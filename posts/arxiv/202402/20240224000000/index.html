<!doctype html><html><head><title>arXiv @ 2024.02.24</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.24"><meta property="og:description" content="Primary Categories cs.AI (5) cs.AR (1) cs.CC (1) cs.CE (1) cs.CG (2) cs.CL (87) cs.CR (6) cs.CV (50) cs.CY (3) cs.DC (2) cs.DS (3) cs.GT (5) cs.HC (6) cs.IR (9) cs.IT (2) cs.LG (60) cs.LO (1) cs.MA (1) cs.NI (2) cs.PF (1) cs.RO (15) cs.SD (2) cs.SE (5) cs.SI (1) eess.AS (2) eess.IV (3) eess.SY (6) math.CO (2) math.DS (1) math.NA (2) math.OC (1) q-bio.BM (1) q-bio.NC (1) quant-ph (3) stat."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240224000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-24T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-24T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.24"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240224000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Saturday, Feb 24, 2024</p></div><div class=title><h1>arXiv @ 2024.02.24</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#csai-5>cs.AI (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#cscc-1>cs.CC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#cscg-2>cs.CG (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#cscl-87>cs.CL (87)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#cscr-6>cs.CR (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#cscv-50>cs.CV (50)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#cscy-3>cs.CY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#csdc-2>cs.DC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#csds-3>cs.DS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#csgt-5>cs.GT (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#cshc-6>cs.HC (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#csir-9>cs.IR (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#csit-2>cs.IT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#cslg-60>cs.LG (60)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#csni-2>cs.NI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#cspf-1>cs.PF (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#csro-15>cs.RO (15)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#csse-5>cs.SE (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#cssi-1>cs.SI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#eessas-2>eess.AS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#eessiv-3>eess.IV (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#eesssy-6>eess.SY (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#mathco-2>math.CO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#mathds-1>math.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#q-biobm-1>q-bio.BM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#q-bionc-1>q-bio.NC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#quant-ph-3>quant-ph (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/#statml-6>stat.ML (6)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Adversarial Attack</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td>1</td><td>1</td><td>1</td></tr><tr><td>Autoencoder</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>BERT</td><td>3</td><td></td><td></td><td></td></tr><tr><td>BLEU</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>3</td><td></td></tr><tr><td>Bard</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td>26</td><td>12</td><td>12</td><td>3</td></tr><tr><td>Black Box</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Chain-of-thought Prompt</td><td>2</td><td></td><td></td><td></td></tr><tr><td>ChatGPT</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td>1</td><td>1</td><td>4</td><td>1</td></tr><tr><td>Code Generation</td><td>1</td><td></td><td>1</td><td>1</td></tr><tr><td>Common-sense Reasoning</td><td>2</td><td></td><td></td><td>1</td></tr><tr><td>Continual Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Continuous Time</td><td></td><td></td><td></td><td>2</td></tr><tr><td>Contrastive Learning</td><td>1</td><td>3</td><td></td><td></td></tr><tr><td>ControlNet</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td>4</td><td>2</td><td>1</td></tr><tr><td>Convolutional Neural Network</td><td></td><td>1</td><td>2</td><td>1</td></tr><tr><td>Counter-factual</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Data Augmentation</td><td>3</td><td>3</td><td></td><td>1</td></tr><tr><td>Dialogue System</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>7</td><td>1</td><td></td></tr><tr><td>Direct Preference Optimization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Distribution Shift</td><td></td><td></td><td>4</td><td></td></tr><tr><td>Domain Adaptation</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Face Recognition</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fairness</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>5</td><td></td></tr><tr><td>Few-shot</td><td>5</td><td>2</td><td>2</td><td></td></tr><tr><td>Few-shot Learning</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>22</td><td>9</td><td>7</td><td></td></tr><tr><td>Foundation Model</td><td>1</td><td>4</td><td>1</td><td>1</td></tr><tr><td>GLUE</td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT</td><td>9</td><td>3</td><td>1</td><td>1</td></tr><tr><td>GPT-3</td><td>5</td><td></td><td></td><td>1</td></tr><tr><td>GPT-3.5</td><td>5</td><td></td><td></td><td>1</td></tr><tr><td>GPT-4</td><td>6</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Gemini</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Generative Adversarial Network</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Geometry</td><td></td><td>4</td><td></td><td></td></tr><tr><td>Graph</td><td>3</td><td>4</td><td>11</td><td>2</td></tr><tr><td>Graph Attention Networks</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Graph Classification</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph Embedding</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>5</td><td>2</td></tr><tr><td>Grounding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>High-Resource</td><td>2</td><td></td><td></td><td></td></tr><tr><td>In-context Learning</td><td>5</td><td>3</td><td>2</td><td></td></tr><tr><td>Information Retrieval</td><td>4</td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td>5</td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Knowledge Base Question Answering</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Knowledge Based Question Answering</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>4</td><td>3</td><td>4</td><td></td></tr><tr><td>Knowledge Graph</td><td>3</td><td></td><td>2</td><td></td></tr><tr><td>Knowledge Transfer</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>LLaMA</td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Language Generation</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>108</td><td>5</td><td>16</td><td>2</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Low-Resource</td><td>4</td><td></td><td>1</td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Masked Language Model</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Massive Multitask Language Understanding (MMLU)</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Mathematical Reasoning</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Meta Learning</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Multi-modal</td><td>11</td><td>6</td><td>2</td><td>4</td></tr><tr><td>Mutual Information</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Named Entity Recognition</td><td>6</td><td></td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>6</td><td></td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Object Detection</td><td></td><td>4</td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Out-of-domain</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>PaLM</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>6</td><td></td><td>1</td><td></td></tr><tr><td>Prompt</td><td>17</td><td>5</td><td>3</td><td></td></tr><tr><td>Pruning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Question Answering</td><td>12</td><td>2</td><td>1</td><td></td></tr><tr><td>Reasoning</td><td>20</td><td>2</td><td>5</td><td>1</td></tr><tr><td>Recommendation</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Recommender System</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reconstruction Loss</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reinforcement Learning</td><td>2</td><td></td><td>5</td><td>2</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>2</td><td></td><td>6</td><td></td></tr><tr><td>Relation Extraction</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td>1</td><td>2</td><td>1</td><td></td></tr><tr><td>Rerank</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td>1</td><td>3</td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td>1</td><td>12</td><td>4</td><td></td></tr><tr><td>Self-supervised Pre-training</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Semantic Parsing</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Sentence Embedding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td></td><td>4</td><td>7</td></tr><tr><td>Simulator</td><td></td><td></td><td>4</td><td>7</td></tr><tr><td>Slot Filling</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Stance Detection</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Stemming</td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Summarization</td><td>2</td><td></td><td>2</td><td></td></tr><tr><td>SuperGLUE</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>2</td><td>5</td><td>2</td><td></td></tr><tr><td>T5</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Temporal Knowledge Graph</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Text Classification</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Text Embedding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Generation</td><td>4</td><td>1</td><td>1</td><td></td></tr><tr><td>Text Mining</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Understanding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text-to-speech</td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Text2SQL</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>5</td><td></td><td></td></tr><tr><td>Textual Entailment</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Tokenization</td><td>3</td><td>1</td><td></td><td></td></tr><tr><td>Topic Model</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Topic Modeling</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Transformer</td><td>5</td><td>11</td><td>5</td><td>2</td></tr><tr><td>Unsupervised Learning</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>6</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td>1</td><td>5</td><td></td><td>1</td></tr><tr><td>Visual Question Answering</td><td>2</td><td>3</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Yolo</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>6</td><td>1</td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=csro-15>cs.RO (15)</h2><h3 id=115--1299-roboscript-code-generation-for-free-form-manipulation-tasks-across-real-and-simulation-junting-chen-et-al-2024>(1/15 | 1/299) RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation (Junting Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junting Chen, Yao Mu, Qiaojun Yu, Tianming Wei, Silang Wu, Zhecheng Yuan, Zhixuan Liang, Chao Yang, Kaipeng Zhang, Wenqi Shao, Yu Qiao, Huazhe Xu, Mingyu Ding, Ping Luo. (2024)<br><strong>RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation</strong><br><button class=copy-to-clipboard title="RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: I-2-7; I-2-8; I-2-9; I-2-10, cs-AI, cs-CL, cs-CV, cs-RO, cs.RO<br>Keyword Score: 106<br>Keywords: Benchmarking, Multi-modal, Simulation, Simulator, GPT, GPT-3, GPT-3.5, GPT-4, Gemini, Code Generation, Common-sense Reasoning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14623v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14623v1.pdf filename=2402.14623v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rapid progress in high-level task planning and <b>code</b> <b>generation</b> for open-world robot manipulation has been witnessed in Embodied AI. However, previous studies put much effort into general common sense <b>reasoning</b> and task planning capabilities of large-scale language or <b>multi-modal</b> models, relatively little effort on ensuring the deployability of generated <b>code</b> <b>on</b> real robots, and other fundamental components of autonomous robot systems including robot perception, motion planning, and control. To bridge this ``ideal-to-real&rsquo;&rsquo; gap, this paper presents \textbf{RobotScript}, a platform for 1) a deployable robot manipulation pipeline powered by <b>code</b> <b>generation;</b> and 2) a <b>code</b> <b>generation</b> <b>benchmark</b> for robot manipulation tasks in free-form natural language. The RobotScript platform addresses this gap by emphasizing the unified interface with both <b>simulation</b> and real robots, based on abstraction from the Robot Operating System (ROS), ensuring syntax compliance and <b>simulation</b> validation with Gazebo. We demonstrate the adaptability of our <b>code</b> <b>generation</b> framework across multiple robot embodiments, including the Franka and UR5 robot arms, and multiple grippers. Additionally, our <b>benchmark</b> assesses <b>reasoning</b> abilities for physical space and constraints, highlighting the differences between <b>GPT-3.5,</b> <b>GPT-4,</b> and <b>Gemini</b> in handling complex physical interactions. Finally, we present a thorough evaluation on the whole system, exploring how each module in the pipeline: <b>code</b> <b>generation,</b> perception, motion planning, and even object geometric properties, impact the overall performance of the system.</p></p class="citation"></blockquote><h3 id=215--2299-reinforcement-learning-with-elastic-time-steps-dong-wang-et-al-2024>(2/15 | 2/299) Reinforcement Learning with Elastic Time Steps (Dong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dong Wang, Giovanni Beltrame. (2024)<br><strong>Reinforcement Learning with Elastic Time Steps</strong><br><button class=copy-to-clipboard title="Reinforcement Learning with Elastic Time Steps" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Continuous Time, Continuous Time, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14961v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14961v1.pdf filename=2402.14961v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional <b>Reinforcement</b> <b>Learning</b> (RL) algorithms are usually applied in robotics to learn controllers that act with a fixed control rate. Given the discrete nature of RL algorithms, they are oblivious to the effects of the choice of control rate: finding the correct control rate can be difficult and mistakes often result in excessive use of computing resources or even lack of convergence. We propose Soft Elastic Actor-Critic (SEAC), a novel off-policy actor-critic algorithm to address this issue. SEAC implements elastic time steps, time steps with a known, variable duration, which allow the agent to change its control frequency to adapt to the situation. In practice, SEAC applies control only when necessary, minimizing computational resources and data usage. We evaluate SEAC&rsquo;s capabilities in <b>simulation</b> in a Newtonian kinematics maze navigation task and on a 3D racing video game, Trackmania. SEAC outperforms the SAC baseline in terms of energy efficiency and overall time management, and most importantly without the need to identify a control frequency for the learned controller. SEAC demonstrated faster and more stable training speeds than SAC, especially at control rates where SAC struggled to converge. We also compared SEAC with a similar approach, the <b>Continuous-Time</b> <b>Continuous-Options</b> <b>(CTCO)</b> model, and SEAC resulted in better task performance. These findings highlight the potential of SEAC for practical, real-world RL applications in robotics.</p></p class="citation"></blockquote><h3 id=315--3299-path-planning-based-on-2d-object-bounding-box-yanliang-huang-et-al-2024>(3/15 | 3/299) Path Planning based on 2D Object Bounding-box (Yanliang Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanliang Huang, Liguo Zhou, Chang Liu, Alois Knoll. (2024)<br><strong>Path Planning based on 2D Object Bounding-box</strong><br><button class=copy-to-clipboard title="Path Planning based on 2D Object Bounding-box" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14933v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14933v1.pdf filename=2402.14933v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The implementation of Autonomous Driving (AD) technologies within urban environments presents significant challenges. These challenges necessitate the development of advanced perception systems and motion planning algorithms capable of managing situations of considerable complexity. Although the end-to-end AD method utilizing LiDAR sensors has achieved significant success in this scenario, we argue that its drawbacks may hinder its practical application. Instead, we propose the vision-centric AD as a promising alternative offering a streamlined model without compromising performance. In this study, we present a path planning method that utilizes 2D bounding boxes of objects, developed through imitation learning in urban driving scenarios. This is achieved by integrating high-definition (HD) map data with images captured by surrounding cameras. Subsequent perception tasks involve bounding-box detection and tracking, while the planning phase employs both local embeddings via <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN)</b> and global embeddings via <b>Transformer</b> for temporal-spatial feature aggregation, ultimately producing optimal path planning information. We evaluated our model on the nuPlan planning task and observed that it performs competitively in comparison to existing vision-centric methods.</p></p class="citation"></blockquote><h3 id=415--4299-practice-makes-perfect-planning-to-learn-skill-parameter-policies-nishanth-kumar-et-al-2024>(4/15 | 4/299) Practice Makes Perfect: Planning to Learn Skill Parameter Policies (Nishanth Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nishanth Kumar, Tom Silver, Willie McClinton, Linfeng Zhao, Stephen Proulx, Tomás Lozano-Pérez, Leslie Pack Kaelbling, Jennifer Barry. (2024)<br><strong>Practice Makes Perfect: Planning to Learn Skill Parameter Policies</strong><br><button class=copy-to-clipboard title="Practice Makes Perfect: Planning to Learn Skill Parameter Policies" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Active Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15025v1.pdf filename=2402.15025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One promising approach towards effective robot decision making in complex, long-horizon tasks is to sequence together parameterized skills. We consider a setting where a robot is initially equipped with (1) a library of parameterized skills, (2) an AI planner for sequencing together the skills given a goal, and (3) a very general prior distribution for selecting skill parameters. Once deployed, the robot should rapidly and autonomously learn to improve its performance by specializing its skill parameter selection policy to the particular objects, goals, and constraints in its environment. In this work, we focus on the <b>active</b> <b>learning</b> problem of choosing which skills to practice to maximize expected future task success. We propose that the robot should estimate the competence of each skill, extrapolate the competence (asking: &ldquo;how much would the competence improve through practice?&rdquo;), and situate the skill in the task distribution through competence-aware planning. This approach is implemented within a fully autonomous system where the robot repeatedly plans, practices, and learns without any environment resets. Through experiments in <b>simulation,</b> we find that our approach learns effective parameter policies more sample-efficiently than several baselines. Experiments in the real-world demonstrate our approach&rsquo;s ability to handle noise from perception and control and improve the robot&rsquo;s ability to solve two long-horizon mobile-manipulation tasks after a few hours of autonomous practice.</p></p class="citation"></blockquote><h3 id=515--5299-radarmoseve-a-spatial-temporal-transformer-network-for-radar-only-moving-object-segmentation-and-ego-velocity-estimation-changsong-pang-et-al-2024>(5/15 | 5/299) RadarMOSEVE: A Spatial-Temporal Transformer Network for Radar-Only Moving Object Segmentation and Ego-Velocity Estimation (Changsong Pang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changsong Pang, Xieyuanli Chen, Yimin Liu, Huimin Lu, Yuwei Cheng. (2024)<br><strong>RadarMOSEVE: A Spatial-Temporal Transformer Network for Radar-Only Moving Object Segmentation and Ego-Velocity Estimation</strong><br><button class=copy-to-clipboard title="RadarMOSEVE: A Spatial-Temporal Transformer Network for Radar-Only Moving Object Segmentation and Ego-Velocity Estimation" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14380v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14380v1.pdf filename=2402.14380v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Moving object segmentation (MOS) and Ego velocity estimation (EVE) are vital capabilities for mobile systems to achieve full autonomy. Several approaches have attempted to achieve MOSEVE using a LiDAR sensor. However, LiDAR sensors are typically expensive and susceptible to adverse weather conditions. Instead, millimeter-wave radar (MWR) has gained popularity in robotics and autonomous driving for real applications due to its cost-effectiveness and resilience to bad weather. Nonetheless, publicly available MOSEVE datasets and approaches using radar data are limited. Some existing methods adopt point <b>convolutional</b> <b>networks</b> from LiDAR-based approaches, ignoring the specific artifacts and the valuable radial velocity information of radar measurements, leading to suboptimal performance. In this paper, we propose a novel <b>transformer</b> network that effectively addresses the sparsity and noise issues and leverages the radial velocity measurements of radar points using our devised radar self- and cross-attention mechanisms. Based on that, our method achieves accurate EVE of the robot and performs MOS using only radar data simultaneously. To thoroughly evaluate the MOSEVE performance of our method, we annotated the radar points in the public View-of-Delft (VoD) dataset and additionally constructed a new radar dataset in various environments. The experimental results demonstrate the superiority of our approach over existing state-of-the-art methods. The code is available at <a href=https://github.com/ORCA-Uboat/RadarMOSEVE>https://github.com/ORCA-Uboat/RadarMOSEVE</a>.</p></p class="citation"></blockquote><h3 id=615--6299-we-choose-to-go-to-space-agent-driven-human-and-multi-robot-collaboration-in-microgravity-miao-xin-et-al-2024>(6/15 | 6/299) We Choose to Go to Space: Agent-driven Human and Multi-Robot Collaboration in Microgravity (Miao Xin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miao Xin, Zhongrui You, Zihan Zhang, Taoran Jiang, Tingjia Xu, Haotian Liang, Guojing Ge, Yuchen Ji, Shentong Mo, Jian Cheng. (2024)<br><strong>We Choose to Go to Space: Agent-driven Human and Multi-Robot Collaboration in Microgravity</strong><br><button class=copy-to-clipboard title="We Choose to Go to Space: Agent-driven Human and Multi-Robot Collaboration in Microgravity" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Foundation Model, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14299v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14299v1.pdf filename=2402.14299v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present SpaceAgents-1, a system for learning human and multi-robot collaboration (HMRC) strategies under microgravity conditions. Future space exploration requires humans to work together with robots. However, acquiring proficient robot skills and adept collaboration under microgravity conditions poses significant challenges within ground laboratories. To address this issue, we develop a microgravity <b>simulation</b> environment and present three typical configurations of intra-cabin robots. We propose a hierarchical heterogeneous multi-agent collaboration architecture: guided by <b>foundation</b> <b>models,</b> a Decision-Making Agent serves as a task planner for human-robot collaboration, while individual Skill-Expert Agents manage the embodied control of robots. This mechanism empowers the SpaceAgents-1 system to execute a range of intricate long-horizon HMRC tasks.</p></p class="citation"></blockquote><h3 id=715--7299-towards-diverse-behaviors-a-benchmark-for-imitation-learning-with-human-demonstrations-xiaogang-jia-et-al-2024>(7/15 | 7/299) Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations (Xiaogang Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaogang Jia, Denis Blessing, Xinkai Jiang, Moritz Reuss, Atalay Donat, Rudolf Lioutikov, Gerhard Neumann. (2024)<br><strong>Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations</strong><br><button class=copy-to-clipboard title="Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 26<br>Keywords: Benchmarking, Multi-modal, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14606v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14606v1.pdf filename=2402.14606v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Imitation learning with human data has demonstrated remarkable success in teaching robots in a wide range of skills. However, the inherent diversity in human behavior leads to the emergence of <b>multi-modal</b> data distributions, thereby presenting a formidable challenge for existing imitation learning algorithms. Quantifying a model&rsquo;s capacity to capture and replicate this diversity effectively is still an open problem. In this work, we introduce <b>simulation</b> <b>benchmark</b> environments and the corresponding Datasets with Diverse human Demonstrations for Imitation Learning (D3IL), designed explicitly to evaluate a model&rsquo;s ability to learn <b>multi-modal</b> behavior. Our environments are designed to involve multiple sub-tasks that need to be solved, consider manipulation of multiple objects which increases the diversity of the behavior and can only be solved by policies that rely on closed loop sensory feedback. Other available datasets are missing at least one of these challenging properties. To address the challenge of diversity quantification, we introduce tractable metrics that provide valuable insights into a model&rsquo;s ability to acquire and reproduce diverse behaviors. These metrics offer a practical means to assess the robustness and versatility of imitation learning algorithms. Furthermore, we conduct a thorough evaluation of state-of-the-art methods on the proposed task suite. This evaluation serves as a <b>benchmark</b> for assessing their capability to learn diverse behaviors. Our findings shed light on the effectiveness of these methods in tackling the intricate problem of capturing and generalizing <b>multi-modal</b> human behaviors, offering a valuable reference for the design of future imitation learning algorithms.</p></p class="citation"></blockquote><h3 id=815--8299-enhancing-robotic-manipulation-with-ai-feedback-from-multimodal-large-language-models-jinyi-liu-et-al-2024>(8/15 | 8/299) Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models (Jinyi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinyi Liu, Yifu Yuan, Jianye Hao, Fei Ni, Lingzhi Fu, Yibin Chen, Yan Zheng. (2024)<br><strong>Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14245v1.pdf filename=2402.14245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been considerable attention towards leveraging <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to enhance decision-making processes. However, aligning the natural language text instructions generated by <b>LLMs</b> with the vectorized operations required for execution presents a significant challenge, often necessitating task-specific details. To circumvent the need for such task-specific granularity, inspired by preference-based policy learning approaches, we investigate the utilization of <b>multimodal</b> <b>LLMs</b> to provide automated preference feedback solely from image inputs to guide decision-making. In this study, we train a <b>multimodal</b> <b>LLM,</b> termed CriticGPT, capable of understanding trajectory videos in robot manipulation tasks, serving as a critic to offer analysis and preference feedback. Subsequently, we validate the effectiveness of preference labels generated by CriticGPT from a reward modeling perspective. Experimental evaluation of the algorithm&rsquo;s preference accuracy demonstrates its effective generalization ability to new tasks. Furthermore, performance on Meta-World tasks reveals that CriticGPT&rsquo;s reward model efficiently guides policy learning, surpassing rewards based on state-of-the-art pre-trained representation models.</p></p class="citation"></blockquote><h3 id=915--9299-autonomy-oriented-digital-twins-for-real2sim2real-autoware-deployment-chinmay-vilas-samak-et-al-2024>(9/15 | 9/299) Autonomy Oriented Digital Twins for Real2Sim2Real Autoware Deployment (Chinmay Vilas Samak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chinmay Vilas Samak, Tanmay Vilas Samak. (2024)<br><strong>Autonomy Oriented Digital Twins for Real2Sim2Real Autoware Deployment</strong><br><button class=copy-to-clipboard title="Autonomy Oriented Digital Twins for Real2Sim2Real Autoware Deployment" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14739v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14739v1.pdf filename=2402.14739v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modeling and <b>simulation</b> of autonomous vehicles plays a crucial role in achieving enterprise-scale realization that aligns with technical, business and regulatory requirements. Contemporary trends in digital lifecycle treatment have proven beneficial to support SBD as well as V&amp;V of these complex systems. Although, the development of appropriate fidelity <b>simulation</b> models capable of capturing the intricate real-world physics and graphics (real2sim), while enabling real-time interactivity for decision-making, has remained a challenge. Nevertheless, recent advances in AI-based tools and workflows, such as online deep-learning algorithms leveraging live-streaming data sources, offer the tantalizing potential for real-time system-identification and adaptive modeling to simulate vehicles, environments, as well as their interactions. This transition from virtual prototypes to digital twins not only improves <b>simulation</b> fidelity and real-time factor, but can also support the development of online adaption/augmentation techniques that can help bridge the gap between <b>simulation</b> and reality (sim2real). In such a milieu, this work focuses on developing autonomy-oriented digital twins of vehicles across different scales and configurations to help support the streamlined development and deployment of Autoware stack, using a unified real2sim2real toolchain. Particularly, the core deliverable for this project was to integrate the Autoware stack with AutoDRIVE Ecosystem to demonstrate end-to-end task of map-based autonomous navigation. This work discusses the development of vehicle and environment digital twins using AutoDRIVE Ecosystem, along with various APIs and HMIs to connect with the same, followed by a detailed section on AutoDRIVE-Autoware integration. Furthermore, this study describes the first-ever off-road deployment of the Autoware stack, expanding the ODD beyond on-road autonomous navigation.</p></p class="citation"></blockquote><h3 id=1015--10299-a-collision-aware-cable-grasping-method-in-cluttered-environment-lei-zhang-et-al-2024>(10/15 | 10/299) A Collision-Aware Cable Grasping Method in Cluttered Environment (Lei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Zhang, Kaixin Bai, Qiang Li, Zhaopeng Chen, Jianwei Zhang. (2024)<br><strong>A Collision-Aware Cable Grasping Method in Cluttered Environment</strong><br><button class=copy-to-clipboard title="A Collision-Aware Cable Grasping Method in Cluttered Environment" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14498v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14498v1.pdf filename=2402.14498v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a Cable Grasping-Convolutional Neural Network designed to facilitate robust cable grasping in cluttered environments. Utilizing physics <b>simulations,</b> we generate an extensive dataset that mimics the intricacies of cable grasping, factoring in potential collisions between cables and robotic grippers. We employ the Approximate Convex Decomposition technique to dissect the non-convex cable model, with grasp quality autonomously labeled based on simulated grasping attempts. The CG-CNN is refined using this simulated dataset and enhanced through domain randomization techniques. Subsequently, the trained model predicts grasp quality, guiding the optimal grasp pose to the robot controller for execution. Grasping efficacy is assessed across both synthetic and real-world settings. Given our model implicit collision sensitivity, we achieved commendable success rates of 92.3% for known cables and 88.4% for unknown cables, surpassing contemporary state-of-the-art approaches. Supplementary materials can be found at <a href=https://leizhang-public.github.io/cg-cnn/>https://leizhang-public.github.io/cg-cnn/</a> .</p></p class="citation"></blockquote><h3 id=1115--11299-ground-fusion-a-low-cost-ground-slam-system-robust-to-corner-cases-jie-yin-et-al-2024>(11/15 | 11/299) Ground-Fusion: A Low-cost Ground SLAM System Robust to Corner Cases (Jie Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Yin, Ang Li, Wei Xi, Wenxian Yu, Danping Zou. (2024)<br><strong>Ground-Fusion: A Low-cost Ground SLAM System Robust to Corner Cases</strong><br><button class=copy-to-clipboard title="Ground-Fusion: A Low-cost Ground SLAM System Robust to Corner Cases" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Graph, Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14308v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14308v1.pdf filename=2402.14308v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Ground-Fusion, a low-cost sensor fusion simultaneous localization and mapping (SLAM) system for ground vehicles. Our system features efficient initialization, effective sensor <b>anomaly</b> <b>detection</b> and handling, real-time dense color mapping, and robust localization in diverse environments. We tightly integrate RGB-D images, inertial measurements, wheel odometer and GNSS signals within a factor <b>graph</b> to achieve accurate and reliable localization both indoors and outdoors. To ensure successful initialization, we propose an efficient strategy that comprises three different methods: stationary, visual, and dynamic, tailored to handle diverse cases. Furthermore, we develop mechanisms to detect sensor anomalies and degradation, handling them adeptly to maintain system accuracy. Our experimental results on both public and self-collected datasets demonstrate that Ground-Fusion outperforms existing low-cost SLAM systems in corner cases. We release the code and datasets at <a href=https://github.com/SJTU-ViSYS/Ground-Fusion>https://github.com/SJTU-ViSYS/Ground-Fusion</a>.</p></p class="citation"></blockquote><h3 id=1215--12299-vision-language-navigation-with-embodied-intelligence-a-survey-peng-gao-et-al-2024>(12/15 | 12/299) Vision-Language Navigation with Embodied Intelligence: A Survey (Peng Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Gao, Peng Wang, Feng Gao, Fei Wang, Ruyue Yuan. (2024)<br><strong>Vision-Language Navigation with Embodied Intelligence: A Survey</strong><br><button class=copy-to-clipboard title="Vision-Language Navigation with Embodied Intelligence: A Survey" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Benchmarking, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14304v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14304v1.pdf filename=2402.14304v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a long-term vision in the field of artificial intelligence, the core goal of embodied intelligence is to improve the perception, understanding, and interaction capabilities of agents and the environment. <b>Vision-language</b> navigation (VLN), as a critical research path to achieve embodied intelligence, focuses on exploring how agents use natural language to communicate effectively with humans, receive and understand instructions, and ultimately rely on visual information to achieve accurate navigation. VLN integrates artificial intelligence, natural language processing, computer vision, and robotics. This field faces technical challenges but shows potential for application such as human-computer interaction. However, due to the complex process involved from language understanding to action execution, VLN faces the problem of aligning visual information and language instructions, improving generalization ability, and many other challenges. This survey systematically reviews the research progress of VLN and details the research direction of VLN with embodied intelligence. After a detailed summary of its system architecture and research based on methods and commonly used <b>benchmark</b> datasets, we comprehensively analyze the problems and challenges faced by current research and explore the future development direction of this field, aiming to provide a practical reference for researchers.</p></p class="citation"></blockquote><h3 id=1315--13299-cyberdemo-augmenting-simulated-human-demonstration-for-real-world-dexterous-manipulation-jun-wang-et-al-2024>(13/15 | 13/299) CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation (Jun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Wang, Yuzhe Qin, Kaiming Kuang, Yigit Korkmaz, Akhilan Gurumoorthy, Hao Su, Xiaolong Wang. (2024)<br><strong>CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation</strong><br><button class=copy-to-clipboard title="CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14795v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14795v1.pdf filename=2402.14795v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce CyberDemo, a novel approach to robotic imitation learning that leverages simulated human demonstrations for real-world tasks. By incorporating extensive <b>data</b> <b>augmentation</b> in a simulated environment, CyberDemo outperforms traditional in-domain real-world demonstrations when transferred to the real world, handling diverse physical and visual conditions. Regardless of its affordability and convenience in <b>data</b> <b>collection,</b> CyberDemo outperforms baseline methods in terms of success rates across various tasks and exhibits generalizability with previously unseen objects. For example, it can rotate novel tetra-valve and penta-valve, despite human demonstrations only involving tri-valves. Our research demonstrates the significant potential of simulated human demonstrations for real-world dexterous manipulation tasks. More details can be found at <a href=https://cyber-demo.github.io>https://cyber-demo.github.io</a></p></p class="citation"></blockquote><h3 id=1415--14299-transformable-gaussian-reward-function-for-socially-aware-navigation-with-deep-reinforcement-learning-jinyeob-kim-et-al-2024>(14/15 | 14/299) Transformable Gaussian Reward Function for Socially-Aware Navigation with Deep Reinforcement Learning (Jinyeob Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinyeob Kim, Sumin Kang, Sungwoo Yang, Beomjoon Kim, Jargalbaatar Yura, Donghan Kim. (2024)<br><strong>Transformable Gaussian Reward Function for Socially-Aware Navigation with Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Transformable Gaussian Reward Function for Socially-Aware Navigation with Deep Reinforcement Learning" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14569v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14569v1.pdf filename=2402.14569v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robot navigation has transitioned from prioritizing obstacle avoidance to adopting socially aware navigation strategies that accommodate human presence. As a result, the recognition of socially aware navigation within dynamic human-centric environments has gained prominence in the field of robotics. Although <b>reinforcement</b> <b>learning</b> technique has fostered the advancement of socially aware navigation, defining appropriate reward functions, especially in congested environments, has posed a significant challenge. These rewards, crucial in guiding robot actions, demand intricate human-crafted design due to their complex nature and inability to be automatically set. The multitude of manually designed rewards poses issues with hyperparameter redundancy, imbalance, and inadequate representation of unique object characteristics. To address these challenges, we introduce a transformable gaussian reward function (TGRF). The TGRF significantly reduces the burden of hyperparameter tuning, displays adaptability across various reward functions, and demonstrates accelerated learning rates, particularly excelling in crowded environments utilizing deep <b>reinforcement</b> <b>learning</b> (DRL). We introduce and validate TGRF through sections highlighting its conceptual background, characteristics, experiments, and real-world application, paving the way for a more effective and adaptable approach in robotics.The complete source code is available on <a href=https://github.com/JinnnK/TGRF>https://github.com/JinnnK/TGRF</a></p></p class="citation"></blockquote><h3 id=1515--15299-transition-state-clustering-for-interaction-segmentation-and-learning-fabian-hahne-et-al-2024>(15/15 | 15/299) Transition State Clustering for Interaction Segmentation and Learning (Fabian Hahne et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fabian Hahne, Vignesh Prasad, Alap Kshirsagar, Dorothea Koert, Ruth Maria Stock-Homburg, Jan Peters, Georgia Chalvatzaki. (2024)<br><strong>Transition State Clustering for Interaction Segmentation and Learning</strong><br><button class=copy-to-clipboard title="Transition State Clustering for Interaction Segmentation and Learning" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14548v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14548v1.pdf filename=2402.14548v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hidden Markov Models with an underlying Mixture of Gaussian structure have proven effective in learning Human-Robot Interactions from demonstrations for various interactive tasks via Gaussian Mixture Regression. However, a mismatch occurs when segmenting the interaction using only the observed state of the human compared to the joint state of the human and the robot. To enhance this underlying segmentation and subsequently the predictive abilities of such Gaussian Mixture-based approaches, we take a hierarchical approach by learning an additional mixture distribution on the states at the transition boundary. This helps prevent misclassifications that usually occur in such states. We find that our framework improves the performance of the underlying Gaussian Mixture-based approach, which we evaluate on various interactive tasks such as handshaking and fistbumps.</p></p class="citation"></blockquote><h2 id=cscl-87>cs.CL (87)</h2><h3 id=187--16299-tokenization-counts-the-impact-of-tokenization-on-arithmetic-in-frontier-llms-aaditya-k-singh-et-al-2024>(1/87 | 16/299) Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs (Aaditya K. Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aaditya K. Singh, DJ Strouse. (2024)<br><strong>Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs</strong><br><button class=copy-to-clipboard title="Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 100<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, PaLM, Reasoning, Tokenization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14903v1.pdf filename=2402.14903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Tokenization,</b> the division of input text into input tokens, is an often overlooked aspect of the <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> pipeline and could be the source of useful or harmful inductive biases. Historically, <b>LLMs</b> have relied on byte pair encoding, without care to specific input domains. With the increased use of <b>LLMs</b> for <b>reasoning,</b> various number-specific <b>tokenization</b> schemes have been adopted, with popular models like <b>LLaMa</b> and <b>PaLM</b> opting for single-digit <b>tokenization</b> while <b>GPT-3.5</b> and <b>GPT-4</b> have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical <b>reasoning</b> through the use of arithmetic tasks. We consider left-to-right and right-to-left <b>tokenization</b> for <b>GPT-3.5</b> and -4, finding that right-to-left <b>tokenization</b> (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-right <b>tokenization</b> follow stereotyped error patterns, suggesting that model computations are systematic rather than approximate. We show that the model is able to convert between <b>tokenizations</b> easily, thus allowing chain-of-thought-inspired approaches to recover performance on left-to-right tokenized inputs. We also find the gap between <b>tokenization</b> directions decreases when models are scaled, possibly indicating that larger models are better able to override this <b>tokenization-dependent</b> inductive bias. In summary, our work performs the first study of how number <b>tokenization</b> choices lead to differences in model performance on arithmetic tasks, accompanied by a thorough analysis of error patterns. We hope this work inspires practitioners to more carefully ablate number <b>tokenization-related</b> choices when working towards general models of numerical <b>reasoning.</b></p></p class="citation"></blockquote><h3 id=287--17299-can-large-language-models-detect-misinformation-in-scientific-news-reporting-yupeng-cao-et-al-2024>(2/87 | 17/299) Can Large Language Models Detect Misinformation in Scientific News Reporting? (Yupeng Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yupeng Cao, Aishwarya Muralidharan Nair, Elyon Eyimife, Nastaran Jamalipour Soofi, K. P. Subbalakshmi, John R. Wullert II, Chumki Basu, David Shallcross. (2024)<br><strong>Can Large Language Models Detect Misinformation in Scientific News Reporting?</strong><br><button class=copy-to-clipboard title="Can Large Language Models Detect Misinformation in Scientific News Reporting?" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-SI, cs.CL<br>Keyword Score: 100<br>Keywords: Few-shot, Zero-shot, GPT, GPT-3, GPT-3.5, GPT-4, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14268v1.pdf filename=2402.14268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scientific facts are often spun in the popular press with the intent to influence public opinion and action, as was evidenced during the COVID-19 pandemic. Automatic detection of misinformation in the scientific domain is challenging because of the distinct styles of writing in these two media types and is still in its nascence. Most research on the validity of scientific reporting treats this problem as a claim verification challenge. In doing so, significant expert human effort is required to generate appropriate claims. Our solution bypasses this step and addresses a more real-world scenario where such explicit, labeled claims may not be available. The central research question of this paper is whether it is possible to use <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to detect misinformation in scientific reporting. To this end, we first present a new labeled dataset SciNews, containing 2.4k scientific news stories drawn from trusted and untrustworthy sources, paired with related abstracts from the CORD-19 database. Our dataset includes both human-written and <b>LLM-generated</b> news articles, making it more comprehensive in terms of capturing the growing trend of using <b>LLMs</b> to generate popular press articles. Then, we identify dimensions of scientific validity in science news articles and explore how this can be integrated into the automated detection of scientific misinformation. We propose several baseline architectures using <b>LLMs</b> to automatically detect false representations of scientific findings in the popular press. For each of these architectures, we use several <b>prompt</b> engineering strategies including <b>zero-shot,</b> <b>few-shot,</b> and <b>chain-of-thought</b> <b>prompting.</b> We also test these architectures and <b>prompting</b> strategies on <b>GPT-3.5,</b> <b>GPT-4,</b> and Llama2-7B, Llama2-13B.</p></p class="citation"></blockquote><h3 id=387--18299-unintended-impacts-of-llm-alignment-on-global-representation-michael-j-ryan-et-al-2024>(3/87 | 18/299) Unintended Impacts of LLM Alignment on Global Representation (Michael J. Ryan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael J. Ryan, William Held, Diyi Yang. (2024)<br><strong>Unintended Impacts of LLM Alignment on Global Representation</strong><br><button class=copy-to-clipboard title="Unintended Impacts of LLM Alignment on Global Representation" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs-LG, cs.CL<br>Keyword Score: 93<br>Keywords: Benchmarking, Direct Preference Optimization, Recommendation, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Instruction Following, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15018v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15018v1.pdf filename=2402.15018v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Before being deployed for user-facing applications, developers align <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to user preferences through a variety of procedures, such as <b>Reinforcement</b> <b>Learning</b> <b>From</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> and <b>Direct</b> <b>Preference</b> <b>Optimization</b> (DPO). Current evaluations of these procedures focus on <b>benchmarks</b> of <b>instruction</b> <b>following,</b> <b>reasoning,</b> and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and <b>recommendations</b> for more equitable preference tuning.</p></p class="citation"></blockquote><h3 id=487--19299-leveraging-large-language-models-for-concept-graph-recovery-and-question-answering-in-nlp-education-rui-yang-et-al-2024>(4/87 | 19/299) Leveraging Large Language Models for Concept Graph Recovery and Question Answering in NLP Education (Rui Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Yang, Boming Yang, Sixun Ouyang, Tianwei She, Aosong Feng, Yuang Jiang, Freddy Lecue, Jinghui Lu, Irene Li. (2024)<br><strong>Leveraging Large Language Models for Concept Graph Recovery and Question Answering in NLP Education</strong><br><button class=copy-to-clipboard title="Leveraging Large Language Models for Concept Graph Recovery and Question Answering in NLP Education" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 86<br>Keywords: Graph, Benchmarking, Supervised Learning, Zero-shot, Question Answering, Question Answering, Reasoning, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14293v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14293v1.pdf filename=2402.14293v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the domain of Natural Language Processing (NLP), <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated promise in <b>text-generation</b> <b>tasks.</b> However, their educational applications, particularly for domain-specific queries, remain underexplored. This study investigates <b>LLMs&rsquo;</b> capabilities in educational scenarios, focusing on concept <b>graph</b> recovery and <b>question-answering</b> <b>(QA).</b> We assess <b>LLMs&rsquo;</b> <b>zero-shot</b> performance in creating domain-specific concept <b>graphs</b> and introduce TutorQA, a new expert-verified NLP-focused <b>benchmark</b> for scientific <b>graph</b> <b>reasoning</b> and <b>QA.</b> TutorQA consists of five tasks with 500 <b>QA</b> pairs. To tackle TutorQA queries, we present CGLLM, a pipeline integrating concept <b>graphs</b> with <b>LLMs</b> for answering diverse <b>questions.</b> <b>Our</b> results indicate that <b>LLMs&rsquo;</b> <b>zero-shot</b> concept <b>graph</b> recovery is competitive with <b>supervised</b> methods, showing an average 3% F1 score improvement. In TutorQA tasks, <b>LLMs</b> achieve up to 26% F1 score enhancement. Moreover, human evaluation and analysis show that CGLLM generates answers with more fine-grained concepts.</p></p class="citation"></blockquote><h3 id=587--20299-hint-before-solving-prompting-guiding-llms-to-effectively-utilize-encoded-knowledge-jinlan-fu-et-al-2024>(5/87 | 20/299) Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge (Jinlan Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinlan Fu, Shenzhen Huangfu, Hang Yan, See-Kiong Ng, Xipeng Qiu. (2024)<br><strong>Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge</strong><br><button class=copy-to-clipboard title="Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Fine-tuning, GPT, GPT-3, GPT-3.5, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14310v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14310v1.pdf filename=2402.14310v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have recently showcased remarkable generalizability in various domains. Despite their extensive knowledge, <b>LLMs</b> still face challenges in efficiently utilizing encoded knowledge to develop accurate and logical <b>reasoning</b> processes. To mitigate this problem, we introduced Hint-before-Solving <b>Prompting</b> (HSP), which guides the model to generate hints (e.g., specific knowledge or key ideas) for solving the problem and then generate solutions containing intermediate <b>reasoning</b> steps. Since HSP is orthogonal to <b>prompting</b> methods (e.g., Chain-of-Thought (CoT)), we applied HSP to CoT, Least-to-Most, Plan-and-Solve, and Standard <b>promptings.</b> The results of extensive experiments on 6 <b>reasoning</b> <b>benchmarks</b> and 4 open-source <b>LLMs</b> demonstrate that HSP can effectively improve the accuracy of <b>reasoning</b> tasks: (1) By applying high-quality hint-enhanced HSP to CoT <b>prompting,</b> Llama2-70B-Chat shows an improvement of 9.7. (2) Beyond exploring training-free <b>LLM</b> capabilities, we built the HSPMATH dataset based on HSP and <b>fine-tuned</b> Llemma-7B, reaching 64.3 accuracy, surpassing <b>GPT-3.5</b> and WizardMath-13B. We make our code and dataset publicly available at \url{https://github.com/jinlanfu/HSP}.</p></p class="citation"></blockquote><h3 id=687--21299-is-chatgpt-the-future-of-causal-text-mining-a-comprehensive-evaluation-and-analysis-takehiro-takayanagi-et-al-2024>(6/87 | 21/299) Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis (Takehiro Takayanagi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Takehiro Takayanagi, Masahiro Suzuki, Ryotaro Kobayashi, Hiroki Sakaji, Kiyoshi Izumi. (2024)<br><strong>Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis</strong><br><button class=copy-to-clipboard title="Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, ChatGPT, GPT, GPT-4, Text Mining, Domain Adaptation, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14484v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14484v2.pdf filename=2402.14484v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Causality is fundamental in human cognition and has drawn attention in diverse research fields. With growing volumes of textual data, discerning causalities within <b>text</b> <b>data</b> is crucial, and causal <b>text</b> <b>mining</b> plays a pivotal role in extracting meaningful patterns. This study conducts comprehensive evaluations of <b>ChatGPT&rsquo;s</b> causal <b>text</b> <b>mining</b> capabilities. Firstly, we introduce a <b>benchmark</b> that extends beyond general English datasets, including <b>domain-specific</b> <b>and</b> non-English datasets. We also provide an evaluation framework to ensure fair comparisons between <b>ChatGPT</b> and previous approaches. Finally, our analysis outlines the limitations and future challenges in employing <b>ChatGPT</b> for causal <b>text</b> <b>mining.</b> Specifically, our analysis reveals that <b>ChatGPT</b> serves as a good starting point for various datasets. However, when equipped with a sufficient amount of training data, previous models still surpass <b>ChatGPT&rsquo;s</b> performance. Additionally, <b>ChatGPT</b> suffers from the tendency to falsely recognize non-causal sequences as causal sequences. These issues become even more pronounced with advanced versions of the model, such as <b>GPT-4.</b> In addition, we highlight the constraints of <b>ChatGPT</b> in handling complex causality types, including both intra/inter-sentential and implicit causality. The model also faces challenges with effectively leveraging <b>in-context</b> <b>learning</b> and <b>domain</b> <b>adaptation.</b> We release our code to support further research and development in this field.</p></p class="citation"></blockquote><h3 id=787--22299-on-the-tip-of-the-tongue-analyzing-conceptual-representation-in-large-language-models-with-reverse-dictionary-probe-ningyu-xu-et-al-2024>(7/87 | 22/299) On the Tip of the Tongue: Analyzing Conceptual Representation in Large Language Models with Reverse-Dictionary Probe (Ningyu Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ningyu Xu, Qi Zhang, Menghan Zhang, Peng Qian, Xuanjing Huang. (2024)<br><strong>On the Tip of the Tongue: Analyzing Conceptual Representation in Large Language Models with Reverse-Dictionary Probe</strong><br><button class=copy-to-clipboard title="On the Tip of the Tongue: Analyzing Conceptual Representation in Large Language Models with Reverse-Dictionary Probe" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Common-sense Reasoning, Reasoning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14404v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14404v2.pdf filename=2402.14404v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Probing and enhancing <b>large</b> <b>language</b> <b>models&rsquo;</b> <b>reasoning</b> capacity remains a crucial open question. Here we re-purpose the reverse dictionary task as a case study to probe <b>LLMs&rsquo;</b> capacity for conceptual inference. We use <b>in-context</b> <b>learning</b> to guide the models to generate the term for an object concept implied in a linguistic description. Models robustly achieve high accuracy in this task, and their representation space encodes information about object categories and fine-grained features. Further experiments suggest that the conceptual inference ability as probed by the reverse-dictionary task predicts model&rsquo;s general <b>reasoning</b> performance across multiple <b>benchmarks,</b> despite similar syntactic generalization behaviors across models. Explorative analyses suggest that <b>prompting</b> <b>LLMs</b> with description$\Rightarrow$word examples may induce generalization beyond surface-level differences in task construals and facilitate models on broader <b>commonsense</b> <b>reasoning</b> problems.</p></p class="citation"></blockquote><h3 id=887--23299-rethinking-scientific-summarization-evaluation-grounding-explainable-metrics-on-facet-aware-benchmark-xiuying-chen-et-al-2024>(8/87 | 23/299) Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark (Xiuying Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiuying Chen, Tairan Wang, Qingqing Zhu, Taicheng Guo, Shen Gao, Zhiyong Lu, Xin Gao, Xiangliang Zhang. (2024)<br><strong>Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark</strong><br><button class=copy-to-clipboard title="Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Grounding, Question Answering, In-context Learning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14359v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14359v1.pdf filename=2402.14359v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>summarization</b> capabilities of pretrained and <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been widely validated in general areas, but their use in scientific corpus, which involves complex sentences and specialized knowledge, has been less assessed. This paper presents conceptual and experimental analyses of scientific <b>summarization,</b> highlighting the inadequacies of traditional evaluation methods, such as $n$-gram, embedding comparison, and <b>QA,</b> particularly in providing explanations, grasping scientific concepts, or identifying key content. Subsequently, we introduce the Facet-aware Metric (FM), employing <b>LLMs</b> for advanced semantic matching to evaluate summaries based on different aspects. This facet-aware approach offers a thorough evaluation of abstracts by decomposing the evaluation task into simpler subtasks.Recognizing the absence of an evaluation <b>benchmark</b> in this domain, we curate a Facet-based scientific <b>summarization</b> Dataset (FD) with facet-level annotations. Our findings confirm that FM offers a more logical approach to evaluating scientific summaries. In addition, <b>fine-tuned</b> smaller models can compete with <b>LLMs</b> in scientific contexts, while <b>LLMs</b> have limitations in learning from <b>in-context</b> information in scientific domains. This suggests an area for future enhancement of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=987--24299-assessing-generalization-capability-of-text-ranking-models-in-polish-sławomir-dadas-et-al-2024>(9/87 | 24/299) Assessing generalization capability of text ranking models in Polish (Sławomir Dadas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sławomir Dadas, Małgorzata Grębowiec. (2024)<br><strong>Assessing generalization capability of text ranking models in Polish</strong><br><button class=copy-to-clipboard title="Assessing generalization capability of text ranking models in Polish" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Out-of-domain, Rerank, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Information Retrieval, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14318v1.pdf filename=2402.14318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Retrieval-augmented</b> <b>generation</b> <b>(RAG)</b> is becoming an increasingly popular technique for integrating internal knowledge bases with <b>large</b> <b>language</b> <b>models.</b> In a typical <b>RAG</b> pipeline, three models are used, responsible for the <b>retrieval,</b> <b>reranking,</b> <b>and</b> generation stages. In this article, we focus on the <b>reranking</b> problem for the Polish language, examining the performance of rerankers and comparing their results with available <b>retrieval</b> <b>models.</b> <b>We</b> conduct a comprehensive evaluation of existing models and those trained by us, utilizing a <b>benchmark</b> of 41 diverse <b>information</b> <b>retrieval</b> <b>tasks</b> <b>for</b> the Polish language. The results of our experiments show that most models struggle with <b>out-of-domain</b> generalization. However, a combination of effective optimization method and a <b>large</b> <b>training</b> <b>dataset</b> allows for building rerankers that are both compact in size and capable of generalization. The best of our models establishes a new state-of-the-art for <b>reranking</b> in the Polish language, outperforming existing models with up to 30 times more parameters.</p></p class="citation"></blockquote><h3 id=1087--25299-llm-da-data-augmentation-via-large-language-models-for-few-shot-named-entity-recognition-junjie-ye-et-al-2024>(10/87 | 25/299) LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition (Junjie Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junjie Ye, Nuo Xu, Yikun Wang, Jie Zhou, Qi Zhang, Tao Gui, Xuanjing Huang. (2024)<br><strong>LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition</strong><br><button class=copy-to-clipboard title="LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Data Augmentation, Few-shot, Information Retrieval, Named Entity Recognition, Named Entity Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14568v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14568v1.pdf filename=2402.14568v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the impressive capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> their performance on <b>information</b> <b>extraction</b> tasks is still not entirely satisfactory. However, their remarkable rewriting capabilities and extensive world knowledge offer valuable insights to improve these tasks. In this paper, we propose $LLM-DA$, a novel <b>data</b> <b>augmentation</b> technique based on <b>LLMs</b> for the <b>few-shot</b> <b>NER</b> task. To overcome the limitations of existing <b>data</b> <b>augmentation</b> methods that compromise semantic integrity and address the uncertainty inherent in <b>LLM-generated</b> text, we leverage the distinctive characteristics of the <b>NER</b> task by augmenting the original <b>data</b> <b>at</b> both the contextual and entity levels. Our approach involves employing 14 contextual rewriting strategies, designing entity replacements of the same type, and incorporating noise injection to enhance robustness. Extensive experiments demonstrate the effectiveness of our approach in enhancing <b>NER</b> model performance with limited <b>data.</b> <b>Furthermore,</b> additional analyses provide further evidence supporting the assertion that the quality of the <b>data</b> <b>we</b> generate surpasses that of other existing methods.</p></p class="citation"></blockquote><h3 id=1187--26299-whose-llm-is-it-anyway-linguistic-comparison-and-llm-attribution-for-gpt-35-gpt-4-and-bard-ariel-rosenfeld-et-al-2024>(11/87 | 26/299) Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard (Ariel Rosenfeld et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ariel Rosenfeld, Teddy Lazebnik. (2024)<br><strong>Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard</strong><br><button class=copy-to-clipboard title="Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Bard, GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14533v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14533v1.pdf filename=2402.14533v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are capable of generating text that is similar to or surpasses human quality. However, it is unclear whether <b>LLMs</b> tend to exhibit distinctive linguistic styles akin to how human authors do. Through a comprehensive linguistic analysis, we compare the vocabulary, Part-Of-Speech (POS) distribution, dependency distribution, and sentiment of texts generated by three of the most popular <b>LLMS</b> today <b>(GPT-3.5,</b> <b>GPT-4,</b> and <b>Bard)</b> to diverse inputs. The results point to significant linguistic variations which, in turn, enable us to attribute a given text to its <b>LLM</b> origin with a favorable 88% accuracy using a simple off-the-shelf classification model. Theoretical and practical implications of this intriguing finding are discussed.</p></p class="citation"></blockquote><h3 id=1287--27299-noise-bert-a-unified-perturbation-robust-framework-with-noise-alignment-pre-training-for-noisy-slot-filling-task-jinxu-zhao-et-al-2024>(12/87 | 27/299) Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment Pre-training for Noisy Slot Filling Task (Jinxu Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinxu Zhao, Guanting Dong, Yueyan Qiu, Tingfeng Hui, Xiaoshuai Song, Daichi Guo, Weiran Xu. (2024)<br><strong>Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment Pre-training for Noisy Slot Filling Task</strong><br><button class=copy-to-clipboard title="Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment Pre-training for Noisy Slot Filling Task" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Contrastive Learning, Data Augmentation, Fine-tuning, Dialogue System, Slot Filling, Pre-trained Language Model, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14494v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14494v2.pdf filename=2402.14494v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In a realistic <b>dialogue</b> <b>system,</b> the input information from users is often subject to various types of input perturbations, which affects the <b>slot-filling</b> <b>task.</b> Although rule-based <b>data</b> <b>augmentation</b> methods have achieved satisfactory results, they fail to exhibit the desired generalization when faced with unknown noise disturbances. In this study, we address the challenges posed by input perturbations in <b>slot</b> <b>filling</b> by proposing Noise-BERT, a unified Perturbation-Robust Framework with Noise Alignment Pre-training. Our framework incorporates two Noise Alignment Pre-training tasks: <b>Slot</b> <b>Masked</b> Prediction and Sentence Noisiness Discrimination, aiming to guide the <b>pre-trained</b> <b>language</b> <b>model</b> in capturing accurate <b>slot</b> <b>information</b> and noise distribution. During <b>fine-tuning,</b> we employ a <b>contrastive</b> <b>learning</b> loss to enhance the semantic representation of entities and labels. Additionally, we introduce an <b>adversarial</b> <b>attack</b> training strategy to improve the model&rsquo;s robustness. Experimental results demonstrate the superiority of our proposed approach over state-of-the-art models, and further analysis confirms its effectiveness and generalization ability.</p></p class="citation"></blockquote><h3 id=1387--28299-small-language-model-is-a-good-guide-for-large-language-model-in-chinese-entity-relation-extraction-xuemei-tang-et-al-2024>(13/87 | 28/299) Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction (Xuemei Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuemei Tang, Jun Wang, Qi Su. (2024)<br><strong>Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction</strong><br><button class=copy-to-clipboard title="Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, Few-shot Learning, Relation Extraction, Large Language Model, Large Language Model, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14373v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14373v1.pdf filename=2402.14373v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been successful in <b>relational</b> <b>extraction</b> (RE) tasks, especially in the <b>few-shot</b> <b>learning.</b> An important problem in the field of RE is long-tailed data, while not much attention is currently paid to this problem using <b>LLM</b> approaches. Therefore, in this paper, we propose SLCoLM, a model collaboration framework, to mitigate the data long-tail problem. In our framework, We use the ``\textit{Training-Guide-Predict}&rsquo;&rsquo; strategy to combine the strengths of <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> and <b>LLMs,</b> where a task-specific <b>PLM</b> framework acts as a tutor, transfers task knowledge to the <b>LLM,</b> and guides the <b>LLM</b> in performing RE tasks. Our experiments on a RE dataset rich in <b>relation</b> <b>types</b> show that the approach in this paper facilitates RE of long-tail <b>relation</b> <b>types.</b></p></p class="citation"></blockquote><h3 id=1487--29299-rule-or-story-which-is-a-better-commonsense-expression-for-talking-with-large-language-models-ning-bian-et-al-2024>(14/87 | 29/299) Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models? (Ning Bian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ning Bian, Xianpei Han, Hongyu Lin, Yaojie Lu, Ben He, Le Sun. (2024)<br><strong>Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?</strong><br><button class=copy-to-clipboard title="Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Self-supervised Learning, Common-sense Reasoning, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14355v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14355v1.pdf filename=2402.14355v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Building machines with <b>commonsense</b> <b>has</b> been a longstanding challenge in NLP due to the reporting bias of <b>commonsense</b> <b>rules</b> and the exposure bias of rule-based <b>commonsense</b> <b>reasoning.</b> In contrast, humans convey and pass down <b>commonsense</b> <b>implicitly</b> through stories. This paper investigates the inherent <b>commonsense</b> <b>ability</b> of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> expressed through storytelling. We systematically investigate and compare stories and rules for retrieving and leveraging <b>commonsense</b> <b>in</b> <b>LLMs.</b> Experimental results on 28 <b>commonsense</b> <b>QA</b> datasets show that stories outperform rules as the expression for retrieving <b>commonsense</b> <b>from</b> <b>LLMs,</b> exhibiting higher generation confidence and <b>commonsense</b> <b>accuracy.</b> Moreover, stories are the more effective <b>commonsense</b> <b>expression</b> for answering questions regarding daily events, while rules are more effective for scientific questions. This aligns with the reporting bias of <b>commonsense</b> <b>in</b> text corpora. We further show that the correctness and relevance of <b>commonsense</b> <b>stories</b> can be further improved via iterative <b>self-supervised</b> <b>fine-tuning.</b> These findings emphasize the importance of using appropriate language to express, retrieve, and leverage <b>commonsense</b> <b>for</b> <b>LLMs,</b> highlighting a promising direction for better exploiting their <b>commonsense</b> <b>abilities.</b></p></p class="citation"></blockquote><h3 id=1587--30299-mitigating-biases-of-large-language-models-in-stance-detection-with-calibration-ang-li-et-al-2024>(15/87 | 30/299) Mitigating Biases of Large Language Models in Stance Detection with Calibration (Ang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ang Li, Jingqian Zhao, Bin Liang, Lin Gui, Hui Wang, Xi Zeng, Kam-Fai Wong, Ruifeng Xu. (2024)<br><strong>Mitigating Biases of Large Language Models in Stance Detection with Calibration</strong><br><button class=copy-to-clipboard title="Mitigating Biases of Large Language Models in Stance Detection with Calibration" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Graph Attention Networks, Counter-factual, Zero-shot, Reasoning, Stance Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14296v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14296v1.pdf filename=2402.14296v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved remarkable progress in many natural language processing tasks. However, our experiment reveals that, in <b>stance</b> <b>detection</b> tasks, <b>LLMs</b> may generate biased <b>stances</b> <b>due</b> to spurious sentiment-stance correlation and preference towards certain individuals and topics, thus harming their performance. Therefore, in this paper, we propose to Mitigate Biases of <b>LLMs</b> in <b>stance</b> <b>detection</b> with Calibration (MB-Cal). In which, a novel <b>gated</b> calibration network is devised to mitigate the biases on the <b>stance</b> <b>reasoning</b> results from <b>LLMs.</b> Further, to make the calibration more accurate and generalizable, we construct <b>counterfactual</b> augmented data to rectify <b>stance</b> <b>biases.</b> Experimental results on in-target and <b>zero-shot</b> <b>stance</b> <b>detection</b> tasks show that the proposed MB-Cal can effectively mitigate biases of <b>LLMs,</b> achieving state-of-the-art results.</p></p class="citation"></blockquote><h3 id=1687--31299-learning-to-reduce-optimal-representations-of-structured-data-in-prompting-large-language-models-younghun-lee-et-al-2024>(16/87 | 31/299) Learning to Reduce: Optimal Representations of Structured Data in Prompting Large Language Models (Younghun Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Younghun Lee, Sungchul Kim, Tong Yu, Ryan A. Rossi, Xiang Chen. (2024)<br><strong>Learning to Reduce: Optimal Representations of Structured Data in Prompting Large Language Models</strong><br><button class=copy-to-clipboard title="Learning to Reduce: Optimal Representations of Structured Data in Prompting Large Language Models" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 65<br>Keywords: Fine-tuning, Knowledge Graph, Reinforcement Learning, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14195v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14195v1.pdf filename=2402.14195v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have been widely used as general-purpose AI agents showing comparable performance on many downstream tasks. However, existing work shows that it is challenging for <b>LLMs</b> to integrate structured data (e.g. <b>KG,</b> tables, DBs) into their <b>prompts;</b> <b>LLMs</b> need to either understand long text data or select the most relevant evidence prior to inference, and both approaches are not trivial. In this paper, we propose a framework, Learning to Reduce, that <b>fine-tunes</b> a language model to generate a reduced version of an input context, given a task description and context input. The model learns to reduce the input context using On-Policy <b>Reinforcement</b> <b>Learning</b> and aims to improve the <b>reasoning</b> performance of a fixed <b>LLM.</b> Experimental results illustrate that our model not only achieves comparable accuracies in selecting the relevant evidence from an input context, but also shows generalizability on different datasets. We further show that our model helps improve the <b>LLM&rsquo;s</b> performance on downstream tasks especially when the context is long.</p></p class="citation"></blockquote><h3 id=1787--32299-conceptmath-a-bilingual-concept-wise-benchmark-for-measuring-mathematical-reasoning-of-large-language-models-yanan-wu-et-al-2024>(17/87 | 32/299) ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models (Yanan Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhanhui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, Wanli Ouyang, Wenbo Su, Bo Zheng. (2024)<br><strong>ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models</strong><br><button class=copy-to-clipboard title="ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, Foundation Model, Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14660v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14660v2.pdf filename=2402.14660v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained <b>benchmark</b> that evaluates concept-wise <b>mathematical</b> <b>reasoning</b> of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Unlike traditional <b>benchmarks</b> that evaluate general <b>mathematical</b> <b>reasoning</b> with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that <b>mathematical</b> <b>reasoning</b> can be evaluated at different granularity with concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range of <b>LLMs,</b> and we observe existing <b>LLMs,</b> though achieving high average accuracies on traditional <b>benchmarks,</b> exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones. Besides, we also introduce an efficient <b>fine-tuning</b> strategy to enhance the weaknesses of existing <b>LLMs.</b> Finally, we hope ConceptMath could guide the developers to understand the fine-grained <b>mathematical</b> <b>abilities</b> of their models and facilitate the growth of <b>foundation</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=1887--33299-enhancing-systematic-decompositional-natural-language-inference-using-informal-logic-nathaniel-weir-et-al-2024>(18/87 | 33/299) Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic (Nathaniel Weir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nathaniel Weir, Kate Sanders, Orion Weller, Shreya Sharma, Dongwei Jiang, Zhengping Jiang, Bhavana Dalvi Mishra, Oyvind Tafjord, Peter Jansen, Peter Clark, Benjamin Van Durme. (2024)<br><strong>Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic</strong><br><button class=copy-to-clipboard title="Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Knowledge Distillation, Knowledge Distillation, Natural Language Inference, Reasoning, Textual Entailment, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14798v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14798v2.pdf filename=2402.14798v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contemporary language models enable new opportunities for structured <b>reasoning</b> with text, such as the construction and evaluation of intuitive, proof-like <b>textual</b> <b>entailment</b> trees without relying on brittle formal logic. However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what valid compositional entailment is. This absence causes noisy datasets and limited performance gains by modern neuro-symbolic engines. To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment datasets, and evaluate its impact on <b>LLM-based</b> <b>textual</b> <b>inference.</b> We find that our resulting dataset, RDTE (Recognizing Decompositional <b>Textual</b> <b>Entailment),</b> has a substantially higher internal consistency (+9%) than prior decompositional entailment datasets, suggesting that RDTE is a significant step forward in the long-standing problem of forming a clear protocol for discerning entailment. We also find that training an RDTE-oriented entailment classifier via <b>knowledge</b> <b>distillation</b> and employing it in a modern neuro-symbolic <b>reasoning</b> engine significantly improves results (both accuracy and proof quality) over other entailment classifier baselines, illustrating the practical benefit of this advance for <b>textual</b> <b>inference.</b></p></p class="citation"></blockquote><h3 id=1987--34299-zero-shot-cross-lingual-transfer-in-instruction-tuning-of-large-language-model-nadezhda-chirkova-et-al-2024>(19/87 | 34/299) Zero-shot cross-lingual transfer in instruction tuning of large language model (Nadezhda Chirkova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nadezhda Chirkova, Vassilina Nikoulina. (2024)<br><strong>Zero-shot cross-lingual transfer in instruction tuning of large language model</strong><br><button class=copy-to-clipboard title="Zero-shot cross-lingual transfer in instruction tuning of large language model" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Zero-shot, Instruction Following, Instruction Tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14778v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14778v1.pdf filename=2402.14778v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction</b> <b>tuning</b> (IT) is widely used to teach pretrained <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to follow arbitrary <b>instructions,</b> <b>but</b> is under-studied in multilingual settings. In this work, we conduct a systematic study of <b>zero-shot</b> cross-lingual transfer in IT, when an <b>LLM</b> is <b>instruction-tuned</b> <b>on</b> English-only data and then tested on user <b>prompts</b> in other languages. We investigate the influence of model configuration choices and devise a multi-facet evaluation strategy for multilingual <b>instruction</b> <b>following.</b> We find that cross-lingual transfer does happen successfully in IT even if all stages of model training are English-centric, but only if multiliguality is taken into account in hyperparameter tuning and with <b>large</b> <b>enough</b> <b>IT</b> data. English-trained <b>LLMs</b> are capable of generating correct-language, comprehensive and helpful responses in the other languages, but suffer from low factuality and may occasionally have fluency errors.</p></p class="citation"></blockquote><h3 id=2087--35299-annotation-and-classification-of-relevant-clauses-in-terms-and-conditions-contracts-pietro-giovanni-bizzaro-et-al-2024>(20/87 | 35/299) Annotation and Classification of Relevant Clauses in Terms-and-Conditions Contracts (Pietro Giovanni Bizzaro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pietro Giovanni Bizzaro, Elena Della Valentina, Maurizio Napolitano, Nadia Mana, Massimo Zancanaro. (2024)<br><strong>Annotation and Classification of Relevant Clauses in Terms-and-Conditions Contracts</strong><br><button class=copy-to-clipboard title="Annotation and Classification of Relevant Clauses in Terms-and-Conditions Contracts" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Few-shot, Fine-tuning, BERT, T5, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14457v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14457v1.pdf filename=2402.14457v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a new annotation scheme to classify different types of clauses in Terms-and-Conditions contracts with the ultimate goal of supporting legal experts to quickly identify and assess problematic issues in this type of legal documents. To this end, we built a small corpus of Terms-and-Conditions contracts and finalized an annotation scheme of 14 categories, eventually reaching an inter-annotator agreement of 0.92. Then, for 11 of them, we experimented with binary classification tasks using <b>few-shot</b> <b>prompting</b> with a multilingual <b>T5</b> and two <b>fine-tuned</b> versions of two <b>BERT-based</b> <b>LLMs</b> for Italian. Our experiments showed the feasibility of automatic classification of our categories by reaching accuracies ranging from .79 to .95 on validation tasks.</p></p class="citation"></blockquote><h3 id=2187--36299-transferring-bert-capabilities-from-high-resource-to-low-resource-languages-using-vocabulary-matching-piotr-rybak-2024>(21/87 | 36/299) Transferring BERT Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching (Piotr Rybak, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Piotr Rybak. (2024)<br><strong>Transferring BERT Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching</strong><br><button class=copy-to-clipboard title="Transferring BERT Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: High-Resource, Low-Resource, BERT, Transformer, Natural Language Understanding, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14408v1.pdf filename=2402.14408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pre-trained</b> <b>language</b> <b>models</b> have revolutionized the <b>natural</b> <b>language</b> <b>understanding</b> landscape, most notably <b>BERT</b> (Bidirectional Encoder Representations from <b>Transformers).</b> However, a significant challenge remains for <b>low-resource</b> languages, where limited data hinders the effective training of such models. This work presents a novel approach to bridge this gap by transferring <b>BERT</b> capabilities from <b>high-resource</b> to <b>low-resource</b> languages using vocabulary matching. We conduct experiments on the Silesian and Kashubian languages and demonstrate the effectiveness of our approach to improve the performance of <b>BERT</b> models even when the target language has minimal training data. Our results highlight the potential of the proposed technique to effectively train <b>BERT</b> models for <b>low-resource</b> languages, thus democratizing access to advanced language understanding models.</p></p class="citation"></blockquote><h3 id=2287--37299-instraug-automatic-instruction-augmentation-for-multimodal-instruction-fine-tuning-wei-han-et-al-2024>(22/87 | 37/299) INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction Fine-tuning (Wei Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Han, Hui Chen, Soujanya Poria. (2024)<br><strong>INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction Fine-tuning</strong><br><button class=copy-to-clipboard title="INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction Fine-tuning" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 59<br>Keywords: Benchmarking, Fine-tuning, Multi-modal, Multi-modal, Zero-shot, Instruction Following, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14492v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14492v1.pdf filename=2402.14492v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> on multi-task <b>instruction-following</b> <b>data</b> has been proven to be a powerful learning paradigm for improving their <b>zero-shot</b> capabilities on new tasks. Recent works about high-quality <b>instruction-following</b> <b>data</b> generation and selection require amounts of human labor to conceive model-understandable <b>instructions</b> <b>for</b> the given tasks and carefully filter the <b>LLM-generated</b> data. In this work, we introduce an automatic <b>instruction</b> <b>augmentation</b> method named INSTRAUG in <b>multimodal</b> tasks. It starts from a handful of basic and straightforward meta <b>instructions</b> <b>but</b> can expand an <b>instruction-following</b> <b>dataset</b> by 30 times. Results on two popular <b>multimodal</b> instructionfollowing <b>benchmarks</b> MULTIINSTRUCT and InstructBLIP show that INSTRAUG can significantly improve the alignment of <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) across 12 <b>multimodal</b> tasks, which is even equivalent to the benefits of scaling up training data multiple times.</p></p class="citation"></blockquote><h3 id=2387--38299-enhancing-temporal-knowledge-graph-forecasting-with-large-language-models-via-chain-of-history-reasoning-yuwei-xia-et-al-2024>(23/87 | 38/299) Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning (Yuwei Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuwei Xia, Ding Wang, Qiang Liu, Liang Wang, Shu Wu, Xiaoyu Zhang. (2024)<br><strong>Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning</strong><br><button class=copy-to-clipboard title="Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 58<br>Keywords: Graph, Knowledge Graph, Reasoning, Large Language Model, Large Language Model, Temporal Knowledge Graph, Temporal Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14382v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14382v1.pdf filename=2402.14382v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Temporal</b> <b>Knowledge</b> <b>Graph</b> <b>(TKG)</b> forecasting aims to predict future facts based on given histories. Most recent <b>graph-based</b> models excel at capturing structural information within <b>TKGs</b> but lack semantic comprehension abilities. Nowadays, with the surge of <b>LLMs,</b> the <b>LLM-based</b> <b>TKG</b> prediction model has emerged. However, the existing <b>LLM-based</b> model exhibits three shortcomings: (1) It only focuses on the first-order history for prediction while ignoring high-order historical information, resulting in the provided information for <b>LLMs</b> being extremely limited. (2) <b>LLMs</b> struggle with optimal <b>reasoning</b> performance under heavy historical information loads. (3) For <b>TKG</b> prediction, the <b>temporal</b> <b>reasoning</b> <b>capability</b> of <b>LLM</b> alone is limited. To address the first two challenges, we propose Chain-of-History (CoH) <b>reasoning</b> which explores high-order histories step-by-step, achieving effective utilization of high-order historical information for <b>LLMs</b> on <b>TKG</b> prediction. To address the third issue, we design CoH as a paly-and-plug module to enhance the performance of <b>graph-based</b> models for <b>TKG</b> prediction. Extensive experiments on three datasets and backbones demonstrate the effectiveness of CoH.</p></p class="citation"></blockquote><h3 id=2487--39299-gate-x-e--a-challenge-set-for-gender-fair-translations-from-weakly-gendered-languages-spencer-rarrick-et-al-2024>(24/87 | 39/299) GATE X-E : A Challenge Set for Gender-Fair Translations from Weakly-Gendered Languages (Spencer Rarrick et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Spencer Rarrick, Ranjita Naik, Sundar Poudel, Vishal Chowdhary. (2024)<br><strong>GATE X-E : A Challenge Set for Gender-Fair Translations from Weakly-Gendered Languages</strong><br><button class=copy-to-clipboard title="GATE X-E : A Challenge Set for Gender-Fair Translations from Weakly-Gendered Languages" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, GPT, GPT-4, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14277v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14277v1.pdf filename=2402.14277v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Neural</b> <b>Machine</b> <b>Translation</b> <b>(NMT)</b> continues to improve in quality and adoption, yet the inadvertent perpetuation of gender bias remains a significant concern. Despite numerous studies on gender bias in translations into English from weakly gendered-languages, there are no <b>benchmarks</b> for evaluating this phenomenon or for assessing mitigation strategies. To address this gap, we introduce GATE X-E, an extension to the GATE (Rarrick et al., 2023) corpus, that consists of human translations from Turkish, Hungarian, Finnish, and Persian into English. Each translation is accompanied by feminine, masculine, and neutral variants. The dataset, which contains between 1250 and 1850 instances for each of the four language pairs, features natural sentences with a wide range of sentence lengths and domains, challenging translation rewriters on various linguistic phenomena. Additionally, we present a translation gender rewriting solution built with <b>GPT-4</b> and use GATE X-E to evaluate it. We open source our contributions to encourage further research on gender debiasing.</p></p class="citation"></blockquote><h3 id=2587--40299-iepile-unearthing-large-scale-schema-based-information-extraction-corpus-honghao-gui-et-al-2024>(25/87 | 40/299) IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus (Honghao Gui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Honghao Gui, Hongbin Ye, Lin Yuan, Ningyu Zhang, Mengshu Sun, Lei Liang, Huajun Chen. (2024)<br><strong>IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus</strong><br><button class=copy-to-clipboard title="IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-DB, cs-IR, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Zero-shot, LLaMA, Information Retrieval, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14710v1.pdf filename=2402.14710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in <b>Information</b> <b>Extraction</b> (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of <b>LLMs,</b> while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a <b>large-scale</b> <b>corpus.</b> <b>Experimental</b> results on <b>LLaMA</b> and Baichuan demonstrate that using IEPile can enhance the performance of <b>LLMs</b> for IE, especially the <b>zero-shot</b> generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.</p></p class="citation"></blockquote><h3 id=2687--41299-data-augmentation-is-dead-long-live-data-augmentation-frédéric-piedboeuf-et-al-2024>(26/87 | 41/299) Data Augmentation is Dead, Long Live Data Augmentation (Frédéric Piedboeuf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frédéric Piedboeuf, Philippe Langlais. (2024)<br><strong>Data Augmentation is Dead, Long Live Data Augmentation</strong><br><button class=copy-to-clipboard title="Data Augmentation is Dead, Long Live Data Augmentation" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Data Augmentation, Few-shot, Fine-tuning, ChatGPT, Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14895v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14895v1.pdf filename=2402.14895v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Textual <b>data</b> <b>augmentation</b> (DA) is a prolific field of study where novel techniques to create artificial <b>data</b> <b>are</b> regularly proposed, and that has demonstrated great efficiency on small <b>data</b> <b>settings,</b> at least for <b>text</b> <b>classification</b> tasks. In this paper, we challenge those results, showing that classical <b>data</b> <b>augmentation</b> is simply a way of performing better <b>fine-tuning,</b> and that spending more time <b>fine-tuning</b> before applying <b>data</b> <b>augmentation</b> negates its effect. This is a significant contribution as it answers several questions that were left open in recent years, namely~: which DA technique performs best (all of them as long as they generate <b>data</b> <b>close</b> enough to the training set as to not impair training) and why did DA show positive results (facilitates training of network). We furthermore show that zero and <b>few-shot</b> <b>data</b> <b>generation</b> via conversational agents such as <b>ChatGPT</b> or LLama2 can increase performances, concluding that this form of <b>data</b> <b>augmentation</b> does still work, even if classical methods do not.</p></p class="citation"></blockquote><h3 id=2787--42299-towards-unified-task-embeddings-across-multiple-models-bridging-the-gap-for-prompt-based-large-language-models-and-beyond-xinyu-wang-et-al-2024>(27/87 | 42/299) Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond (Xinyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Wang, Hainiu Xu, Lin Gui, Yulan He. (2024)<br><strong>Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond</strong><br><button class=copy-to-clipboard title="Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Meta Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14522v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14522v1.pdf filename=2402.14522v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Task embedding, a <b>meta-learning</b> <b>technique</b> that captures task-specific information, has become prevalent, especially in areas such as multi-task learning, model editing, and interpretability. However, it faces challenges with the emergence of <b>prompt-guided</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> operating in a gradientfree manner. Existing task embedding methods rely on <b>fine-tuned,</b> task-specific language models, which hinders the adaptability of task embeddings across diverse models, especially <b>prompt-based</b> <b>LLMs.</b> To unleash the power of task embedding in the era of <b>LLMs,</b> we propose a framework for unified task embeddings (FUTE), harmonizing task embeddings from various models, including smaller language models and <b>LLMs</b> with varied <b>prompts,</b> within a single vector space. Such uniformity enables the comparison and analysis of similarities amongst different models, extending the scope and utility of existing task embedding methods in addressing multi-model scenarios, whilst maintaining their performance to be comparable to architecture-specific methods.</p></p class="citation"></blockquote><h3 id=2887--43299-malaysian-english-news-decoded-a-linguistic-resource-for-named-entity-and-relation-extraction-mohan-raj-chanthran-et-al-2024>(28/87 | 43/299) Malaysian English News Decoded: A Linguistic Resource for Named Entity and Relation Extraction (Mohan Raj Chanthran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohan Raj Chanthran, Lay-Ki Soon, Huey Fang Ong, Bhawani Selvaretnam. (2024)<br><strong>Malaysian English News Decoded: A Linguistic Resource for Named Entity and Relation Extraction</strong><br><button class=copy-to-clipboard title="Malaysian English News Decoded: A Linguistic Resource for Named Entity and Relation Extraction" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, Named Entity Recognition, Named Entity Recognition, Relation Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14521v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14521v1.pdf filename=2402.14521v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Standard English and Malaysian English exhibit notable differences, posing challenges for natural language processing (NLP) tasks on Malaysian English. Unfortunately, most of the existing datasets are mainly based on standard English and therefore inadequate for improving NLP tasks in Malaysian English. An experiment using state-of-the-art <b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER)</b> solutions on Malaysian English news articles highlights that they cannot handle morphosyntactic variations in Malaysian English. To the best of our knowledge, there is no annotated dataset available to improvise the model. To address these issues, we constructed a Malaysian English News (MEN) dataset, which contains 200 news articles that are manually annotated with entities and <b>relations.</b> <b>We</b> then <b>fine-tuned</b> the spaCy <b>NER</b> tool and validated that having a dataset tailor-made for Malaysian English could improve the performance of <b>NER</b> in Malaysian English significantly. This paper presents our effort in the data acquisition, annotation methodology, and thorough analysis of the annotated dataset. To validate the quality of the annotation, inter-annotator agreement was used, followed by adjudication of disagreements by a subject matter expert. Upon completion of these tasks, we managed to develop a dataset with 6,061 entities and 3,268 <b>relation</b> <b>instances.</b> Finally, we discuss on spaCy <b>fine-tuning</b> setup and analysis on the <b>NER</b> performance. This unique dataset will contribute significantly to the advancement of NLP research in Malaysian English, allowing researchers to accelerate their progress, particularly in <b>NER</b> and <b>relation</b> <b>extraction.</b> The dataset and annotation guideline has been published on Github.</p></p class="citation"></blockquote><h3 id=2987--44299-my-answer-is-c-first-token-probabilities-do-not-match-text-answers-in-instruction-tuned-language-models-xinpeng-wang-et-al-2024>(29/87 | 44/299) &lsquo;My Answer is C&rsquo;: First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models (Xinpeng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinpeng Wang, Bolei Ma, Chengzhi Hu, Leon Weber-Genzel, Paul Röttger, Frauke Kreuter, Dirk Hovy, Barbara Plank. (2024)<br><strong>&lsquo;My Answer is C&rsquo;: First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models</strong><br><button class=copy-to-clipboard title="'My Answer is C': First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Language Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14499v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14499v1.pdf filename=2402.14499v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The open-ended nature of <b>language</b> <b>generation</b> makes the evaluation of autoregressive <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> challenging. One common evaluation approach uses multiple-choice questions (MCQ) to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model&rsquo;s diverse response styles such as starting with &ldquo;Sure&rdquo; or refusing to answer. Consequently, MCQ evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under <b>prompt</b> perturbation. Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60%. Models heavily <b>fine-tuned</b> on conversational or safety data are especially impacted. Crucially, models remain misaligned even when we increasingly constrain <b>prompts,</b> i.e., force them to start with an option letter or example template. Our findings i) underscore the importance of inspecting the text output, too and ii) caution against relying solely on first-token evaluation.</p></p class="citation"></blockquote><h3 id=3087--45299-word-sequence-entropy-towards-uncertainty-estimation-in-free-form-medical-question-answering-applications-and-beyond-zhiyuan-wang-et-al-2024>(30/87 | 45/299) Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond (Zhiyuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyuan Wang, Jinhao Duan, Chenxi Yuan, Qingyu Chen, Tianlong Chen, Huaxiu Yao, Yue Zhang, Ren Wang, Kaidi Xu, Xiaoshuang Shi. (2024)<br><strong>Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond</strong><br><button class=copy-to-clipboard title="Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14259v1.pdf filename=2402.14259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Uncertainty estimation plays a pivotal role in ensuring the reliability of safety-critical human-AI interaction systems, particularly in the medical domain. However, a general method for quantifying the uncertainty of free-form answers has yet to be established in open-ended medical <b>question-answering</b> <b>(QA)</b> tasks, where irrelevant words and sequences with limited semantic information can be the primary source of uncertainty due to the presence of generative inequality. In this paper, we propose the Word-Sequence Entropy (WSE), which calibrates the uncertainty proportion at both the word and sequence levels according to the semantic relevance, with greater emphasis placed on keywords and more relevant sequences when performing uncertainty quantification. We compare WSE with 6 baseline methods on 5 free-form medical <b>QA</b> datasets, utilizing 7 &ldquo;off-the-shelf&rdquo; <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> and show that WSE exhibits superior performance on accurate uncertainty measurement under two standard criteria for correctness evaluation (e.g., WSE outperforms existing state-of-the-art method by 3.23% AUROC on the MedQA dataset). Additionally, in terms of the potential for real-world medical <b>QA</b> applications, we achieve a significant enhancement in the performance of <b>LLMs</b> when employing sequences with lower uncertainty, identified by WSE, as final answers (e.g., +6.36% accuracy improvement on the COVID-QA dataset), without requiring any additional task-specific <b>fine-tuning</b> or architectural modifications.</p></p class="citation"></blockquote><h3 id=3187--46299-eagle-ethical-dataset-given-from-real-interactions-masahiro-kaneko-et-al-2024>(31/87 | 46/299) Eagle: Ethical Dataset Given from Real Interactions (Masahiro Kaneko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masahiro Kaneko, Danushka Bollegala, Timothy Baldwin. (2024)<br><strong>Eagle: Ethical Dataset Given from Real Interactions</strong><br><button class=copy-to-clipboard title="Eagle: Ethical Dataset Given from Real Interactions" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: ChatGPT, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14258v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14258v1.pdf filename=2402.14258v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have demonstrated that <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have ethical-related problems such as social biases, lack of moral <b>reasoning,</b> and generation of offensive content. The existing evaluation metrics and methods to address these ethical challenges use datasets intentionally created by instructing humans to create instances including ethical problems. Therefore, the data does not reflect <b>prompts</b> that users actually provide when utilizing <b>LLM</b> services in everyday contexts. This may not lead to the development of safe <b>LLMs</b> that can address ethical challenges arising in real-world applications. In this paper, we create Eagle datasets extracted from real interactions between <b>ChatGPT</b> and users that exhibit social biases, toxicity, and immoral problems. Our experiments show that Eagle captures complementary aspects, not covered by existing datasets proposed for evaluation and mitigation of such ethical challenges. Our code is publicly available at <a href=https://huggingface.co/datasets/MasahiroKaneko/eagle>https://huggingface.co/datasets/MasahiroKaneko/eagle</a>.</p></p class="citation"></blockquote><h3 id=3287--47299-towards-understanding-counseling-conversations-domain-knowledge-and-large-language-models-younghun-lee-et-al-2024>(32/87 | 47/299) Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models (Younghun Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Younghun Lee, Dan Goldwasser, Laura Schwab Reese. (2024)<br><strong>Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models</strong><br><button class=copy-to-clipboard title="Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, Transformer, Large Language Model, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14200v1.pdf filename=2402.14200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the dynamics of counseling conversations is an important task, yet it is a challenging NLP problem regardless of the recent advance of <b>Transformer-based</b> <b>pre-trained</b> <b>language</b> <b>models.</b> This paper proposes a systematic approach to examine the efficacy of domain knowledge and <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in better representing conversations between a crisis counselor and a help seeker. We empirically show that state-of-the-art language models such as <b>Transformer-based</b> models and <b>GPT</b> models fail to predict the conversation outcome. To provide richer context to conversations, we incorporate human-annotated domain knowledge and <b>LLM-generated</b> features; simple integration of domain knowledge and <b>LLM</b> features improves the model performance by approximately 15%. We argue that both domain knowledge and <b>LLM-generated</b> features can be exploited to better characterize counseling conversations when they are used as an additional context to conversations.</p></p class="citation"></blockquote><h3 id=3387--48299-palo-a-polyglot-large-multimodal-model-for-5b-people-muhammad-maaz-et-al-2024>(33/87 | 48/299) PALO: A Polyglot Large Multimodal Model for 5B People (Muhammad Maaz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Maaz, Hanoona Rasheed, Abdelrahman Shaker, Salman Khan, Hisham Cholakal, Rao M. Anwer, Tim Baldwin, Michael Felsberg, Fahad S. Khan. (2024)<br><strong>PALO: A Polyglot Large Multimodal Model for 5B People</strong><br><button class=copy-to-clipboard title="PALO: A Polyglot Large Multimodal Model for 5B People" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 49<br>Keywords: Benchmarking, Fine-tuning, Multi-modal, Multi-modal, Reasoning, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14818v1.pdf filename=2402.14818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In pursuit of more inclusive <b>Vision-Language</b> Models (VLMs), this study introduces a <b>Large</b> <b>Multilingual</b> <b>Multimodal</b> Model called \textsc{Palo}. \textsc{Palo} offers visual <b>reasoning</b> capabilities in 10 major languages, including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese, that span a total of $\sim$5B people (65% of the world population). Our approach involves a semi-automated translation approach to adapt the <b>multimodal</b> instruction dataset from English to the target languages using a <b>fine-tuned</b> <b>Large</b> <b>Language</b> <b>Model,</b> thereby ensuring high linguistic fidelity while allowing scalability due to minimal manual effort. The incorporation of diverse instruction sets helps us boost overall performance across multiple languages especially those that are underrepresented like Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three scales (1.7B, 7B and 13B parameters) to show the generalization and scalability where we observe substantial improvements compared to strong baselines. We also propose the first multilingual <b>multimodal</b> <b>benchmark</b> for the forthcoming approaches to evaluate their <b>vision-language</b> <b>reasoning</b> capabilities across languages. Code: <a href=https://github.com/mbzuai-oryx/PALO>https://github.com/mbzuai-oryx/PALO</a>.</p></p class="citation"></blockquote><h3 id=3487--49299-tinybenchmarks-evaluating-llms-with-fewer-examples-felipe-maia-polo-et-al-2024>(34/87 | 49/299) tinyBenchmarks: evaluating LLMs with fewer examples (Felipe Maia Polo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, Mikhail Yurochkin. (2024)<br><strong>tinyBenchmarks: evaluating LLMs with fewer examples</strong><br><button class=copy-to-clipboard title="tinyBenchmarks: evaluating LLMs with fewer examples" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL, stat-ML<br>Keyword Score: 43<br>Keywords: Benchmarking, Massive Multitask Language Understanding (MMLU), Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14992v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14992v1.pdf filename=2402.14992v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The versatility of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> led to the creation of diverse <b>benchmarks</b> that thoroughly test a variety of language models&rsquo; abilities. These <b>benchmarks</b> consist of tens of thousands of examples making evaluation of <b>LLMs</b> very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an <b>LLM</b> on several key <b>benchmarks.</b> For example, we show that to accurately estimate the performance of an <b>LLM</b> on <b>MMLU,</b> a popular multiple-choice <b>QA</b> <b>benchmark</b> consisting of 14K examples, it is sufficient to evaluate this <b>LLM</b> on 100 curated examples. We release evaluation tools and tiny versions of popular <b>benchmarks:</b> Open <b>LLM</b> Leaderboard, <b>MMLU,</b> HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny <b>benchmarks</b> are sufficient to reliably and efficiently reproduce the original evaluation results.</p></p class="citation"></blockquote><h3 id=3587--50299-mt-bench-101-a-fine-grained-benchmark-for-evaluating-large-language-models-in-multi-turn-dialogues-ge-bai-et-al-2024>(35/87 | 50/299) MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues (Ge Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, Wanli Ouyang. (2024)<br><strong>MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues</strong><br><button class=copy-to-clipboard title="MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Dialogue System, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14762v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14762v1.pdf filename=2402.14762v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has drastically enhanced <b>dialogue</b> <b>systems.</b> However, comprehensively evaluating the <b>dialogue</b> <b>abilities</b> of <b>LLMs</b> remains a challenge. Previous <b>benchmarks</b> have primarily focused on single-turn <b>dialogues</b> <b>or</b> provided coarse-grained and incomplete assessments of multi-turn <b>dialogues,</b> <b>overlooking</b> the complexity and fine-grained nuances of real-life <b>dialogues.</b> <b>To</b> address this issue, we introduce <b>MT-Bench-101,</b> specifically designed to evaluate the fine-grained abilities of <b>LLMs</b> in multi-turn <b>dialogues.</b> <b>By</b> conducting a detailed analysis of real multi-turn <b>dialogue</b> <b>data,</b> we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn <b>dialogues</b> <b>in</b> 13 distinct tasks. We then evaluate 21 popular <b>LLMs</b> based on <b>MT-Bench-101,</b> conducting comprehensive analyses from both ability and task perspectives and observing differing trends in <b>LLMs</b> performance across <b>dialogue</b> <b>turns</b> within various tasks. Further analysis indicates that neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of <b>LLMs.</b> Extensive case studies suggest that our designed tasks accurately assess the corresponding multi-turn abilities.</p></p class="citation"></blockquote><h3 id=3687--51299-an-llm-enhanced-adversarial-editing-system-for-lexical-simplification-keren-tan-et-al-2024>(36/87 | 51/299) An LLM-Enhanced Adversarial Editing System for Lexical Simplification (Keren Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keren Tan, Kangyang Luo, Yunshi Lan, Zheng Yuan, Jinlong Shu. (2024)<br><strong>An LLM-Enhanced Adversarial Editing System for Lexical Simplification</strong><br><button class=copy-to-clipboard title="An LLM-Enhanced Adversarial Editing System for Lexical Simplification" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Knowledge Distillation, Low-Resource, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14704v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14704v2.pdf filename=2402.14704v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lexical Simplification (LS) aims to simplify text at the lexical level. Existing methods rely heavily on annotated data, making it challenging to apply in <b>low-resource</b> scenarios. In this paper, we propose a novel LS method without parallel corpora. This method employs an Adversarial Editing System with guidance from a confusion loss and an invariance loss to predict lexical edits in the original sentences. Meanwhile, we introduce an innovative <b>LLM-enhanced</b> loss to enable the <b>distillation</b> of knowledge from <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> into a small-size LS system. From that, complex words within sentences are masked and a Difficulty-aware Filling module is crafted to replace masked positions with simpler words. At last, extensive experimental results and analyses on three <b>benchmark</b> LS datasets demonstrate the effectiveness of our proposed method.</p></p class="citation"></blockquote><h3 id=3787--52299-triad-a-framework-leveraging-a-multi-role-llm-based-agent-to-solve-knowledge-base-question-answering-chang-zong-et-al-2024>(37/87 | 52/299) Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering (Chang Zong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chang Zong, Yuchen Yan, Weiming Lu, Eliot Huang, Jian Shao, Yueting Zhuang. (2024)<br><strong>Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering</strong><br><button class=copy-to-clipboard title="Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Knowledge Base Question Answering, Knowledge Based Question Answering, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14320v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14320v1.pdf filename=2402.14320v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent progress with <b>LLM-based</b> agents has shown promising results across various tasks. However, their use in answering <b>questions</b> <b>from</b> <b>knowledge</b> <b>bases</b> <b>remains</b> <b>largely</b> unexplored. Implementing a <b>KBQA</b> system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an <b>LLM-based</b> agent with three roles for <b>KBQA</b> tasks. The agent is assigned three roles to tackle different <b>KBQA</b> subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering <b>questions</b> <b>with</b> <b>knowledge.</b> <b>Our</b> <b>KBQA</b> <b>framework</b> is executed in four phases, involving the collaboration of the agent&rsquo;s multiple roles. We evaluated the performance of our framework using three <b>benchmark</b> datasets, and the results show that our framework outperforms state-of-the-art systems on the LC-QuAD and YAGO-QA <b>benchmarks,</b> yielding F1 scores of 11.8% and 20.7%, respectively.</p></p class="citation"></blockquote><h3 id=3887--53299-divide-or-conquer-which-part-should-you-distill-your-llm-zhuofeng-wu-et-al-2024>(38/87 | 53/299) Divide-or-Conquer? Which Part Should You Distill Your LLM? (Zhuofeng Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuofeng Wu, He Bai, Aonan Zhang, Jiatao Gu, VG Vinod Vydiswaran, Navdeep Jaitly, Yizhe Zhang. (2024)<br><strong>Divide-or-Conquer? Which Part Should You Distill Your LLM?</strong><br><button class=copy-to-clipboard title="Divide-or-Conquer? Which Part Should You Distill Your LLM?" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15000v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15000v1.pdf filename=2402.15000v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent methods have demonstrated that <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can solve <b>reasoning</b> tasks better when they are encouraged to solve subtasks of the main task first. In this paper we devise a similar strategy that breaks down <b>reasoning</b> tasks into a problem decomposition phase and a problem solving phase and show that the strategy is able to outperform a single stage solution. Further, we hypothesize that the decomposition should be easier to <b>distill</b> into a smaller model compared to the problem solving because the latter requires <b>large</b> <b>amounts</b> <b>of</b> domain knowledge while the former only requires learning general problem solving strategies. We propose methods to <b>distill</b> these two capabilities and evaluate their impact on <b>reasoning</b> outcomes and inference cost. We find that we can <b>distill</b> the problem decomposition phase and at the same time achieve good generalization across tasks, datasets, and models. However, it is harder to <b>distill</b> the problem solving capability without losing performance and the resulting <b>distilled</b> model struggles with generalization. These results indicate that by using smaller, <b>distilled</b> problem decomposition models in combination with problem solving <b>LLMs</b> we can achieve <b>reasoning</b> with cost-efficient inference and local adaptation.</p></p class="citation"></blockquote><h3 id=3987--54299-fine-tuning-enhances-existing-mechanisms-a-case-study-on-entity-tracking-nikhil-prakash-et-al-2024>(39/87 | 54/299) Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking (Nikhil Prakash et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, David Bau. (2024)<br><strong>Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking</strong><br><button class=copy-to-clipboard title="Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Code Generation, Instruction Following<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14811v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14811v1.pdf filename=2402.14811v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> on generalized tasks such as <b>instruction</b> <b>following,</b> <b>code</b> <b>generation,</b> and mathematics has been shown to enhance language models&rsquo; performance on a range of tasks. Nevertheless, explanations of how such <b>fine-tuning</b> influences the internal computations in these models remain elusive. We study how <b>fine-tuning</b> affects the internal mechanisms implemented in language models. As a case study, we explore the property of entity tracking, a crucial facet of language comprehension, where models <b>fine-tuned</b> on mathematics have substantial performance gains. We identify the mechanism that enables entity tracking and show that (i) in both the original model and its <b>fine-tuned</b> versions primarily the same circuit implements entity tracking. In fact, the entity tracking circuit of the original model on the <b>fine-tuned</b> versions performs better than the full original model. (ii) The circuits of all the models implement roughly the same functionality: Entity tracking is performed by tracking the position of the correct entity in both the original model and its <b>fine-tuned</b> versions. (iii) Performance boost in the <b>fine-tuned</b> models is primarily attributed to its improved ability to handle the augmented positional information. To uncover these findings, we employ: Patch Patching, DCM, which automatically detects model components responsible for specific semantics, and CMAP, a new approach for patching activations across models to reveal improved mechanisms. Our findings suggest that <b>fine-tuning</b> enhances, rather than fundamentally alters, the mechanistic operation of the model.</p></p class="citation"></blockquote><h3 id=4087--55299-identifying-multiple-personalities-in-large-language-models-with-external-evaluation-xiaoyang-song-et-al-2024>(40/87 | 55/299) Identifying Multiple Personalities in Large Language Models with External Evaluation (Xiaoyang Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur. (2024)<br><strong>Identifying Multiple Personalities in Large Language Models with External Evaluation</strong><br><button class=copy-to-clipboard title="Identifying Multiple Personalities in Large Language Models with External Evaluation" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14805v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14805v1.pdf filename=2402.14805v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are integrated with human daily applications rapidly, many societal and ethical concerns are raised regarding the behavior of <b>LLMs.</b> One of the ways to comprehend <b>LLMs&rsquo;</b> behavior is to analyze their personalities. Many recent studies quantify <b>LLMs&rsquo;</b> personalities using self-assessment tests that are created for humans. Yet many critiques question the applicability and reliability of these self-assessment tests when applied to <b>LLMs.</b> In this paper, we investigate <b>LLM</b> personalities using an alternate personality measurement method, which we refer to as the external evaluation method, where instead of <b>prompting</b> <b>LLMs</b> with multiple-choice questions in the Likert scale, we evaluate <b>LLMs&rsquo;</b> personalities by analyzing their responses toward open-ended situational questions using an external machine learning model. We first <b>fine-tuned</b> a Llama2-7B model as the MBTI personality predictor that outperforms the state-of-the-art models as the tool to analyze <b>LLMs&rsquo;</b> responses. Then, we <b>prompt</b> the <b>LLMs</b> with situational questions and ask them to generate Twitter posts and comments, respectively, in order to assess their personalities when playing two different roles. Using the external personality evaluation method, we identify that the obtained personality types for <b>LLMs</b> are significantly different when generating posts versus comments, whereas humans show a consistent personality profile in these two different situations. This shows that <b>LLMs</b> can exhibit different personalities based on different scenarios, thus highlighting a fundamental difference between personality in <b>LLMs</b> and humans. With our work, we call for a re-evaluation of personality definition and measurement in <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=4187--56299-compass-computational-mapping-of-patient-therapist-alliance-strategies-with-language-modeling-baihan-lin-et-al-2024>(41/87 | 56/299) COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling (Baihan Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baihan Lin, Djallel Bouneffouf, Yulia Landa, Rachel Jespersen, Cheryl Corcoran, Guillermo Cecchi. (2024)<br><strong>COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling</strong><br><button class=copy-to-clipboard title="COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs-LG, cs.CL, q-bio-NC<br>Keyword Score: 40<br>Keywords: Topic Model, Large Language Model, Prompt, Topic Modeling<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14701v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14701v1.pdf filename=2402.14701v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The therapeutic working alliance is a critical factor in predicting the success of psychotherapy treatment. Traditionally, working alliance assessment relies on questionnaires completed by both therapists and patients. In this paper, we present COMPASS, a novel framework to directly infer the therapeutic working alliance from the natural language used in psychotherapy sessions. Our approach utilizes advanced <b>large</b> <b>language</b> <b>models</b> to analyze transcripts of psychotherapy sessions and compare them with distributed representations of statements in the working alliance inventory. Analyzing a dataset of over 950 sessions covering diverse psychiatric conditions, we demonstrate the effectiveness of our method in microscopically mapping patient-therapist alignment trajectories and providing interpretability for clinical psychiatry and in identifying emerging patterns related to the condition being treated. By employing various neural <b>topic</b> <b>modeling</b> techniques in combination with generative language <b>prompting,</b> we analyze the <b>topical</b> <b>characteristics</b> of different psychiatric conditions and incorporate temporal modeling to capture the evolution of <b>topics</b> <b>at</b> a turn-level resolution. This combined framework enhances the understanding of therapeutic interactions, enabling timely feedback for therapists regarding conversation quality and providing interpretable insights to improve the effectiveness of psychotherapy.</p></p class="citation"></blockquote><h3 id=4287--57299-ufo-a-unified-and-flexible-framework-for-evaluating-factuality-of-large-language-models-zhaoheng-huang-et-al-2024>(42/87 | 57/299) UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models (Zhaoheng Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoheng Huang, Zhicheng Dou, Yutao Zhu, Ji-rong Wen. (2024)<br><strong>UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models</strong><br><button class=copy-to-clipboard title="UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Question Answering, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14690v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14690v1.pdf filename=2402.14690v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> may generate <b>text</b> <b>that</b> lacks consistency with human knowledge, leading to factual inaccuracies or \textit{hallucination}. Existing research for evaluating the factuality of <b>LLMs</b> involves extracting fact claims using an <b>LLM</b> and verifying them against a predefined fact source. However, these evaluation metrics are task-specific, and not scalable, and the substitutability of fact sources in different tasks is under-explored. To address these challenges, we categorize four available fact sources: human-written evidence, reference documents, search engine results, and <b>LLM</b> knowledge, along with five <b>text</b> <b>generation</b> tasks containing six representative datasets. Then, we propose \texttt{UFO}, an <b>LLM-based</b> unified and flexible evaluation framework to verify facts against plug-and-play fact sources. We implement five evaluation scenarios based on this framework. Experimental results show that for most <b>QA</b> tasks, human-written evidence and reference documents are crucial, and they can substitute for each other in retrieval-augmented <b>QA</b> tasks. In news fact generation tasks, search engine results and <b>LLM</b> knowledge are essential. Our dataset and code are available at \url{https://github.com/WaldenRUC/UFO}.</p></p class="citation"></blockquote><h3 id=4387--58299-middleware-for-llms-tools-are-instrumental-for-language-agents-in-complex-environments-yu-gu-et-al-2024>(43/87 | 58/299) Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments (Yu Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Gu, Yiheng Shu, Hao Yu, Xiao Liu, Yuxiao Dong, Jie Tang, Jayanth Srinivasa, Hugo Latapie, Yu Su. (2024)<br><strong>Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments</strong><br><button class=copy-to-clipboard title="Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14672v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14672v1.pdf filename=2402.14672v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The applications of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have expanded well beyond the confines of text processing, signaling a new era where <b>LLMs</b> are envisioned as generalist language agents capable of operating within complex real-world environments. These environments are often highly expansive, making it impossible for the <b>LLM</b> to process them within its short-term memory. Motivated by recent research on extending the capabilities of <b>LLMs</b> with tools, this paper investigates the intriguing potential of tools to augment <b>LLMs</b> in handling such complexity. To this end, we design customized tools to aid in the proactive exploration within these massive environments. Such tools can serve as a middleware layer shielding the <b>LLM</b> from environmental complexity. In two representative complex environments &ndash; knowledge bases (KBs) and databases &ndash; we demonstrate the significant potential of augmenting language agents with tools in complex environments. Notably, equipped with these tools, <b>GPT-4</b> achieves 2.8X the performance of the best baseline in tasks requiring access to database content and 2.2X in KB tasks. Our findings illuminate the path for advancing language agents in complex real-world applications.</p></p class="citation"></blockquote><h3 id=4487--59299-llms-with-industrial-lens-deciphering-the-challenges-and-prospects----a-survey-ashok-urlana-et-al-2024>(44/87 | 59/299) LLMs with Industrial Lens: Deciphering the Challenges and Prospects &ndash; A Survey (Ashok Urlana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashok Urlana, Charaka Vinayak Kumar, Ajeet Kumar Singh, Bala Mallikarjunarao Garlapati, Srinivasa Rao Chalamala, Rahul Mishra. (2024)<br><strong>LLMs with Industrial Lens: Deciphering the Challenges and Prospects &ndash; A Survey</strong><br><button class=copy-to-clipboard title="LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A Survey" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Recommendation, Sentiment Analysis, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14558v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14558v1.pdf filename=2402.14558v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have become the secret ingredient driving numerous industrial applications, showcasing their remarkable versatility across a diverse spectrum of tasks. From natural language processing and <b>sentiment</b> <b>analysis</b> to content generation and personalized <b>recommendations,</b> their unparalleled adaptability has facilitated widespread adoption across industries. This transformative shift driven by <b>LLMs</b> underscores the need to explore the underlying associated challenges and avenues for enhancement in their utilization. In this paper, our objective is to unravel and evaluate the obstacles and opportunities inherent in leveraging <b>LLMs</b> within an industrial context. To this end, we conduct a survey involving a group of industry practitioners, develop four research questions derived from the insights gathered, and examine 68 industry papers to address these questions and derive meaningful conclusions.</p></p class="citation"></blockquote><h3 id=4587--60299-does-the-generator-mind-its-contexts-an-analysis-of-generative-model-faithfulness-under-context-transfer-xinshuo-hu-et-al-2024>(45/87 | 60/299) Does the Generator Mind its Contexts? An Analysis of Generative Model Faithfulness under Context Transfer (Xinshuo Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinshuo Hu, Baotian Hu, Dongfang Li, Xiaoguang Li, Lifeng Shang. (2024)<br><strong>Does the Generator Mind its Contexts? An Analysis of Generative Model Faithfulness under Context Transfer</strong><br><button class=copy-to-clipboard title="Does the Generator Mind its Contexts? An Analysis of Generative Model Faithfulness under Context Transfer" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Neural Machine Translation, Question Answering, Stemming, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14488v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14488v1.pdf filename=2402.14488v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The present study introduces the knowledge-augmented generator, which is specifically designed to produce information that remains grounded in contextual knowledge, regardless of alterations in the context. Previous research has predominantly focused on examining hallucinations <b>stemming</b> from static input, such as in the domains of <b>summarization</b> or <b>machine</b> <b>translation.</b> However, our investigation delves into the faithfulness of generative <b>question</b> <b>answering</b> in the presence of dynamic knowledge. Our objective is to explore the existence of hallucinations arising from parametric memory when contextual knowledge undergoes changes, while also analyzing the underlying causes for their occurrence. In order to efficiently address this issue, we propose a straightforward yet effective measure for detecting such hallucinations. Intriguingly, our investigation uncovers that all models exhibit a tendency to generate previous answers as hallucinations. To gain deeper insights into the underlying causes of this phenomenon, we conduct a series of experiments that verify the critical role played by context in hallucination, both during training and testing, from various perspectives.</p></p class="citation"></blockquote><h3 id=4687--61299-do-llms-implicitly-determine-the-suitable-text-difficulty-for-users-seiji-gobara-et-al-2024>(46/87 | 61/299) Do LLMs Implicitly Determine the Suitable Text Difficulty for Users? (Seiji Gobara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seiji Gobara, Hidetaka Kamigaito, Taro Watanabe. (2024)<br><strong>Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?</strong><br><button class=copy-to-clipboard title="Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Question Answering, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14453v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14453v1.pdf filename=2402.14453v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Education that suits the individual learning level is necessary to improve students&rsquo; understanding. The first step in achieving this purpose by using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> is to adjust the textual difficulty of the response to students. This work analyzes how <b>LLMs</b> can implicitly adjust text difficulty between user input and its generated text. To conduct the experiments, we created a new dataset from Stack-Overflow to explore the performance of <b>question-answering-based</b> <b>conversation.</b> Experimental results on the Stack-Overflow dataset and the TSCC dataset, including multi-turn conversation show that <b>LLMs</b> can implicitly handle text difficulty between user input and its generated response. We also observed that some <b>LLMs</b> can surpass humans in handling text difficulty and the importance of instruction-tuning.</p></p class="citation"></blockquote><h3 id=4787--62299-kocosa-korean-context-aware-sarcasm-detection-dataset-yumin-kim-et-al-2024>(47/87 | 62/299) KoCoSa: Korean Context-aware Sarcasm Detection Dataset (Yumin Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yumin Kim, Heejae Suh, Mingi Kim, Dongyeon Won, Hwanhee Lee. (2024)<br><strong>KoCoSa: Korean Context-aware Sarcasm Detection Dataset</strong><br><button class=copy-to-clipboard title="KoCoSa: Korean Context-aware Sarcasm Detection Dataset" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: GPT, GPT-3, GPT-3.5, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14428v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14428v1.pdf filename=2402.14428v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sarcasm is a way of verbal irony where someone says the opposite of what they mean, often to ridicule a person, situation, or idea. It is often difficult to detect sarcasm in the dialogue since detecting sarcasm should reflect the context (i.e., dialogue history). In this paper, we introduce a new dataset for the Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware Sarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and the labels for this task on the last response. To build the dataset, we propose an efficient sarcasm detection dataset generation pipeline: 1) generating new sarcastic dialogues from source dialogues with <b>large</b> <b>language</b> <b>models,</b> 2) automatic and manual filtering of abnormal and toxic dialogues, and 3) human annotation for the sarcasm detection task. We also provide a simple but effective baseline for the Korean sarcasm detection task trained on our dataset. Experimental results on the dataset show that our baseline system outperforms strong baselines like <b>large</b> <b>language</b> <b>models,</b> such as <b>GPT-3.5,</b> in the Korean sarcasm detection task. We show that the sarcasm detection task relies deeply on the existence of sufficient context. We will release the dataset at <a href=https://anonymous.4open.science/r/KoCoSa-2372>https://anonymous.4open.science/r/KoCoSa-2372</a>.</p></p class="citation"></blockquote><h3 id=4887--63299-qsnail-a-questionnaire-dataset-for-sequential-question-generation-yan-lei-et-al-2024>(48/87 | 63/299) Qsnail: A Questionnaire Dataset for Sequential Question Generation (Yan Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Lei, Liang Pang, Yuanzhuo Wang, Huawei Shen, Xueqi Cheng. (2024)<br><strong>Qsnail: A Questionnaire Dataset for Sequential Question Generation</strong><br><button class=copy-to-clipboard title="Qsnail: A Questionnaire Dataset for Sequential Question Generation" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Chain-of-thought Prompt, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14272v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14272v1.pdf filename=2402.14272v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The questionnaire is a professional research methodology used for both qualitative and quantitative analysis of human opinions, preferences, attitudes, and behaviors. However, designing and evaluating questionnaires demands significant effort due to their intricate and complex structure. Questionnaires entail a series of questions that must conform to intricate constraints involving the questions, options, and overall structure. Specifically, the questions should be relevant and specific to the given research topic and intent. The options should be tailored to the questions, ensuring they are mutually exclusive, completed, and ordered sensibly. Moreover, the sequence of questions should follow a logical order, grouping similar topics together. As a result, automatically generating questionnaires presents a significant challenge and this area has received limited attention primarily due to the scarcity of high-quality datasets. To address these issues, we present Qsnail, the first dataset specifically constructed for the questionnaire generation task, which comprises 13,168 human-written questionnaires gathered from online platforms. We further conduct experiments on Qsnail, and the results reveal that retrieval models and traditional generative models do not fully align with the given research topic and intents. <b>Large</b> <b>language</b> <b>models,</b> while more closely related to the research topic and intents, exhibit significant limitations in terms of diversity and specificity. Despite enhancements through the <b>chain-of-thought</b> <b>prompt</b> and <b>finetuning,</b> questionnaires generated by language models still fall short of human-written questionnaires. Therefore, questionnaire generation is challenging and needs to be further explored. The dataset is available at: <a href=https://github.com/LeiyanGithub/qsnail>https://github.com/LeiyanGithub/qsnail</a>.</p></p class="citation"></blockquote><h3 id=4987--64299-content-conditional-debiasing-for-fair-text-embedding-wenlong-deng-et-al-2024>(49/87 | 64/299) Content Conditional Debiasing for Fair Text Embedding (Wenlong Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenlong Deng, Blair Chen, Xiaoxiao Li, Christos Thrampoulidis. (2024)<br><strong>Content Conditional Debiasing for Fair Text Embedding</strong><br><button class=copy-to-clipboard title="Content Conditional Debiasing for Fair Text Embedding" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fairness, Large Language Model, Large Language Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14208v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14208v2.pdf filename=2402.14208v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP). Yet, only a few studies focus on fair <b>text</b> <b>embeddings,</b> which are crucial yet challenging for real-world applications. In this paper, we propose a novel method for learning fair <b>text</b> <b>embeddings.</b> We achieve <b>fairness</b> while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and <b>text</b> <b>embeddings</b> conditioned on the content. Specifically, we enforce that embeddings of <b>texts</b> <b>with</b> different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral <b>text.</b> <b>Furthermore,</b> we address the issue of lacking proper training data by using <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to augment <b>texts</b> <b>into</b> different sensitive groups. Our extensive evaluations demonstrate that our approach effectively improves <b>fairness</b> while preserving the utility of embeddings, representing a pioneering effort in achieving conditional independence for fair <b>text</b> <b>embeddings.</b></p></p class="citation"></blockquote><h3 id=5087--65299-criticbench-benchmarking-llms-for-critique-correct-reasoning-zicheng-lin-et-al-2024>(50/87 | 65/299) CriticBench: Benchmarking LLMs for Critique-Correct Reasoning (Zicheng Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu Yang. (2024)<br><strong>CriticBench: Benchmarking LLMs for Critique-Correct Reasoning</strong><br><button class=copy-to-clipboard title="CriticBench: Benchmarking LLMs for Critique-Correct Reasoning" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 36<br>Keywords: Benchmarking, Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14809v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14809v1.pdf filename=2402.14809v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to critique and refine their <b>reasoning</b> is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive <b>benchmark</b> designed to assess <b>LLMs&rsquo;</b> abilities to critique and rectify their <b>reasoning</b> across a variety of tasks. CriticBench encompasses five <b>reasoning</b> domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three <b>LLM</b> families. Utilizing CriticBench, we evaluate and dissect the performance of 17 <b>LLMs</b> in generation, critique, and correction <b>reasoning,</b> i.e., GQC <b>reasoning.</b> Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsistencies that decrease as model size increases; and (4) an intriguing inter-model critiquing dynamic, where stronger models are better at critiquing weaker ones, while weaker models can surprisingly surpass stronger ones in their self-critique. We hope these insights into the nuanced critique-correct <b>reasoning</b> of <b>LLMs</b> will foster further research in <b>LLM</b> critique and self-improvement.</p></p class="citation"></blockquote><h3 id=5187--66299-2d-matryoshka-sentence-embeddings-xianming-li-et-al-2024>(51/87 | 66/299) 2D Matryoshka Sentence Embeddings (Xianming Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xianming Li, Zongxi Li, Jing Li, Haoran Xie, Qing Li. (2024)<br><strong>2D Matryoshka Sentence Embeddings</strong><br><button class=copy-to-clipboard title="2D Matryoshka Sentence Embeddings" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 35<br>Keywords: Representation Learning, Transformer, Sentence Embedding, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14776v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14776v1.pdf filename=2402.14776v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Common approaches rely on fixed-length embedding vectors from language models as <b>sentence</b> <b>embeddings</b> for downstream tasks such as semantic textual similarity (STS). Such methods are limited in their flexibility due to unknown computational constraints and budgets across various applications. Matryoshka <b>Representation</b> <b>Learning</b> (MRL) (Kusupati et al., 2022) encodes information at finer granularities, i.e., with lower embedding dimensions, to adaptively accommodate ad hoc tasks. Similar accuracy can be achieved with a smaller embedding size, leading to speedups in downstream tasks. Despite its improved efficiency, MRL still requires traversing all <b>Transformer</b> layers before obtaining the embedding, which remains the dominant factor in time and memory consumption. This <b>prompts</b> consideration of whether the fixed number of <b>Transformer</b> layers affects <b>representation</b> <b>quality</b> and whether using intermediate layers for <b>sentence</b> <b>representation</b> <b>is</b> feasible. In this paper, we introduce a novel <b>sentence</b> <b>embedding</b> model called Two-dimensional Matryoshka <b>Sentence</b> <b>Embedding</b> (2DMSE). It supports elastic settings for both embedding sizes and <b>Transformer</b> layers, offering greater flexibility and efficiency than MRL. We conduct extensive experiments on STS tasks and downstream applications. The experimental results demonstrate the effectiveness of our proposed model in dynamically supporting different embedding sizes and <b>Transformer</b> layers, allowing it to be highly adaptable to various scenarios.</p></p class="citation"></blockquote><h3 id=5287--67299-re-examine-distantly-supervised-ner-a-new-benchmark-and-a-simple-approach-yuepei-li-et-al-2024>(52/87 | 67/299) Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach (Yuepei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuepei Li, Kang Zhou, Qiao Qiao, Qing Wang, Qi Li. (2024)<br><strong>Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach</strong><br><button class=copy-to-clipboard title="Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Supervised Learning, Named Entity Recognition, Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14948v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14948v2.pdf filename=2402.14948v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper delves into <b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER)</b> under the framework of Distant Supervision (DS-NER), where the main challenge lies in the compromised quality of labels due to inherent errors such as false positives, false negatives, and positive type errors. We critically assess the efficacy of current DS-NER methodologies using a real-world <b>benchmark</b> dataset <b>named</b> <b>QTL,</b> <b>revealing</b> that their performance often does not meet expectations. To tackle the prevalent issue of label noise, we introduce a simple yet effective approach, Curriculum-based Positive-Unlabeled Learning CuPUL, which strategically starts on &ldquo;easy&rdquo; and cleaner samples during the training process to enhance model resilience to noisy samples. Our empirical results highlight the capability of CuPUL to significantly reduce the impact of noisy labels and outperform existing methods. QTL dataset and our code is available on GitHub.</p></p class="citation"></blockquote><h3 id=5387--68299-instructir-a-benchmark-for-instruction-following-of-information-retrieval-models-hanseok-oh-et-al-2024>(53/87 | 68/299) INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval Models (Hanseok Oh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanseok Oh, Hyunji Lee, Seonghyeon Ye, Haebin Shin, Hansol Jang, Changwook Jun, Minjoon Seo. (2024)<br><strong>INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval Models</strong><br><button class=copy-to-clipboard title="INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval Models" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Information Retrieval, Instruction Following<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14334v1.pdf filename=2402.14334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the critical need to align search targets with users&rsquo; intention, retrievers often only prioritize query <b>information</b> <b>without</b> delving into the users&rsquo; intended search context. Enhancing the capability of retrievers to understand intentions and preferences of users, akin to language model <b>instructions,</b> <b>has</b> the potential to yield more aligned search targets. Prior studies restrict the application of <b>instructions</b> <b>in</b> <b>information</b> <b>retrieval</b> to a task description format, neglecting the broader context of diverse and evolving search scenarios. Furthermore, the prevailing <b>benchmarks</b> utilized for evaluation lack explicit tailoring to assess <b>instruction-following</b> <b>ability,</b> thereby hindering progress in this field. In response to these limitations, we propose a novel <b>benchmark,INSTRUCTIR,</b> specifically designed to evaluate <b>instruction-following</b> <b>ability</b> in <b>information</b> <b>retrieval</b> tasks. Our approach focuses on user-aligned <b>instructions</b> <b>tailored</b> to each query instance, reflecting the diverse characteristics inherent in real-world search scenarios. Through experimental analysis, we observe that retrievers <b>fine-tuned</b> to follow task-style <b>instructions,</b> <b>such</b> as INSTRUCTOR, can underperform compared to their non-instruction-tuned counterparts. This underscores potential overfitting issues inherent in constructing retrievers trained on existing <b>instruction-aware</b> <b>retrieval</b> datasets.</p></p class="citation"></blockquote><h3 id=5487--69299-genception-evaluate-multimodal-llms-with-unlabeled-unimodal-data-lele-cao-et-al-2024>(54/87 | 69/299) GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data (Lele Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lele Cao, Valentin Buchner, Zineb Senane, Fangkai Yang. (2024)<br><strong>GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data</strong><br><button class=copy-to-clipboard title="GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-7; I-4, cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 32<br>Keywords: Benchmarking, Benchmarking, Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14973v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14973v1.pdf filename=2402.14973v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) are commonly evaluated using costly annotated <b>multimodal</b> <b>benchmarks.</b> However, these <b>benchmarks</b> often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models&rsquo; inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption&rsquo;s efficacy, showing strong correlations with popular MLLM <b>benchmarking</b> results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.</p></p class="citation"></blockquote><h3 id=5587--70299-commvqa-situating-visual-question-answering-in-communicative-contexts-nandita-shankar-naik-et-al-2024>(55/87 | 70/299) CommVQA: Situating Visual Question Answering in Communicative Contexts (Nandita Shankar Naik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nandita Shankar Naik, Christopher Potts, Elisa Kreiss. (2024)<br><strong>CommVQA: Situating Visual Question Answering in Communicative Contexts</strong><br><button class=copy-to-clipboard title="CommVQA: Situating Visual Question Answering in Communicative Contexts" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 30<br>Keywords: Question Answering, Visual Question Answering, Visual Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15002v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15002v1.pdf filename=2402.15002v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current <b>visual</b> <b>question</b> <b>answering</b> <b>(VQA)</b> models tend to be trained and evaluated on image-question pairs in isolation. However, the <b>questions</b> <b>people</b> ask are dependent on their informational needs and prior knowledge about the image content. To evaluate how situating images within naturalistic contexts shapes <b>visual</b> <b>questions,</b> <b>we</b> introduce CommVQA, a <b>VQA</b> dataset consisting of images, image descriptions, real-world communicative scenarios where the image might appear (e.g., a travel website), and follow-up <b>questions</b> <b>and</b> answers conditioned on the scenario. We show that CommVQA poses a challenge for current models. Providing contextual information to <b>VQA</b> models improves performance broadly, highlighting the relevance of situating systems within a communicative scenario.</p></p class="citation"></blockquote><h3 id=5687--71299-multils-a-multi-task-lexical-simplification-framework-kai-north-et-al-2024>(56/87 | 71/299) MultiLS: A Multi-task Lexical Simplification Framework (Kai North et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai North, Tharindu Ranasinghe, Matthew Shardlow, Marcos Zampieri. (2024)<br><strong>MultiLS: A Multi-task Lexical Simplification Framework</strong><br><button class=copy-to-clipboard title="MultiLS: A Multi-task Lexical Simplification Framework" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14972v1.pdf filename=2402.14972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lexical Simplification (LS) automatically replaces difficult to read words for easier alternatives while preserving a sentence&rsquo;s original meaning. LS is a precursor to Text Simplification with the aim of improving text accessibility to various target demographics, including children, second language learners, individuals with reading disabilities or low literacy. Several datasets exist for LS. These LS datasets specialize on one or two sub-tasks within the LS pipeline. However, as of this moment, no single LS dataset has been developed that covers all LS sub-tasks. We present MultiLS, the first LS framework that allows for the creation of a multi-task LS dataset. We also present MultiLS-PT, the first dataset to be created using the MultiLS framework. We demonstrate the potential of MultiLS-PT by carrying out all LS sub-tasks of (1). lexical complexity prediction (LCP), (2). substitute generation, and (3). substitute ranking for Portuguese. Model performances are reported, ranging from <b>transformer-based</b> models to more recent <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b></p></p class="citation"></blockquote><h3 id=5787--72299-mirror-a-multiple-perspective-self-reflection-method-for-knowledge-rich-reasoning-hanqi-yan-et-al-2024>(57/87 | 72/299) Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning (Hanqi Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanqi Yan, Qinglin Zhu, Xinyu Wang, Lin Gui, Yulan He. (2024)<br><strong>Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning</strong><br><button class=copy-to-clipboard title="Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14963v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14963v1.pdf filename=2402.14963v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have the capability to iteratively reflect on their own outputs, recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of <b>LLMs</b> in self-assessment, we also observe that <b>LLMs</b> struggle to revisit their predictions despite receiving explicit negative feedback. Therefore, We propose Mirror, a Multiple-perspective self-reflection method for knowledge-rich <b>reasoning,</b> to avoid getting stuck at a particular reflection iteration. Mirror enables <b>LLMs</b> to reflect from multiple-perspective clues, achieved through a heuristic interaction between a Navigator and a Reasoner. It guides agents toward diverse yet plausibly reliable <b>reasoning</b> trajectory without access to ground truth by encouraging (1) diversity of directions generated by Navigator and (2) agreement among strategically induced perturbations in responses generated by the Reasoner. The experiments on five <b>reasoning</b> datasets demonstrate that Mirror&rsquo;s superiority over several contemporary self-reflection approaches. Additionally, the ablation study studies clearly indicate that our strategies alleviate the aforementioned challenges.</p></p class="citation"></blockquote><h3 id=5887--73299-relayattention-for-efficient-large-language-model-serving-with-long-system-prompts-lei-zhu-et-al-2024>(58/87 | 73/299) RelayAttention for Efficient Large Language Model Serving with Long System Prompts (Lei Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Zhu, Xinjiang Wang, Wayne Zhang, Rynson W. H. Lau. (2024)<br><strong>RelayAttention for Efficient Large Language Model Serving with Long System Prompts</strong><br><button class=copy-to-clipboard title="RelayAttention for Efficient Large Language Model Serving with Long System Prompts" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14808v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14808v1.pdf filename=2402.14808v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Practical <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> services may involve a long system <b>prompt,</b> which specifies the instructions, examples, and knowledge documents of the task and is reused across numerous requests. However, the long system <b>prompt</b> causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length. This paper aims to improve the efficiency of <b>LLM</b> services that involve long system <b>prompts.</b> Our key observation is that handling these system <b>prompts</b> requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (i.e., key-value pairs) of system <b>prompts</b> are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once for a batch of input tokens. RelayAttention is a free lunch: it maintains the generation quality while requiring no model retraining, as it is based on a mathematical reformulation of causal attention.</p></p class="citation"></blockquote><h3 id=5987--74299-not-all-experts-are-equal-efficient-expert-pruning-and-skipping-for-mixture-of-experts-large-language-models-xudong-lu-et-al-2024>(59/87 | 74/299) Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models (Xudong Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, Hongsheng Li. (2024)<br><strong>Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models</strong><br><button class=copy-to-clipboard title="Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Pruning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14800v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14800v1.pdf filename=2402.14800v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A pivotal advancement in the progress of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> is the emergence of the Mixture-of-Experts (MoE) <b>LLMs.</b> Compared to traditional <b>LLMs,</b> MoE <b>LLMs</b> can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight <b>pruning</b> methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE <b>LLMs</b> by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert <b>pruning</b> and skipping of MoE <b>LLMs,</b> tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining satisfactory performance. Data and code will be available at <a href=https://github.com/Lucky-Lance/Expert_Sparsity>https://github.com/Lucky-Lance/Expert_Sparsity</a>.</p></p class="citation"></blockquote><h3 id=6087--75299-dependency-annotation-of-ottoman-turkish-with-multilingual-bert-şaziye-betül-özateş-et-al-2024>(60/87 | 75/299) Dependency Annotation of Ottoman Turkish with Multilingual BERT (Şaziye Betül Özateş et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Şaziye Betül Özateş, Tarık Emre Tıraş, Efe Eren Genç, Esma Fatıma Bilgin Taşdemir. (2024)<br><strong>Dependency Annotation of Ottoman Turkish with Multilingual BERT</strong><br><button class=copy-to-clipboard title="Dependency Annotation of Ottoman Turkish with Multilingual BERT" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, BERT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14743v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14743v1.pdf filename=2402.14743v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces a pretrained <b>large</b> <b>language</b> <b>model-based</b> annotation methodology for the first dependency treebank in Ottoman Turkish. Our experimental results show that, iteratively, i) pseudo-annotating data using a multilingual <b>BERT-based</b> parsing model, ii) manually correcting the pseudo-annotations, and iii) <b>fine-tuning</b> the parsing model with the corrected annotations, we speed up and simplify the challenging dependency annotation process. The resulting treebank, that will be a part of the Universal Dependencies (UD) project, will facilitate automated analysis of Ottoman Turkish documents, unlocking the linguistic richness embedded in this historical heritage.</p></p class="citation"></blockquote><h3 id=6187--76299-chain-of-thought-unfaithfulness-as-disguised-accuracy-oliver-bentham-et-al-2024>(61/87 | 76/299) Chain-of-Thought Unfaithfulness as Disguised Accuracy (Oliver Bentham et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oliver Bentham, Nathan Stringham, Ana Marasović. (2024)<br><strong>Chain-of-Thought Unfaithfulness as Disguised Accuracy</strong><br><button class=copy-to-clipboard title="Chain-of-Thought Unfaithfulness as Disguised Accuracy" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14897v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14897v1.pdf filename=2402.14897v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the extent to which Chain-of-Thought (CoT) generations align with a <b>large</b> <b>language</b> <b>model&rsquo;s</b> <b>(LLM)</b> internal computations is critical for deciding whether to trust an <b>LLM&rsquo;s</b> output. As a proxy for CoT faithfulness, arXiv:2307.13702 propose a metric that measures a model&rsquo;s dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that <b>LLMs</b> exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all <b>LLMs.</b> We replicate their experimental setup with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, we discover that simply changing the order of answer choices in the <b>prompt</b> can reduce the metric by 73 percentage points. The faithfulness metric is also highly correlated ($R^2$ = 0.91) with accuracy, raising doubts about its validity as a construct for evaluating faithfulness.</p></p class="citation"></blockquote><h3 id=6287--77299-efficient-and-effective-vocabulary-expansion-towards-multilingual-large-language-models-seungduk-kim-et-al-2024>(62/87 | 77/299) Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models (Seungduk Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seungduk Kim, Seungtaek Choi, Myeongho Jeong. (2024)<br><strong>Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models</strong><br><button class=copy-to-clipboard title="Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Text Understanding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14714v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14714v1.pdf filename=2402.14714v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This report introduces \texttt{EEVE-Korean-v1.0}, a Korean adaptation of <b>large</b> <b>language</b> <b>models</b> that exhibit remarkable capabilities across English and Korean <b>text</b> <b>understanding.</b> Building on recent highly capable but English-centric <b>LLMs,</b> such as SOLAR-10.7B and Phi-2, where non-English <b>texts</b> <b>are</b> inefficiently processed with English-centric tokenizers, we present an efficient and effective vocabulary expansion (EEVE) method, which encompasses parameter freezing and subword initialization. In contrast to previous efforts that believe new embeddings require trillions of training tokens, we show that our method can significantly boost non-English proficiency within just 2 billion tokens. Surpassing most instruction-tuned <b>LLMs</b> on the Open Ko-LLM Leaderboard, as of January 2024, our model \texttt{EEVE-Korean-10.8B-v1.0} ranks as the leading Korean pre-trained model in the open-source community, according to Hugging Face&rsquo;s leaderboard. We open-source our models on Huggingface to empower the open research community in various languages.</p></p class="citation"></blockquote><h3 id=6387--78299-two-counterexamples-to-textittokenization-and-the-noiseless-channel-marco-cognetta-et-al-2024>(63/87 | 78/299) Two Counterexamples to \textit{Tokenization and the Noiseless Channel} (Marco Cognetta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Cognetta, Vilém Zouhar, Sangwhan Moon, Naoaki Okazaki. (2024)<br><strong>Two Counterexamples to \textit{Tokenization and the Noiseless Channel}</strong><br><button class=copy-to-clipboard title="Two Counterexamples to \textit{Tokenization and the Noiseless Channel}" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Neural Machine Translation, Tokenization, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14614v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14614v1.pdf filename=2402.14614v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In \textit{Tokenization and the Noiseless Channel} \cite{zouhar-etal-2023-tokenization}, R'enyi efficiency is suggested as an intrinsic mechanism for evaluating a tokenizer: for NLP tasks, the tokenizer which leads to the highest R'enyi efficiency of the unigram distribution should be chosen. The R'enyi efficiency is thus treated as a predictor of downstream performance (e.g., predicting <b>BLEU</b> for a <b>machine</b> <b>translation</b> task), without the expensive step of training multiple models with different tokenizers. Although useful, the predictive power of this metric is not perfect, and the authors note there are additional qualities of a good <b>tokenization</b> scheme that R'enyi efficiency alone cannot capture. We describe two variants of BPE <b>tokenization</b> which can arbitrarily increase R'enyi efficiency while decreasing the downstream model performance. These counterexamples expose cases where R'enyi efficiency fails as an intrinsic <b>tokenization</b> metric and thus give insight for building more accurate predictors.</p></p class="citation"></blockquote><h3 id=6487--79299-should-we-respect-llms-a-cross-lingual-study-on-the-influence-of-prompt-politeness-on-llm-performance-ziqi-yin-et-al-2024>(64/87 | 79/299) Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance (Ziqi Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziqi Yin, Hao Wang, Kaito Horio, Daisuke Kawahara, Satoshi Sekine. (2024)<br><strong>Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance</strong><br><button class=copy-to-clipboard title="Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14531v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14531v1.pdf filename=2402.14531v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the impact of politeness levels in <b>prompts</b> on the performance of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Polite language in human communications often garners more compliance and effectiveness, while rudeness can cause aversion, impacting response quality. We consider that <b>LLMs</b> mirror human communication traits, suggesting they align with human cultural norms. We assess the impact of politeness in <b>prompts</b> on <b>LLMs</b> across English, Chinese, and Japanese tasks. We observed that impolite <b>prompts</b> often result in poor performance, but overly polite language does not guarantee better outcomes. The best politeness level is different according to the language. This phenomenon suggests that <b>LLMs</b> not only reflect human behavior but are also influenced by language, particularly in different cultural contexts. Our findings highlight the need to factor in politeness for cross-cultural natural language processing and <b>LLM</b> usage.</p></p class="citation"></blockquote><h3 id=6587--80299-understanding-and-patching-compositional-reasoning-in-llms-zhaoyi-li-et-al-2024>(65/87 | 80/299) Understanding and Patching Compositional Reasoning in LLMs (Zhaoyi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoyi Li, Gangwei Jiang, Hong Xie, Linqi Song, Defu Lian, Ying Wei. (2024)<br><strong>Understanding and Patching Compositional Reasoning in LLMs</strong><br><button class=copy-to-clipboard title="Understanding and Patching Compositional Reasoning in LLMs" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14328v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14328v1.pdf filename=2402.14328v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>LLMs</b> have marked a revolutonary shift, yet they falter when faced with compositional <b>reasoning</b> tasks. Our research embarks on a quest to uncover the root causes of compositional <b>reasoning</b> failures of <b>LLMs,</b> uncovering that most of them stem from the improperly generated or leveraged implicit <b>reasoning</b> results. Inspired by our empirical findings, we resort to Logit Lens and an intervention experiment to dissect the inner hidden states of <b>LLMs.</b> This deep dive reveals that implicit <b>reasoning</b> results indeed surface within middle layers and play a causative role in shaping the final explicit <b>reasoning</b> results. Our exploration further locates multi-head <b>self-attention</b> (MHSA) modules within these layers, which emerge as the linchpins in accurate generation and leveraing of implicit <b>reasoning</b> results. Grounded on the above findings, we develop CREME, a lightweight method to patch errors in compositional <b>reasoning</b> via editing the located MHSA modules. Our empirical evidence stands testament to CREME&rsquo;s effectiveness, paving the way for autonomously and continuously enhancing compositional <b>reasoning</b> capabilities in language models.</p></p class="citation"></blockquote><h3 id=6687--81299-cev-lm-controlled-edit-vector-language-model-for-shaping-natural-language-generations-samraj-moorjani-et-al-2024>(66/87 | 81/299) CEV-LM: Controlled Edit Vector Language Model for Shaping Natural Language Generations (Samraj Moorjani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samraj Moorjani, Adit Krishnan, Hari Sundaram. (2024)<br><strong>CEV-LM: Controlled Edit Vector Language Model for Shaping Natural Language Generations</strong><br><button class=copy-to-clipboard title="CEV-LM: Controlled Edit Vector Language Model for Shaping Natural Language Generations" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Language Generation, Natural Language Generation, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14290v1.pdf filename=2402.14290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As large-scale <b>language</b> <b>models</b> become the standard for <b>text</b> <b>generation,</b> there is a greater need to tailor the generations to be more or less concise, targeted, and informative, depending on the audience/application. Existing control approaches primarily adjust the semantic (e.g., emotion, topics), structural (e.g., syntax tree, parts-of-speech), and lexical (e.g., keyword/phrase inclusion) properties of <b>text,</b> <b>but</b> are insufficient to accomplish complex objectives such as pacing which control the complexity and readability of the <b>text.</b> <b>In</b> this paper, we introduce CEV-LM - a lightweight, semi-autoregressive <b>language</b> <b>model</b> that utilizes constrained edit vectors to control three complementary metrics (speed, volume, and circuitousness) that quantify the shape of <b>text</b> <b>(e.g.,</b> pacing of content). We study an extensive set of state-of-the-art CTG models and find that CEV-LM provides significantly more targeted and precise control of these three metrics while preserving semantic content, using less training data, and containing fewer parameters.</p></p class="citation"></blockquote><h3 id=6787--82299-can-language-models-act-as-knowledge-bases-at-scale-qiyuan-he-et-al-2024>(67/87 | 82/299) Can Language Models Act as Knowledge Bases at Scale? (Qiyuan He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiyuan He, Yizhong Wang, Wenya Wang. (2024)<br><strong>Can Language Models Act as Knowledge Bases at Scale?</strong><br><button class=copy-to-clipboard title="Can Language Models Act as Knowledge Bases at Scale?" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14273v1.pdf filename=2402.14273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated remarkable proficiency in understanding and generating responses to complex queries through <b>large-scale</b> <b>pre-training.</b> <b>However,</b> the efficacy of these models in memorizing and <b>reasoning</b> among <b>large-scale</b> <b>structured</b> <b>knowledge,</b> especially world knowledge that explicitly covers abundant factual information remains questionable. Addressing this gap, our research investigates whether <b>LLMs</b> can effectively store, recall, and reason with knowledge on a <b>large</b> <b>scale</b> <b>comparable</b> to latest knowledge bases (KBs) such as Wikidata. Specifically, we focus on three crucial aspects to study the viability: (1) the efficiency of <b>LLMs</b> with different sizes in memorizing the exact knowledge in the <b>large-scale</b> <b>KB;</b> <b>(2)</b> the flexibility of recalling the memorized knowledge in response to natural language queries; (3) the capability to infer new knowledge through <b>reasoning.</b> Our findings indicate that while <b>LLMs</b> hold promise as <b>large-scale</b> <b>KBs</b> <b>capable</b> of retrieving and responding with flexibility, enhancements in their <b>reasoning</b> capabilities are necessary to fully realize their potential.</p></p class="citation"></blockquote><h3 id=6887--83299-multi-modal-stance-detection-new-datasets-and-model-bin-liang-et-al-2024>(68/87 | 83/299) Multi-modal Stance Detection: New Datasets and Model (Bin Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bin Liang, Ang Li, Jingqian Zhao, Lin Gui, Min Yang, Yue Yu, Kam-Fai Wong, Ruifeng Xu. (2024)<br><strong>Multi-modal Stance Detection: New Datasets and Model</strong><br><button class=copy-to-clipboard title="Multi-modal Stance Detection: New Datasets and Model" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 26<br>Keywords: Benchmarking, Multi-modal, Stance Detection, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14298v1.pdf filename=2402.14298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Stance</b> <b>detection</b> is a challenging task that aims to identify public opinion from social media platforms with respect to specific targets. Previous work on <b>stance</b> <b>detection</b> largely focused on pure texts. In this paper, we study <b>multi-modal</b> <b>stance</b> <b>detection</b> for tweets consisting of texts and images, which are prevalent in today&rsquo;s fast-growing social media platforms where people often post <b>multi-modal</b> messages. To this end, we create five new <b>multi-modal</b> <b>stance</b> <b>detection</b> datasets of different domains based on Twitter, in which each example consists of a text and an image. In addition, we propose a simple yet effective Targeted <b>Multi-modal</b> <b>Prompt</b> Tuning framework (TMPT), where target information is leveraged to learn <b>multi-modal</b> <b>stance</b> <b>features</b> from textual and visual modalities. Experimental results on our three <b>benchmark</b> datasets show that the proposed TMPT achieves state-of-the-art performance in <b>multi-modal</b> <b>stance</b> <b>detection.</b></p></p class="citation"></blockquote><h3 id=6987--84299-is-cognition-and-action-consistent-or-not-investigating-large-language-models-personality-yiming-ai-et-al-2024>(69/87 | 84/299) Is Cognition and Action Consistent or Not: Investigating Large Language Model&rsquo;s Personality (Yiming Ai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiming Ai, Zhiwei He, Ziyin Zhang, Wenhong Zhu, Hongkun Hao, Kai Yu, Lingjun Chen, Rui Wang. (2024)<br><strong>Is Cognition and Action Consistent or Not: Investigating Large Language Model&rsquo;s Personality</strong><br><button class=copy-to-clipboard title="Is Cognition and Action Consistent or Not: Investigating Large Language Model's Personality" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14679v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14679v1.pdf filename=2402.14679v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we investigate the reliability of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in professing human-like personality traits through responses to personality questionnaires. Our goal is to evaluate the consistency between <b>LLMs&rsquo;</b> professed personality inclinations and their actual &ldquo;behavior&rdquo;, examining the extent to which these models can emulate human-like personality patterns. Through a comprehensive analysis of <b>LLM</b> outputs against established human <b>benchmarks,</b> we seek to understand the cognition-action divergence in <b>LLMs</b> and propose hypotheses for the observed results based on psychological theories and metrics.</p></p class="citation"></blockquote><h3 id=7087--85299-balanced-data-sampling-for-language-model-training-with-clustering-yunfan-shao-et-al-2024>(70/87 | 85/299) Balanced Data Sampling for Language Model Training with Clustering (Yunfan Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunfan Shao, Linyang Li, Zhaoye Fei, Hang Yan, Dahua Lin, Xipeng Qiu. (2024)<br><strong>Balanced Data Sampling for Language Model Training with Clustering</strong><br><button class=copy-to-clipboard title="Balanced Data Sampling for Language Model Training with Clustering" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Clustering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14526v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14526v1.pdf filename=2402.14526v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data plays a fundamental role in the training of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question. Most <b>LLMs</b> are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal. In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training. Specifically, ClusterClip Sampling utilizes data <b>clustering</b> to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results. A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters. Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperforms random sampling and other cluster-based sampling variants under various training datasets and <b>large</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=7187--86299-vygotsky-distance-measure-for-benchmark-task-similarity-maxim-k-surkov-et-al-2024>(71/87 | 86/299) Vygotsky Distance: Measure for Benchmark Task Similarity (Maxim K. Surkov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maxim K. Surkov, Ivan P. Yamshchikov. (2024)<br><strong>Vygotsky Distance: Measure for Benchmark Task Similarity</strong><br><button class=copy-to-clipboard title="Vygotsky Distance: Measure for Benchmark Task Similarity" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T01, 97P80, 97C30, 68Q32, H-1-1; I-2-4; I-2-6; F-2-0, cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, GLUE, SuperGLUE<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14890v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14890v2.pdf filename=2402.14890v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluation plays a significant role in modern natural language processing. Most modern NLP <b>benchmarks</b> consist of arbitrary sets of tasks that neither guarantee any generalization potential for the model once applied outside the test set nor try to minimize the resource consumption needed for model evaluation. This paper presents a theoretical instrument and a practical algorithm to calculate similarity between <b>benchmark</b> tasks, we call this similarity measure &ldquo;Vygotsky distance&rdquo;. The core idea of this similarity measure is that it is based on relative performance of the &ldquo;students&rdquo; on a given task, rather that on the properties of the task itself. If two tasks are close to each other in terms of Vygotsky distance the models tend to have similar relative performance on them. Thus knowing Vygotsky distance between tasks one can significantly reduce the number of evaluation tasks while maintaining a high validation quality. Experiments on various <b>benchmarks,</b> including <b>GLUE,</b> <b>SuperGLUE,</b> CLUE, and RussianSuperGLUE, demonstrate that a vast majority of NLP <b>benchmarks</b> could be at least 40% smaller in terms of the tasks included. Most importantly, Vygotsky distance could also be used for the validation of new tasks thus increasing the generalization potential of the future NLP models.</p></p class="citation"></blockquote><h3 id=7287--87299-cobias-contextual-reliability-in-bias-assessment-priyanshul-govil-et-al-2024>(72/87 | 87/299) COBIAS: Contextual Reliability in Bias Assessment (Priyanshul Govil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Priyanshul Govil, Vamshi Krishna Bonagiri, Manas Gaur, Ponnurangam Kumaraguru, Sanorita Dey. (2024)<br><strong>COBIAS: Contextual Reliability in Bias Assessment</strong><br><button class=copy-to-clipboard title="COBIAS: Contextual Reliability in Bias Assessment" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14889v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14889v1.pdf filename=2402.14889v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are trained on inherently biased data. Previous works on debiasing models rely on <b>benchmark</b> datasets to measure model performance. However, these datasets suffer from several pitfalls due to the extremely subjective understanding of bias, highlighting a critical need for contextual exploration. We propose understanding the context of user inputs with consideration of the diverse situations in which input statements are possible. This approach would allow for frameworks that foster bias awareness rather than guardrails that hurt user engagement. Our contribution is twofold: (i) we create a dataset of 2287 stereotyped statements augmented with points for adding context; (ii) we develop the Context-Oriented Bias Indicator and Assessment Score (COBIAS) to assess statements&rsquo; contextual reliability in measuring bias. Our metric is a significant predictor of the contextual reliability of bias-benchmark datasets ($\chi^2=71.02, p&lt;2.2 \cdot 10^{-16})$. COBIAS can be used to create reliable datasets, resulting in an improvement in bias mitigation works.</p></p class="citation"></blockquote><h3 id=7387--88299-a-usage-centric-take-on-intent-understanding-in-e-commerce-wendi-zhou-et-al-2024>(73/87 | 88/299) A Usage-centric Take on Intent Understanding in E-Commerce (Wendi Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wendi Zhou, Tianyi Li, Pavlos Vougiouklis, Mark Steedman, Jeff Z. Pan. (2024)<br><strong>A Usage-centric Take on Intent Understanding in E-Commerce</strong><br><button class=copy-to-clipboard title="A Usage-centric Take on Intent Understanding in E-Commerce" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 21<br>Keywords: Graph, Benchmarking, Knowledge Graph, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14901v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14901v1.pdf filename=2402.14901v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying and understanding user intents is a pivotal task for E-Commerce. Despite its popularity, intent understanding has not been consistently defined or accurately <b>benchmarked.</b> In this paper, we focus on predicative user intents as &ldquo;how a customer uses a product&rdquo;, and pose intent understanding as a natural language <b>reasoning</b> task, independent of product ontologies. We identify two weaknesses of FolkScope, the SOTA E-Commerce Intent <b>Knowledge</b> <b>Graph,</b> that limit its capacity to reason about user intents and to recommend diverse useful products. Following these observations, we introduce a Product Recovery <b>Benchmark</b> including a novel evaluation framework and an example dataset. We further validate the above FolkScope weaknesses on this <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=7487--89299-ar-spider-text-to-sql-in-arabic-saleh-almohaimeed-et-al-2024>(74/87 | 89/299) Ar-Spider: Text-to-SQL in Arabic (Saleh Almohaimeed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saleh Almohaimeed, Saad Almohaimeed, Mansour Al Ghanim, Liqiang Wang. (2024)<br><strong>Ar-Spider: Text-to-SQL in Arabic</strong><br><button class=copy-to-clipboard title="Ar-Spider: Text-to-SQL in Arabic" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Semantic Parsing, Text2SQL<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15012v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15012v1.pdf filename=2402.15012v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Natural Language Processing (NLP), one of the most important tasks is <b>text-to-SQL</b> <b>semantic</b> <b>parsing,</b> which focuses on enabling users to interact with the database in a more natural manner. In recent years, <b>text-to-SQL</b> has made significant progress, but most were English-centric. In this paper, we introduce Ar-Spider 1, the first Arabic cross-domain <b>text-to-SQL</b> dataset. Due to the unique nature of the language, two major challenges have been encountered, namely schema linguistic and SQL structural challenges. In order to handle these issues and conduct the experiments, we adopt two baseline models LGESQL [4] and S2SQL [12], both of which are tested with two cross-lingual models to alleviate the effects of schema linguistic and SQL structure linking challenges. The baselines demonstrate decent single-language performance on our Arabic <b>text-to-SQL</b> dataset, Ar-Spider, achieving 62.48% for S2SQL and 65.57% for LGESQL, only 8.79% below the highest results achieved by the baselines when trained in English dataset. To achieve better performance on Arabic <b>text-to-SQL,</b> we propose the context similarity relationship (CSR) approach, which results in a significant increase in the overall performance of about 1.52% for S2SQL and 1.06% for LGESQL and closes the gap between Arabic and English languages to 7.73%.</p></p class="citation"></blockquote><h3 id=7587--90299-how-important-is-tokenization-in-french-medical-masked-language-models-yanis-labrak-et-al-2024>(75/87 | 90/299) How Important Is Tokenization in French Medical Masked Language Models? (Yanis Labrak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanis Labrak, Adrien Bazoge, Beatrice Daille, Mickael Rouvier, Richard Dufour. (2024)<br><strong>How Important Is Tokenization in French Medical Masked Language Models?</strong><br><button class=copy-to-clipboard title="How Important Is Tokenization in French Medical Masked Language Models?" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Tokenization, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15010v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15010v1.pdf filename=2402.15010v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Subword <b>tokenization</b> has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of <b>pre-trained</b> <b>language</b> <b>models.</b> This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword <b>tokenization</b> consistently outperforms character and word-level <b>tokenization,</b> the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tokenizers, and the role of morphological information in Indo-European languages remain insufficiently explored. This is particularly pertinent for biomedical terminology, characterized by specific rules governing morpheme combinations. Despite the agglutinative nature of biomedical terminology, existing language models do not explicitly incorporate this knowledge, leading to inconsistent <b>tokenization</b> strategies for common terms. In this paper, we seek to delve into the complexities of subword <b>tokenization</b> in French biomedical domain across a variety of NLP tasks and pinpoint areas where further enhancements can be made. We analyze classical <b>tokenization</b> algorithms, including BPE and SentencePiece, and introduce an original <b>tokenization</b> strategy that integrates morpheme-enriched word segmentation into existing <b>tokenization</b> methods.</p></p class="citation"></blockquote><h3 id=7687--91299-unveiling-linguistic-regions-in-large-language-models-zhihao-zhang-et-al-2024>(76/87 | 91/299) Unveiling Linguistic Regions in Large Language Models (Zhihao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihao Zhang, Jun Zhao, Qi Zhang, Tao Gui, Xuanjing Huang. (2024)<br><strong>Unveiling Linguistic Regions in Large Language Models</strong><br><button class=copy-to-clipboard title="Unveiling Linguistic Regions in Large Language Models" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14700v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14700v1.pdf filename=2402.14700v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving <b>LLMs&rsquo;</b> cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how <b>LLMs</b> achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of <b>LLMs.</b> We discover a core region in <b>LLMs</b> that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependency, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct regions exist for different monolingual families, and disruption to these specific regions substantially reduces the <b>LLMs&rsquo;</b> proficiency in those corresponding languages. Our research also indicates that freezing the core linguistic region during further pre-training can mitigate the issue of catastrophic forgetting (CF), a common occurrence observed during further pre-training of <b>LLMs.</b> Overall, exploring the <b>LLMs&rsquo;</b> functional regions provides insights into the foundation of their intelligence.</p></p class="citation"></blockquote><h3 id=7787--92299-novi-jezički-modeli-za-srpski-jezik-mihailo-škorić-2024>(77/87 | 92/299) Novi jezički modeli za srpski jezik (Mihailo Škorić, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mihailo Škorić. (2024)<br><strong>Novi jezički modeli za srpski jezik</strong><br><button class=copy-to-clipboard title="Novi jezički modeli za srpski jezik" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Transformer, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14379v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14379v2.pdf filename=2402.14379v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper will briefly present the development history of <b>transformer-based</b> language models for the Serbian language. Several new models for <b>text</b> <b>generation</b> and vectorization, trained on the resources of the Society for Language Resources and Technologies, will also be presented. Ten selected vectorization models for Serbian, including two new ones, will be compared on four natural language processing tasks. Paper will analyze which models are the best for each selected task, how does their size and the size of their training sets affect the performance on those tasks, and what is the optimal setting to train the best language models for the Serbian language.</p></p class="citation"></blockquote><h3 id=7887--93299-aura-natural-language-reasoning-for-aleatoric-uncertainty-in-rationales-hazel-kim-2024>(78/87 | 93/299) AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales (Hazel Kim, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hazel Kim. (2024)<br><strong>AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales</strong><br><button class=copy-to-clipboard title="AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Low-Resource, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14337v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14337v1.pdf filename=2402.14337v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rationales behind answers not only explain model decisions but boost language models to reason well on complex <b>reasoning</b> tasks. However, obtaining impeccable rationales is often impossible. Besides, it is non-trivial to estimate the degree to which the rationales are faithful enough to encourage model performance. Thus, such <b>reasoning</b> tasks often compel models to output correct answers under undesirable rationales and are sub-optimal compared to what the models are fully capable of. In this work, we propose how to deal with imperfect rationales causing aleatoric uncertainty. We first define the ambiguous rationales with entropy scores of given rationales, using model prior beliefs as informativeness. We then guide models to select one of two different <b>reasoning</b> models according to the ambiguity of rationales. We empirically argue that our proposed method produces robust performance superiority against the adversarial quality of rationales and <b>low-resource</b> settings.</p></p class="citation"></blockquote><h3 id=7987--94299-mitigating-the-linguistic-gap-with-phonemic-representations-for-robust-multilingual-language-understanding-haeji-jung-et-al-2024>(79/87 | 94/299) Mitigating the Linguistic Gap with Phonemic Representations for Robust Multilingual Language Understanding (Haeji Jung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haeji Jung, Changdae Oh, Jooeon Kang, Jimin Sohn, Kyungwoo Song, Jinkyu Kim, David R. Mortensen. (2024)<br><strong>Mitigating the Linguistic Gap with Phonemic Representations for Robust Multilingual Language Understanding</strong><br><button class=copy-to-clipboard title="Mitigating the Linguistic Gap with Phonemic Representations for Robust Multilingual Language Understanding" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: High-Resource, Low-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14279v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14279v1.pdf filename=2402.14279v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Approaches to improving multilingual language understanding often require multiple languages during the training phase, rely on complicated training techniques, and &ndash; importantly &ndash; struggle with significant performance gaps between <b>high-resource</b> and <b>low-resource</b> languages. We hypothesize that the performance gaps between languages are affected by linguistic gaps between those languages and provide a novel solution for robust multilingual language modeling by employing phonemic representations (specifically, using phonemes as input tokens to LMs rather than subwords). We present quantitative evidence from three cross-lingual tasks that demonstrate the effectiveness of phonemic representation, which is further justified by a theoretical analysis of the cross-lingual performance gap.</p></p class="citation"></blockquote><h3 id=8087--95299-llmbind-a-unified-modality-task-integration-framework-bin-zhu-et-al-2024>(80/87 | 95/299) LLMBind: A Unified Modality-Task Integration Framework (Bin Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bin Zhu, Peng Jin, Munan Ning, Bin Lin, Jinfa Huang, Qi Song, Junwu Zhang, Zhenyu Tang, Mingjun Pan, Xing Zhou, Li Yuan. (2024)<br><strong>LLMBind: A Unified Modality-Task Integration Framework</strong><br><button class=copy-to-clipboard title="LLMBind: A Unified Modality-Task Integration Framework" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14891v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14891v2.pdf filename=2402.14891v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While recent progress in <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> tackles various modality tasks, they posses limited integration capabilities for complex multi-modality tasks, consequently constraining the development of the field. In this work, we take the initiative to explore and propose the LLMBind, a unified framework for modality task integration, which binds <b>Large</b> <b>Language</b> <b>Models</b> and corresponding pre-trained task models with task-specific tokens. Consequently, LLMBind can interpret inputs and produce outputs in versatile combinations of image, text, video, and audio. Specifically, we introduce a Mixture-of-Experts technique to enable effective learning for different <b>multimodal</b> tasks through collaboration among diverse experts. Furthermore, we create a multi-task dataset comprising 400k instruction data, which unlocks the ability for interactive visual generation and editing tasks. Extensive experiments show the effectiveness of our framework across various tasks, including image, video, audio generation, image segmentation, and image editing. More encouragingly, our framework can be easily extended to other modality tasks, showcasing the promising potential of creating a unified AI agent for modeling universal modalities.</p></p class="citation"></blockquote><h3 id=8187--96299-scaling-efficient-llms-b-n-kausik-2024>(81/87 | 96/299) Scaling Efficient LLMs (B. N. Kausik, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>B. N. Kausik. (2024)<br><strong>Scaling Efficient LLMs</strong><br><button class=copy-to-clipboard title="Scaling Efficient LLMs" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14746v1.pdf filename=2402.14746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trained <b>LLMs</b> are typically sparse in that most of the parameters are zero, raising questions on efficiency. In response, we inquire into efficient <b>LLMs,</b> i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, we compare theoretical and empirical estimates for training loss at current scale to obtain upper and lower bounds on the number of unique sequences in a natural training corpus as a function of its size. Our result implies (1) to double the number of skills represented in a training corpus, the corpus must scale roughly between three and five fold (2) for efficient <b>LLMs,</b> the number of parameters $N$ and the size $D$ of a natural training corpus scale as $N \sim D^{0.58}$ (3) if the number of parameters of an <b>LLM</b> is smaller than the number of unique sequences in the training corpus, scaling up can uncover emergent skills.</p></p class="citation"></blockquote><h3 id=8287--97299-inffeed-influence-functions-as-a-feedback-to-improve-the-performance-of-subjective-tasks-somnath-banerjee-et-al-2024>(82/87 | 97/299) InfFeed: Influence Functions as a Feedback to Improve the Performance of Subjective Tasks (Somnath Banerjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Somnath Banerjee, Maulindu Sarkar, Punyajoy Saha, Binny Mathew, Animesh Mukherjee. (2024)<br><strong>InfFeed: Influence Functions as a Feedback to Improve the Performance of Subjective Tasks</strong><br><button class=copy-to-clipboard title="InfFeed: Influence Functions as a Feedback to Improve the Performance of Subjective Tasks" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14702v1.pdf filename=2402.14702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, influence functions present an apparatus for achieving explainability for deep neural models by quantifying the perturbation of individual train instances that might impact a test prediction. Our objectives in this paper are twofold. First we incorporate influence functions as a feedback into the model to improve its performance. Second, in a dataset extension exercise, using influence functions to automatically identify data points that have been initially `silver&rsquo; annotated by some existing method and need to be cross-checked (and corrected) by annotators to improve the model performance. To meet these objectives, in this paper, we introduce InfFeed, which uses influence functions to compute the influential instances for a target instance. Toward the first objective, we adjust the label of the target instance based on its influencer(s) label. In doing this, InfFeed outperforms the state-of-the-art baselines (including <b>LLMs)</b> by a maximum macro F1-score margin of almost 4% for hate speech classification, 3.5% for stance classification, and 3% for irony and 2% for sarcasm detection. Toward the second objective we show that manually re-annotating only those silver annotated data points in the extension set that have a negative influence can immensely improve the model performance bringing it very close to the scenario where all the data points in the extension set have gold labels. This allows for huge reduction of the number of data points that need to be manually annotated since out of the silver annotated extension dataset, the influence function scheme picks up ~1/1000 points that need manual correction.</p></p class="citation"></blockquote><h3 id=8387--98299-domain-generalization-via-causal-adjustment-for-cross-domain-sentiment-analysis-siyin-wang-et-al-2024>(83/87 | 98/299) Domain Generalization via Causal Adjustment for Cross-Domain Sentiment Analysis (Siyin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyin Wang, Jie Zhou, Qin Chen, Qi Zhang, Tao Gui, Xuanjing Huang. (2024)<br><strong>Domain Generalization via Causal Adjustment for Cross-Domain Sentiment Analysis</strong><br><button class=copy-to-clipboard title="Domain Generalization via Causal Adjustment for Cross-Domain Sentiment Analysis" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14536v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14536v1.pdf filename=2402.14536v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Domain adaption has been widely adapted for cross-domain <b>sentiment</b> <b>analysis</b> to transfer knowledge from the source domain to the target domain. Whereas, most methods are proposed under the assumption that the target (test) domain is known, making them fail to generalize well on unknown test data that is not always available in practice. In this paper, we focus on the problem of domain generalization for cross-domain <b>sentiment</b> <b>analysis.</b> Specifically, we propose a backdoor adjustment-based causal model to disentangle the domain-specific and domain-invariant representations that play essential roles in tackling domain shift. First, we rethink the cross-domain <b>sentiment</b> <b>analysis</b> task in a causal view to model the causal-and-effect relationships among different variables. Then, to learn an invariant feature representation, we remove the effect of domain confounders (e.g., domain knowledge) using the backdoor adjustment. A series of experiments over many homologous and diverse datasets show the great performance and robustness of our model by comparing it with the state-of-the-art domain generalization baselines.</p></p class="citation"></blockquote><h3 id=8487--99299-daisy-tts-simulating-wider-spectrum-of-emotions-via-prosody-embedding-decomposition-rendi-chevi-et-al-2024>(84/87 | 99/299) Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding Decomposition (Rendi Chevi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rendi Chevi, Alham Fikri Aji. (2024)<br><strong>Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding Decomposition</strong><br><button class=copy-to-clipboard title="Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding Decomposition" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 10<br>Keywords: Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14523v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14523v1.pdf filename=2402.14523v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We often verbally express emotions in a multifaceted manner, they may vary in their intensities and may be expressed not just as a single but as a mixture of emotions. This wide spectrum of emotions is well-studied in the structural model of emotions, which represents variety of emotions as derivative products of primary emotions with varying degrees of intensity. In this paper, we propose an emotional <b>text-to-speech</b> design to simulate a wider spectrum of emotions grounded on the structural model. Our proposed design, Daisy-TTS, incorporates a prosody encoder to learn emotionally-separable prosody embedding as a proxy for emotion. This emotion representation allows the model to simulate: (1) Primary emotions, as learned from the training samples, (2) Secondary emotions, as a mixture of primary emotions, (3) Intensity-level, by scaling the emotion embedding, and (4) Emotions polarity, by negating the emotion embedding. Through a series of perceptual evaluations, Daisy-TTS demonstrated overall higher emotional speech naturalness and emotion perceiveability compared to the baseline.</p></p class="citation"></blockquote><h3 id=8587--100299-nlas-multi-a-multilingual-corpus-of-automatically-generated-natural-language-argumentation-schemes-ramon-ruiz-dolz-et-al-2024>(85/87 | 100/299) NLAS-multi: A Multilingual Corpus of Automatically Generated Natural Language Argumentation Schemes (Ramon Ruiz-Dolz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ramon Ruiz-Dolz, Joaquin Taverner, John Lawrence, Chris Reed. (2024)<br><strong>NLAS-multi: A Multilingual Corpus of Automatically Generated Natural Language Argumentation Schemes</strong><br><button class=copy-to-clipboard title="NLAS-multi: A Multilingual Corpus of Automatically Generated Natural Language Argumentation Schemes" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14458v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14458v1.pdf filename=2402.14458v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Some of the major limitations identified in the areas of argument mining, argument generation, and natural language argument analysis are related to the complexity of annotating argumentatively rich data, the limited size of these corpora, and the constraints that represent the different languages and domains in which these data is annotated. To address these limitations, in this paper we present the following contributions: (i) an effective methodology for the automatic generation of natural language arguments in different topics and languages, (ii) the largest publicly available corpus of natural language argumentation schemes, and (iii) a set of solid baselines and <b>fine-tuned</b> models for the automatic identification of argumentation schemes.</p></p class="citation"></blockquote><h3 id=8687--101299-assisting-in-writing-wikipedia-like-articles-from-scratch-with-large-language-models-yijia-shao-et-al-2024>(86/87 | 101/299) Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models (Yijia Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, Monica S. Lam. (2024)<br><strong>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</strong><br><button class=copy-to-clipboard title="Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14207v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14207v1.pdf filename=2402.14207v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study how to apply <b>large</b> <b>language</b> <b>models</b> to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline. For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from experienced Wikipedia editors. Compared to articles generated by an outline-driven retrieval-augmented baseline, more of STORM&rsquo;s articles are deemed to be organized (by a 25% absolute increase) and broad in coverage (by 10%). The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.</p></p class="citation"></blockquote><h3 id=8787--102299-less-is-more-mitigating-multimodal-hallucination-from-an-eos-decision-perspective-zihao-yue-et-al-2024>(87/87 | 102/299) Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective (Zihao Yue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihao Yue, Liang Zhang, Qin Jin. (2024)<br><strong>Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective</strong><br><button class=copy-to-clipboard title="Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14545v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14545v1.pdf filename=2402.14545v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>Multimodal</b> Models (LMMs) often suffer from <b>multimodal</b> hallucinations, wherein they may create content that is not present in the visual inputs. In this paper, we explore a new angle of this issue: overly detailed training data hinders the model&rsquo;s ability to timely terminate generation, leading to continued outputs beyond visual perception limits. By investigating how the model decides to terminate generation with EOS, the special end-of-sentence token, we find that the model assesses the completeness of the entire sequence by comparing the generated text with the image. This observation suggests that the model possesses an inherent potential of making proper EOS decisions based on its visual perception to avoid overly lengthy outputs. To take advantage of such potential, we explore two methods to mitigate <b>multimodal</b> hallucinations: a training objective that enables the model to reduce hallucinations by learning from regular instruction data, and a data filtering strategy to prevent harmful training data from exacerbating model hallucinations. Both methods significantly improve the hallucination performance of LMMs, without requiring any additional data or knowledge.</p></p class="citation"></blockquote><h2 id=cslg-60>cs.LG (60)</h2><h3 id=160--103299-copr-continual-human-preference-learning-via-optimal-policy-regularization-han-zhang-et-al-2024>(1/60 | 103/299) COPR: Continual Human Preference Learning via Optimal Policy Regularization (Han Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Zhang, Lin Gui, Yu Lei, Yuanzhao Zhai, Yehong Zhang, Yulan He, Hui Wang, Yue Yu, Kam-Fai Wong, Bin Liang, Ruifeng Xu. (2024)<br><strong>COPR: Continual Human Preference Learning via Optimal Policy Regularization</strong><br><button class=copy-to-clipboard title="COPR: Continual Human Preference Learning via Optimal Policy Regularization" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 83<br>Keywords: Benchmarking, Continual Learning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14228v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14228v2.pdf filename=2402.14228v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> is commonly utilized to improve the alignment of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with human preferences. Given the evolving nature of human preferences, <b>continual</b> <b>alignment</b> becomes more crucial and practical in comparison to traditional static alignment. Nevertheless, making <b>RLHF</b> compatible with <b>Continual</b> <b>Learning</b> (CL) is challenging due to its complex process. Meanwhile, directly learning new human preferences may lead to Catastrophic Forgetting (CF) of historical preferences, resulting in helpless or harmful outputs. To overcome these challenges, we propose the <b>Continual</b> <b>Optimal</b> Policy Regularization (COPR) method, which draws inspiration from the optimal policy theory. COPR utilizes a sampling distribution as a demonstration and regularization constraints for CL. It adopts the Lagrangian Duality (LD) method to dynamically regularize the current policy based on the historically optimal policy, which prevents CF and avoids over-emphasizing unbalanced objectives. We also provide formal proof for the learnability of COPR. The experimental results show that COPR outperforms strong CL baselines on our proposed <b>benchmark,</b> in terms of reward-based, <b>GPT-4</b> evaluations and human assessment. Furthermore, we validate the robustness of COPR under various CL settings, including different backbones, replay memory sizes, and learning orders.</p></p class="citation"></blockquote><h3 id=260--104299-generalizing-reward-modeling-for-out-of-distribution-preference-learning-chen-jia-2024>(2/60 | 104/299) Generalizing Reward Modeling for Out-of-Distribution Preference Learning (Chen Jia, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Jia. (2024)<br><strong>Generalizing Reward Modeling for Out-of-Distribution Preference Learning</strong><br><button class=copy-to-clipboard title="Generalizing Reward Modeling for Out-of-Distribution Preference Learning" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 80<br>Keywords: Meta Learning, Out-of-distribution, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14760v1.pdf filename=2402.14760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Preference learning (PL) with <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> aims to align the <b>LLMs&rsquo;</b> generations with human preferences. Previous work on <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF)</b> has demonstrated promising results in in-distribution PL. However, due to the difficulty of obtaining human feedback, discretely training reward models for every encountered distribution is challenging. Thus, <b>out-of-distribution</b> (OOD) PL is practically useful for enhancing the generalization ability of <b>LLMs</b> with limited preference feedback. This work addresses OOD PL by optimizing a general reward model through a <b>meta-learning</b> <b>approach.</b> During <b>meta-training,</b> <b>a</b> bilevel optimization algorithm is utilized to learn a reward model capable of guiding policy learning to align with human preferences across various distributions. When encountering a test distribution, the <b>meta-test</b> <b>procedure</b> conducts regularized policy optimization using the learned reward model for PL. We theoretically demonstrate the convergence rate of the bilevel optimization algorithm under reasonable assumptions. Additionally, we conduct experiments on two <b>text</b> <b>generation</b> tasks across 20 held-out domains and outperform a variety of strong baselines across various evaluation metrics.</p></p class="citation"></blockquote><h3 id=360--105299-efficient-data-selection-employing-semantic-similarity-based-graph-structures-for-model-training-roxana-petcu-et-al-2024>(3/60 | 105/299) Efficient data selection employing Semantic Similarity-based Graph Structures for model training (Roxana Petcu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roxana Petcu, Subhadeep Maji. (2024)<br><strong>Efficient data selection employing Semantic Similarity-based Graph Structures for model training</strong><br><button class=copy-to-clipboard title="Efficient data selection employing Semantic Similarity-based Graph Structures for model training" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 73<br>Keywords: Message-Passing, Graph, Fine-tuning, Low-Resource, Automatic Speech Recognition, Automatic Speech Recognition, Text-to-speech, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14888v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14888v1.pdf filename=2402.14888v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent developments in natural language processing (NLP) have highlighted the need for substantial amounts of data for models to capture textual information accurately. This raises concerns regarding the computational resources and time required for training such models. This paper introduces Semantics for data SAliency in Model performance Estimation (SeSaME). It is an efficient data sampling mechanism solely based on textual information without passing the data through a compute-heavy model or other intensive pre-processing transformations. The application of this approach is demonstrated in the use case of <b>low-resource</b> automated <b>speech</b> <b>recognition</b> <b>(ASR)</b> models, which excessively rely on <b>text-to-speech</b> <b>(TTS)</b> calls when using augmented data. SeSaME learns to categorize new incoming data points into <b>speech</b> <b>recognition</b> difficulty buckets by employing semantic similarity-based <b>graph</b> structures and discrete <b>ASR</b> information from homophilous neighbourhoods through message passing. The results indicate reliable projections of <b>ASR</b> performance, with a 93% accuracy increase when using the proposed method compared to random predictions, bringing non-trivial information on the impact of textual representations in <b>speech</b> <b>models.</b> Furthermore, a series of experiments show both the benefits and challenges of using the <b>ASR</b> information on incoming data to <b>fine-tune</b> the model. We report a 7% drop in validation loss compared to random sampling, 7% WER drop with non-local aggregation when evaluating against a highly difficult dataset, and 1.8% WER drop with local aggregation and high semantic similarity between datasets.</p></p class="citation"></blockquote><h3 id=460--106299-link-prediction-under-heterophily-a-physics-inspired-graph-neural-network-approach-andrea-giuseppe-di-francesco-et-al-2024>(4/60 | 106/299) Link Prediction under Heterophily: A Physics-Inspired Graph Neural Network Approach (Andrea Giuseppe Di Francesco et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Giuseppe Di Francesco, Francesco Caso, Maria Sofia Bucarelli, Fabrizio Silvestri. (2024)<br><strong>Link Prediction under Heterophily: A Physics-Inspired Graph Neural Network Approach</strong><br><button class=copy-to-clipboard title="Link Prediction under Heterophily: A Physics-Inspired Graph Neural Network Approach" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IR, cs-LG, cs-SI, cs.LG<br>Keyword Score: 56<br>Keywords: Message-Passing, Node Classification, Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14802v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14802v1.pdf filename=2402.14802v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the past years, <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have become the `de facto&rsquo; standard in various deep learning domains, thanks to their flexibility in modeling real-world phenomena represented as <b>graphs.</b> <b>However,</b> <b>the</b> <b>message-passing</b> mechanism of <b>GNNs</b> faces challenges in learnability and expressivity, hindering high performance on heterophilic <b>graphs,</b> <b>where</b> <b>adjacent</b> <b>nodes</b> <b>frequently</b> have different labels. Most existing solutions addressing these challenges are primarily confined to specific <b>benchmarks</b> focused on <b>node</b> <b>classification</b> tasks. This narrow focus restricts the potential impact that link prediction under heterophily could offer in several applications, including <b>recommender</b> <b>systems.</b> For example, in social networks, two users may be connected for some latent reason, making it challenging to predict such connections in advance. Physics-Inspired <b>GNNs</b> such as GRAFF provided a significant contribution to enhance <b>node</b> <b>classification</b> performance under heterophily, thanks to the adoption of physics biases in the <b>message-passing.</b> Drawing inspiration from these findings, we advocate that the methodology employed by GRAFF can improve link prediction performance as well. To further explore this hypothesis, we introduce GRAFF-LP, an extension of GRAFF to link prediction. We evaluate its efficacy within a recent collection of heterophilic <b>graphs,</b> <b>establishing</b> <b>a</b> new <b>benchmark</b> for link prediction under heterophily. Our approach surpasses previous methods, in most of the datasets, showcasing a strong flexibility in different contexts, and achieving relative AUROC improvements of up to 26.7%.</p></p class="citation"></blockquote><h3 id=560--107299-how-transformers-learn-causal-structure-with-gradient-descent-eshaan-nichani-et-al-2024>(5/60 | 107/299) How Transformers Learn Causal Structure with Gradient Descent (Eshaan Nichani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eshaan Nichani, Alex Damian, Jason D. Lee. (2024)<br><strong>How Transformers Learn Causal Structure with Gradient Descent</strong><br><button class=copy-to-clipboard title="How Transformers Learn Causal Structure with Gradient Descent" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IT, cs-LG, cs.LG, math-IT, stat-ML<br>Keyword Score: 53<br>Keywords: Graph, Mutual Information, Transformer, In-context Learning, In-context Learning, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14735v1.pdf filename=2402.14735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The incredible success of <b>transformers</b> on sequence modeling tasks can be largely attributed to the <b>self-attention</b> mechanism, which allows information to be transferred between different parts of a sequence. <b>Self-attention</b> allows <b>transformers</b> to encode causal structure which makes them particularly suitable for sequence modeling. However, the process by which <b>transformers</b> learn such causal structure via gradient-based training algorithms remains poorly understood. To better understand this process, we introduce an <b>in-context</b> <b>learning</b> task that requires learning latent causal structure. We prove that gradient descent on a simplified two-layer <b>transformer</b> learns to solve this task by encoding the latent causal <b>graph</b> in the first attention layer. The key insight of our proof is that the gradient of the attention matrix encodes the <b>mutual</b> <b>information</b> between tokens. As a consequence of the data processing inequality, the largest entries of this gradient correspond to edges in the latent causal <b>graph.</b> As a special case, when the sequences are generated from <b>in-context</b> <b>Markov</b> chains, we prove that <b>transformers</b> learn an induction head (Olsson et al., 2022). We confirm our theoretical findings by showing that <b>transformers</b> trained on our <b>in-context</b> <b>learning</b> task are able to recover a wide variety of causal structures.</p></p class="citation"></blockquote><h3 id=660--108299-practical-insights-into-knowledge-distillation-for-pre-trained-models-norah-alballa-et-al-2024>(6/60 | 108/299) Practical Insights into Knowledge Distillation for Pre-Trained Models (Norah Alballa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Norah Alballa, Marco Canini. (2024)<br><strong>Practical Insights into Knowledge Distillation for Pre-Trained Models</strong><br><button class=copy-to-clipboard title="Practical Insights into Knowledge Distillation for Pre-Trained Models" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Federated Learning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Knowledge Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14922v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14922v1.pdf filename=2402.14922v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research investigates the enhancement of <b>knowledge</b> <b>distillation</b> <b>(KD)</b> processes in pre-trained models, an emerging field in <b>knowledge</b> <b>transfer</b> with significant implications for distributed training and <b>federated</b> <b>learning</b> environments. These environments benefit from reduced communication demands and accommodate various model architectures. Despite the adoption of numerous <b>KD</b> approaches for transferring <b>knowledge</b> <b>among</b> pre-trained models, a comprehensive understanding of <b>KD&rsquo;s</b> application in these scenarios is lacking. Our study conducts an extensive comparison of multiple <b>KD</b> techniques, including standard <b>KD,</b> tuned <b>KD</b> (via optimized temperature and weight parameters), deep mutual learning, and data partitioning <b>KD.</b> We assess these methods across various data distribution strategies to identify the most effective contexts for each. Through detailed examination of hyperparameter tuning, informed by extensive grid search evaluations, we pinpoint when adjustments are crucial to enhance model performance. This paper sheds light on optimal hyperparameter settings for distinct data partitioning scenarios and investigates <b>KD&rsquo;s</b> role in improving <b>federated</b> <b>learning</b> by minimizing communication rounds and expediting the training process. By filling a notable void in current research, our findings serve as a practical framework for leveraging <b>KD</b> in pre-trained models within collaborative and <b>federated</b> <b>learning</b> frameworks.</p></p class="citation"></blockquote><h3 id=760--109299-back-to-basics-revisiting-reinforce-style-optimization-for-learning-from-human-feedback-in-llms-arash-ahmadian-et-al-2024>(7/60 | 109/299) Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs (Arash Ahmadian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, Sara Hooker. (2024)<br><strong>Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs</strong><br><button class=copy-to-clipboard title="Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-7, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14740v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14740v2.pdf filename=2402.14740v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AI alignment in the shape of <b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> is increasingly treated as a crucial ingredient for high performance <b>large</b> <b>language</b> <b>models.</b> Proximal Policy Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of <b>RLHF.</b> However, it involves both high computational cost and sensitive hyperparameter tuning. We posit that most of the motivational principles that led to the development of PPO are less of a practical concern in <b>RLHF</b> and advocate for a less computationally expensive method that preserves and even increases performance. We revisit the formulation of alignment from human preferences in the context of RL. Keeping simplicity as a guiding principle, we show that many components of PPO are unnecessary in an <b>RLHF</b> context and that far simpler REINFORCE-style optimization variants outperform both PPO and newly proposed &ldquo;RL-free&rdquo; methods such as DPO and RAFT. Our work suggests that careful adaptation to <b>LLMs</b> alignment characteristics enables benefiting from online RL optimization at low cost.</p></p class="citation"></blockquote><h3 id=860--110299-q-probe-a-lightweight-approach-to-reward-maximization-for-language-models-kenneth-li-et-al-2024>(8/60 | 110/299) Q-Probe: A Lightweight Approach to Reward Maximization for Language Models (Kenneth Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kenneth Li, Samy Jelassi, Hugh Zhang, Sham Kakade, Martin Wattenberg, David Brandfonbrener. (2024)<br><strong>Q-Probe: A Lightweight Approach to Reward Maximization for Language Models</strong><br><button class=copy-to-clipboard title="Q-Probe: A Lightweight Approach to Reward Maximization for Language Models" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Few-shot, Fine-tuning, Code Generation, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14688v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14688v1.pdf filename=2402.14688v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present an approach called Q-probing to adapt a <b>pre-trained</b> <b>language</b> <b>model</b> to maximize a task-specific reward function. At a high level, Q-probing sits between heavier approaches such as <b>finetuning</b> and lighter approaches such as few shot <b>prompting,</b> but can also be combined with either. The idea is to learn a simple linear function on a model&rsquo;s embedding space that can be used to reweight candidate completions. We theoretically show that this sampling procedure is equivalent to a KL-constrained maximization of the Q-probe as the number of samples increases. To train the Q-probes we consider either reward modeling or a class of novel direct policy learning objectives based on importance weighted policy gradients. With this technique, we see gains in domains with ground-truth rewards <b>(code</b> <b>generation)</b> as well as implicit rewards defined by preference data, even outperforming <b>finetuning</b> in data-limited regimes. Moreover, a Q-probe can be trained on top of an API since it only assumes access to sampling and embeddings. <b>Code:</b> <b><a href=https://github.com/likenneth/q_probe>https://github.com/likenneth/q_probe</a></b> .</p></p class="citation"></blockquote><h3 id=960--111299-fedcqa-answering-complex-queries-on-multi-source-knowledge-graphs-via-federated-learning-qi-hu-et-al-2024>(9/60 | 111/299) FedCQA: Answering Complex Queries on Multi-Source Knowledge Graphs via Federated Learning (Qi Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Hu, Weifeng Jiang, Haoran Li, Zihao Wang, Jiaxin Bai, Qianren Mao, Yangqiu Song, Lixin Fan, Jianxin Li. (2024)<br><strong>FedCQA: Answering Complex Queries on Multi-Source Knowledge Graphs via Federated Learning</strong><br><button class=copy-to-clipboard title="FedCQA: Answering Complex Queries on Multi-Source Knowledge Graphs via Federated Learning" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-DB, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph, Graph Embedding, Federated Learning, Knowledge Graph, Knowledge Graph, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14609v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14609v2.pdf filename=2402.14609v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Complex logical query answering is a challenging task in <b>knowledge</b> <b>graphs</b> <b>(KGs)</b> that has been widely studied. The ability to perform complex logical <b>reasoning</b> is essential and supports various <b>graph</b> <b>reasoning-based</b> downstream tasks, such as search engines. Recent approaches are proposed to represent <b>KG</b> entities and logical queries into embedding vectors and find answers to logical queries from the <b>KGs.</b> However, existing proposed methods mainly focus on querying a single <b>KG</b> and cannot be applied to multiple <b>graphs.</b> <b>In</b> addition, directly sharing <b>KGs</b> with sensitive information may incur privacy risks, making it impractical to share and construct an aggregated <b>KG</b> for <b>reasoning</b> to retrieve query answers. Thus, it remains unknown how to answer queries on multi-source <b>KGs.</b> An entity can be involved in various <b>knowledge</b> <b>graphs</b> <b>and</b> <b>reasoning</b> on multiple <b>KGs</b> and answering complex queries on multi-source <b>KGs</b> is important in discovering <b>knowledge</b> <b>cross</b> <b>graphs.</b> <b>Fortunately,</b> <b>federated</b> <b>learning</b> is utilized in <b>knowledge</b> <b>graphs</b> <b>to</b> collaboratively learn representations with privacy preserved. <b>Federated</b> <b>knowledge</b> <b>graph</b> <b>embeddings</b> enrich the relations in <b>knowledge</b> <b>graphs</b> <b>to</b> improve the representation quality. However, these methods only focus on one-hop relations and cannot perform complex <b>reasoning</b> tasks. In this paper, we apply <b>federated</b> <b>learning</b> to complex query-answering tasks to reason over multi-source <b>knowledge</b> <b>graphs</b> <b>while</b> preserving privacy. We propose a <b>Federated</b> <b>Complex</b> Query Answering framework (FedCQA), to reason over multi-source <b>KGs</b> avoiding sensitive raw data transmission to protect privacy. We conduct extensive experiments on three real-world datasets and evaluate retrieval performance on various types of complex queries.</p></p class="citation"></blockquote><h3 id=1060--112299-rethinking-invariance-regularization-in-adversarial-training-to-improve-robustness-accuracy-trade-off-futa-waseda-et-al-2024>(10/60 | 112/299) Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off (Futa Waseda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Futa Waseda, Isao Echizen. (2024)<br><strong>Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off</strong><br><button class=copy-to-clipboard title="Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Adversarial Learning, Knowledge Distillation, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14648v1.pdf filename=2402.14648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although <b>adversarial</b> <b>training</b> has been the state-of-the-art approach to defend against <b>adversarial</b> <b>examples</b> (AEs), they suffer from a robustness-accuracy trade-off. In this work, we revisit representation-based invariance regularization to learn discriminative yet adversarially invariant representations, aiming to mitigate this trade-off. We empirically identify two key issues hindering invariance regularization: (1) a &ldquo;gradient conflict&rdquo; between invariance loss and classification objectives, indicating the existence of &ldquo;collapsing solutions,&rdquo; and (2) the mixture distribution problem arising from diverged distributions of clean and <b>adversarial</b> <b>inputs.</b> To address these issues, we propose Asymmetrically Representation-regularized <b>Adversarial</b> <b>Training</b> (AR-AT), which incorporates a stop-gradient operation and a pre-dictor in the invariance loss to avoid &ldquo;collapsing solutions,&rdquo; inspired by a recent non-contrastive <b>self-supervised</b> <b>learning</b> approach, and a split-BatchNorm (BN) structure to resolve the mixture distribution problem. Our method significantly improves the robustness-accuracy trade-off by learning adversarially invariant representations without sacrificing discriminative power. Furthermore, we discuss the relevance of our findings to <b>knowledge-distillation-based</b> <b>defense</b> methods, contributing to a deeper understanding of their relative successes.</p></p class="citation"></blockquote><h3 id=1160--113299-data-science-with-llms-and-interpretable-models-sebastian-bordt-et-al-2024>(11/60 | 113/299) Data Science with LLMs and Interpretable Models (Sebastian Bordt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sebastian Bordt, Ben Lengerich, Harsha Nori, Rich Caruana. (2024)<br><strong>Data Science with LLMs and Interpretable Models</strong><br><button class=copy-to-clipboard title="Data Science with LLMs and Interpretable Models" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Question Answering, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14474v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14474v1.pdf filename=2402.14474v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent years have seen important advances in the building of interpretable models, machine learning models that are designed to be easily understood by humans. In this work, we show that <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are remarkably good at working with interpretable models, too. In particular, we show that <b>LLMs</b> can describe, interpret, and debug Generalized Additive Models (GAMs). Combining the flexibility of <b>LLMs</b> with the breadth of statistical patterns accurately described by GAMs enables dataset <b>summarization,</b> <b>question</b> <b>answering,</b> and model critique. <b>LLMs</b> can also improve the interaction between domain experts and interpretable models, and generate hypotheses about the underlying phenomenon. We release \url{https://github.com/interpretml/TalkToEBM} as an open-source <b>LLM-GAM</b> interface.</p></p class="citation"></blockquote><h3 id=1260--114299-large-scale-actionless-video-pre-training-via-discrete-diffusion-for-efficient-policy-learning-haoran-he-et-al-2024>(12/60 | 114/299) Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning (Haoran He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, Xuelong Li. (2024)<br><strong>Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning</strong><br><button class=copy-to-clipboard title="Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs-RO, cs.LG<br>Keyword Score: 40<br>Keywords: Diffusion Model, Fine-tuning, Fine-tuning, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14407v1.pdf filename=2402.14407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily <b>stemming</b> from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete <b>diffusion</b> <b>to</b> combine generative pre-training on human videos and policy <b>fine-tuning</b> on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete <b>diffusion</b> <b>model</b> with a mask-and-replace <b>diffusion</b> <b>strategy</b> to predict future video tokens in the latent space. In the <b>fine-tuning</b> stage, we harness the imagined future videos to guide low-level action learning trained on a limited set of robot data. Experiments demonstrate that our method generates high-fidelity future videos for planning and enhances the <b>fine-tuned</b> policies compared to previous state-of-the-art approaches with superior generalization ability. Our project website is available at <a href=https://video-diff.github.io/>https://video-diff.github.io/</a>.</p></p class="citation"></blockquote><h3 id=1360--115299-generative-adversarial-network-with-soft-dynamic-time-warping-and-parallel-reconstruction-for-energy-time-series-anomaly-detection-hardik-prabhu-et-al-2024>(13/60 | 115/299) Generative Adversarial Network with Soft-Dynamic Time Warping and Parallel Reconstruction for Energy Time Series Anomaly Detection (Hardik Prabhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hardik Prabhu, Jayaraman Valadi, Pandarasamy Arjunan. (2024)<br><strong>Generative Adversarial Network with Soft-Dynamic Time Warping and Parallel Reconstruction for Energy Time Series Anomaly Detection</strong><br><button class=copy-to-clipboard title="Generative Adversarial Network with Soft-Dynamic Time Warping and Parallel Reconstruction for Energy Time Series Anomaly Detection" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Anomaly Detection, Convolution, Generative Adversarial Network, Reconstruction Loss<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14384v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14384v1.pdf filename=2402.14384v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we employ a 1D deep <b>convolutional</b> <b>generative</b> <b>adversarial</b> <b>network</b> (DCGAN) for sequential <b>anomaly</b> <b>detection</b> in energy time series data. <b>Anomaly</b> <b>detection</b> involves gradient descent to reconstruct energy sub-sequences, identifying the noise vector that closely generates them through the generator network. Soft-DTW is used as a differentiable alternative for the <b>reconstruction</b> <b>loss</b> and is found to be superior to Euclidean distance. Combining <b>reconstruction</b> <b>loss</b> and the latent space&rsquo;s prior probability distribution serves as the <b>anomaly</b> <b>score.</b> Our novel method accelerates detection by parallel computation of <b>reconstruction</b> <b>of</b> multiple points and shows promise in identifying anomalous energy consumption in buildings, as evidenced by performing experiments on hourly energy time series from 15 buildings.</p></p class="citation"></blockquote><h3 id=1460--116299-estimating-unknown-population-sizes-using-the-hypergeometric-distribution-liam-hodgson-et-al-2024>(14/60 | 116/299) Estimating Unknown Population Sizes Using the Hypergeometric Distribution (Liam Hodgson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liam Hodgson, Danilo Bzdok. (2024)<br><strong>Estimating Unknown Population Sizes Using the Hypergeometric Distribution</strong><br><button class=copy-to-clipboard title="Estimating Unknown Population Sizes Using the Hypergeometric Distribution" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ME, stat-ML<br>Keyword Score: 40<br>Keywords: Autoencoder, Simulation, Simulator, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14220v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14220v1.pdf filename=2402.14220v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The multivariate hypergeometric distribution describes sampling without replacement from a discrete population of elements divided into multiple categories. Addressing a gap in the literature, we tackle the challenge of estimating discrete distributions when both the total population size and the sizes of its constituent categories are unknown. Here, we propose a novel solution using the hypergeometric likelihood to solve this estimation challenge, even in the presence of severe under-sampling. We develop our approach to account for a data generating process where the ground-truth is a mixture of distributions conditional on a continuous latent variable, such as with collaborative filtering, using the <b>variational</b> <b>autoencoder</b> framework. Empirical data <b>simulation</b> demonstrates that our method outperforms other likelihood functions used to model count data, both in terms of accuracy of population size estimate and in its ability to learn an informative latent space. We demonstrate our method&rsquo;s versatility through applications in NLP, by inferring and estimating the complexity of latent vocabularies in text excerpts, and in biology, by accurately recovering the true number of gene transcripts from sparse single-cell genomics data.</p></p class="citation"></blockquote><h3 id=1560--117299-mobilellm-optimizing-sub-billion-parameter-language-models-for-on-device-use-cases-zechun-liu-et-al-2024>(15/60 | 117/299) MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases (Zechun Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas Chandra. (2024)<br><strong>MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases</strong><br><button class=copy-to-clipboard title="MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14905v1.pdf filename=2402.14905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the growing need for efficient <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality <b>LLMs</b> with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale <b>LLMs.</b> Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-LS, demonstrate a further accuracy enhancement of 0.7%/0.8% than MobileLLM 125M/350M. Moreover, MobileLLM model family shows significant improvements compared to previous sub-billion models on chat <b>benchmarks,</b> and demonstrates close correctness to <b>LLaMA-v2</b> 7B in API calling tasks, highlighting the capability of small models for common on-device use cases.</p></p class="citation"></blockquote><h3 id=1660--118299-self-guided-masked-autoencoders-for-domain-agnostic-self-supervised-learning-johnathan-xie-et-al-2024>(16/60 | 118/299) Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning (Johnathan Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johnathan Xie, Yoonho Lee, Annie S. Chen, Chelsea Finn. (2024)<br><strong>Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning</strong><br><button class=copy-to-clipboard title="Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Autoencoder, Benchmarking, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14789v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14789v1.pdf filename=2402.14789v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities. Yet, extending <b>self-supervised</b> <b>learning</b> to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task. While masked modeling is promising as a domain-agnostic framework for <b>self-supervised</b> <b>learning</b> because it does not rely on input augmentations, its mask sampling procedure remains domain-specific. We present Self-guided Masked <b>Autoencoders</b> (SMA), a fully domain-agnostic masked modeling method. SMA trains an attention based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions. We evaluate SMA on three <b>self-supervised</b> <b>learning</b> <b>benchmarks</b> in protein biology, chemical property prediction, and particle physics. We find SMA is capable of learning representations without domain-specific knowledge and achieves state-of-the-art performance on these three <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1760--119299-cat-gnn-enhancing-credit-card-fraud-detection-via-causal-temporal-graph-neural-networks-yifan-duan-et-al-2024>(17/60 | 119/299) CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks (Yifan Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Duan, Guibin Zhang, Shilong Wang, Xiaojiang Peng, Wang Ziqi, Junyuan Mao, Hao Wu, Xinke Jiang, Kun Wang. (2024)<br><strong>CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks</strong><br><button class=copy-to-clipboard title="CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, q-fin-ST<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14708v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14708v1.pdf filename=2402.14708v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Credit card fraud poses a significant threat to the economy. While <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN)-based</b> fraud detection methods perform well, they often overlook the causal effect of a node&rsquo;s local structure on predictions. This paper introduces a novel method for credit card fraud detection, the \textbf{\underline{Ca}}usal \textbf{\underline{T}}emporal \textbf{\underline{G}}raph \textbf{\underline{N}}eural \textbf{N}etwork (CaT-GNN), which leverages causal invariant learning to reveal inherent correlations within transaction data. By decomposing the problem into discovery and intervention phases, CaT-GNN identifies causal nodes within the transaction <b>graph</b> <b>and</b> <b>applies</b> a causal mixup strategy to enhance the model&rsquo;s robustness and interpretability. CaT-GNN consists of two key components: Causal-Inspector and Causal-Intervener. The Causal-Inspector utilizes attention weights in the temporal attention mechanism to identify causal and environment nodes without introducing additional parameters. Subsequently, the Causal-Intervener performs a causal mixup enhancement on environment nodes based on the set of nodes. Evaluated on three datasets, including a private financial dataset and two public datasets, CaT-GNN demonstrates superior performance over existing state-of-the-art methods. Our findings highlight the potential of integrating causal <b>reasoning</b> with <b>graph</b> <b>neural</b> <b>networks</b> to improve fraud detection capabilities in financial transactions.</p></p class="citation"></blockquote><h3 id=1860--120299-robust-training-of-federated-models-with-extremely-label-deficiency-yonggang-zhang-et-al-2024>(18/60 | 120/299) Robust Training of Federated Models with Extremely Label Deficiency (Yonggang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yonggang Zhang, Zhiqin Yang, Xinmei Tian, Nannan Wang, Tongliang Liu, Bo Han. (2024)<br><strong>Robust Training of Federated Models with Extremely Label Deficiency</strong><br><button class=copy-to-clipboard title="Robust Training of Federated Models with Extremely Label Deficiency" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Semi-Supervised Learning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14430v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14430v1.pdf filename=2402.14430v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Federated <b>semi-supervised</b> <b>learning</b> (FSSL) has emerged as a powerful paradigm for collaboratively training machine learning models using distributed data with label deficiency. Advanced FSSL methods predominantly focus on training a single model on each client. However, this approach could lead to a discrepancy between the objective functions of labeled and unlabeled data, resulting in gradient conflicts. To alleviate gradient conflict, we propose a novel twin-model paradigm, called Twin-sight, designed to enhance mutual guidance by providing insights from different perspectives of labeled and unlabeled data. In particular, Twin-sight concurrently trains a <b>supervised</b> model with a <b>supervised</b> objective function while training an <b>unsupervised</b> model using an <b>unsupervised</b> objective function. To enhance the synergy between these two models, Twin-sight introduces a neighbourhood-preserving constraint, which encourages the preservation of the neighbourhood relationship among data features extracted by both models. Our comprehensive experiments on four <b>benchmark</b> datasets provide substantial evidence that Twin-sight can significantly outperform state-of-the-art methods across various experimental settings, demonstrating the efficacy of the proposed Twin-sight.</p></p class="citation"></blockquote><h3 id=1960--121299-take-the-bull-by-the-horns-hard-sample-reweighted-continual-training-improves-llm-generalization-xuxi-chen-et-al-2024>(19/60 | 121/299) Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization (Xuxi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuxi Chen, Zhendong Wang, Daouda Sow, Junjie Yang, Tianlong Chen, Yingbin Liang, Mingyuan Zhou, Zhangyang Wang. (2024)<br><strong>Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization</strong><br><button class=copy-to-clipboard title="Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14270v1.pdf filename=2402.14270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly advancing arena of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> a key challenge is to enhance their capabilities amid a looming shortage of high-quality training data. Our study starts from an empirical strategy for the light continual training of <b>LLMs</b> using their original pre-training data sets, with a specific focus on selective retention of samples that incur moderately high losses. These samples are deemed informative and beneficial for model refinement, contrasting with the highest-loss samples, which would be discarded due to their correlation with data noise and complexity. We then formalize this strategy into a principled framework of Instance-Reweighted Distributionally Robust Optimization (IR-DRO). IR-DRO is designed to dynamically prioritize the training focus on informative samples through an instance reweighting mechanism, streamlined by a closed-form solution for straightforward integration into established training protocols. Through rigorous experimentation with various models and datasets, our findings indicate that our sample-targeted methods significantly improve <b>LLM</b> performance across multiple <b>benchmarks,</b> in both continual pre-training and <b>instruction</b> <b>tuning</b> scenarios. Our codes are available at <a href=https://github.com/VITA-Group/HardFocusTraining>https://github.com/VITA-Group/HardFocusTraining</a>.</p></p class="citation"></blockquote><h3 id=2060--122299-towards-few-shot-adaptation-of-foundation-models-via-multitask-finetuning-zhuoyan-xu-et-al-2024>(20/60 | 122/299) Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning (Zhuoyan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuoyan Xu, Zhenmei Shi, Junyi Wei, Fangzhou Mu, Yin Li, Yingyu Liang. (2024)<br><strong>Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning</strong><br><button class=copy-to-clipboard title="Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Few-shot, Fine-tuning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15017v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15017v1.pdf filename=2402.15017v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> have emerged as a powerful tool for many AI problems. Despite the tremendous success of <b>foundation</b> <b>models,</b> effective adaptation to new tasks, particularly those with limited labels, remains an open question and lacks theoretical understanding. An emerging solution with recent success in vision and NLP involves <b>finetuning</b> a <b>foundation</b> <b>model</b> on a selection of relevant tasks, before its adaptation to a target task with limited labeled samples. In this paper, we study the theoretical justification of this multitask <b>finetuning</b> approach. Our theoretical analysis reveals that with a diverse set of related tasks, this multitask <b>finetuning</b> leads to reduced error in the target task, in comparison to directly adapting the same pretrained model. We quantify the relationship between <b>finetuning</b> tasks and target tasks by diversity and consistency metrics, and further propose a practical task selection algorithm. We substantiate our theoretical claims with extensive empirical evidence. Further, we present results affirming our task selection algorithm adeptly chooses related <b>finetuning</b> tasks, providing advantages to the model performance on target tasks. We believe our study shed new light on the effective adaptation of <b>foundation</b> <b>models</b> to new tasks that lack abundant labels. Our code is available at <a href=https://github.com/OliverXUZY/Foudation-Model_Multitask>https://github.com/OliverXUZY/Foudation-Model_Multitask</a>.</p></p class="citation"></blockquote><h3 id=2160--123299-prompting-a-pretrained-transformer-can-be-a-universal-approximator-aleksandar-petrov-et-al-2024>(21/60 | 123/299) Prompting a Pretrained Transformer Can Be a Universal Approximator (Aleksandar Petrov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aleksandar Petrov, Philip H. S. Torr, Adel Bibi. (2024)<br><strong>Prompting a Pretrained Transformer Can Be a Universal Approximator</strong><br><button class=copy-to-clipboard title="Prompting a Pretrained Transformer Can Be a Universal Approximator" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-FA<br>Keyword Score: 30<br>Keywords: Fine-tuning, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14753v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14753v1.pdf filename=2402.14753v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the widespread adoption of <b>prompting,</b> <b>prompt</b> tuning and prefix-tuning of <b>transformer</b> models, our theoretical understanding of these <b>fine-tuning</b> methods remains limited. A key question is whether one can arbitrarily modify the behavior of pretrained model by <b>prompting</b> or prefix-tuning it. Formally, whether <b>prompting</b> and prefix-tuning a pretrained model can universally approximate sequence-to-sequence functions. This paper answers in the affirmative and demonstrates that much smaller pretrained models than previously thought can be universal approximators when prefixed. In fact, the attention mechanism is uniquely suited for universal approximation with prefix-tuning a single attention head being sufficient to approximate any continuous function. Moreover, any sequence-to-sequence function can be approximated by prefixing a <b>transformer</b> with depth linear in the sequence length. Beyond these density-type results, we also offer Jackson-type bounds on the length of the prefix needed to approximate a function to a desired precision.</p></p class="citation"></blockquote><h3 id=2260--124299-clifford-steerable-convolutional-neural-networks-maksim-zhdanov-et-al-2024>(22/60 | 124/299) Clifford-Steerable Convolutional Neural Networks (Maksim Zhdanov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maksim Zhdanov, David Ruhe, Maurice Weiler, Ana Lucic, Johannes Brandstetter, Patrick Forré. (2024)<br><strong>Clifford-Steerable Convolutional Neural Networks</strong><br><button class=copy-to-clipboard title="Clifford-Steerable Convolutional Neural Networks" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14730v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14730v1.pdf filename=2402.14730v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Clifford-Steerable <b>Convolutional</b> <b>Neural</b> <b>Networks</b> (CS-CNNs), a novel class of $\mathrm{E}(p, q)$-equivariant <b>CNNs.</b> CS-CNNs process multivector fields on pseudo-Euclidean spaces $\mathbb{R}^{p,q}$. They cover, for instance, $\mathrm{E}(3)$-equivariance on $\mathbb{R}^3$ and Poincar'e-equivariance on Minkowski spacetime $\mathbb{R}^{1,3}$. Our approach is based on an implicit parametrization of $\mathrm{O}(p,q)$-steerable kernels via Clifford group equivariant neural networks. We significantly and consistently outperform baseline methods on fluid dynamics as well as relativistic electrodynamics forecasting tasks.</p></p class="citation"></blockquote><h3 id=2360--125299-opentab-advancing-large-language-models-as-open-domain-table-reasoners-kezhi-kong-et-al-2024>(23/60 | 125/299) OpenTab: Advancing Large Language Models as Open-domain Table Reasoners (Kezhi Kong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kezhi Kong, Jiani Zhang, Zhengyuan Shen, Balasubramaniam Srinivasan, Chuan Lei, Christos Faloutsos, Huzefa Rangwala, George Karypis. (2024)<br><strong>OpenTab: Advancing Large Language Models as Open-domain Table Reasoners</strong><br><button class=copy-to-clipboard title="OpenTab: Advancing Large Language Models as Open-domain Table Reasoners" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14361v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14361v1.pdf filename=2402.14361v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> trained on <b>large</b> <b>volumes</b> <b>of</b> data excel at various natural language tasks, but they cannot handle tasks requiring knowledge that has not been trained on previously. One solution is to use a retriever that fetches relevant information to expand <b>LLM&rsquo;s</b> knowledge scope. However, existing textual-oriented retrieval-based <b>LLMs</b> are not ideal on structured table data due to diversified data modalities and <b>large</b> <b>table</b> <b>sizes.</b> In this work, we propose OpenTab, an open-domain table <b>reasoning</b> framework powered by <b>LLMs.</b> Overall, OpenTab leverages table retriever to fetch relevant tables and then generates SQL programs to parse the retrieved tables efficiently. Utilizing the intermediate data derived from the SQL executions, it conducts grounded inference to produce accurate response. Extensive experimental evaluation shows that OpenTab significantly outperforms baselines in both open- and closed-domain settings, achieving up to 21.5% higher accuracy. We further run ablation studies to validate the efficacy of our proposed designs of the system.</p></p class="citation"></blockquote><h3 id=2460--126299-applying-reinforcement-learning-to-optimize-traffic-light-cycles-seungah-son-et-al-2024>(24/60 | 126/299) Applying Reinforcement Learning to Optimize Traffic Light Cycles (Seungah Son et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seungah Son, Juhee Jin. (2024)<br><strong>Applying Reinforcement Learning to Optimize Traffic Light Cycles</strong><br><button class=copy-to-clipboard title="Applying Reinforcement Learning to Optimize Traffic Light Cycles" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14886v1.pdf filename=2402.14886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Manual optimization of traffic light cycles is a complex and time-consuming task, necessitating the development of automated solutions. In this paper, we propose the application of <b>reinforcement</b> <b>learning</b> to optimize traffic light cycles in real-time. We present a case study using the <b>Simulation</b> Urban Mobility simulator to train a Deep Q-Network algorithm. The experimental results showed 44.16% decrease in the average number of Emergency stops, showing the potential of our approach to reduce traffic congestion and improve traffic flow. Furthermore, we discuss avenues for future research and enhancements to the <b>reinforcement</b> <b>learning</b> model.</p></p class="citation"></blockquote><h3 id=2560--127299-quaternion-recurrent-neural-network-with-real-time-recurrent-learning-and-maximum-correntropy-criterion-pauline-bourigault-et-al-2024>(25/60 | 127/299) Quaternion recurrent neural network with real-time recurrent learning and maximum correntropy criterion (Pauline Bourigault et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pauline Bourigault, Dongpo Xu, Danilo P. Mandic. (2024)<br><strong>Quaternion recurrent neural network with real-time recurrent learning and maximum correntropy criterion</strong><br><button class=copy-to-clipboard title="Quaternion recurrent neural network with real-time recurrent learning and maximum correntropy criterion" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14227v1.pdf filename=2402.14227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We develop a robust quaternion <b>recurrent</b> <b>neural</b> <b>network</b> (QRNN) for real-time processing of 3D and 4D data with outliers. This is achieved by combining the real-time <b>recurrent</b> <b>learning</b> <b>(RTRL)</b> algorithm and the maximum correntropy criterion (MCC) as a loss function. While both the mean square error and maximum correntropy criterion are viable cost functions, it is shown that the non-quadratic maximum correntropy loss function is less sensitive to outliers, making it suitable for applications with multidimensional noisy or uncertain data. Both algorithms are derived based on the novel generalised HR (GHR) calculus, which allows for the differentiation of real functions of quaternion variables and offers the product and chain rules, thus enabling elegant and compact derivations. <b>Simulation</b> results in the context of motion prediction of chest internal markers for lung cancer radiotherapy, which includes regular and irregular breathing sequences, support the analysis.</p></p class="citation"></blockquote><h3 id=2660--128299-betail-behavior-transformer-adversarial-imitation-learning-from-human-racing-gameplay-catherine-weaver-et-al-2024>(26/60 | 128/299) BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human Racing Gameplay (Catherine Weaver et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Catherine Weaver, Chen Tang, Ce Hao, Kenta Kawamoto, Masayoshi Tomizuka, Wei Zhan. (2024)<br><strong>BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human Racing Gameplay</strong><br><button class=copy-to-clipboard title="BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human Racing Gameplay" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-RO, cs.LG<br>Keyword Score: 30<br>Keywords: Distribution Shift, Distribution Shift, Out-of-distribution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14194v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14194v1.pdf filename=2402.14194v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Imitation learning learns a policy from demonstrations without requiring hand-designed reward functions. In many robotic tasks, such as autonomous racing, imitated policies must model complex environment dynamics and human decision-making. Sequence modeling is highly effective in capturing intricate patterns of motion sequences but struggles to adapt to new environments or <b>distribution</b> <b>shifts</b> that are common in real-world robotics tasks. In contrast, Adversarial Imitation Learning (AIL) can mitigate this effect, but struggles with sample inefficiency and handling complex motion patterns. Thus, we propose BeTAIL: Behavior <b>Transformer</b> Adversarial Imitation Learning, which combines a Behavior <b>Transformer</b> (BeT) policy from human demonstrations with online AIL. BeTAIL adds an AIL residual policy to the BeT policy to model the sequential decision-making process of human experts and correct for <b>out-of-distribution</b> states or shifts in environment dynamics. We test BeTAIL on three challenges with expert-level demonstrations of real human gameplay in Gran Turismo Sport. Our proposed residual BeTAIL reduces environment interactions and improves racing performance and stability, even when the BeT is pretrained on different tracks than downstream learning. Videos and code available at: <a href=https://sites.google.com/berkeley.edu/BeTAIL/home>https://sites.google.com/berkeley.edu/BeTAIL/home</a>.</p></p class="citation"></blockquote><h3 id=2760--129299-graph-parsing-networks-yunchong-song-et-al-2024>(27/60 | 129/299) Graph Parsing Networks (Yunchong Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunchong Song, Siyuan Huang, Xinbing Wang, Chenghu Zhou, Zhouhan Lin. (2024)<br><strong>Graph Parsing Networks</strong><br><button class=copy-to-clipboard title="Graph Parsing Networks" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 29<br>Keywords: Graph Classification, Node Classification, Graph, Benchmarking, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14393v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14393v1.pdf filename=2402.14393v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>pooling</b> compresses <b>graph</b> <b>information</b> into a compact representation. State-of-the-art <b>graph</b> <b>pooling</b> methods follow a hierarchical approach, which reduces the <b>graph</b> <b>size</b> step-by-step. These methods must balance memory efficiency with preserving <b>node</b> <b>information,</b> depending on whether they use <b>node</b> <b>dropping</b> or <b>node</b> <b>clustering.</b> Additionally, fixed pooling ratios or numbers of pooling layers are predefined for all <b>graphs,</b> <b>which</b> prevents personalized pooling structures from being captured for each individual <b>graph.</b> <b>In</b> this work, inspired by bottom-up grammar induction, we propose an efficient <b>graph</b> <b>parsing</b> algorithm to infer the pooling structure, which then drives <b>graph</b> <b>pooling.</b> The resulting <b>Graph</b> <b>Parsing</b> Network (GPN) adaptively learns personalized pooling structure for each individual <b>graph.</b> <b>GPN</b> benefits from the discrete assignments generated by the <b>graph</b> <b>parsing</b> algorithm, allowing good memory efficiency while preserving <b>node</b> <b>information</b> intact. Experimental results on standard <b>benchmarks</b> demonstrate that GPN outperforms state-of-the-art <b>graph</b> <b>pooling</b> methods in <b>graph</b> <b>classification</b> tasks while being able to achieve competitive performance in <b>node</b> <b>classification</b> tasks. We also conduct a <b>graph</b> <b>reconstruction</b> task to show GPN&rsquo;s ability to preserve <b>node</b> <b>information</b> and measure both memory and time efficiency through relevant tests.</p></p class="citation"></blockquote><h3 id=2860--130299-securing-transactions-a-hybrid-dependable-ensemble-machine-learning-model-using-iht-lr-and-grid-search-md-alamin-talukder-et-al-2024>(28/60 | 130/299) Securing Transactions: A Hybrid Dependable Ensemble Machine Learning Model using IHT-LR and Grid Search (Md. Alamin Talukder et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md. Alamin Talukder, Rakib Hossen, Md Ashraf Uddin, Mohammed Nasir Uddin, Uzzal Kumar Acharjee. (2024)<br><strong>Securing Transactions: A Hybrid Dependable Ensemble Machine Learning Model using IHT-LR and Grid Search</strong><br><button class=copy-to-clipboard title="Securing Transactions: A Hybrid Dependable Ensemble Machine Learning Model using IHT-LR and Grid Search" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-fin-GN<br>Keyword Score: 23<br>Keywords: Benchmarking, Logistic Regression, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14389v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14389v1.pdf filename=2402.14389v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Financial institutions and businesses face an ongoing challenge from fraudulent transactions, <b>prompting</b> the need for effective detection methods. Detecting credit card fraud is crucial for identifying and preventing unauthorized transactions.Timely detection of fraud enables investigators to take swift actions to mitigate further losses. However, the investigation process is often time-consuming, limiting the number of alerts that can be thoroughly examined each day. Therefore, the primary objective of a fraud detection model is to provide accurate alerts while minimizing false alarms and missed fraud cases. In this paper, we introduce a state-of-the-art hybrid ensemble (ENS) dependable Machine learning (ML) model that intelligently combines multiple algorithms with proper weighted optimization using Grid search, including Decision Tree (DT), Random Forest (RF), K-Nearest Neighbor (KNN), and Multilayer Perceptron (MLP), to enhance fraud identification. To address the data imbalance issue, we employ the Instant Hardness Threshold (IHT) technique in conjunction with <b>Logistic</b> <b>Regression</b> (LR), surpassing conventional approaches. Our experiments are conducted on a publicly available credit card dataset comprising 284,807 transactions. The proposed model achieves impressive accuracy rates of 99.66%, 99.73%, 98.56%, and 99.79%, and a perfect 100% for the DT, RF, KNN, MLP and ENS models, respectively. The hybrid ensemble model outperforms existing works, establishing a new <b>benchmark</b> for detecting fraudulent transactions in high-frequency scenarios. The results highlight the effectiveness and reliability of our approach, demonstrating superior performance metrics and showcasing its exceptional potential for real-world fraud detection applications.</p></p class="citation"></blockquote><h3 id=2960--131299-probabilistically-sound-beam-search-with-masked-language-models-charlie-cowen-breen-et-al-2024>(29/60 | 131/299) Probabilistically-sound beam search with masked language models (Charlie Cowen-Breen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Charlie Cowen-Breen, Creston Brooks, Robert Calef, Anna Sappington. (2024)<br><strong>Probabilistically-sound beam search with masked language models</strong><br><button class=copy-to-clipboard title="Probabilistically-sound beam search with masked language models" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Masked Language Model, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15020v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15020v1.pdf filename=2402.15020v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Beam search with <b>masked</b> <b>language</b> <b>models</b> <b>(MLMs)</b> is challenging in part because joint probability distributions over sequences are not readily available, unlike for autoregressive models. Nevertheless, estimating such distributions has applications in many domains, including protein engineering and ancient text restoration. We present probabilistically-sound methods for beam search with <b>MLMs.</b> First, we clarify the conditions under which it is theoretically sound to perform text infilling with <b>MLMs</b> using standard beam search. When these conditions fail, we provide a probabilistically-sound modification with no additional computational complexity and demonstrate that it is superior to the aforementioned beam search in the expected conditions. We then present empirical results comparing several infilling approaches with <b>MLMs</b> across several domains.</p></p class="citation"></blockquote><h3 id=3060--132299-consistency-guided-temperature-scaling-using-style-and-content-information-for-out-of-domain-calibration-wonjeong-choi-et-al-2024>(30/60 | 132/299) Consistency-Guided Temperature Scaling Using Style and Content Information for Out-of-Domain Calibration (Wonjeong Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wonjeong Choi, Jungwuk Park, Dong-Jun Han, Younghyun Park, Jaekyun Moon. (2024)<br><strong>Consistency-Guided Temperature Scaling Using Style and Content Information for Out-of-Domain Calibration</strong><br><button class=copy-to-clipboard title="Consistency-Guided Temperature Scaling Using Style and Content Information for Out-of-Domain Calibration" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Out-of-domain, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15019v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15019v1.pdf filename=2402.15019v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Research interests in the robustness of deep neural networks against domain shifts have been rapidly increasing in recent years. Most existing works, however, focus on improving the accuracy of the model, not the calibration performance which is another important requirement for trustworthy AI systems. Temperature scaling (TS), an accuracy-preserving post-hoc calibration method, has been proven to be effective in in-domain settings, but not in <b>out-of-domain</b> (OOD) due to the difficulty in obtaining a validation set for the unseen domain beforehand. In this paper, we propose consistency-guided temperature scaling (CTS), a new temperature scaling strategy that can significantly enhance the OOD calibration performance by providing mutual supervision among data samples in the source domains. Motivated by our observation that over-confidence <b>stemming</b> from inconsistent sample predictions is the main obstacle to OOD calibration, we propose to guide the scaling process by taking consistencies into account in terms of two different aspects &ndash; style and content &ndash; which are the key components that can well-represent data samples in multi-domain settings. Experimental results demonstrate that our proposed strategy outperforms existing works, achieving superior OOD calibration performance on various datasets. This can be accomplished by employing only the source domains without compromising accuracy, making our scheme directly applicable to various trustworthy AI systems.</p></p class="citation"></blockquote><h3 id=3160--133299-optimizing-language-models-for-human-preferences-is-a-causal-inference-problem-victoria-lin-et-al-2024>(31/60 | 133/299) Optimizing Language Models for Human Preferences is a Causal Inference Problem (Victoria Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victoria Lin, Eli Ben-Michael, Louis-Philippe Morency. (2024)<br><strong>Optimizing Language Models for Human Preferences is a Causal Inference Problem</strong><br><button class=copy-to-clipboard title="Optimizing Language Models for Human Preferences is a Causal Inference Problem" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG, stat-ME<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14979v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14979v1.pdf filename=2402.14979v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences. In this paper, we present an initial exploration of language model optimization for human preferences from direct outcome datasets, where each sample consists of a text and an associated numerical outcome measuring the reader&rsquo;s response. We first propose that language model optimization should be viewed as a causal problem to ensure that the model correctly learns the relationship between the text and the outcome. We formalize this causal language optimization problem, and we develop a method&ndash;causal preference optimization (CPO)&ndash;that solves an unbiased surrogate objective for the problem. We further extend CPO with doubly robust CPO (DR-CPO), which reduces the variance of the surrogate objective while retaining provably strong guarantees on bias. Finally, we empirically demonstrate the effectiveness of (DR-)CPO in optimizing state-of-the-art <b>LLMs</b> for human preferences on direct outcome data, and we validate the robustness of DR-CPO under difficult confounding conditions.</p></p class="citation"></blockquote><h3 id=3260--134299-federated-fairness-without-access-to-sensitive-groups-afroditi-papadaki-et-al-2024>(32/60 | 134/299) Federated Fairness without Access to Sensitive Groups (Afroditi Papadaki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Afroditi Papadaki, Natalia Martinez, Martin Bertran, Guillermo Sapiro, Miguel Rodrigues. (2024)<br><strong>Federated Fairness without Access to Sensitive Groups</strong><br><button class=copy-to-clipboard title="Federated Fairness without Access to Sensitive Groups" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-DC, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fairness, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14929v1.pdf filename=2402.14929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current approaches to group <b>fairness</b> in <b>federated</b> <b>learning</b> assume the existence of predefined and labeled sensitive groups during training. However, due to factors ranging from emerging regulations to dynamics and location-dependency of protected groups, this assumption may be unsuitable in many real-world scenarios. In this work, we propose a new approach to guarantee group <b>fairness</b> that does not rely on any predefined definition of sensitive groups or additional labels. Our objective allows the federation to learn a Pareto efficient global model ensuring worst-case group <b>fairness</b> and it enables, via a single hyper-parameter, trade-offs between <b>fairness</b> and utility, subject only to a group size constraint. This implies that any sufficiently large subset of the population is guaranteed to receive at least a minimum level of utility performance from the model. The proposed objective encompasses existing approaches as special cases, such as empirical risk minimization and subgroup robustness objectives from centralized machine learning. We provide an algorithm to solve this problem in federation that enjoys convergence and excess risk guarantees. Our empirical results indicate that the proposed approach can effectively improve the worst-performing group that may be present without unnecessarily hurting the average performance, exhibits superior or comparable performance to relevant baselines, and achieves a large set of solutions with different <b>fairness-utility</b> trade-offs.</p></p class="citation"></blockquote><h3 id=3360--135299-bandits-with-abstention-under-expert-advice-stephen-pasteris-et-al-2024>(33/60 | 135/299) Bandits with Abstention under Expert Advice (Stephen Pasteris et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephen Pasteris, Alberto Rumi, Maximilian Thiessen, Shota Saito, Atsushi Miyauchi, Fabio Vitale, Mark Herbster. (2024)<br><strong>Bandits with Abstention under Expert Advice</strong><br><button class=copy-to-clipboard title="Bandits with Abstention under Expert Advice" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14585v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14585v1.pdf filename=2402.14585v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the classic problem of prediction with expert advice under <b>bandit</b> <b>feedback.</b> Our model assumes that one action, corresponding to the learner&rsquo;s abstention from play, has no reward or loss on every trial. We propose the CBA algorithm, which exploits this assumption to obtain reward bounds that can significantly improve those of the classical Exp4 algorithm. We can view our problem as the aggregation of confidence-rated predictors when the learner has the option of abstention from play. Importantly, we are the first to achieve bounds on the expected cumulative reward for general confidence-rated predictors. In the special case of specialists we achieve a novel reward bound, significantly improving previous bounds of SpecialistExp (treating abstention as another action). As an example application, we discuss learning unions of balls in a finite metric space. In this contextual setting, we devise an efficient implementation of CBA, reducing the runtime from quadratic to almost linear in the number of contexts. Preliminary experiments show that CBA improves over existing <b>bandit</b> <b>algorithms.</b></p></p class="citation"></blockquote><h3 id=3460--136299-dyngma-a-robust-approach-for-learning-stochastic-differential-equations-from-data-aiqing-zhu-et-al-2024>(34/60 | 136/299) DynGMA: a robust approach for learning stochastic differential equations from data (Aiqing Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aiqing Zhu, Qianxiao Li. (2024)<br><strong>DynGMA: a robust approach for learning stochastic differential equations from data</strong><br><button class=copy-to-clipboard title="DynGMA: a robust approach for learning stochastic differential equations from data" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NA, cs.LG, math-NA, physics-comp-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14475v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14475v1.pdf filename=2402.14475v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning unknown stochastic differential equations (SDEs) from observed data is a significant and challenging task with applications in various fields. Current approaches often use neural networks to represent drift and diffusion functions, and construct likelihood-based loss by approximating the transition density to train these networks. However, these methods often rely on one-step stochastic numerical schemes, necessitating data with sufficiently high time resolution. In this paper, we introduce novel approximations to the transition density of the parameterized SDE: a Gaussian density approximation inspired by the random perturbation theory of dynamical systems, and its extension, the dynamical Gaussian mixture approximation (DynGMA). Benefiting from the robust density approximation, our method exhibits superior accuracy compared to baseline methods in learning the fully unknown drift and diffusion functions and computing the invariant distribution from trajectory data. And it is capable of handling trajectory data with low time resolution and variable, even uncontrollable, time step sizes, such as data generated from Gillespie&rsquo;s stochastic <b>simulations.</b> We then conduct several experiments across various scenarios to verify the advantages and robustness of the proposed method.</p></p class="citation"></blockquote><h3 id=3560--137299-global-safe-sequential-learning-via-efficient-knowledge-transfer-cen-you-li-et-al-2024>(35/60 | 137/299) Global Safe Sequential Learning via Efficient Knowledge Transfer (Cen-You Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cen-You Li, Olaf Duennbier, Marc Toussaint, Barbara Rakitsch, Christoph Zimmer. (2024)<br><strong>Global Safe Sequential Learning via Efficient Knowledge Transfer</strong><br><button class=copy-to-clipboard title="Global Safe Sequential Learning via Efficient Knowledge Transfer" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Active Learning, Knowledge Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14402v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14402v1.pdf filename=2402.14402v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sequential learning methods such as <b>active</b> <b>learning</b> and Bayesian optimization select the most informative data to learn about a task. In many medical or engineering applications, the data selection is constrained by a priori unknown safety conditions. A promissing line of safe learning methods utilize Gaussian processes (GPs) to model the safety probability and perform data selection in areas with high safety confidence. However, accurate safety modeling requires prior <b>knowledge</b> <b>or</b> consumes data. In addition, the safety confidence centers around the given observations which leads to local exploration. As transferable source <b>knowledge</b> <b>is</b> often available in safety critical experiments, we propose to consider transfer safe sequential learning to accelerate the learning of safety. We further consider a pre-computation of source components to reduce the additional computational load that is introduced by incorporating source data. In this paper, we theoretically analyze the maximum explorable safe regions of conventional safe learning methods. Furthermore, we empirically demonstrate that our approach 1) learns a task with lower data consumption, 2) globally explores multiple disjoint safe regions under guidance of the source <b>knowledge,</b> <b>and</b> 3) operates with computation comparable to conventional safe learning methods.</p></p class="citation"></blockquote><h3 id=3660--138299-representation-learning-for-frequent-subgraph-mining-rex-ying-et-al-2024>(36/60 | 138/299) Representation Learning for Frequent Subgraph Mining (Rex Ying et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rex Ying, Tianyu Fu, Andrew Wang, Jiaxuan You, Yu Wang, Jure Leskovec. (2024)<br><strong>Representation Learning for Frequent Subgraph Mining</strong><br><button class=copy-to-clipboard title="Representation Learning for Frequent Subgraph Mining" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 18<br>Keywords: Graph, Graph Neural Network, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14367v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14367v1.pdf filename=2402.14367v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying frequent subgraphs, also called network motifs, is crucial in analyzing and predicting properties of real-world networks. However, finding large commonly-occurring motifs remains a challenging problem not only due to its NP-hard subroutine of subgraph counting, but also the exponential growth of the number of possible subgraphs patterns. Here we present Subgraph Pattern Miner (SPMiner), a novel neural approach for approximately finding frequent subgraphs in a large target <b>graph.</b> <b>SPMiner</b> <b>combines</b> <b>graph</b> <b>neural</b> <b>networks,</b> order embedding space, and an efficient search strategy to identify network subgraph patterns that appear most frequently in the target <b>graph.</b> <b>SPMiner</b> <b>first</b> decomposes the target <b>graph</b> <b>into</b> <b>many</b> overlapping subgraphs and then encodes each subgraph into an order embedding space. SPMiner then uses a monotonic walk in the order embedding space to identify frequent motifs. Compared to existing approaches and possible neural alternatives, SPMiner is more accurate, faster, and more scalable. For 5- and 6-node motifs, we show that SPMiner can almost perfectly identify the most frequent motifs while being 100x faster than exact enumeration methods. In addition, SPMiner can also reliably identify frequent 10-node motifs, which is well beyond the size limit of exact enumeration approaches. And last, we show that SPMiner can find large up to 20 node motifs with 10-100x higher frequency than those found by current approximate methods.</p></p class="citation"></blockquote><h3 id=3760--139299-rao-blackwellising-bayesian-causal-inference-christian-toth-et-al-2024>(37/60 | 139/299) Rao-Blackwellising Bayesian Causal Inference (Christian Toth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Toth, Christian Knoll, Franz Pernkopf, Robert Peharz. (2024)<br><strong>Rao-Blackwellising Bayesian Causal Inference</strong><br><button class=copy-to-clipboard title="Rao-Blackwellising Bayesian Causal Inference" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ME, stat-ML<br>Keyword Score: 16<br>Keywords: Graph, Benchmarking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14781v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14781v1.pdf filename=2402.14781v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bayesian causal inference, i.e., inferring a posterior over causal models for the use in downstream causal <b>reasoning</b> tasks, poses a hard computational inference problem that is little explored in literature. In this work, we combine techniques from order-based MCMC structure learning with recent advances in gradient-based <b>graph</b> learning into an effective Bayesian causal inference framework. Specifically, we decompose the problem of inferring the causal structure into (i) inferring a topological order over variables and (ii) inferring the parent sets for each variable. When limiting the number of parents per variable, we can exactly marginalise over the parent sets in polynomial time. We further use Gaussian processes to model the unknown causal mechanisms, which also allows their exact marginalisation. This introduces a Rao-Blackwellization scheme, where all components are eliminated from the model, except for the causal order, for which we learn a distribution via gradient-based optimisation. The combination of Rao-Blackwellization with our sequential inference procedure for causal orders yields state-of-the-art on linear and non-linear additive noise <b>benchmarks</b> with scale-free and Erdos-Renyi <b>graph</b> structures.</p></p class="citation"></blockquote><h3 id=3860--140299-stable-neural-stochastic-differential-equations-in-analyzing-irregular-time-series-data-yongkyung-oh-et-al-2024>(38/60 | 140/299) Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data (YongKyung Oh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>YongKyung Oh, Dongyoung Lim, Sungil Kim. (2024)<br><strong>Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data</strong><br><button class=copy-to-clipboard title="Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14989v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14989v1.pdf filename=2402.14989v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs&rsquo; performance. In this study, we propose three stable classes of Neural SDEs: Langevin-type SDE, Linear Noise SDE, and Geometric SDE. Then, we rigorously demonstrate their robustness in maintaining excellent performance under <b>distribution</b> <b>shift,</b> while effectively preventing overfitting. To assess the effectiveness of our approach, we conduct extensive experiments on four <b>benchmark</b> datasets for interpolation, forecasting, and classification tasks, and analyze the robustness of our methods with 30 public datasets under different missing rates. Our results demonstrate the efficacy of the proposed method in handling real-world irregular time series data.</p></p class="citation"></blockquote><h3 id=3960--141299-boosting-gets-full-attention-for-relational-learning-mathieu-guillame-bert-et-al-2024>(39/60 | 141/299) Boosting gets full Attention for Relational Learning (Mathieu Guillame-Bert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mathieu Guillame-Bert, Richard Nock. (2024)<br><strong>Boosting gets full Attention for Relational Learning</strong><br><button class=copy-to-clipboard title="Boosting gets full Attention for Relational Learning" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-6, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14926v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14926v1.pdf filename=2402.14926v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>More often than not in <b>benchmark</b> <b>supervised</b> ML, tabular data is flat, i.e. consists of a single $m \times d$ (rows, columns) file, but cases abound in the real world where observations are described by a set of tables with structural relationships. Neural nets-based deep models are a classical fit to incorporate general topological dependence among description features (pixels, words, etc.), but their suboptimality to tree-based models on tabular data is still well documented. In this paper, we introduce an attention mechanism for structured data that blends well with tree-based models in the training context of (gradient) boosting. Each aggregated model is a tree whose training involves two steps: first, simple tabular models are learned descending tables in a top-down fashion with boosting&rsquo;s class residuals on tables&rsquo; features. Second, what has been learned progresses back bottom-up via attention and aggregation mechanisms, progressively crafting new features that complete at the end the set of observation features over which a single tree is learned, boosting&rsquo;s iteration clock is incremented and new class residuals are computed. Experiments on simulated and real-world domains display the competitiveness of our method against a state of the art containing both tree-based and neural nets-based models.</p></p class="citation"></blockquote><h3 id=4060--142299-imbalanced-data-clustering-using-equilibrium-k-means-yudong-he-2024>(40/60 | 142/299) Imbalanced Data Clustering using Equilibrium K-Means (Yudong He, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yudong He. (2024)<br><strong>Imbalanced Data Clustering using Equilibrium K-Means</strong><br><button class=copy-to-clipboard title="Imbalanced Data Clustering using Equilibrium K-Means" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 13<br>Keywords: MNIST, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14490v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14490v1.pdf filename=2402.14490v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Imbalanced data, characterized by an unequal distribution of data points across different clusters, poses a challenge for traditional hard and fuzzy <b>clustering</b> algorithms, such as hard K-means (HKM, or Lloyd&rsquo;s algorithm) and fuzzy K-means (FKM, or Bezdek&rsquo;s algorithm). This paper introduces equilibrium K-means (EKM), a novel and simple K-means-type algorithm that alternates between just two steps, yielding significantly improved <b>clustering</b> results for imbalanced data by reducing the tendency of centroids to crowd together in the center of large clusters. We also present a unifying perspective for HKM, FKM, and EKM, showing they are essentially gradient descent algorithms with an explicit relationship to Newton&rsquo;s method. EKM has the same time and space complexity as FKM but offers a clearer physical meaning for its membership definition. We illustrate the performance of EKM on two synthetic and ten real datasets, comparing it to various <b>clustering</b> algorithms, including HKM, FKM, maximum-entropy fuzzy <b>clustering,</b> two FKM variations designed for imbalanced data, and the Gaussian mixture model. The results demonstrate that EKM performs competitively on balanced data while significantly outperforming other techniques on imbalanced data. For high-dimensional data <b>clustering,</b> we demonstrate that a more discriminative representation can be obtained by mapping high-dimensional data via deep neural networks into a low-dimensional, EKM-friendly space. Deep <b>clustering</b> with EKM improves <b>clustering</b> accuracy by 35% on an imbalanced dataset derived from <b>MNIST</b> compared to deep <b>clustering</b> based on HKM.</p></p class="citation"></blockquote><h3 id=4160--143299-comparing-graph-transformers-via-positional-encodings-mitchell-black-et-al-2024>(41/60 | 143/299) Comparing Graph Transformers via Positional Encodings (Mitchell Black et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mitchell Black, Zhengchao Wan, Gal Mishne, Amir Nayyeri, Yusu Wang. (2024)<br><strong>Comparing Graph Transformers via Positional Encodings</strong><br><button class=copy-to-clipboard title="Comparing Graph Transformers via Positional Encodings" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14202v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14202v1.pdf filename=2402.14202v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The distinguishing power of <b>graph</b> <b>transformers</b> is closely tied to the choice of positional encoding: features used to augment the base <b>transformer</b> with information about the <b>graph.</b> There are two primary types of positional encoding: absolute positional encodings (APEs) and relative positional encodings (RPEs). APEs assign features to each node and are given as input to the <b>transformer.</b> RPEs instead assign a feature to each pair of nodes, e.g., <b>graph</b> distance, and are used to augment the attention block. A priori, it is unclear which method is better for maximizing the power of the resulting <b>graph</b> <b>transformer.</b> In this paper, we aim to understand the relationship between these different types of positional encodings. Interestingly, we show that <b>graph</b> <b>transformers</b> using APEs and RPEs are equivalent in terms of distinguishing power. In particular, we demonstrate how to interchange APEs and RPEs while maintaining their distinguishing power in terms of <b>graph</b> <b>transformers.</b> Based on our theoretical results, we provide a study on several APEs and RPEs (including the resistance distance and the recently introduced stable and expressive positional encoding (SPE)) and compare their distinguishing power in terms of <b>transformers.</b> We believe our work will help navigate the huge number of choices of positional encoding and will provide guidance on the future design of positional encodings for <b>graph</b> <b>transformers.</b></p></p class="citation"></blockquote><h3 id=4260--144299-privacy-enhancing-collaborative-information-sharing-through-federated-learning----a-case-of-the-insurance-industry-panyi-dong-et-al-2024>(42/60 | 144/299) Privacy-Enhancing Collaborative Information Sharing through Federated Learning &ndash; A Case of the Insurance Industry (Panyi Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Panyi Dong, Zhiyu Quan, Brandon Edwards, Shih-han Wang, Runhuan Feng, Tianyang Wang, Patrick Foley, Prashant Shah. (2024)<br><strong>Privacy-Enhancing Collaborative Information Sharing through Federated Learning &ndash; A Case of the Insurance Industry</strong><br><button class=copy-to-clipboard title="Privacy-Enhancing Collaborative Information Sharing through Federated Learning -- A Case of the Insurance Industry" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG, q-fin-RM<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14983v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14983v1.pdf filename=2402.14983v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The report demonstrates the benefits (in terms of improved claims loss modeling) of harnessing the value of <b>Federated</b> <b>Learning</b> (FL) to learn a single model across multiple insurance industry datasets without requiring the datasets themselves to be shared from one company to another. The application of FL addresses two of the most pressing concerns: limited data volume and data variety, which are caused by privacy concerns, the rarity of claim events, the lack of informative rating factors, etc.. During each round of FL, collaborators compute improvements on the model using their local private data, and these insights are combined to update a global model. Such aggregation of insights allows for an increase to the effectiveness in forecasting claims losses compared to models individually trained at each collaborator. Critically, this approach enables machine learning collaboration without the need for raw data to leave the compute infrastructure of each respective data owner. Additionally, the open-source framework, OpenFL, that is used in our experiments is designed so that it can be run using confidential computing as well as with additional algorithmic protections against leakage of information via the shared model updates. In such a way, FL is implemented as a privacy-enhancing collaborative learning technique that addresses the challenges posed by the sensitivity and privacy of data in traditional machine learning solutions. This paper&rsquo;s application of FL can also be expanded to other areas including fraud detection, catastrophe modeling, etc., that have a similar need to incorporate data privacy into machine learning collaborations. Our framework and empirical results provide a foundation for future collaborations among insurers, regulators, academic researchers, and InsurTech experts.</p></p class="citation"></blockquote><h3 id=4360--145299-enhancing-power-quality-event-classification-with-ai-transformer-models-ahmad-mohammad-saber-et-al-2024>(43/60 | 145/299) Enhancing Power Quality Event Classification with AI Transformer Models (Ahmad Mohammad Saber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmad Mohammad Saber, Amr Youssef, Davor Svetinovic, Hatem Zeineldin, Deepa Kundur, Ehab El-Saadany. (2024)<br><strong>Enhancing Power Quality Event Classification with AI Transformer Models</strong><br><button class=copy-to-clipboard title="Enhancing Power Quality Event Classification with AI Transformer Models" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14949v1.pdf filename=2402.14949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been a growing interest in utilizing machine learning for accurate classification of power quality events (PQEs). However, most of these studies are performed assuming an ideal situation, while in reality, we can have measurement noise, DC offset, and variations in the voltage signal&rsquo;s amplitude and frequency. Building on the prior PQE classification works using deep learning, this paper proposes a deep-learning framework that leverages attention-enabled <b>Transformers</b> as a tool to accurately classify PQEs under the aforementioned considerations. The proposed framework can operate directly on the voltage signals with no need for a separate feature extraction or calculation phase. Our results show that the proposed framework outperforms recently proposed learning-based techniques. It can accurately classify PQEs under the aforementioned conditions with an accuracy varying between 99.81%$-$91.43% depending on the signal-to-noise ratio, DC offsets, and variations in the signal amplitude and frequency.</p></p class="citation"></blockquote><h3 id=4460--146299-on-the-curses-of-future-and-history-in-future-dependent-value-functions-for-off-policy-evaluation-yuheng-zhang-et-al-2024>(44/60 | 146/299) On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation (Yuheng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuheng Zhang, Nan Jiang. (2024)<br><strong>On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation</strong><br><button class=copy-to-clipboard title="On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14703v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14703v1.pdf filename=2402.14703v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study off-policy evaluation (OPE) in partially observable environments with complex observations, with the goal of developing estimators whose guarantee avoids exponential dependence on the horizon. While such estimators exist for <b>MDPs</b> and POMDPs can be converted to history-based <b>MDPs,</b> their estimation errors depend on the state-density ratio for <b>MDPs</b> which becomes history ratios after conversion, an exponential object. Recently, Uehara et al. (2022) proposed future-dependent value functions as a promising framework to address this issue, where the guarantee for memoryless policies depends on the density ratio over the latent state space. However, it also depends on the boundedness of the future-dependent value function and other related quantities, which we show could be exponential-in-length and thus erasing the advantage of the method. In this paper, we discover novel coverage assumptions tailored to the structure of POMDPs, such as outcome coverage and belief coverage. These assumptions not only enable polynomial bounds on the aforementioned quantities, but also lead to the discovery of new algorithms with complementary properties.</p></p class="citation"></blockquote><h3 id=4560--147299-bayesian-off-policy-evaluation-and-learning-for-large-action-spaces-imad-aouali-et-al-2024>(45/60 | 147/299) Bayesian Off-Policy Evaluation and Learning for Large Action Spaces (Imad Aouali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Imad Aouali, Victor-Emmanuel Brunel, David Rohde, Anna Korba. (2024)<br><strong>Bayesian Off-Policy Evaluation and Learning for Large Action Spaces</strong><br><button class=copy-to-clipboard title="Bayesian Off-Policy Evaluation and Learning for Large Action Spaces" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14664v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14664v1.pdf filename=2402.14664v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In interactive systems, actions are often correlated, presenting an opportunity for more sample-efficient off-policy evaluation (OPE) and learning (OPL) in large action spaces. We introduce a unified Bayesian framework to capture these correlations through structured and informative priors. In this framework, we propose sDM, a generic Bayesian approach designed for OPE and OPL, grounded in both algorithmic and theoretical foundations. Notably, sDM leverages action correlations without compromising computational efficiency. Moreover, inspired by online Bayesian <b>bandits,</b> we introduce Bayesian metrics that assess the average performance of algorithms across multiple problem instances, deviating from the conventional worst-case assessments. We analyze sDM in OPE and OPL, highlighting the benefits of leveraging action correlations. Empirical evidence showcases the strong performance of sDM.</p></p class="citation"></blockquote><h3 id=4660--148299-federated-learning-on-transcriptomic-data-model-quality-and-performance-trade-offs-anika-hannemann-et-al-2024>(46/60 | 148/299) Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs (Anika Hannemann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anika Hannemann, Jan Ewald, Leo Seeger, Erik Buchmann. (2024)<br><strong>Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs</strong><br><button class=copy-to-clipboard title="Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG, q-bio-GN<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14527v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14527v1.pdf filename=2402.14527v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning on large-scale genomic or transcriptomic data is important for many novel health applications. For example, precision medicine tailors medical treatments to patients on the basis of individual biomarkers, cellular and molecular states, etc. However, the data required is sensitive, voluminous, heterogeneous, and typically distributed across locations where dedicated machine learning hardware is not available. Due to privacy and regulatory reasons, it is also problematic to aggregate all data at a trusted third party.Federated learning is a promising solution to this dilemma, because it enables decentralized, collaborative machine learning without exchanging raw data. In this paper, we perform comparative experiments with the <b>federated</b> <b>learning</b> frameworks TensorFlow <b>Federated</b> <b>and</b> Flower. Our test case is the training of disease prognosis and cell type classification models. We train the models with distributed transcriptomic data, considering both data heterogeneity and architectural heterogeneity. We measure model quality, robustness against privacy-enhancing noise, computational performance and resource overhead. Each of the <b>federated</b> <b>learning</b> frameworks has different strengths. However, our experiments confirm that both frameworks can readily build models on transcriptomic data, without transferring personal raw data to a third party with abundant computational resources.</p></p class="citation"></blockquote><h3 id=4760--149299-towards-automated-causal-discovery-a-case-study-on-5g-telecommunication-data-konstantina-biza-et-al-2024>(47/60 | 149/299) Towards Automated Causal Discovery: a case study on 5G telecommunication data (Konstantina Biza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Konstantina Biza, Antonios Ntroumpogiannis, Sofia Triantafillou, Ioannis Tsamardinos. (2024)<br><strong>Towards Automated Causal Discovery: a case study on 5G telecommunication data</strong><br><button class=copy-to-clipboard title="Towards Automated Causal Discovery: a case study on 5G telecommunication data" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ME<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14481v1.pdf filename=2402.14481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the concept of Automated Causal Discovery (AutoCD), defined as any system that aims to fully automate the application of causal discovery and causal <b>reasoning</b> methods. AutoCD&rsquo;s goal is to deliver all causal information that an expert human analyst would and answer a user&rsquo;s causal queries. We describe the architecture of such a platform, and illustrate its performance on synthetic data sets. As a case study, we apply it on temporal telecommunication data. The system is general and can be applied to a plethora of causal discovery problems.</p></p class="citation"></blockquote><h3 id=4860--150299-text-me-the-data-generating-ground-pressure-sequence-from-textual-descriptions-for-har-lala-shakti-swarup-ray-et-al-2024>(48/60 | 150/299) Text me the data: Generating Ground Pressure Sequence from Textual Descriptions for HAR (Lala Shakti Swarup Ray et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lala Shakti Swarup Ray, Bo Zhou, Sungho Suh, Lars Krupp, Vitor Fortes Rey, Paul Lukowicz. (2024)<br><strong>Text me the data: Generating Ground Pressure Sequence from Textual Descriptions for HAR</strong><br><button class=copy-to-clipboard title="Text me the data: Generating Ground Pressure Sequence from Textual Descriptions for HAR" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG, eess-SP<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14427v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14427v1.pdf filename=2402.14427v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In human activity recognition (HAR), the availability of substantial ground truth is necessary for training efficient models. However, acquiring ground pressure data through physical sensors itself can be cost-prohibitive, time-consuming. To address this critical need, we introduce Text-to-Pressure (T2P), a framework designed to generate extensive ground pressure sequences from textual descriptions of human activities using deep learning techniques. We show that the combination of vector <b>quantization</b> of sensor data along with simple text conditioned auto regressive strategy allows us to obtain high-quality generated pressure sequences from textual descriptions with the help of discrete latent correlation between text and pressure maps. We achieved comparable performance on the consistency between text and generated motion with an R squared value of 0.722, Masked R squared value of 0.892, and FID score of 1.83. Additionally, we trained a HAR model with the the synthesized data and evaluated it on pressure dynamics collected by a real pressure sensor which is on par with a model trained on only real data. Combining both real and synthesized training data increases the overall macro F1 score by 5.9 percent.</p></p class="citation"></blockquote><h3 id=4960--151299-hyperfast-instant-classification-for-tabular-data-david-bonet-et-al-2024>(49/60 | 151/299) HyperFast: Instant Classification for Tabular Data (David Bonet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Bonet, Daniel Mas Montserrat, Xavier Giró-i-Nieto, Alexander G. Ioannidis. (2024)<br><strong>HyperFast: Instant Classification for Tabular Data</strong><br><button class=copy-to-clipboard title="HyperFast: Instant Classification for Tabular Data" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14335v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14335v1.pdf filename=2402.14335v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training deep learning models and performing hyperparameter tuning can be computationally demanding and time-consuming. Meanwhile, traditional machine learning methods like gradient-boosting algorithms remain the preferred choice for most tabular data applications, while neural network alternatives require extensive hyperparameter tuning or work only in toy datasets under limited settings. In this paper, we introduce HyperFast, a meta-trained hypernetwork designed for instant classification of tabular data in a single forward pass. HyperFast generates a task-specific neural network tailored to an unseen dataset that can be directly used for classification inference, removing the need for training a model. We report extensive experiments with OpenML and genomic data, comparing HyperFast to competing tabular data neural networks, traditional ML methods, AutoML systems, and boosting machines. HyperFast shows highly competitive results, while being significantly faster. Additionally, our approach demonstrates robust adaptability across a variety of classification tasks with little to no <b>fine-tuning,</b> positioning HyperFast as a strong solution for numerous applications and rapid model deployment. HyperFast introduces a promising paradigm for fast classification, with the potential to substantially decrease the computational burden of deep learning. Our code, which offers a scikit-learn-like interface, along with the trained HyperFast model, can be found at <a href=https://github.com/AI-sandbox/HyperFast>https://github.com/AI-sandbox/HyperFast</a>.</p></p class="citation"></blockquote><h3 id=5060--152299-deep-generative-model-based-synthesis-of-four-bar-linkage-mechanisms-with-target-conditions-sumin-lee-et-al-2024>(50/60 | 152/299) Deep Generative Model-based Synthesis of Four-bar Linkage Mechanisms with Target Conditions (Sumin Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sumin Lee, Jihoon Kim, Namwoo Kang. (2024)<br><strong>Deep Generative Model-based Synthesis of Four-bar Linkage Mechanisms with Target Conditions</strong><br><button class=copy-to-clipboard title="Deep Generative Model-based Synthesis of Four-bar Linkage Mechanisms with Target Conditions" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CE, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14882v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14882v1.pdf filename=2402.14882v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mechanisms are essential components designed to perform specific tasks in various mechanical systems. However, designing a mechanism that satisfies certain kinematic or quasi-static requirements is a challenging task. The kinematic requirements may include the workspace of a mechanism, while the quasi-static requirements of a mechanism may include its torque transmission, which refers to the ability of the mechanism to transfer power and torque effectively. In this paper, we propose a deep learning-based <b>generative</b> <b>model</b> <b>for</b> generating multiple crank-rocker four-bar linkage mechanisms that satisfy both the kinematic and quasi-static requirements aforementioned. The proposed model is based on a conditional <b>generative</b> <b>adversarial</b> <b>network</b> (cGAN) with modifications for mechanism synthesis, which is trained to learn the relationship between the requirements of a mechanism with respect to linkage lengths. The results demonstrate that the proposed model successfully generates multiple distinct mechanisms that satisfy specific kinematic and quasi-static requirements. To evaluate the novelty of our approach, we provide a comparison of the samples synthesized by the proposed cGAN, traditional cVAE and NSGA-II. Our approach has several advantages over traditional design methods. It enables designers to efficiently generate multiple diverse and feasible design candidates while exploring a large design space. Also, the proposed model considers both the kinematic and quasi-static requirements, which can lead to more efficient and effective mechanisms for real-world use, making it a promising tool for linkage mechanism design.</p></p class="citation"></blockquote><h3 id=5160--153299-reconstruction-based-anomaly-localization-via-knowledge-informed-self-training-cheng-qian-et-al-2024>(51/60 | 153/299) Reconstruction-Based Anomaly Localization via Knowledge-Informed Self-Training (Cheng Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Qian, Xiaoxian Lao, Chunguang Li. (2024)<br><strong>Reconstruction-Based Anomaly Localization via Knowledge-Informed Self-Training</strong><br><button class=copy-to-clipboard title="Reconstruction-Based Anomaly Localization via Knowledge-Informed Self-Training" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14246v1.pdf filename=2402.14246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Anomaly localization, which involves localizing anomalous regions within images, is a significant industrial task. Reconstruction-based methods are widely adopted for anomaly localization because of their low complexity and high interpretability. Most existing reconstruction-based methods only use normal samples to construct model. If anomalous samples are appropriately utilized in the process of anomaly localization, the localization performance can be improved. However, usually only weakly labeled anomalous samples are available, which limits the improvement. In many cases, we can obtain some knowledge of anomalies <b>summarized</b> by domain experts. Taking advantage of such knowledge can help us better utilize the anomalous samples and thus further improve the localization performance. In this paper, we propose a novel reconstruction-based method named knowledge-informed self-training (KIST) which integrates knowledge into reconstruction model through self-training. Specifically, KIST utilizes weakly labeled anomalous samples in addition to the normal ones and exploits knowledge to yield pixel-level pseudo-labels of the anomalous samples. Based on the pseudo labels, a novel loss which promotes the reconstruction of normal pixels while suppressing the reconstruction of anomalous pixels is used. We conduct experiments on different datasets and demonstrate the advantages of KIST over the existing reconstruction-based methods.</p></p class="citation"></blockquote><h3 id=5260--154299-automated-design-and-optimization-of-distributed-filtering-circuits-via-reinforcement-learning-peng-gao-et-al-2024>(52/60 | 154/299) Automated Design and Optimization of Distributed Filtering Circuits via Reinforcement Learning (Peng Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Gao, Tao Yu, Fei Wang, Ru-Yue Yuan. (2024)<br><strong>Automated Design and Optimization of Distributed Filtering Circuits via Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Automated Design and Optimization of Distributed Filtering Circuits via Reinforcement Learning" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-AR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14236v1.pdf filename=2402.14236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Designing distributed filtering circuits (DFCs) is complex and time-consuming, with the circuit performance relying heavily on the expertise and experience of electronics engineers. However, manual design methods tend to have exceedingly low-efficiency. This study proposes a novel end-to-end automated method for fabricating circuits to improve the design of DFCs. The proposed method harnesses <b>reinforcement</b> <b>learning</b> (RL) algorithms, eliminating the dependence on the design experience of engineers. Thus, it significantly reduces the subjectivity and constraints associated with circuit design. The experimental findings demonstrate clear improvements in both design efficiency and quality when comparing the proposed method with traditional engineer-driven methods. In particular, the proposed method achieves superior performance when designing complex or rapidly evolving DFCs. Furthermore, compared to existing circuit automation design techniques, the proposed method demonstrates superior design efficiency, highlighting the substantial potential of RL in circuit design automation.</p></p class="citation"></blockquote><h3 id=5360--155299-diversity-aware-ensembling-of-language-models-based-on-topological-data-analysis-polina-proskura-et-al-2024>(53/60 | 155/299) Diversity-Aware Ensembling of Language Models Based on Topological Data Analysis (Polina Proskura et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Polina Proskura, Alexey Zaytsev. (2024)<br><strong>Diversity-Aware Ensembling of Language Models Based on Topological Data Analysis</strong><br><button class=copy-to-clipboard title="Diversity-Aware Ensembling of Language Models Based on Topological Data Analysis" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14184v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14184v1.pdf filename=2402.14184v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensembles are important tools for improving the performance of machine learning models. In cases related to natural language processing, ensembles boost the performance of a method due to multiple large models available in open source. However, existing approaches mostly rely on simple averaging of predictions by ensembles with equal weights for each model, ignoring differences in the quality and conformity of models. We propose to estimate weights for ensembles of NLP models using not only knowledge of their individual performance but also their similarity to each other. By adopting distance measures based on Topological Data Analysis (TDA), we improve our ensemble. The quality improves for both <b>text</b> <b>classification</b> accuracy and relevant uncertainty estimation.</p></p class="citation"></blockquote><h3 id=5460--156299-tinyllava-a-framework-of-small-scale-large-multimodal-models-baichuan-zhou-et-al-2024>(54/60 | 156/299) TinyLLaVA: A Framework of Small-scale Large Multimodal Models (Baichuan Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, Lei Huang. (2024)<br><strong>TinyLLaVA: A Framework of Small-scale Large Multimodal Models</strong><br><button class=copy-to-clipboard title="TinyLLaVA: A Framework of Small-scale Large Multimodal Models" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14289v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14289v1.pdf filename=2402.14289v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the TinyLLaVA framework that provides a unified perspective in designing and analyzing the small-scale Large <b>Multimodal</b> Models (LMMs). We empirically study the effects of different vision encoders, connection modules, language models, training data and training recipes. Our extensive experiments showed that better quality of data combined with better training recipes, smaller LMMs can consistently achieve on-par performances compared to bigger LMMs. Under our framework, we train a family of small-scale LMMs. Our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as baselines for future research in terms of data scaling, training setups and model selections. Our model weights and codes will be made public.</p></p class="citation"></blockquote><h3 id=5560--157299-omnipred-language-models-as-universal-regressors-xingyou-song-et-al-2024>(55/60 | 157/299) OmniPred: Language Models as Universal Regressors (Xingyou Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyou Song, Oscar Li, Chansoo Lee, Bangding Yang, Daiyi Peng, Sagi Perel, Yutian Chen. (2024)<br><strong>OmniPred: Language Models as Universal Regressors</strong><br><button class=copy-to-clipboard title="OmniPred: Language Models as Universal Regressors" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-DB, cs-LG, cs.LG<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14547v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14547v2.pdf filename=2402.14547v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.</p></p class="citation"></blockquote><h3 id=5660--158299-latrend-a-framework-for-clustering-longitudinal-data-niek-den-teuling-et-al-2024>(56/60 | 158/299) latrend: A Framework for Clustering Longitudinal Data (Niek Den Teuling et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niek Den Teuling, Steffen Pauws, Edwin van den Heuvel. (2024)<br><strong>latrend: A Framework for Clustering Longitudinal Data</strong><br><button class=copy-to-clipboard title="latrend: A Framework for Clustering Longitudinal Data" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14621v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14621v1.pdf filename=2402.14621v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Clustering</b> of longitudinal data is used to explore common trends among subjects over time for a numeric measurement of interest. Various R packages have been introduced throughout the years for identifying clusters of longitudinal patterns, summarizing the variability in trajectories between subject in terms of one or more trends. We introduce the R package &ldquo;latrend&rdquo; as a framework for the unified application of methods for longitudinal <b>clustering,</b> enabling comparisons between methods with minimal coding. The package also serves as an interface to commonly used packages for <b>clustering</b> longitudinal data, including &ldquo;dtwclust&rdquo;, &ldquo;flexmix&rdquo;, &ldquo;kml&rdquo;, &ldquo;lcmm&rdquo;, &ldquo;mclust&rdquo;, &ldquo;mixAK&rdquo;, and &ldquo;mixtools&rdquo;. This enables researchers to easily compare different approaches, implementations, and method specifications. Furthermore, researchers can build upon the standard tools provided by the framework to quickly implement new cluster methods, enabling rapid prototyping. We demonstrate the functionality and application of the latrend package on a synthetic dataset based on the therapy adherence patterns of patients with sleep apnea.</p></p class="citation"></blockquote><h3 id=5760--159299-ace--off-policy-actor-critic-with-causality-aware-entropy-regularization-tianying-ji-et-al-2024>(57/60 | 159/299) ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization (Tianying Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianying Ji, Yongyuan Liang, Yan Zeng, Yu Luo, Guowei Xu, Jiawei Guo, Ruijie Zheng, Furong Huang, Fuchun Sun, Huazhe Xu. (2024)<br><strong>ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization</strong><br><button class=copy-to-clipboard title="ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2, cs-AI, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14528v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14528v1.pdf filename=2402.14528v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, ACE: Off-policy Actor-critic with Causality-aware Entropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which underscores the effectiveness, versatility, and efficient sample efficiency of our approach. <b>Benchmark</b> results and videos are available at <a href=https://ace-rl.github.io/>https://ace-rl.github.io/</a>.</p></p class="citation"></blockquote><h3 id=5860--160299-from-large-to-small-datasets-size-generalization-for-clustering-algorithm-selection-vaggos-chatziafratis-et-al-2024>(58/60 | 160/299) From Large to Small Datasets: Size Generalization for Clustering Algorithm Selection (Vaggos Chatziafratis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vaggos Chatziafratis, Ishani Karmarkar, Ellen Vitercik. (2024)<br><strong>From Large to Small Datasets: Size Generalization for Clustering Algorithm Selection</strong><br><button class=copy-to-clipboard title="From Large to Small Datasets: Size Generalization for Clustering Algorithm Selection" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14332v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14332v2.pdf filename=2402.14332v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>clustering</b> algorithm selection, we are given a massive dataset and must efficiently select which <b>clustering</b> algorithm to use. We study this problem in a semi-supervised setting, with an unknown ground-truth <b>clustering</b> that we can only access through expensive oracle queries. Ideally, the <b>clustering</b> algorithm&rsquo;s output will be structurally close to the ground truth. We approach this problem by introducing a notion of size generalization for <b>clustering</b> algorithm accuracy. We identify conditions under which we can (1) subsample the massive <b>clustering</b> instance, (2) evaluate a set of candidate algorithms on the smaller instance, and (3) guarantee that the algorithm with the best accuracy on the small instance will have the best accuracy on the original big instance. We provide theoretical size generalization guarantees for three classic <b>clustering</b> algorithms: single-linkage, k-means++, and (a smoothed variant of) Gonzalez&rsquo;s k-centers heuristic. We validate our theoretical analysis with empirical results, observing that on real-world <b>clustering</b> instances, we can use a subsample of as little as 5% of the data to identify which algorithm is best on the full dataset.</p></p class="citation"></blockquote><h3 id=5960--161299-high-arity-pac-learning-via-exchangeability-leonardo-n-coregliano-et-al-2024>(59/60 | 161/299) High-arity PAC learning via exchangeability (Leonardo N. Coregliano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leonardo N. Coregliano, Maryanthe Malliaris. (2024)<br><strong>High-arity PAC learning via exchangeability</strong><br><button class=copy-to-clipboard title="High-arity PAC learning via exchangeability" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: Primary: 68Q32- Secondary: 60F05, 60F15, 03C99, cs-LG, cs.LG, math-LO, math-ST, stat-TH<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14294v1.pdf filename=2402.14294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We develop a theory of high-arity PAC learning, which is statistical learning in the presence of &ldquo;structured correlation&rdquo;. In this theory, hypotheses are either <b>graphs,</b> hypergraphs or, more generally, structures in finite relational languages, and i.i.d. sampling is replaced by sampling an induced substructure, producing an exchangeable distribution. We prove a high-arity version of the fundamental theorem of statistical learning by characterizing high-arity (agnostic) PAC learnability in terms of finiteness of a purely combinatorial dimension and in terms of an appropriate version of uniform convergence.</p></p class="citation"></blockquote><h3 id=6060--162299-a-hierarchical-decomposition-for-explaining-ml-performance-discrepancies-jean-feng-et-al-2024>(60/60 | 162/299) A hierarchical decomposition for explaining ML performance discrepancies (Jean Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jean Feng, Harvineet Singh, Fan Xia, Adarsh Subbaswamy, Alexej Gossmann. (2024)<br><strong>A hierarchical decomposition for explaining ML performance discrepancies</strong><br><button class=copy-to-clipboard title="A hierarchical decomposition for explaining ML performance discrepancies" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14254v1.pdf filename=2402.14254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning (ML) algorithms can often differ in performance across domains. Understanding $\textit{why}$ their performance differs is crucial for determining what types of interventions (e.g., algorithmic or operational) are most effective at closing the performance gaps. Existing methods focus on $\textit{aggregate decompositions}$ of the total performance gap into the impact of a shift in the distribution of features $p(X)$ versus the impact of a shift in the conditional distribution of the outcome $p(Y|X)$; however, such coarse explanations offer only a few options for how one can close the performance gap. $\textit{Detailed variable-level decompositions}$ that quantify the importance of each variable to each term in the aggregate decomposition can provide a much deeper understanding and suggest much more targeted interventions. However, existing methods assume knowledge of the full causal <b>graph</b> or make strong parametric assumptions. We introduce a nonparametric hierarchical framework that provides both aggregate and detailed decompositions for explaining why the performance of an ML algorithm differs across domains, without requiring causal knowledge. We derive debiased, computationally-efficient estimators, and statistical inference procedures for asymptotically valid confidence intervals.</p></p class="citation"></blockquote><h2 id=cscr-6>cs.CR (6)</h2><h3 id=16--163299-mitigating-fine-tuning-jailbreak-attack-with-backdoor-enhanced-alignment-jiongxiao-wang-et-al-2024>(1/6 | 163/299) Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment (Jiongxiao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi, Junjie Hu, Yixuan Li, Patrick McDaniel, Muhao Chen, Bo Li, Chaowei Xiao. (2024)<br><strong>Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment</strong><br><button class=copy-to-clipboard title="Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs.CR<br>Keyword Score: 80<br>Keywords: Fine-tuning, Fine-tuning, GPT, GPT-4, LLaMA, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14968v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14968v2.pdf filename=2402.14968v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the general capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>GPT-4</b> and <b>Llama-2,</b> these models still request <b>fine-tuning</b> or adaptation with customized data when it comes to meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the <b>Fine-tuning</b> based Jailbreak Attack (FJAttack), where incorporating just a few harmful examples into the <b>fine-tuning</b> dataset can significantly compromise the model safety. Though potential defenses have been proposed by incorporating safety examples into the <b>fine-tuning</b> dataset to reduce the safety issues, such approaches require incorporating a substantial amount of safety examples, making it inefficient. To effectively defend against the FJAttack with limited safety examples, we propose a Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In particular, we construct prefixed safety examples by integrating a secret <b>prompt,</b> acting as a &ldquo;backdoor trigger&rdquo;, that is prefixed to safety examples. Our comprehensive experiments demonstrate that through the Backdoor Enhanced Safety Alignment with adding as few as 11 prefixed safety examples, the maliciously <b>fine-tuned</b> <b>LLMs</b> will achieve similar safety performance as the original aligned models. Furthermore, we also explore the effectiveness of our method in a more practical setting where the <b>fine-tuning</b> data consists of both FJAttack examples and the <b>fine-tuning</b> task data. Our method shows great efficacy in defending against FJAttack without harming the performance of <b>fine-tuning</b> tasks.</p></p class="citation"></blockquote><h3 id=26--164299-watermarking-makes-language-models-radioactive-tom-sander-et-al-2024>(2/6 | 164/299) Watermarking Makes Language Models Radioactive (Tom Sander et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tom Sander, Pierre Fernandez, Alain Durmus, Matthijs Douze, Teddy Furon. (2024)<br><strong>Watermarking Makes Language Models Radioactive</strong><br><button class=copy-to-clipboard title="Watermarking Makes Language Models Radioactive" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 30<br>Keywords: Fine-tuning, Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14904v1.pdf filename=2402.14904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the radioactivity of <b>LLM-generated</b> texts, i.e. whether it is possible to detect that such input was used as training data. Conventional methods like membership inference can carry out this detection with some level of accuracy. We show that watermarked training data leaves traces easier to detect and much more reliable than membership inference. We link the contamination level to the watermark robustness, its proportion in the training set, and the <b>fine-tuning</b> process. We notably demonstrate that training on watermarked synthetic instructions can be detected with high confidence (p-value &lt; 1e-5) even when as little as 5% of training text is watermarked. Thus, <b>LLM</b> watermarking, originally designed for detecting machine-generated text, gives the ability to easily identify if the outputs of a watermarked <b>LLM</b> were used to <b>fine-tune</b> another <b>LLM.</b></p></p class="citation"></blockquote><h3 id=36--165299-mudjacking-patching-backdoor-vulnerabilities-in-foundation-models-hongbin-liu-et-al-2024>(3/6 | 165/299) Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models (Hongbin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongbin Liu, Michael K. Reiter, Neil Zhenqiang Gong. (2024)<br><strong>Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models</strong><br><button class=copy-to-clipboard title="Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CV, cs-LG, cs.CR<br>Keyword Score: 23<br>Keywords: Benchmarking, Foundation Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14977v1.pdf filename=2402.14977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>model</b> has become the backbone of the AI ecosystem. In particular, a <b>foundation</b> <b>model</b> can be used as a general-purpose feature extractor to build various downstream classifiers. However, <b>foundation</b> <b>models</b> are vulnerable to backdoor attacks and a backdoored <b>foundation</b> <b>model</b> is a single-point-of-failure of the AI ecosystem, e.g., multiple downstream classifiers inherit the backdoor vulnerabilities simultaneously. In this work, we propose Mudjacking, the first method to patch <b>foundation</b> <b>models</b> to remove backdoors. Specifically, given a misclassified trigger-embedded input detected after a backdoored <b>foundation</b> <b>model</b> is deployed, Mudjacking adjusts the parameters of the <b>foundation</b> <b>model</b> to remove the backdoor. We formulate patching a <b>foundation</b> <b>model</b> as an optimization problem and propose a gradient descent based method to solve it. We evaluate Mudjacking on both vision and language <b>foundation</b> <b>models,</b> eleven <b>benchmark</b> datasets, five existing backdoor attacks, and thirteen adaptive backdoor attacks. Our results show that Mudjacking can remove backdoor from a <b>foundation</b> <b>model</b> while maintaining its utility.</p></p class="citation"></blockquote><h3 id=46--166299-double-i-watermark-protecting-model-copyright-for-llm-fine-tuning-shen-li-et-al-2024>(4/6 | 166/299) Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning (Shen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shen Li, Liuyi Yao, Jinyang Gao, Lan Zhang, Yaliang Li. (2024)<br><strong>Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning</strong><br><button class=copy-to-clipboard title="Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR<br>Keyword Score: 20<br>Keywords: Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14883v1.pdf filename=2402.14883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To support various applications, business owners often seek the customized models that are obtained by <b>fine-tuning</b> a pre-trained <b>LLM</b> through the API provided by <b>LLM</b> owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during <b>LLM</b> <b>fine-tuning</b> has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named &ldquo;Double-I watermark&rdquo;. Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging <b>LLM&rsquo;s</b> learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during <b>fine-tuning,</b> which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed &ldquo;Double-I watermark&rdquo; under various <b>fine-tuning</b> methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both theoretical analysis and experimental verification.</p></p class="citation"></blockquote><h3 id=56--167299-bionib-blockchain-based-iot-using-novelty-index-in-bridge-health-monitoring-divija-swetha-gadiraju-et-al-2024>(5/6 | 167/299) BIONIB: Blockchain-based IoT using Novelty Index in Bridge Health Monitoring (Divija Swetha Gadiraju et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Divija Swetha Gadiraju, Ryan McMaster, Saeed Eftekhar Azam, Deepak Khazanchi. (2024)<br><strong>BIONIB: Blockchain-based IoT using Novelty Index in Bridge Health Monitoring</strong><br><button class=copy-to-clipboard title="BIONIB: Blockchain-based IoT using Novelty Index in Bridge Health Monitoring" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14902v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14902v1.pdf filename=2402.14902v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bridge health monitoring becomes crucial with the deployment of IoT sensors. The challenge lies in securely storing vast amounts of data and extracting useful information to promptly identify unhealthy bridge conditions. To address this challenge, we propose BIONIB, wherein real-time IoT data is stored on the blockchain for monitoring bridges. One of the emerging blockchains, EOSIO is used because of its exceptional scaling capabilities for monitoring the health of bridges. The approach involves collecting data from IoT sensors and using an <b>unsupervised</b> machine learning-based technique called the Novelty Index (NI) to observe meaningful patterns in the data. Smart contracts of EOSIO are used in implementation because of their efficiency, security, and programmability, making them well-suited for handling complex transactions and automating processes within decentralized applications. BIONIB provides secure storage benefits of blockchain, as well as useful predictions based on the NI. Performance analysis uses real-time data collected from IoT sensors at the bridge in healthy and unhealthy states. The data is collected with extensive experimentation with different loads, climatic conditions, and the health of the bridge. The performance of BIONIB under varying numbers of sensors and various numbers of participating blockchain nodes is observed. We observe a tradeoff between throughput, latency, and computational resources. Storage efficiency can be increased by manifolds with a slight increase in latency caused by NI calculation. As latency is not a significant concern in bridge health applications, the results demonstrate that BIONIB has high throughput, parallel processing, and high security while efficiently scaled.</p></p class="citation"></blockquote><h3 id=66--168299-a-new-hope-contextual-privacy-policies-for-mobile-applications-and-an-approach-toward-automated-generation-shidong-pan-et-al-2024>(6/6 | 168/299) {A New Hope}: Contextual Privacy Policies for Mobile Applications and An Approach Toward Automated Generation (Shidong Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shidong Pan, Zhen Tao, Thong Hoang, Dawen Zhang, Tianshi Li, Zhenchang Xing, Sherry Xu, Mark Staples, Thierry Rakotoarivelo, David Lo. (2024)<br><strong>{A New Hope}: Contextual Privacy Policies for Mobile Applications and An Approach Toward Automated Generation</strong><br><button class=copy-to-clipboard title="{A New Hope}: Contextual Privacy Policies for Mobile Applications and An Approach Toward Automated Generation" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-SE, cs.CR<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14544v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14544v1.pdf filename=2402.14544v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Privacy policies have emerged as the predominant approach to conveying privacy notices to mobile application users. In an effort to enhance both readability and user engagement, the concept of contextual privacy policies (CPPs) has been proposed by researchers. The aim of CPPs is to fragment privacy policies into concise snippets, displaying them only within the corresponding contexts within the application&rsquo;s graphical user interfaces (GUIs). In this paper, we first formulate CPP in mobile application scenario, and then present a novel <b>multimodal</b> framework, named SeePrivacy, specifically designed to automatically generate CPPs for mobile applications. This method uniquely integrates vision-based GUI understanding with privacy policy analysis, achieving 0.88 precision and 0.90 recall to detect contexts, as well as 0.98 precision and 0.96 recall in extracting corresponding policy segments. A human evaluation shows that 77% of the extracted privacy policy segments were perceived as well-aligned with the detected contexts. These findings suggest that SeePrivacy could serve as a significant tool for bolstering user interaction with, and understanding of, privacy policies. Furthermore, our solution has the potential to make privacy notices more accessible and inclusive, thus appealing to a broader demographic. A demonstration of our work can be accessed at <a href=https://cpp4app.github.io/SeePrivacy/>https://cpp4app.github.io/SeePrivacy/</a></p></p class="citation"></blockquote><h2 id=cscv-50>cs.CV (50)</h2><h3 id=150--169299-visual-hallucinations-of-multi-modal-large-language-models-wen-huang-et-al-2024>(1/50 | 169/299) Visual Hallucinations of Multi-modal Large Language Models (Wen Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wen Huang, Hongbin Liu, Minxin Guo, Neil Zhenqiang Gong. (2024)<br><strong>Visual Hallucinations of Multi-modal Large Language Models</strong><br><button class=copy-to-clipboard title="Visual Hallucinations of Multi-modal Large Language Models" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 76<br>Keywords: Benchmarking, Fine-tuning, Multi-modal, GPT, Question Answering, Text2image, Visual Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14683v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14683v1.pdf filename=2402.14683v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Visual</b> <b>hallucination</b> <b>(VH)</b> means that a <b>multi-modal</b> <b>LLM</b> (MLLM) imagines incorrect details about an image in <b>visual</b> <b>question</b> <b>answering.</b> Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs&rsquo; performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a <b>text-to-image</b> generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a <b>benchmark</b> dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as <b>GPT-4V,</b> LLaVA-1.5, and MiniGPT-v2 hallucinate for a <b>large</b> <b>fraction</b> <b>of</b> the instances in our <b>benchmark.</b> Moreover, we find that <b>fine-tuning</b> an MLLM using our <b>benchmark</b> dataset reduces its likelihood to hallucinate without sacrificing its performance on other <b>benchmarks.</b> Our <b>benchmarks</b> are publicly available: <a href=https://github.com/wenhuang2000/VHTest>https://github.com/wenhuang2000/VHTest</a>.</p></p class="citation"></blockquote><h3 id=250--170299-unsupervised-domain-adaptation-within-deep-foundation-latent-spaces-dmitry-kangin-et-al-2024>(2/50 | 170/299) Unsupervised Domain Adaptation within Deep Foundation Latent Spaces (Dmitry Kangin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dmitry Kangin, Plamen Angelov. (2024)<br><strong>Unsupervised Domain Adaptation within Deep Foundation Latent Spaces</strong><br><button class=copy-to-clipboard title="Unsupervised Domain Adaptation within Deep Foundation Latent Spaces" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 70<br>Keywords: Vision Transformer, Fine-tuning, Foundation Model, Unsupervised Learning, Transformer, Domain Adaptation, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14976v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14976v1.pdf filename=2402.14976v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>vision</b> <b>transformer-based</b> <b>foundation</b> <b>models,</b> such as ViT or Dino-V2, are aimed at solving problems with little or no <b>finetuning</b> of features. Using a setting of prototypical networks, we analyse to what extent such <b>foundation</b> <b>models</b> can solve <b>unsupervised</b> <b>domain</b> <b>adaptation</b> without <b>finetuning</b> over the source or target <b>domain.</b> <b>Through</b> quantitative analysis, as well as qualitative interpretations of decision making, we demonstrate that the suggested method can improve upon existing baselines, as well as showcase the limitations of such approach yet to be solved.</p></p class="citation"></blockquote><h3 id=350--171299-zero-shot-pediatric-tuberculosis-detection-in-chest-x-rays-using-self-supervised-learning-daniel-capellán-martín-et-al-2024>(3/50 | 171/299) Zero-Shot Pediatric Tuberculosis Detection in Chest X-Rays using Self-Supervised Learning (Daniel Capellán-Martín et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Capellán-Martín, Abhijeet Parida, Juan J. Gómez-Valverde, Ramon Sanchez-Jacob, Pooneh Roshanitabrizi, Marius G. Linguraru, María J. Ledesma-Carbayo, Syed M. Anwar. (2024)<br><strong>Zero-Shot Pediatric Tuberculosis Detection in Chest X-Rays using Self-Supervised Learning</strong><br><button class=copy-to-clipboard title="Zero-Shot Pediatric Tuberculosis Detection in Chest X-Rays using Self-Supervised Learning" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 70<br>Keywords: Vision Transformer, Self-supervised Learning, Self-supervised Learning, Self-supervised Pre-training, Zero-shot, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14741v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14741v1.pdf filename=2402.14741v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tuberculosis (TB) remains a significant global health challenge, with pediatric cases posing a major concern. The World Health Organization (WHO) advocates for chest X-rays (CXRs) for TB screening. However, visual interpretation by radiologists can be subjective, time-consuming and prone to error, especially in pediatric TB. Artificial intelligence (AI)-driven computer-aided detection (CAD) tools, especially those utilizing deep learning, show promise in enhancing lung disease detection. However, challenges include data scarcity and lack of generalizability. In this context, we propose a novel <b>self-supervised</b> <b>paradigm</b> leveraging <b>Vision</b> <b>Transformers</b> (ViT) for improved TB detection in CXR, enabling <b>zero-shot</b> pediatric TB detection. We demonstrate improvements in TB detection performance ($\sim$12.7% and $\sim$13.4% top AUC/AUPR gains in adults and children, respectively) when conducting <b>self-supervised</b> <b>pre-training</b> when compared to fully-supervised (i.e., non pre-trained) ViT models, achieving top performances of 0.959 AUC and 0.962 AUPR in adult TB detection, and 0.697 AUC and 0.607 AUPR in <b>zero-shot</b> pediatric TB detection. As a result, this work demonstrates that <b>self-supervised</b> <b>learning</b> on adult CXRs effectively extends to challenging downstream tasks such as pediatric TB detection, where data are scarce.</p></p class="citation"></blockquote><h3 id=450--172299-clce-an-approach-to-refining-cross-entropy-and-contrastive-learning-for-optimized-learning-fusion-zijun-long-et-al-2024>(4/50 | 172/299) CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for Optimized Learning Fusion (Zijun Long et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijun Long, George Killick, Lipeng Zhuang, Gerardo Aragon-Camarasa, Zaiqiao Meng, Richard Mccreadie. (2024)<br><strong>CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for Optimized Learning Fusion</strong><br><button class=copy-to-clipboard title="CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for Optimized Learning Fusion" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 63<br>Keywords: Benchmarking, Contrastive Learning, Few-shot, Few-shot Learning, Fine-tuning, Transfer Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14551v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14551v1.pdf filename=2402.14551v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>State-of-the-art pre-trained image models predominantly adopt a two-stage approach: initial <b>unsupervised</b> pre-training on large-scale datasets followed by task-specific <b>fine-tuning</b> using Cross-Entropy loss~(CE). However, it has been demonstrated that CE can compromise model generalization and stability. While recent works employing <b>contrastive</b> <b>learning</b> address some of these limitations by enhancing the quality of embeddings and producing better decision boundaries, they often overlook the importance of hard negative mining and rely on resource intensive and slow training using large sample batches. To counter these issues, we introduce a novel approach named CLCE, which integrates Label-Aware <b>Contrastive</b> <b>Learning</b> with CE. Our approach not only maintains the strengths of both loss functions but also leverages hard negative mining in a synergistic way to enhance performance. Experimental results demonstrate that CLCE significantly outperforms CE in Top-1 accuracy across twelve <b>benchmarks,</b> achieving gains of up to 3.52% in <b>few-shot</b> <b>learning</b> scenarios and 3.41% in <b>transfer</b> <b>learning</b> settings with the BEiT-3 model. Importantly, our proposed CLCE approach effectively mitigates the dependency of <b>contrastive</b> <b>learning</b> on large batch sizes such as 4096 samples per batch, a limitation that has previously constrained the application of <b>contrastive</b> <b>learning</b> in budget-limited hardware environments.</p></p class="citation"></blockquote><h3 id=550--173299-uncertainty-aware-evaluation-for-vision-language-models-vasily-kostumov-et-al-2024>(5/50 | 173/299) Uncertainty-Aware Evaluation for Vision-Language Models (Vasily Kostumov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vasily Kostumov, Bulat Nutfullin, Oleg Pilipenko, Eugene Ilyushin. (2024)<br><strong>Uncertainty-Aware Evaluation for Vision-Language Models</strong><br><button class=copy-to-clipboard title="Uncertainty-Aware Evaluation for Vision-Language Models" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Benchmarking, GPT, GPT-4, Question Answering, Visual Question Answering, Visual Question Answering, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14418v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14418v2.pdf filename=2402.14418v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-Language</b> Models like <b>GPT-4,</b> LLaVA, and CogVLM have surged in popularity recently due to their impressive performance in several <b>vision-language</b> tasks. Current evaluation methods, however, overlook an essential component: uncertainty, which is crucial for a comprehensive assessment of VLMs. Addressing this oversight, we present a <b>benchmark</b> incorporating uncertainty quantification into evaluating VLMs. Our analysis spans 20+ VLMs, focusing on the multiple-choice <b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA)</b> task. We examine models on 5 datasets that evaluate various <b>vision-language</b> capabilities. Using conformal prediction as an uncertainty estimation approach, we demonstrate that the models&rsquo; uncertainty is not aligned with their accuracy. Specifically, we show that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs. Our empirical findings also reveal a correlation between model uncertainty and its language model part.</p></p class="citation"></blockquote><h3 id=650--174299-self-supervised-visualisation-of-medical-image-datasets-ifeoma-veronica-nwabufo-et-al-2024>(6/50 | 174/299) Self-supervised Visualisation of Medical Image Datasets (Ifeoma Veronica Nwabufo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ifeoma Veronica Nwabufo, Jan Niklas Böhm, Philipp Berens, Dmitry Kobak. (2024)<br><strong>Self-supervised Visualisation of Medical Image Datasets</strong><br><button class=copy-to-clipboard title="Self-supervised Visualisation of Medical Image Datasets" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Contrastive Learning, Data Augmentation, Fine-tuning, Self-supervised Learning, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14566v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14566v1.pdf filename=2402.14566v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> methods based on <b>data</b> <b>augmentations,</b> such as SimCLR, BYOL, or DINO, allow obtaining semantically meaningful representations of image datasets and are widely used prior to <b>supervised</b> <b>fine-tuning.</b> A recent <b>self-supervised</b> <b>learning</b> method, $t$-SimCNE, uses <b>contrastive</b> <b>learning</b> to directly train a 2D representation suitable for visualisation. When applied to natural image datasets, $t$-SimCNE yields 2D visualisations with semantically meaningful clusters. In this work, we used $t$-SimCNE to visualise medical image datasets, including examples from dermatology, histology, and blood microscopy. We found that increasing the set of <b>data</b> <b>augmentations</b> to include arbitrary rotations improved the results in terms of class separability, compared to <b>data</b> <b>augmentations</b> used for natural images. Our 2D representations show medically relevant structures and can be used to aid <b>data</b> <b>exploration</b> and annotation, improving on common approaches for <b>data</b> <b>visualisation.</b></p></p class="citation"></blockquote><h3 id=750--175299-weaksam-segment-anything-meets-weakly-supervised-instance-level-recognition-lianghui-zhu-et-al-2024>(7/50 | 175/299) WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition (Lianghui Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lianghui Zhu, Junwei Zhou, Yan Liu, Xin Hao, Wenyu Liu, Xinggang Wang. (2024)<br><strong>WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition</strong><br><button class=copy-to-clipboard title="WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Object Detection, Benchmarking, Foundation Model, Supervised Learning, Weakly-supervised Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14812v1.pdf filename=2402.14812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Weakly <b>supervised</b> visual recognition using inexact supervision is a critical yet challenging learning problem. It significantly reduces human labeling costs and traditionally relies on multi-instance learning and pseudo-labeling. This paper introduces WeakSAM and solves the <b>weakly-supervised</b> <b>object</b> <b>detection</b> (WSOD) and segmentation by utilizing the pre-learned world knowledge contained in a vision <b>foundation</b> <b>model,</b> i.e., the Segment Anything Model (SAM). WeakSAM addresses two critical limitations in traditional WSOD retraining, i.e., pseudo ground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT generation and Region of Interest (RoI) drop regularization. It also addresses the SAM&rsquo;s problems of requiring <b>prompts</b> and category unawareness for automatic <b>object</b> <b>detection</b> and segmentation. Our results indicate that WeakSAM significantly surpasses previous state-of-the-art methods in WSOD and WSIS <b>benchmarks</b> with large margins, i.e. average improvements of 7.4% and 8.5%, respectively. The code is available at \url{https://github.com/hustvl/WeakSAM}.</p></p class="citation"></blockquote><h3 id=850--176299-subobject-level-image-tokenization-delong-chen-et-al-2024>(8/50 | 176/299) Subobject-level Image Tokenization (Delong Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Delong Chen, Samuel Cahyawijaya, Jianfeng Liu, Baoyuan Wang, Pascale Fung. (2024)<br><strong>Subobject-level Image Tokenization</strong><br><button class=copy-to-clipboard title="Subobject-level Image Tokenization" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Autoencoder, Transformer, Tokenization, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14327v1.pdf filename=2402.14327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> vision models typically tokenize images into fixed-size square patches as input units, which lacks the adaptability to image content and overlooks the inherent pixel grouping structure. Inspired by the subword <b>tokenization</b> widely adopted in language models, we propose an image tokenizer at a subobject level, where the subobjects are represented by semantically meaningful image segments obtained by segmentation models (e.g., segment anything models). To implement a learning system based on subobject <b>tokenization,</b> we first introduced a Sequence-to-sequence <b>AutoEncoder</b> (SeqAE) to compress subobject segments of varying sizes and shapes into compact embedding vectors, then fed the subobject embeddings into a <b>large</b> <b>language</b> <b>model</b> for vision language learning. Empirical results demonstrated that our subobject-level <b>tokenization</b> significantly facilitates efficient learning of translating images into object and attribute descriptions compared to the traditional patch-level <b>tokenization.</b> Codes and models will be open-sourced at <a href=https://github.com/ChenDelong1999/subobjects>https://github.com/ChenDelong1999/subobjects</a>.</p></p class="citation"></blockquote><h3 id=950--177299-typographic-text-generation-with-off-the-shelf-diffusion-model-khaytze-peong-et-al-2024>(9/50 | 177/299) Typographic Text Generation with Off-the-Shelf Diffusion Model (KhayTze Peong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>KhayTze Peong, Seiichi Uchida, Daichi Haraguchi. (2024)<br><strong>Typographic Text Generation with Off-the-Shelf Diffusion Model</strong><br><button class=copy-to-clipboard title="Typographic Text Generation with Off-the-Shelf Diffusion Model" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: ControlNet, Diffusion Model, Text Generation, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14314v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14314v1.pdf filename=2402.14314v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>diffusion-based</b> <b>generative</b> models show promise in their ability to generate <b>text</b> <b>images,</b> but limitations in specifying the styles of the generated <b>texts</b> <b>render</b> them insufficient in the realm of typographic design. This paper proposes a typographic <b>text</b> <b>generation</b> system to add and modify <b>text</b> <b>on</b> typographic designs while specifying font styles, colors, and <b>text</b> <b>effects.</b> The proposed system is a novel combination of two off-the-shelf methods for <b>diffusion</b> <b>models,</b> <b>ControlNet</b> and Blended Latent <b>Diffusion.</b> <b>The</b> former functions to generate <b>text</b> <b>images</b> under the guidance of edge conditions specifying stroke contours. The latter blends latent noise in Latent <b>Diffusion</b> <b>Models</b> (LDM) to add typographic <b>text</b> <b>naturally</b> onto an existing background. We first show that given appropriate <b>text</b> <b>edges,</b> <b>ControlNet</b> can generate <b>texts</b> <b>in</b> specified fonts while incorporating effects described by <b>prompts.</b> We further introduce <b>text</b> <b>edge</b> manipulation as an intuitive and customizable way to produce <b>texts</b> <b>with</b> complex effects such as <code>shadows'' and </code>reflections&rsquo;&rsquo;. Finally, with the proposed system, we successfully add and modify <b>texts</b> <b>on</b> a predefined background while preserving its overall coherence.</p></p class="citation"></blockquote><h3 id=1050--178299-a-simple-framework-uniting-visual-in-context-learning-with-masked-image-modeling-to-improve-ultrasound-segmentation-yuyue-zhou-et-al-2024>(10/50 | 178/299) A Simple Framework Uniting Visual In-context Learning with Masked Image Modeling to Improve Ultrasound Segmentation (Yuyue Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuyue Zhou, Banafshe Felfeliyan, Shrimanti Ghosh, Jessica Knight, Fatima Alves-Pereira, Christopher Keen, Jessica Küpper, Abhilash Rakkunedeth Hareendranathan, Jacob L. Jaremko. (2024)<br><strong>A Simple Framework Uniting Visual In-context Learning with Masked Image Modeling to Improve Ultrasound Segmentation</strong><br><button class=copy-to-clipboard title="A Simple Framework Uniting Visual In-context Learning with Masked Image Modeling to Improve Ultrasound Segmentation" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Self-supervised Learning, Self-supervised Learning, In-context Learning, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14300v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14300v1.pdf filename=2402.14300v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional deep learning models deal with images one-by-one, requiring costly and time-consuming expert labeling in the field of medical imaging, and domain-specific restriction limits model generalizability. Visual <b>in-context</b> <b>learning</b> <b>(ICL)</b> is a new and exciting area of research in computer vision. Unlike conventional deep learning, <b>ICL</b> emphasizes the model&rsquo;s ability to adapt to new tasks based on given examples quickly. Inspired by MAE-VQGAN, we proposed a new simple visual <b>ICL</b> method called SimICL, combining visual <b>ICL</b> pairing images with masked image modeling (MIM) designed for <b>self-supervised</b> <b>learning.</b> We validated our method on bony structures segmentation in a wrist ultrasound (US) dataset with limited annotations, where the clinical objective was to segment bony structures to help with further fracture detection. We used a test set containing 3822 images from 18 patients for bony region segmentation. SimICL achieved an remarkably high Dice coeffient (DC) of 0.96 and Jaccard Index (IoU) of 0.92, surpassing state-of-the-art segmentation and visual <b>ICL</b> models (a maximum DC 0.86 and IoU 0.76), with SimICL DC and IoU increasing up to 0.10 and 0.16. This remarkably high agreement with limited manual annotations indicates SimICL could be used for training AI models even on small US datasets. This could dramatically decrease the human expert time required for image labeling compared to conventional approaches, and enhance the real-world use of AI assistance in US image analysis.</p></p class="citation"></blockquote><h3 id=1150--179299-modeling-3d-infant-kinetics-using-adaptive-graph-convolutional-networks-daniel-holmberg-et-al-2024>(11/50 | 179/299) Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks (Daniel Holmberg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Holmberg, Manu Airaksinen, Viviana Marchi, Andrea Guzzetta, Anna Kivi, Leena Haataja, Sampsa Vanhatalo, Teemu Roos. (2024)<br><strong>Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks</strong><br><button class=copy-to-clipboard title="Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68T06, I-2; I-4; J-3, cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 43<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14400v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14400v1.pdf filename=2402.14400v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reliable methods for the neurodevelopmental assessment of infants are essential for early detection of medical issues that may need <b>prompt</b> interventions. Spontaneous motor activity, or `kinetics&rsquo;, is shown to provide a powerful surrogate measure of upcoming neurodevelopment. However, its assessment is by and large qualitative and subjective, focusing on visually identified, age-specific gestures. Here, we follow an alternative approach, predicting infants&rsquo; neurodevelopmental maturation based on data-driven evaluation of individual motor patterns. We utilize 3D video recordings of infants processed with pose-estimation to extract spatio-temporal series of anatomical landmarks, and apply adaptive <b>graph</b> <b>convolutional</b> <b>networks</b> to predict the actual age. We show that our data-driven approach achieves improvement over traditional machine learning baselines based on manually engineered features.</p></p class="citation"></blockquote><h3 id=1250--180299-overcoming-dimensional-collapse-in-self-supervised-contrastive-learning-for-medical-image-segmentation-jamshid-hassanpour-et-al-2024>(12/50 | 180/299) Overcoming Dimensional Collapse in Self-supervised Contrastive Learning for Medical Image Segmentation (Jamshid Hassanpour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jamshid Hassanpour, Vinkle Srivastav, Didier Mutter, Nicolas Padoy. (2024)<br><strong>Overcoming Dimensional Collapse in Self-supervised Contrastive Learning for Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="Overcoming Dimensional Collapse in Self-supervised Contrastive Learning for Medical Image Segmentation" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Contrastive Learning, Fine-tuning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14611v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14611v2.pdf filename=2402.14611v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> (SSL) approaches have achieved great success when the amount of labeled data is limited. Within SSL, models learn robust feature representations by solving pretext tasks. One such pretext task is <b>contrastive</b> <b>learning,</b> which involves forming pairs of similar and dissimilar input samples, guiding the model to distinguish between them. In this work, we investigate the application of <b>contrastive</b> <b>learning</b> to the domain of medical image analysis. Our findings reveal that MoCo v2, a state-of-the-art <b>contrastive</b> <b>learning</b> method, encounters dimensional collapse when applied to medical images. This is attributed to the high degree of inter-image similarity shared between the medical images. To address this, we propose two key contributions: local feature learning and feature decorrelation. Local feature learning improves the ability of the model to focus on the local regions of the image, while feature decorrelation removes the linear dependence among the features. Our experimental findings demonstrate that our contributions significantly enhance the model&rsquo;s performance in the downstream task of medical segmentation, both in the linear evaluation and full <b>fine-tuning</b> settings. This work illustrates the importance of effectively adapting SSL techniques to the characteristics of medical imaging tasks. The source code will be made publicly available at: <a href=https://github.com/CAMMA-public/med-moco>https://github.com/CAMMA-public/med-moco</a></p></p class="citation"></blockquote><h3 id=1350--181299-tie-kd-teacher-independent-and-explainable-knowledge-distillation-for-monocular-depth-estimation-sangwon-choi-et-al-2024>(13/50 | 181/299) TIE-KD: Teacher-Independent and Explainable Knowledge Distillation for Monocular Depth Estimation (Sangwon Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sangwon Choi, Daejune Choi, Duksu Kim. (2024)<br><strong>TIE-KD: Teacher-Independent and Explainable Knowledge Distillation for Monocular Depth Estimation</strong><br><button class=copy-to-clipboard title="TIE-KD: Teacher-Independent and Explainable Knowledge Distillation for Monocular Depth Estimation" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Knowledge Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14340v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14340v1.pdf filename=2402.14340v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monocular depth estimation (MDE) is essential for numerous applications yet is impeded by the substantial computational demands of accurate deep learning models. To mitigate this, we introduce a novel Teacher-Independent Explainable <b>Knowledge</b> <b>Distillation</b> (TIE-KD) framework that streamlines the <b>knowledge</b> <b>transfer</b> from complex teacher models to compact student networks, eliminating the need for architectural similarity. The cornerstone of TIE-KD is the Depth Probability Map (DPM), an explainable feature map that interprets the teacher&rsquo;s output, enabling feature-based <b>knowledge</b> <b>distillation</b> solely from the teacher&rsquo;s response. This approach allows for efficient student learning, leveraging the strengths of feature-based <b>distillation.</b> Extensive evaluation of the KITTI dataset indicates that TIE-KD not only outperforms conventional response-based <b>KD</b> methods but also demonstrates consistent efficacy across diverse teacher and student architectures. The robustness and adaptability of TIE-KD underscore its potential for applications requiring efficient and interpretable models, affirming its practicality for real-world deployment.</p></p class="citation"></blockquote><h3 id=1450--182299-mvd2-efficient-multiview-3d-reconstruction-for-multiview-diffusion-xin-yang-zheng-et-al-2024>(14/50 | 182/299) MVD$^2$: Efficient Multiview 3D Reconstruction for Multiview Diffusion (Xin-Yang Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin-Yang Zheng, Hao Pan, Yu-Xiao Guo, Xin Tong, Yang Liu. (2024)<br><strong>MVD$^2$: Efficient Multiview 3D Reconstruction for Multiview Diffusion</strong><br><button class=copy-to-clipboard title="MVD$^2$: Efficient Multiview 3D Reconstruction for Multiview Diffusion" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Convolution, Fine-tuning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14253v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14253v1.pdf filename=2402.14253v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a promising 3D generation technique, multiview <b>diffusion</b> <b>(MVD)</b> has received a lot of attention due to its advantages in terms of generalizability, quality, and efficiency. By <b>finetuning</b> pretrained large image <b>diffusion</b> <b>models</b> with 3D data, the MVD methods first generate multiple views of a 3D object based on an image or text <b>prompt</b> and then reconstruct 3D shapes with multiview 3D reconstruction. However, the sparse views and inconsistent details in the generated images make 3D reconstruction challenging. We present MVD$^2$, an efficient 3D reconstruction method for multiview <b>diffusion</b> <b>(MVD)</b> images. MVD$^2$ aggregates image features into a 3D feature volume by projection and <b>convolution</b> and then decodes volumetric features into a 3D mesh. We train MVD$^2$ with 3D shape collections and MVD images <b>prompted</b> by rendered views of 3D shapes. To address the discrepancy between the generated multiview images and ground-truth views of the 3D shapes, we design a simple-yet-efficient view-dependent training scheme. MVD$^2$ improves the 3D generation quality of MVD and is fast and robust to various MVD methods. After training, it can efficiently decode 3D meshes from multiview images within one second. We train MVD$^2$ with Zero-123++ and ObjectVerse-LVIS 3D dataset and demonstrate its superior performance in generating 3D models from multiview images generated by different MVD methods, using both synthetic and real images as <b>prompts.</b></p></p class="citation"></blockquote><h3 id=1550--183299-hint-high-quality-inpainting-transformer-with-mask-aware-encoding-and-enhanced-attention-shuang-chen-et-al-2024>(15/50 | 183/299) HINT: High-quality INPainting Transformer with Mask-Aware Encoding and Enhanced Attention (Shuang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuang Chen, Amir Atapour-Abarghouei, Hubert P. H. Shum. (2024)<br><strong>HINT: High-quality INPainting Transformer with Mask-Aware Encoding and Enhanced Attention</strong><br><button class=copy-to-clipboard title="HINT: High-quality INPainting Transformer with Mask-Aware Encoding and Enhanced Attention" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Transformer, Automatic Speech Recognition, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14185v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14185v1.pdf filename=2402.14185v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing image inpainting methods leverage <b>convolution-based</b> downsampling approaches to reduce spatial dimensions. This may result in information loss from corrupted images where the available information is inherently sparse, especially for the scenario of large missing regions. Recent advances in <b>self-attention</b> mechanisms within <b>transformers</b> have led to significant improvements in many computer vision tasks including inpainting. However, limited by the computational costs, existing methods cannot fully exploit the efficacy of long-range modelling capabilities of such models. In this paper, we propose an end-to-end High-quality INpainting <b>Transformer,</b> abbreviated as HINT, which consists of a novel mask-aware pixel-shuffle downsampling module (MPD) to preserve the visible information extracted from the corrupted image while maintaining the integrity of the information available for high-level inferences made within the model. Moreover, we propose a Spatially-activated Channel Attention Layer (SCAL), an efficient <b>self-attention</b> mechanism interpreting spatial awareness to model the corrupted image at multiple scales. To further enhance the effectiveness of SCAL, motivated by recent advanced in <b>speech</b> <b>recognition,</b> we introduce a sandwich structure that places feed-forward networks before and after the SCAL module. We demonstrate the superior performance of HINT compared to contemporary state-of-the-art models on four datasets, CelebA, CelebA-HQ, Places2, and Dunhuang.</p></p class="citation"></blockquote><h3 id=1650--184299-stop-reasoning-when-multimodal-llms-with-chain-of-thought-reasoning-meets-adversarial-images-zefeng-wang-et-al-2024>(16/50 | 184/299) Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images (Zefeng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zefeng Wang, Zhen Han, Shuo Chen, Fan Xue, Zifeng Ding, Xun Xiao, Volker Tresp, Philip Torr, Jindong Gu. (2024)<br><strong>Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images</strong><br><button class=copy-to-clipboard title="Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CR, cs-CV, cs-LG, cs.CV<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Reasoning, Large Language Model, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14899v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14899v1.pdf filename=2402.14899v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>Multimodal</b> <b>LLMs</b> (MLLMs) have shown a great ability to understand images. However, like traditional vision models, they are still vulnerable to <b>adversarial</b> <b>images.</b> Meanwhile, Chain-of-Thought (CoT) <b>reasoning</b> has been widely explored on MLLMs, which not only improves model&rsquo;s performance, but also enhances model&rsquo;s explainability by giving intermediate <b>reasoning</b> steps. Nevertheless, there is still a lack of study regarding MLLMs&rsquo; <b>adversarial</b> <b>robustness</b> with CoT and an understanding of what the rationale looks like when MLLMs infer wrong answers with <b>adversarial</b> <b>images.</b> Our research evaluates the <b>adversarial</b> <b>robustness</b> of MLLMs when employing CoT <b>reasoning,</b> finding that CoT marginally improves <b>adversarial</b> <b>robustness</b> against existing attack methods. Moreover, we introduce a novel stop-reasoning attack technique that effectively bypasses the CoT-induced robustness enhancements. Finally, we demonstrate the alterations in CoT <b>reasoning</b> when MLLMs confront <b>adversarial</b> <b>images,</b> shedding light on their <b>reasoning</b> process under <b>adversarial</b> <b>attacks.</b></p></p class="citation"></blockquote><h3 id=1750--185299-consolidating-attention-features-for-multi-view-image-editing-or-patashnik-et-al-2024>(17/50 | 185/299) Consolidating Attention Features for Multi-view Image Editing (Or Patashnik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Or Patashnik, Rinon Gal, Daniel Cohen-Or, Jun-Yan Zhu, Fernando De la Torre. (2024)<br><strong>Consolidating Attention Features for Multi-view Image Editing</strong><br><button class=copy-to-clipboard title="Consolidating Attention Features for Multi-view Image Editing" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 35<br>Keywords: Geometry, Text2image, Prompt, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14792v1.pdf filename=2402.14792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale <b>text-to-image</b> models enable a wide range of image editing techniques, using text <b>prompts</b> or even spatial controls. However, applying these editing methods to multi-view images depicting a single scene leads to 3D-inconsistent results. In this work, we focus on spatial control-based geometric manipulations and introduce a method to consolidate the editing process across various views. We build on two insights: (1) maintaining consistent features throughout the generative process helps attain consistency in multi-view editing, and (2) the queries in <b>self-attention</b> layers significantly influence the image structure. Hence, we propose to improve the geometric consistency of the edited images by enforcing the consistency of the queries. To do so, we introduce QNeRF, a neural radiance field trained on the internal query features of the edited images. Once trained, QNeRF can render 3D-consistent queries, which are then softly injected back into the <b>self-attention</b> layers during generation, greatly improving multi-view consistency. We refine the process through a progressive, iterative method that better consolidates queries across the diffusion timesteps. We compare our method to a range of existing techniques and demonstrate that it can achieve better multi-view consistency and higher fidelity to the input scene. These advantages allow us to train NeRFs with fewer visual artifacts, that are better aligned with the target <b>geometry.</b></p></p class="citation"></blockquote><h3 id=1850--186299-clove-encoding-compositional-language-in-contrastive-vision-language-models-santiago-castro-et-al-2024>(18/50 | 186/299) CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models (Santiago Castro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Santiago Castro, Amir Ziai, Avneesh Saluja, Zhuoning Yuan, Rada Mihalcea. (2024)<br><strong>CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models</strong><br><button class=copy-to-clipboard title="CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, GPT, Vision-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15021v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15021v1.pdf filename=2402.15021v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent years have witnessed a significant increase in the performance of Vision and Language tasks. Foundational <b>Vision-Language</b> Models (VLMs), such as CLIP, have been leveraged in multiple settings and demonstrated remarkable performance across several tasks. Such models excel at object-centric recognition yet learn text representations that seem invariant to word order, failing to compose known concepts in novel ways. However, no evidence exists that any VLM, including large-scale single-stream models such as <b>GPT-4V,</b> identifies compositions successfully. In this paper, we introduce a framework to significantly improve the ability of existing models to encode compositional language, with over 10% absolute improvement on compositionality <b>benchmarks,</b> while maintaining or improving the performance on standard object-recognition and retrieval <b>benchmarks.</b> Our code and pre-trained models are publicly available at <a href=https://github.com/netflix/clove>https://github.com/netflix/clove</a>.</p></p class="citation"></blockquote><h3 id=1950--187299-multi-hmr-multi-person-whole-body-human-mesh-recovery-in-a-single-shot-fabien-baradel-et-al-2024>(19/50 | 187/299) Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot (Fabien Baradel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fabien Baradel, Matthieu Armando, Salma Galaaoui, Romain Brégier, Philippe Weinzaepfel, Grégory Rogez, Thomas Lucas. (2024)<br><strong>Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot</strong><br><button class=copy-to-clipboard title="Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Vision Transformer, Benchmarking, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14654v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14654v1.pdf filename=2402.14654v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Multi-HMR, a strong single-shot model for multi-person 3D human mesh recovery from a single RGB image. Predictions encompass the whole body, i.e, including hands and facial expressions, using the SMPL-X parametric model and spatial location in the camera coordinate system. Our model detects people by predicting coarse 2D heatmaps of person centers, using features produced by a standard <b>Vision</b> <b>Transformer</b> (ViT) backbone. It then predicts their whole-body pose, shape and spatial location using a new cross-attention module called the Human Prediction Head (HPH), with one query per detected center token, attending to the entire set of features. As direct prediction of SMPL-X parameters yields suboptimal results, we introduce CUFFS; the Close-Up Frames of Full-Body Subjects dataset, containing humans close to the camera with diverse hand poses. We show that incorporating this dataset into training further enhances predictions, particularly for hands, enabling us to achieve state-of-the-art performance. Multi-HMR also optionally accounts for camera intrinsics, if available, by encoding camera ray directions for each image token. This simple design achieves strong performance on whole-body and body-only <b>benchmarks</b> simultaneously. We train models with various backbone sizes and input resolutions. In particular, using a ViT-S backbone and $448\times448$ input images already yields a fast and competitive model with respect to state-of-the-art methods, while considering larger models and higher resolutions further improve performance.</p></p class="citation"></blockquote><h3 id=2050--188299-s2former-or-single-stage-bimodal-transformer-for-scene-graph-generation-in-or-jialun-pei-et-al-2024>(20/50 | 188/299) S^2Former-OR: Single-Stage Bimodal Transformer for Scene Graph Generation in OR (Jialun Pei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialun Pei, Diandian Guo, Jingyang Zhang, Manxi Lin, Yueming Jin, Pheng-Ann Heng. (2024)<br><strong>S^2Former-OR: Single-Stage Bimodal Transformer for Scene Graph Generation in OR</strong><br><button class=copy-to-clipboard title="S^2Former-OR: Single-Stage Bimodal Transformer for Scene Graph Generation in OR" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 31<br>Keywords: Object Detection, Graph, Benchmarking, Geometry, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14461v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14461v1.pdf filename=2402.14461v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene <b>graph</b> generation (SGG) of surgical procedures is crucial in enhancing holistically cognitive intelligence in the operating room (OR). However, previous works have primarily relied on the multi-stage learning that generates semantic scene <b>graphs</b> dependent on intermediate processes with pose estimation and <b>object</b> <b>detection,</b> which may compromise model efficiency and efficacy, also impose extra annotation burden. In this study, we introduce a novel single-stage bimodal <b>transformer</b> framework for SGG in the OR, termed S^2Former-OR, aimed to complementally leverage multi-view 2D scenes and 3D point clouds for SGG in an end-to-end manner. Concretely, our model embraces a View-Sync Transfusion scheme to encourage multi-view visual information interaction. Concurrently, a <b>Geometry-Visual</b> Cohesion operation is designed to integrate the synergic 2D semantic features into 3D point cloud features. Moreover, based on the augmented feature, we propose a novel relation-sensitive <b>transformer</b> decoder that embeds dynamic entity-pair queries and relational trait priors, which enables the direct prediction of entity-pair relations for <b>graph</b> generation without intermediate steps. Extensive experiments have validated the superior SGG performance and lower computational cost of S^2Former-OR on 4D-OR <b>benchmark,</b> compared with current OR-SGG methods, e.g., 3% Precision increase and 24.2M reduction in model parameters. We further compared our method with generic single-stage SGG methods with broader metrics for a comprehensive evaluation, with consistently better performance achieved. The code will be made available.</p></p class="citation"></blockquote><h3 id=2150--189299-the-common-stability-mechanism-behind-most-self-supervised-learning-approaches-abhishek-jha-et-al-2024>(21/50 | 189/299) The Common Stability Mechanism behind most Self-Supervised Learning Approaches (Abhishek Jha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhishek Jha, Matthew B. Blaschko, Yuki M. Asano, Tinne Tuytelaars. (2024)<br><strong>The Common Stability Mechanism behind most Self-Supervised Learning Approaches</strong><br><button class=copy-to-clipboard title="The Common Stability Mechanism behind most Self-Supervised Learning Approaches" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Data Augmentation, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14957v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14957v1.pdf filename=2402.14957v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Last couple of years have witnessed a tremendous progress in <b>self-supervised</b> <b>learning</b> (SSL), the success of which can be attributed to the introduction of useful inductive biases in the learning process to learn meaningful visual representations while avoiding collapse. These inductive biases and constraints manifest themselves in the form of different optimization formulations in the SSL techniques, e.g. by utilizing negative examples in a contrastive formulation, or exponential moving average and predictor in BYOL and SimSiam. In this paper, we provide a framework to explain the stability mechanism of these different SSL techniques: i) we discuss the working mechanism of contrastive techniques like SimCLR, non-contrastive techniques like BYOL, SWAV, SimSiam, Barlow Twins, and DINO; ii) we provide an argument that despite different formulations these methods implicitly optimize a similar objective function, i.e. minimizing the magnitude of the expected representation over all <b>data</b> <b>samples,</b> or the mean of the <b>data</b> <b>distribution,</b> while maximizing the magnitude of the expected representation of individual samples over different <b>data</b> <b>augmentations;</b> iii) we provide mathematical and empirical evidence to support our framework. We formulate different hypotheses and test them using the Imagenet100 dataset.</p></p class="citation"></blockquote><h3 id=2250--190299-two-stage-cytopathological-image-synthesis-for-augmenting-cervical-abnormality-screening-zhenrong-shen-et-al-2024>(22/50 | 190/299) Two-stage Cytopathological Image Synthesis for Augmenting Cervical Abnormality Screening (Zhenrong Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenrong Shen, Manman Fei, Xin Wang, Jiangdong Cai, Sheng Wang, Lichi Zhang, Qian Wang. (2024)<br><strong>Two-stage Cytopathological Image Synthesis for Augmenting Cervical Abnormality Screening</strong><br><button class=copy-to-clipboard title="Two-stage Cytopathological Image Synthesis for Augmenting Cervical Abnormality Screening" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Data Augmentation, Fine-tuning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14707v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14707v2.pdf filename=2402.14707v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic thin-prep cytologic test (TCT) screening can assist pathologists in finding cervical abnormality towards accurate and efficient cervical cancer diagnosis. Current automatic TCT screening systems mostly involve abnormal cervical cell detection, which generally requires large-scale and diverse training <b>data</b> <b>with</b> high-quality annotations to achieve promising performance. Pathological image synthesis is naturally raised to minimize the efforts in <b>data</b> <b>collection</b> and annotation. However, it is challenging to generate realistic large-size cytopathological images while simultaneously synthesizing visually plausible appearances for small-size abnormal cervical cells. In this paper, we propose a two-stage image synthesis framework to create synthetic <b>data</b> <b>for</b> augmenting cervical abnormality screening. In the first Global Image Generation stage, a Normal Image Generator is designed to generate cytopathological images full of normal cervical cells. In the second Local Cell Editing stage, normal cells are randomly selected from the generated images and then are converted to different types of abnormal cells using the proposed Abnormal Cell Synthesizer. Both Normal Image Generator and Abnormal Cell Synthesizer are built upon Stable Diffusion, a pre-trained <b>foundation</b> <b>model</b> for image synthesis, via parameter-efficient <b>fine-tuning</b> methods for customizing cytopathological image contents and extending spatial layout controllability, respectively. Our experiments demonstrate the synthetic image quality, diversity, and controllability of the proposed synthesis framework, and validate its <b>data</b> <b>augmentation</b> effectiveness in enhancing the performance of abnormal cervical cell detection.</p></p class="citation"></blockquote><h3 id=2350--191299-measuring-multimodal-mathematical-reasoning-with-math-vision-dataset-ke-wang-et-al-2024>(23/50 | 191/299) Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset (Ke Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, Hongsheng Li. (2024)<br><strong>Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset</strong><br><button class=copy-to-clipboard title="Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV, math-HO<br>Keyword Score: 29<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Mathematical Reasoning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14804v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14804v1.pdf filename=2402.14804v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in Large <b>Multimodal</b> Models (LMMs) have shown promising results in <b>mathematical</b> <b>reasoning</b> within visual contexts, with models approaching human-level performance on existing <b>benchmarks</b> such as MathVista. However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these <b>benchmarks.</b> To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality <b>mathematical</b> <b>problems</b> with visual contexts sourced from real math competitions. Spanning 16 distinct <b>mathematical</b> <b>disciplines</b> and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating the <b>mathematical</b> <b>reasoning</b> abilities of LMMs. Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on MATH-V, underscoring the imperative for further advancements in LMMs. Moreover, our detailed categorization allows for a thorough error analysis of LMMs, offering valuable insights to guide future research and development. The project is available at <a href=https://mathvision-cuhk.github.io>https://mathvision-cuhk.github.io</a></p></p class="citation"></blockquote><h3 id=2450--192299-a-landmark-aware-visual-navigation-dataset-faith-johnson-et-al-2024>(24/50 | 192/299) A Landmark-Aware Visual Navigation Dataset (Faith Johnson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Faith Johnson, Bryan Bo Cao, Kristin Dana, Shubham Jain, Ashwin Ashok. (2024)<br><strong>A Landmark-Aware Visual Navigation Dataset</strong><br><button class=copy-to-clipboard title="A Landmark-Aware Visual Navigation Dataset" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 28<br>Keywords: Graph, Representation Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14281v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14281v1.pdf filename=2402.14281v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Map <b>representation</b> <b>learned</b> by expert demonstrations has shown promising research value. However, recent advancements in the visual navigation field face challenges due to the lack of human datasets in the real world for efficient <b>supervised</b> <b>representation</b> <b>learning</b> of the environments. We present a Landmark-Aware Visual Navigation (LAVN) dataset to allow for <b>supervised</b> <b>learning</b> of human-centric exploration policies and map building. We collect RGB observation and human point-click pairs as a human annotator explores virtual and real-world environments with the goal of full coverage exploration of the space. The human annotators also provide distinct landmark examples along each trajectory, which we intuit will simplify the task of map or <b>graph</b> building and localization. These human point-clicks serve as direct supervision for waypoint prediction when learning to explore in environments. Our dataset covers a wide spectrum of scenes, including rooms in indoor environments, as well as walkways outdoors. Dataset is available at DOI: 10.5281/zenodo.10608067.</p></p class="citation"></blockquote><h3 id=2550--193299-dualfocus-integrating-macro-and-micro-perspectives-in-multi-modal-large-language-models-yuhang-cao-et-al-2024>(25/50 | 193/299) DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models (Yuhang Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, Jiaqi Wang. (2024)<br><strong>DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models</strong><br><button class=copy-to-clipboard title="DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Benchmarking, Multi-modal, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14767v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14767v1.pdf filename=2402.14767v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present DualFocus, a novel framework for integrating macro and micro perspectives within <b>multi-modal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) to enhance <b>vision-language</b> task performance. Current MLLMs typically singularly focus on inputs at a predefined resolution, resulting in deficiencies in detailed questions involving local regions. We introduced a DualFocus mechanism where the model concentrates on the image from a macro perspective, responses to the question, and identifies suitable sub-regions to zoom in for subsequent micro perspective analysis. Via the integration of answers from both macro and micro perspectives, the model is adept at addressing tasks that encompass global, detailed, and combined considerations. To endows the DualFocus mechanism in MLLMs, we curated a tailored dataset derived from the Visual Genome (VG) and adapted it to align with the training regimen of DualFocus. Through comparative studies across different model sizes and <b>benchmarks,</b> we demonstrate DualFocus&rsquo;s superiority in balancing detailed examination with holistic insight, significantly reducing hallucination instances in MLLMs and improving their performance in various <b>vision-language</b> tasks.</p></p class="citation"></blockquote><h3 id=2650--194299-framenerf-a-simple-and-efficient-framework-for-few-shot-novel-view-synthesis-yan-xing-et-al-2024>(26/50 | 194/299) FrameNeRF: A Simple and Efficient Framework for Few-shot Novel View Synthesis (Yan Xing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Xing, Pan Wang, Ligang Liu, Daolun Li, Li Zhang. (2024)<br><strong>FrameNeRF: A Simple and Efficient Framework for Few-shot Novel View Synthesis</strong><br><button class=copy-to-clipboard title="FrameNeRF: A Simple and Efficient Framework for Few-shot Novel View Synthesis" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Few-shot, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14586v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14586v2.pdf filename=2402.14586v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel framework, called FrameNeRF, designed to apply off-the-shelf fast high-fidelity NeRF models with fast training speed and high rendering quality for <b>few-shot</b> novel view synthesis tasks. The training stability of fast high-fidelity models is typically constrained to dense views, making them unsuitable for <b>few-shot</b> novel view synthesis tasks. To address this limitation, we utilize a regularization model as a data generator to produce dense views from sparse inputs, facilitating subsequent training of fast high-fidelity models. Since these dense views are pseudo ground truth generated by the regularization model, original sparse images are then used to <b>fine-tune</b> the fast high-fidelity model. This process helps the model learn realistic details and correct artifacts introduced in earlier stages. By leveraging an off-the-shelf regularization model and a fast high-fidelity model, our approach achieves state-of-the-art performance across various <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=2750--195299-cameras-as-rays-pose-estimation-via-ray-diffusion-jason-y-zhang-et-al-2024>(27/50 | 195/299) Cameras as Rays: Pose Estimation via Ray Diffusion (Jason Y. Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, Shubham Tulsiani. (2024)<br><strong>Cameras as Rays: Pose Estimation via Ray Diffusion</strong><br><button class=copy-to-clipboard title="Cameras as Rays: Pose Estimation via Ray Diffusion" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14817v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14817v1.pdf filename=2402.14817v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating camera poses is a fundamental task for 3D reconstruction and remains challenging given sparse views (&lt;10). In contrast to existing approaches that pursue top-down prediction of global parametrizations of camera extrinsics, we propose a distributed representation of camera pose that treats a camera as a bundle of rays. This representation allows for a tight coupling with spatial image features improving pose precision. We observe that this representation is naturally suited for set-level level <b>transformers</b> and develop a regression-based approach that maps image patches to corresponding rays. To capture the inherent uncertainties in sparse-view pose inference, we adapt this approach to learn a denoising <b>diffusion</b> <b>model</b> which allows us to sample plausible modes while improving performance. Our proposed methods, both regression- and <b>diffusion-based,</b> <b>demonstrate</b> state-of-the-art performance on camera pose estimation on CO3D while generalizing to unseen object categories and in-the-wild captures.</p></p class="citation"></blockquote><h3 id=2850--196299-customize-a-video-one-shot-motion-customization-of-text-to-video-diffusion-models-yixuan-ren-et-al-2024>(28/50 | 196/299) Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models (Yixuan Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixuan Ren, Yang Zhou, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, Abhinav Shrivastava. (2024)<br><strong>Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models</strong><br><button class=copy-to-clipboard title="Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14780v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14780v1.pdf filename=2402.14780v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image customization has been extensively studied in <b>text-to-image</b> (T2I) <b>diffusion</b> <b>models,</b> leading to impressive outcomes and applications. With the emergence of text-to-video (T2V) <b>diffusion</b> <b>models,</b> its temporal counterpart, motion customization, has not yet been well investigated. To address the challenge of one-shot motion customization, we propose Customize-A-Video that models the motion from a single reference video and adapting it to new subjects and scenes with both spatial and temporal varieties. It leverages low-rank adaptation (LoRA) on temporal attention layers to tailor the pre-trained T2V <b>diffusion</b> <b>model</b> for specific motion modeling from the reference videos. To disentangle the spatial and temporal information during the training pipeline, we introduce a novel concept of appearance absorbers that detach the original appearance from the single reference video prior to motion learning. Our proposed method can be easily extended to various downstream tasks, including custom video generation and editing, video appearance customization, and multiple motion combination, in a plug-and-play fashion. Our project page can be found at <a href=https://anonymous-314.github.io>https://anonymous-314.github.io</a>.</p></p class="citation"></blockquote><h3 id=2950--197299-high-speed-detector-for-low-powered-devices-in-aerial-grasping-ashish-kumar-et-al-2024>(29/50 | 197/299) High-Speed Detector For Low-Powered Devices In Aerial Grasping (Ashish Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashish Kumar, Laxmidhar Behera. (2024)<br><strong>High-Speed Detector For Low-Powered Devices In Aerial Grasping</strong><br><button class=copy-to-clipboard title="High-Speed Detector For Low-Powered Devices In Aerial Grasping" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 20<br>Keywords: Yolo, Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14591v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14591v1.pdf filename=2402.14591v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous aerial harvesting is a highly complex problem because it requires numerous interdisciplinary algorithms to be executed on mini low-powered computing devices. <b>Object</b> <b>detection</b> is one such algorithm that is compute-hungry. In this context, we make the following contributions: (i) Fast Fruit Detector (FFD), a resource-efficient, single-stage, and postprocessing-free <b>object</b> <b>detector</b> based on our novel latent <b>object</b> <b>representation</b> (LOR) module, query assignment, and prediction strategy. FFD achieves 100FPS@FP32 precision on the latest 10W NVIDIA Jetson-NX embedded device while co-existing with other time-critical sub-systems such as control, grasping, SLAM, a major achievement of this work. (ii) a method to generate vast amounts of training data without exhaustive manual labelling of fruit images since they consist of a large number of instances, which increases the labelling cost and time. (iii) an open-source fruit detection dataset having plenty of very small-sized instances that are difficult to detect. Our exhaustive evaluations on our and MinneApple dataset show that FFD, being only a single-scale detector, is more accurate than many representative detectors, e.g. FFD is better than single-scale Faster-RCNN by 10.7AP, multi-scale Faster-RCNN by 2.3AP, and better than latest single-scale <b>YOLO-v8</b> by 8AP and multi-scale <b>YOLO-v8</b> by 0.3 while being considerably faster.</p></p class="citation"></blockquote><h3 id=3050--198299-debiasing-text-to-image-diffusion-models-ruifei-he-et-al-2024>(30/50 | 198/299) Debiasing Text-to-Image Diffusion Models (Ruifei He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruifei He, Chuhui Xue, Haoru Tan, Wenqing Zhang, Yingchen Yu, Song Bai, Xiaojuan Qi. (2024)<br><strong>Debiasing Text-to-Image Diffusion Models</strong><br><button class=copy-to-clipboard title="Debiasing Text-to-Image Diffusion Models" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14577v1.pdf filename=2402.14577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning-based <b>Text-to-Image</b> (TTI) models like Stable <b>Diffusion</b> <b>have</b> revolutionized the way visual content is generated in various domains. However, recent research has shown that nonnegligible social bias exists in current state-of-the-art TTI systems, which raises important concerns. In this work, we target resolving the social bias in TTI <b>diffusion</b> <b>models.</b> We begin by formalizing the problem setting and use the text descriptions of bias groups to establish an unsafe direction for guiding the <b>diffusion</b> <b>process.</b> Next, we simplify the problem into a weight optimization problem and attempt a Reinforcement solver, Policy Gradient, which shows sub-optimal performance with slow convergence. Further, to overcome limitations, we propose an iterative distribution alignment (IDA) method. Despite its simplicity, we show that IDA shows efficiency and fast convergence in resolving the social bias in TTI <b>diffusion</b> <b>models.</b> Our code will be released.</p></p class="citation"></blockquote><h3 id=3150--199299-reimagining-anomalies-what-if-anomalies-were-normal-philipp-liznerski-et-al-2024>(31/50 | 199/299) Reimagining Anomalies: What If Anomalies Were Normal? (Philipp Liznerski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Liznerski, Saurabh Varshneya, Ece Calikus, Sophie Fellenz, Marius Kloft. (2024)<br><strong>Reimagining Anomalies: What If Anomalies Were Normal?</strong><br><button class=copy-to-clipboard title="Reimagining Anomalies: What If Anomalies Were Normal?" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, stat-ML<br>Keyword Score: 20<br>Keywords: Anomaly Detection, Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14469v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14469v1.pdf filename=2402.14469v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning-based methods have achieved a breakthrough in image <b>anomaly</b> <b>detection,</b> but their complexity introduces a considerable challenge to understanding why an instance is predicted to be anomalous. We introduce a novel explanation method that generates multiple <b>counterfactual</b> examples for each <b>anomaly,</b> <b>capturing</b> diverse concepts of anomalousness. A <b>counterfactual</b> example is a modification of the <b>anomaly</b> <b>that</b> is perceived as normal by the <b>anomaly</b> <b>detector.</b> The method provides a high-level semantic explanation of the mechanism that triggered the <b>anomaly</b> <b>detector,</b> allowing users to explore &ldquo;what-if scenarios.&rdquo; Qualitative and quantitative analyses across various image datasets show that the method applied to state-of-the-art <b>anomaly</b> <b>detectors</b> can achieve high-quality semantic explanations of detectors.</p></p class="citation"></blockquote><h3 id=3250--200299-diffusion-model-based-visual-compensation-guidance-and-visual-difference-analysis-for-no-reference-image-quality-assessment-zhaoyang-wang-et-al-2024>(32/50 | 200/299) Diffusion Model Based Visual Compensation Guidance and Visual Difference Analysis for No-Reference Image Quality Assessment (Zhaoyang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoyang Wang, Bo Hu, Mingyang Zhang, Jie Li, Leida Li, Maoguo Gong, Xinbo Gao. (2024)<br><strong>Diffusion Model Based Visual Compensation Guidance and Visual Difference Analysis for No-Reference Image Quality Assessment</strong><br><button class=copy-to-clipboard title="Diffusion Model Based Visual Compensation Guidance and Visual Difference Analysis for No-Reference Image Quality Assessment" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14401v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14401v1.pdf filename=2402.14401v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing free-energy guided No-Reference Image Quality Assessment (NR-IQA) methods still suffer from finding a balance between learning feature information at the pixel level of the image and capturing high-level feature information and the efficient utilization of the obtained high-level feature information remains a challenge. As a novel class of state-of-the-art (SOTA) generative model, the <b>diffusion</b> <b>model</b> exhibits the capability to model intricate relationships, enabling a comprehensive understanding of images and possessing a better learning of both high-level and low-level visual features. In view of these, we pioneer the exploration of the <b>diffusion</b> <b>model</b> into the domain of NR-IQA. Firstly, we devise a new <b>diffusion</b> <b>restoration</b> network that leverages the produced enhanced image and noise-containing images, incorporating nonlinear features obtained during the denoising process of the <b>diffusion</b> <b>model,</b> as high-level visual information. Secondly, two visual evaluation branches are designed to comprehensively analyze the obtained high-level feature information. These include the visual compensation guidance branch, grounded in the <b>transformer</b> architecture and noise embedding strategy, and the visual difference analysis branch, built on the ResNet architecture and the residual transposed attention block. Extensive experiments are conducted on seven public NR-IQA datasets, and the results demonstrate that the proposed model outperforms SOTA methods for NR-IQA.</p></p class="citation"></blockquote><h3 id=3350--201299-learning-to-kern----set-wise-estimation-of-optimal-letter-space-kei-nakatsuru-et-al-2024>(33/50 | 201/299) Learning to Kern &ndash; Set-wise Estimation of Optimal Letter Space (Kei Nakatsuru et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kei Nakatsuru, Seiichi Uchida. (2024)<br><strong>Learning to Kern &ndash; Set-wise Estimation of Optimal Letter Space</strong><br><button class=copy-to-clipboard title="Learning to Kern -- Set-wise Estimation of Optimal Letter Space" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14313v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14313v1.pdf filename=2402.14313v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Kerning is the task of setting appropriate horizontal spaces for all possible letter pairs of a certain font. One of the difficulties of kerning is that the appropriate space differs for each letter pair. Therefore, for a total of 52 capital and small letters, we need to adjust $52 \times 52 = 2704$ different spaces. Another difficulty is that there is neither a general procedure nor criterion for automatic kerning; therefore, kerning is still done manually or with heuristics. In this paper, we tackle kerning by proposing two machine-learning models, called pairwise and set-wise models. The former is a simple deep neural network that estimates the letter space for two given letter images. In contrast, the latter is a <b>Transformer-based</b> model and estimates the letter spaces for three or more given letter images. For example, the set-wise model simultaneously estimates 2704 spaces for 52 letter images for a certain font. Among the two models, the set-wise model is not only more efficient but also more accurate because its internal <b>self-attention</b> mechanism allows for more consistent kerning for all letters. Experimental results on about 2500 Google fonts and their quantitative and qualitative analyses show that the set-wise model has an average estimation error of only about 5.3 pixels when the average letter space of all fonts and letter pairs is about 115 pixels.</p></p class="citation"></blockquote><h3 id=3450--202299-yolo-tla-an-efficient-and-lightweight-small-object-detection-model-based-on-yolov5-peng-gao-et-al-2024>(34/50 | 202/299) YOLO-TLA: An Efficient and Lightweight Small Object Detection Model based on YOLOv5 (Peng Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Gao, Chun-Lin Ji, Tao Yu, Ru-Yue Yuan. (2024)<br><strong>YOLO-TLA: An Efficient and Lightweight Small Object Detection Model based on YOLOv5</strong><br><button class=copy-to-clipboard title="YOLO-TLA: An Efficient and Lightweight Small Object Detection Model based on YOLOv5" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Yolo, Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14309v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14309v1.pdf filename=2402.14309v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Object</b> <b>detection,</b> a crucial aspect of computer vision, has seen significant advancements in accuracy and robustness. Despite these advancements, practical applications still face notable challenges, primarily the inaccurate detection or missed detection of small <b>objects.</b> <b>In</b> this paper, we propose <b>YOLO-TLA,</b> an advanced <b>object</b> <b>detection</b> model building on YOLOv5. We first introduce an additional detection layer for small <b>objects</b> <b>in</b> the neck network pyramid architecture, thereby producing a feature map of a larger scale to discern finer features of small <b>objects.</b> <b>Further,</b> we integrate the C3CrossCovn module into the backbone network. This module uses sliding window feature extraction, which effectively minimizes both computational demand and the number of parameters, rendering the model more compact. Additionally, we have incorporated a global attention mechanism into the backbone network. This mechanism combines the channel information with global information to create a weighted feature map. This feature map is tailored to highlight the attributes of the <b>object</b> <b>of</b> interest, while effectively ignoring irrelevant details. In comparison to the baseline YOLOv5s model, our newly developed <b>YOLO-TLA</b> model has shown considerable improvements on the MS COCO validation dataset, with increases of 4.6% in <a href=mailto:mAP@0.5>mAP@0.5</a> and 4% in <a href=mailto:mAP@0.5>mAP@0.5</a>:0.95, all while keeping the model size compact at 9.49M parameters. Further extending these improvements to the YOLOv5m model, the enhanced version exhibited a 1.7% and 1.9% increase in <a href=mailto:mAP@0.5>mAP@0.5</a> and <a href=mailto:mAP@0.5>mAP@0.5</a>:0.95, respectively, with a total of 27.53M parameters. These results validate the <b>YOLO-TLA</b> model&rsquo;s efficient and effective performance in small <b>object</b> <b>detection,</b> achieving high accuracy with fewer parameters and computational demands.</p></p class="citation"></blockquote><h3 id=3550--203299-a-self-supervised-pressure-map-human-keypoint-detection-approch-optimizing-generalization-and-computational-efficiency-across-datasets-chengzhang-yu-et-al-2024>(35/50 | 203/299) A Self-supervised Pressure Map human keypoint Detection Approch: Optimizing Generalization and Computational Efficiency Across Datasets (Chengzhang Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengzhang Yu, Xianjun Yang, Wenxia Bao, Shaonan Wang, Zhiming Yao. (2024)<br><strong>A Self-supervised Pressure Map human keypoint Detection Approch: Optimizing Generalization and Computational Efficiency Across Datasets</strong><br><button class=copy-to-clipboard title="A Self-supervised Pressure Map human keypoint Detection Approch: Optimizing Generalization and Computational Efficiency Across Datasets" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14241v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14241v1.pdf filename=2402.14241v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In environments where RGB images are inadequate, pressure maps is a viable alternative, garnering scholarly attention. This study introduces a novel <b>self-supervised</b> pressure map keypoint detection (SPMKD) method, addressing the current gap in specialized designs for human keypoint extraction from pressure maps. Central to our contribution is the Encoder-Fuser-Decoder (EFD) model, which is a robust framework that integrates a lightweight encoder for precise human keypoint detection, a fuser for efficient gradient propagation, and a decoder that transforms human keypoints into reconstructed pressure maps. This structure is further enhanced by the Classification-to-Regression Weight Transfer (CRWT) method, which <b>fine-tunes</b> accuracy through initial classification task training. This innovation not only enhances human keypoint generalization without manual annotations but also showcases remarkable efficiency and generalization, evidenced by a reduction to only $5.96%$ in FLOPs and $1.11%$ in parameter count compared to the baseline methods.</p></p class="citation"></blockquote><h3 id=3650--204299-snap-video-scaled-spatiotemporal-transformers-for-text-to-video-synthesis-willi-menapace-et-al-2024>(36/50 | 204/299) Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis (Willi Menapace et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, Sergey Tulyakov. (2024)<br><strong>Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis</strong><br><button class=copy-to-clipboard title="Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14797v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14797v1.pdf filename=2402.14797v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contemporary models for generating images show remarkable quality and versatility. Swayed by these advantages, the research community repurposes them to generate videos. Since video content is highly redundant, we argue that naively bringing advances of image models to the video generation domain reduces motion fidelity, visual quality and impairs scalability. In this work, we build Snap Video, a video-first model that systematically addresses these challenges. To do that, we first extend the EDM framework to take into account spatially and temporally redundant pixels and naturally support video generation. Second, we show that a U-Net - a workhorse behind image generation - scales poorly when generating videos, requiring significant computational overhead. Hence, we propose a new <b>transformer-based</b> architecture that trains 3.31 times faster than U-Nets (and is ~4.5 faster at inference). This allows us to efficiently train a text-to-video model with billions of parameters for the first time, reach state-of-the-art results on a number of <b>benchmarks,</b> and generate videos with substantially higher quality, temporal consistency, and motion complexity. The user studies showed that our model was favored by a large margin over the most recent methods. See our website at <a href=https://snap-research.github.io/snapvideo/>https://snap-research.github.io/snapvideo/</a>.</p></p class="citation"></blockquote><h3 id=3750--205299-a-transformer-model-for-boundary-detection-in-continuous-sign-language-razieh-rastgoo-et-al-2024>(37/50 | 205/299) A Transformer Model for Boundary Detection in Continuous Sign Language (Razieh Rastgoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Razieh Rastgoo, Kourosh Kiani, Sergio Escalera. (2024)<br><strong>A Transformer Model for Boundary Detection in Continuous Sign Language</strong><br><button class=copy-to-clipboard title="A Transformer Model for Boundary Detection in Continuous Sign Language" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14720v1.pdf filename=2402.14720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sign Language Recognition (SLR) has garnered significant attention from researchers in recent years, particularly the intricate domain of Continuous Sign Language Recognition (CSLR), which presents heightened complexity compared to Isolated Sign Language Recognition (ISLR). One of the prominent challenges in CSLR pertains to accurately detecting the boundaries of isolated signs within a continuous video stream. Additionally, the reliance on handcrafted features in existing models poses a challenge to achieving optimal accuracy. To surmount these challenges, we propose a novel approach utilizing a <b>Transformer-based</b> model. Unlike traditional models, our approach focuses on enhancing accuracy while eliminating the need for handcrafted features. The <b>Transformer</b> model is employed for both ISLR and CSLR. The training process involves using isolated sign videos, where hand keypoint features extracted from the input video are enriched using the <b>Transformer</b> model. Subsequently, these enriched features are forwarded to the final classification layer. The trained model, coupled with a post-processing method, is then applied to detect isolated sign boundaries within continuous sign videos. The evaluation of our model is conducted on two distinct datasets, including both continuous signs and their corresponding isolated signs, demonstrates promising results.</p></p class="citation"></blockquote><h3 id=3850--206299-quadruplet-loss-for-improving-the-robustness-to-face-morphing-attacks-iurii-medvedev-et-al-2024>(38/50 | 206/299) Quadruplet Loss For Improving the Robustness to Face Morphing Attacks (Iurii Medvedev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Iurii Medvedev, Nuno Gonçalves. (2024)<br><strong>Quadruplet Loss For Improving the Robustness to Face Morphing Attacks</strong><br><button class=copy-to-clipboard title="Quadruplet Loss For Improving the Robustness to Face Morphing Attacks" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Face Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14665v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14665v1.pdf filename=2402.14665v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in deep learning have revolutionized technology and security measures, necessitating robust identification methods. Biometric approaches, leveraging personalized characteristics, offer a promising solution. However, <b>Face</b> <b>Recognition</b> Systems are vulnerable to sophisticated attacks, notably <b>face</b> <b>morphing</b> techniques, enabling the creation of fraudulent documents. In this study, we introduce a novel quadruplet loss function for increasing the robustness of <b>face</b> <b>recognition</b> systems against morphing attacks. Our approach involves specific sampling of <b>face</b> <b>image</b> quadruplets, combined with <b>face</b> <b>morphs,</b> for network training. Experimental results demonstrate the efficiency of our strategy in improving the robustness of <b>face</b> <b>recognition</b> networks against morphing attacks.</p></p class="citation"></blockquote><h3 id=3950--207299-towards-seamless-adaptation-of-pre-trained-models-for-visual-place-recognition-feng-lu-et-al-2024>(39/50 | 207/299) Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition (Feng Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feng Lu, Lijun Zhang, Xiangyuan Lan, Shuting Dong, Yaowei Wang, Chun Yuan. (2024)<br><strong>Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition</strong><br><button class=copy-to-clipboard title="Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14505v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14505v1.pdf filename=2402.14505v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies show that vision models pre-trained in generic visual learning tasks with large-scale data can provide useful feature representations for a wide range of visual perception problems. However, few attempts have been made to exploit pre-trained <b>foundation</b> <b>models</b> in visual place recognition (VPR). Due to the inherent difference in training objectives and data between the tasks of model pre-training and VPR, how to bridge the gap and fully unleash the capability of pre-trained models for VPR is still a key issue to address. To this end, we propose a novel method to realize seamless adaptation of pre-trained models for VPR. Specifically, to obtain both global and local features that focus on salient landmarks for discriminating places, we design a hybrid adaptation method to achieve both global and local adaptation efficiently, in which only lightweight adapters are tuned without adjusting the pre-trained model. Besides, to guide effective adaptation, we propose a mutual nearest neighbor local feature loss, which ensures proper dense local features are produced for local matching and avoids time-consuming spatial verification in re-ranking. Experimental results show that our method outperforms the state-of-the-art methods with less training data and training time, and uses about only 3% retrieval runtime of the two-stage VPR methods with RANSAC-based spatial verification. It ranks 1st on the MSLS challenge leaderboard (at the time of submission). The code is released at <a href=https://github.com/Lu-Feng/SelaVPR>https://github.com/Lu-Feng/SelaVPR</a>.</p></p class="citation"></blockquote><h3 id=4050--208299-gradual-residuals-alignment-a-dual-stream-framework-for-gan-inversion-and-image-attribute-editing-hao-li-et-al-2024>(40/50 | 208/299) Gradual Residuals Alignment: A Dual-Stream Framework for GAN Inversion and Image Attribute Editing (Hao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Li, Mengqi Huang, Lei Zhang, Bo Hu, Yi Liu, Zhendong Mao. (2024)<br><strong>Gradual Residuals Alignment: A Dual-Stream Framework for GAN Inversion and Image Attribute Editing</strong><br><button class=copy-to-clipboard title="Gradual Residuals Alignment: A Dual-Stream Framework for GAN Inversion and Image Attribute Editing" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14398v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14398v1.pdf filename=2402.14398v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>GAN-based</b> image attribute editing firstly leverages <b>GAN</b> Inversion to project real images into the latent space of <b>GAN</b> and then manipulates corresponding latent codes. Recent inversion methods mainly utilize additional high-bit features to improve image details preservation, as low-bit codes cannot faithfully reconstruct source images, leading to the loss of details. However, during editing, existing works fail to accurately complement the lost details and suffer from poor editability. The main reason is they inject all the lost details indiscriminately at one time, which inherently induces the position and quantity of details to overfit source images, resulting in inconsistent content and artifacts in edited images. This work argues that details should be gradually injected into both the reconstruction and editing process in a multi-stage coarse-to-fine manner for better detail preservation and high editability. Therefore, a novel dual-stream framework is proposed to accurately complement details at each stage. The Reconstruction Stream is employed to embed coarse-to-fine lost details into residual features and then adaptively add them to the <b>GAN</b> generator. In the Editing Stream, residual features are accurately aligned by our Selective Attention mechanism and then injected into the editing process in a multi-stage manner. Extensive experiments have shown the superiority of our framework in both reconstruction accuracy and editing quality compared with existing methods.</p></p class="citation"></blockquote><h3 id=4150--209299-gam-depth-self-supervised-indoor-depth-estimation-leveraging-a-gradient-aware-mask-and-semantic-constraints-anqi-cheng-et-al-2024>(41/50 | 209/299) GAM-Depth: Self-Supervised Indoor Depth Estimation Leveraging a Gradient-Aware Mask and Semantic Constraints (Anqi Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anqi Cheng, Zhiyuan Yang, Haiyue Zhu, Kezhi Mao. (2024)<br><strong>GAM-Depth: Self-Supervised Indoor Depth Estimation Leveraging a Gradient-Aware Mask and Semantic Constraints</strong><br><button class=copy-to-clipboard title="GAM-Depth: Self-Supervised Indoor Depth Estimation Leveraging a Gradient-Aware Mask and Semantic Constraints" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14354v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14354v1.pdf filename=2402.14354v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> depth estimation has evolved into an image reconstruction task that minimizes a photometric loss. While recent methods have made strides in indoor depth estimation, they often produce inconsistent depth estimation in textureless areas and unsatisfactory depth discrepancies at object boundaries. To address these issues, in this work, we propose GAM-Depth, developed upon two novel components: gradient-aware mask and semantic constraints. The gradient-aware mask enables adaptive and robust supervision for both key areas and textureless regions by allocating weights based on gradient magnitudes.The incorporation of semantic constraints for indoor <b>self-supervised</b> depth estimation improves depth discrepancies at object boundaries, leveraging a co-optimization network and proxy semantic labels derived from a pretrained segmentation model. Experimental studies on three indoor datasets, including NYUv2, ScanNet, and InteriorNet, show that GAM-Depth outperforms existing methods and achieves state-of-the-art performance, signifying a meaningful step forward in indoor depth estimation. Our code will be available at <a href=https://github.com/AnqiCheng1234/GAM-Depth>https://github.com/AnqiCheng1234/GAM-Depth</a>.</p></p class="citation"></blockquote><h3 id=4250--210299-font-style-interpolation-with-diffusion-models-tetta-kondo-et-al-2024>(42/50 | 210/299) Font Style Interpolation with Diffusion Models (Tetta Kondo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tetta Kondo, Shumpei Takezaki, Daichi Haraguchi, Seiichi Uchida. (2024)<br><strong>Font Style Interpolation with Diffusion Models</strong><br><button class=copy-to-clipboard title="Font Style Interpolation with Diffusion Models" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14311v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14311v1.pdf filename=2402.14311v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fonts have huge variations in their styles and give readers different impressions. Therefore, generating new fonts is worthy of giving new impressions to readers. In this paper, we employ <b>diffusion</b> <b>models</b> to generate new font styles by interpolating a pair of reference fonts with different styles. More specifically, we propose three different interpolation approaches, image-blending, condition-blending, and noise-blending, with the <b>diffusion</b> <b>models.</b> We perform qualitative and quantitative experimental analyses to understand the style generation ability of the three approaches. According to experimental results, three proposed approaches can generate not only expected font styles but also somewhat serendipitous font styles. We also compare the approaches with a state-of-the-art style-conditional Latin-font generative network model to confirm the validity of using the <b>diffusion</b> <b>models</b> for the style interpolation task.</p></p class="citation"></blockquote><h3 id=4350--211299-swin3d-effective-multi-source-pretraining-for-3d-indoor-scene-understanding-yu-qi-yang-et-al-2024>(43/50 | 211/299) Swin3D++: Effective Multi-Source Pretraining for 3D Indoor Scene Understanding (Yu-Qi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu-Qi Yang, Yu-Xiao Guo, Yang Liu. (2024)<br><strong>Swin3D++: Effective Multi-Source Pretraining for 3D Indoor Scene Understanding</strong><br><button class=copy-to-clipboard title="Swin3D++: Effective Multi-Source Pretraining for 3D Indoor Scene Understanding" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14215v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14215v1.pdf filename=2402.14215v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data diversity and abundance are essential for improving the performance and generalization of models in natural language processing and 2D vision. However, 3D vision domain suffers from the lack of 3D data, and simply combining multiple 3D datasets for pretraining a 3D backbone does not yield significant improvement, due to the domain discrepancies among different 3D datasets that impede effective feature learning. In this work, we identify the main sources of the domain discrepancies between 3D indoor scene datasets, and propose Swin3D++, an enhanced architecture based on Swin3D for efficient pretraining on multi-source 3D point clouds. Swin3D++ introduces domain-specific mechanisms to Swin3D&rsquo;s modules to address domain discrepancies and enhance the network capability on multi-source pretraining. Moreover, we devise a simple source-augmentation strategy to increase the pretraining data scale and facilitate <b>supervised</b> pretraining. We validate the effectiveness of our design, and demonstrate that Swin3D++ surpasses the state-of-the-art 3D pretraining methods on typical indoor scene understanding tasks. Our code and models will be released at <a href=https://github.com/microsoft/Swin3D>https://github.com/microsoft/Swin3D</a></p></p class="citation"></blockquote><h3 id=4450--212299-mip-grid-anti-aliased-grid-representations-for-neural-radiance-fields-seungtae-nam-et-al-2024>(44/50 | 212/299) Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields (Seungtae Nam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seungtae Nam, Daniel Rho, Jong Hwan Ko, Eunbyung Park. (2024)<br><strong>Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields</strong><br><button class=copy-to-clipboard title="Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14196v1.pdf filename=2402.14196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the remarkable achievements of neural radiance fields (NeRF) in representing 3D scenes and generating novel view images, the aliasing issue, rendering &ldquo;jaggies&rdquo; or &ldquo;blurry&rdquo; images at varying camera distances, remains unresolved in most existing approaches. The recently proposed mip-NeRF has addressed this challenge by rendering conical frustums instead of rays. However, it relies on MLP architecture to represent the radiance fields, missing out on the fast training speed offered by the latest grid-based methods. In this work, we present mip-Grid, a novel approach that integrates anti-aliasing techniques into grid-based representations for radiance fields, mitigating the aliasing artifacts while enjoying fast training time. The proposed method generates multi-scale grids by applying simple <b>convolution</b> operations over a shared grid representation and uses the scale-aware coordinate to retrieve features at different scales from the generated multi-scale grids. To test the effectiveness, we integrated the proposed method into the two recent representative grid-based methods, TensoRF and K-Planes. Experimental results demonstrate that mip-Grid greatly improves the rendering performance of both methods and even outperforms mip-NeRF on multi-scale datasets while achieving significantly faster training time. For code and demo videos, please see <a href=https://stnamjef.github.io/mipgrid.github.io/>https://stnamjef.github.io/mipgrid.github.io/</a>.</p></p class="citation"></blockquote><h3 id=4550--213299-distributed-radiance-fields-for-edge-video-compression-and-metaverse-integration-in-autonomous-driving-eugen-šlapak-et-al-2024>(45/50 | 213/299) Distributed Radiance Fields for Edge Video Compression and Metaverse Integration in Autonomous Driving (Eugen Šlapak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eugen Šlapak, Matúš Dopiriak, Mohammad Abdullah Al Faruque, Juraj Gazda, Marco Levorato. (2024)<br><strong>Distributed Radiance Fields for Edge Video Compression and Metaverse Integration in Autonomous Driving</strong><br><button class=copy-to-clipboard title="Distributed Radiance Fields for Edge Video Compression and Metaverse Integration in Autonomous Driving" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-DC, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14642v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14642v1.pdf filename=2402.14642v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The metaverse is a virtual space that combines physical and digital elements, creating immersive and connected digital worlds. For autonomous mobility, it enables new possibilities with edge computing and digital twins (DTs) that offer virtual prototyping, prediction, and more. DTs can be created with 3D scene reconstruction methods that capture the real world&rsquo;s <b>geometry,</b> appearance, and dynamics. However, sending data for real-time DT updates in the metaverse, such as camera images and videos from connected autonomous vehicles (CAVs) to edge servers, can increase network congestion, costs, and latency, affecting metaverse services. Herein, a new method is proposed based on distributed radiance fields (RFs), multi-access edge computing (MEC) network for video compression and metaverse DT updates. RF-based encoder and decoder are used to create and restore representations of camera images. The method is evaluated on a dataset of camera images from the CARLA simulator. Data savings of up to 80% were achieved for H.264 I-frame - P-frame pairs by using RFs instead of I-frames, while maintaining high peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) qualitative metrics for the reconstructed images. Possible uses and challenges for the metaverse and autonomous mobility are also discussed.</p></p class="citation"></blockquote><h3 id=4650--214299-nerf-det-incorporating-semantic-cues-and-perspective-aware-depth-supervision-for-indoor-multi-view-3d-detection-chenxi-huang-et-al-2024>(46/50 | 214/299) NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth Supervision for Indoor Multi-View 3D Detection (Chenxi Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenxi Huang, Yuenan Hou, Weicai Ye, Di Huang, Xiaoshui Huang, Binbin Lin, Deng Cai, Wanli Ouyang. (2024)<br><strong>NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth Supervision for Indoor Multi-View 3D Detection</strong><br><button class=copy-to-clipboard title="NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth Supervision for Indoor Multi-View 3D Detection" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14464v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14464v1.pdf filename=2402.14464v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>NeRF-Det has achieved impressive performance in indoor multi-view 3D detection by innovatively utilizing NeRF to enhance <b>representation</b> <b>learning.</b> Despite its notable performance, we uncover three decisive shortcomings in its current design, including semantic ambiguity, inappropriate sampling, and insufficient utilization of depth supervision. To combat the aforementioned problems, we present three corresponding solutions: 1) Semantic Enhancement. We project the freely available 3D segmentation annotations onto the 2D plane and leverage the corresponding 2D semantic maps as the supervision signal, significantly enhancing the semantic awareness of multi-view detectors. 2) Perspective-aware Sampling. Instead of employing the uniform sampling strategy, we put forward the perspective-aware sampling policy that samples densely near the camera while sparsely in the distance, more effectively collecting the valuable geometric clues. 3)Ordinal Residual Depth Supervision. As opposed to directly regressing the depth values that are difficult to optimize, we divide the depth range of each scene into a fixed number of ordinal bins and reformulate the depth prediction as the combination of the classification of depth bins as well as the regression of the residual depth values, thereby benefiting the depth learning process. The resulting algorithm, NeRF-Det++, has exhibited appealing performance in the ScanNetV2 and ARKITScenes datasets. Notably, in ScanNetV2, NeRF-Det++ outperforms the competitive NeRF-Det by +1.9% in <a href=mailto:mAP@0.25>mAP@0.25</a> and +3.5% in <a href=mailto:mAP@0.50>mAP@0.50</a>$. The code will be publicly at <a href=https://github.com/mrsempress/NeRF-Detplusplus>https://github.com/mrsempress/NeRF-Detplusplus</a>.</p></p class="citation"></blockquote><h3 id=4750--215299-taylorgrid-towards-fast-and-high-quality-implicit-field-learning-via-direct-taylor-based-grid-optimization-renyi-mao-et-al-2024>(47/50 | 215/299) TaylorGrid: Towards Fast and High-Quality Implicit Field Learning via Direct Taylor-based Grid Optimization (Renyi Mao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renyi Mao, Qingshan Xu, Peng Zheng, Ye Wang, Tieru Wu, Rui Ma. (2024)<br><strong>TaylorGrid: Towards Fast and High-Quality Implicit Field Learning via Direct Taylor-based Grid Optimization</strong><br><button class=copy-to-clipboard title="TaylorGrid: Towards Fast and High-Quality Implicit Field Learning via Direct Taylor-based Grid Optimization" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14415v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14415v1.pdf filename=2402.14415v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Coordinate-based neural implicit representation or implicit fields have been widely studied for 3D <b>geometry</b> representation or novel view synthesis. Recently, a series of efforts have been devoted to accelerating the speed and improving the quality of the coordinate-based implicit field learning. Instead of learning heavy MLPs to predict the neural implicit values for the query coordinates, neural voxels or grids combined with shallow MLPs have been proposed to achieve high-quality implicit field learning with reduced optimization time. On the other hand, lightweight field representations such as linear grid have been proposed to further improve the learning speed. In this paper, we aim for both fast and high-quality implicit field learning, and propose TaylorGrid, a novel implicit field representation which can be efficiently computed via direct Taylor expansion optimization on 2D or 3D grids. As a general representation, TaylorGrid can be adapted to different implicit fields learning tasks such as SDF learning or NeRF. From extensive quantitative and qualitative comparisons, TaylorGrid achieves a balance between the linear grid and neural voxels, showing its superiority in fast and high-quality implicit field learning.</p></p class="citation"></blockquote><h3 id=4850--216299-geneoh-diffusion-towards-generalizable-hand-object-interaction-denoising-via-denoising-diffusion-xueyi-liu-et-al-2024>(48/50 | 216/299) GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion (Xueyi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xueyi Liu, Li Yi. (2024)<br><strong>GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion</strong><br><button class=copy-to-clipboard title="GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14810v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14810v1.pdf filename=2402.14810v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we tackle the challenging problem of denoising hand-object interactions (HOI). Given an erroneous interaction sequence, the objective is to refine the incorrect hand trajectory to remove interaction artifacts for a perceptually realistic sequence. This challenge involves intricate interaction noise, including unnatural hand poses and incorrect hand-object relations, alongside the necessity for robust generalization to new interactions and diverse noise patterns. We tackle those challenges through a novel approach, GeneOH Diffusion, incorporating two key designs: an innovative contact-centric HOI representation named GeneOH and a new domain-generalizable denoising scheme. The contact-centric representation GeneOH informatively parameterizes the HOI process, facilitating enhanced generalization across various HOI scenarios. The new denoising scheme consists of a canonical denoising model trained to project noisy data samples from a whitened noise space to a clean data manifold and a &ldquo;denoising via diffusion&rdquo; strategy which can handle input trajectories with various noise patterns by first diffusing them to align with the whitened noise space and cleaning via the canonical denoiser. Extensive experiments on four <b>benchmarks</b> with significant domain variations demonstrate the superior effectiveness of our method. GeneOH Diffusion also shows promise for various downstream applications. Project website: <a href=https://meowuu7.github.io/GeneOH-Diffusion/>https://meowuu7.github.io/GeneOH-Diffusion/</a>.</p></p class="citation"></blockquote><h3 id=4950--217299-ccpa-long-term-person-re-identification-via-contrastive-clothing-and-pose-augmentation-vuong-d-nguyen-et-al-2024>(49/50 | 217/299) CCPA: Long-term Person Re-Identification via Contrastive Clothing and Pose Augmentation (Vuong D. Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vuong D. Nguyen, Shishir K. Shah. (2024)<br><strong>CCPA: Long-term Person Re-Identification via Contrastive Clothing and Pose Augmentation</strong><br><button class=copy-to-clipboard title="CCPA: Long-term Person Re-Identification via Contrastive Clothing and Pose Augmentation" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14454v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14454v1.pdf filename=2402.14454v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Long-term Person Re-Identification (LRe-ID) aims at matching an individual across cameras after a long period of time, presenting variations in clothing, pose, and viewpoint. In this work, we propose CCPA: Contrastive Clothing and Pose Augmentation framework for LRe-ID. Beyond appearance, CCPA captures body shape information which is cloth-invariant using a Relation <b>Graph</b> Attention Network. Training a robust LRe-ID model requires a wide range of clothing variations and expensive cloth labeling, which is lacked in current LRe-ID datasets. To address this, we perform clothing and pose transfer across identities to generate images of more clothing variations and of different persons wearing similar clothing. The augmented batch of images serve as inputs to our proposed Fine-grained Contrastive Losses, which not only supervise the Re-ID model to learn discriminative person embeddings under long-term scenarios but also ensure in-distribution data generation. Results on LRe-ID datasets demonstrate the effectiveness of our CCPA framework.</p></p class="citation"></blockquote><h3 id=5050--218299-semantic-image-synthesis-with-unconditional-generator-jungwoo-chae-et-al-2024>(50/50 | 218/299) Semantic Image Synthesis with Unconditional Generator (Jungwoo Chae et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jungwoo Chae, Hyunin Cho, Sooyeon Go, Kyungmook Choi, Youngjung Uh. (2024)<br><strong>Semantic Image Synthesis with Unconditional Generator</strong><br><button class=copy-to-clipboard title="Semantic Image Synthesis with Unconditional Generator" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14395v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14395v1.pdf filename=2402.14395v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic image synthesis (SIS) aims to generate realistic images that match given semantic masks. Despite recent advances allowing high-quality results and precise spatial control, they require a massive semantic segmentation dataset for training the models. Instead, we propose to employ a pre-trained unconditional generator and rearrange its feature maps according to proxy masks. The proxy masks are prepared from the feature maps of random samples in the generator by simple <b>clustering.</b> The feature rearranger learns to rearrange original feature maps to match the shape of the proxy masks that are either from the original sample itself or from random samples. Then we introduce a semantic mapper that produces the proxy masks from various input conditions including semantic masks. Our method is versatile across various applications such as free-form spatial editing of real images, sketch-to-photo, and even scribble-to-photo. Experiments validate advantages of our method on a range of datasets: human faces, animal faces, and buildings.</p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=11--219299-a-decision-language-model-dlm-for-dynamic-restless-multi-armed-bandit-tasks-in-public-health-nikhil-behari-et-al-2024>(1/1 | 219/299) A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health (Nikhil Behari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikhil Behari, Edwin Zhang, Yunfan Zhao, Aparna Taneja, Dheeraj Nagaraj, Milind Tambe. (2024)<br><strong>A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health</strong><br><button class=copy-to-clipboard title="A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-LG, cs-MA, cs.MA<br>Keyword Score: 70<br>Keywords: Bandit Algorithm, Fine-tuning, Simulation, Simulator, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14807v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14807v2.pdf filename=2402.14807v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efforts to reduce maternal mortality rate, a key UN Sustainable Development target (SDG Target 3.1), rely largely on preventative care programs to spread critical health information to high-risk populations. These programs face two important challenges: efficiently allocating limited health resources to <b>large</b> <b>beneficiary</b> <b>populations,</b> and adapting to evolving policy priorities. While prior works in restless multi-armed <b>bandit</b> (RMAB) demonstrated success in public health allocation tasks, they lack flexibility to adapt to evolving policy priorities. Concurrently, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have emerged as adept, automated planners in various domains, including robotic control and navigation. In this paper, we propose DLM: a Decision Language Model for RMABs. To enable dynamic <b>fine-tuning</b> of RMAB policies for challenging public health settings using human-language commands, we propose using <b>LLMs</b> as automated planners to (1) interpret human policy preference <b>prompts,</b> (2) propose code reward functions for a multi-agent RL environment for RMABs, and (3) iterate on the generated reward using feedback from RMAB <b>simulations</b> to effectively adapt policy outcomes. In collaboration with ARMMAN, an India-based public health organization promoting preventative care for pregnant mothers, we conduct a <b>simulation</b> study, showing DLM can dynamically shape policy outcomes using only human language commands as input.</p></p class="citation"></blockquote><h2 id=eessiv-3>eess.IV (3)</h2><h3 id=13--220299-dicom----diverse-concept-modeling-towards-enhancing-generalizability-in-chest-x-ray-studies-abhieet-parida-et-al-2024>(1/3 | 220/299) DiCoM &ndash; Diverse Concept Modeling towards Enhancing Generalizability in Chest X-Ray Studies (Abhieet Parida et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhieet Parida, Daniel Capellan-Martin, Sara Atito, Muhammad Awais, Maria J. Ledesma-Carbayo, Marius G. Linguraru, Syed Muhammad Anwar. (2024)<br><strong>DiCoM &ndash; Diverse Concept Modeling towards Enhancing Generalizability in Chest X-Ray Studies</strong><br><button class=copy-to-clipboard title="DiCoM -- Diverse Concept Modeling towards Enhancing Generalizability in Chest X-Ray Studies" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 60<br>Keywords: Fine-tuning, Foundation Model, Self-supervised Learning, Self-supervised Pre-training, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15534v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15534v1.pdf filename=2402.15534v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chest X-Ray (CXR) is a widely used clinical imaging modality and has a pivotal role in the diagnosis and prognosis of various lung and heart related conditions. Conventional automated clinical diagnostic tool design strategies relying on radiology reads and <b>supervised</b> <b>learning,</b> entail the cumbersome requirement of high quality annotated training data. To address this challenge, <b>self-supervised</b> <b>pre-training</b> has proven to outperform <b>supervised</b> <b>pre-training</b> in numerous downstream vision tasks, representing a significant breakthrough in the field. However, medical imaging pre-training significantly differs from pre-training with natural images (e.g., ImageNet) due to unique attributes of clinical images. In this context, we introduce Diverse Concept Modeling (DiCoM), a novel <b>self-supervised</b> <b>training</b> paradigm that leverages a student teacher framework for learning diverse concepts and hence effective representation of the CXR data. Hence, expanding beyond merely modeling a single primary label within an image, instead, effectively harnessing the information from all the concepts inherent in the CXR. The pre-trained model is subsequently <b>fine-tuned</b> to address diverse domain-specific tasks. Our proposed paradigm consistently demonstrates robust performance across multiple downstream tasks on multiple datasets, highlighting the success and generalizability of the pre-training strategy. To establish the efficacy of our methods we analyze both the power of learned representations and the speed of convergence (SoC) of our models. For diverse data and tasks, DiCoM is able to achieve in most cases better results compared to other state-of-the-art pre-training strategies. This when combined with the higher SoC and generalization capabilities positions DiCoM to be established as a <b>foundation</b> <b>model</b> for CXRs, a widely used imaging modality.</p></p class="citation"></blockquote><h3 id=23--221299-uncertainty-driven-and-adversarial-calibration-learning-for-epicardial-adipose-tissue-segmentation-kai-zhao-et-al-2024>(2/3 | 221/299) Uncertainty-driven and Adversarial Calibration Learning for Epicardial Adipose Tissue Segmentation (Kai Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Zhao, Zhiming Liu, Jiaqi Liu, Jingbiao Zhou, Bihong Liao, Huifang Tang, Qiuyu Wang, Chunquan Li. (2024)<br><strong>Uncertainty-driven and Adversarial Calibration Learning for Epicardial Adipose Tissue Segmentation</strong><br><button class=copy-to-clipboard title="Uncertainty-driven and Adversarial Calibration Learning for Epicardial Adipose Tissue Segmentation" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14349v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14349v2.pdf filename=2402.14349v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Epicardial adipose tissue (EAT) is a type of visceral fat that can secrete large amounts of adipokines to affect the myocardium and coronary arteries. EAT volume and density can be used as independent risk markers measurement of volume by noninvasive magnetic resonance images is the best method of assessing EAT. However, segmenting EAT is challenging due to the low contrast between EAT and pericardial effusion and the presence of motion artifacts. we propose a novel feature latent space multilevel supervision network (SPDNet) with uncertainty-driven and <b>adversarial</b> <b>calibration</b> learning to enhance segmentation for more accurate EAT volume estimation. The network first addresses the blurring of EAT edges due to the medical images in the open medical environments with low quality or <b>out-of-distribution</b> by modeling the uncertainty as a Gaussian distribution in the feature latent space, which using its Bayesian estimation as a regularization constraint to optimize SwinUNETR. Second, an <b>adversarial</b> <b>training</b> strategy is introduced to calibrate the segmentation feature map and consider the multi-scale feature differences between the uncertainty-guided predictive segmentation and the ground truth segmentation, synthesizing the multi-scale <b>adversarial</b> <b>loss</b> directly improves the ability to discriminate the similarity between organizations. Experiments on both the cardiac public MRI dataset (ACDC) and the real-world clinical cohort EAT dataset show that the proposed network outperforms mainstream models, validating that uncertainty-driven and <b>adversarial</b> <b>calibration</b> learning can be used to provide additional information for modeling multi-scale ambiguities.</p></p class="citation"></blockquote><h3 id=33--222299-towards-spatially-lucid-ai-classification-in-non-euclidean-space-an-application-for-mxif-oncology-data-majid-farhadloo-et-al-2024>(3/3 | 222/299) Towards Spatially-Lucid AI Classification in Non-Euclidean Space: An Application for MxIF Oncology Data (Majid Farhadloo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Majid Farhadloo, Arun Sharma, Jayant Gupta, Alexey Leontovich, Svetomir N. Markovic, Shashi Shekhar. (2024)<br><strong>Towards Spatially-Lucid AI Classification in Non-Euclidean Space: An Application for MxIF Oncology Data</strong><br><button class=copy-to-clipboard title="Towards Spatially-Lucid AI Classification in Non-Euclidean Space: An Application for MxIF Oncology Data" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14974v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14974v1.pdf filename=2402.14974v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given multi-category point sets from different place-types, our goal is to develop a spatially-lucid classifier that can distinguish between two classes based on the arrangements of their points. This problem is important for many applications, such as oncology, for analyzing immune-tumor relationships and designing new immunotherapies. It is challenging due to spatial variability and interpretability needs. Previously proposed techniques require dense training data or have limited ability to handle significant spatial variability within a single place-type. Most importantly, these deep neural network (DNN) approaches are not designed to work in non-Euclidean space, particularly point sets. Existing non-Euclidean DNN methods are limited to one-size-fits-all approaches. We explore a spatial ensemble framework that explicitly uses different training strategies, including weighted-distance learning rate and spatial <b>domain</b> <b>adaptation,</b> on various place-types for spatially-lucid classification. Experimental results on real-world datasets (e.g., MxIF oncology data) show that the proposed framework provides higher prediction accuracy than baseline methods.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--223299-an-fpga-based-accelerator-enabling-efficient-support-for-cnns-with-arbitrary-kernel-sizes-miaoxin-wang-et-al-2024>(1/1 | 223/299) An FPGA-Based Accelerator Enabling Efficient Support for CNNs with Arbitrary Kernel Sizes (Miaoxin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miaoxin Wang, Xiao Wu, Jun Lin, Zhongfeng Wang. (2024)<br><strong>An FPGA-Based Accelerator Enabling Efficient Support for CNNs with Arbitrary Kernel Sizes</strong><br><button class=copy-to-clipboard title="An FPGA-Based Accelerator Enabling Efficient Support for CNNs with Arbitrary Kernel Sizes" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-LG, cs.AR<br>Keyword Score: 60<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14307v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14307v1.pdf filename=2402.14307v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> with large kernels, drawing inspiration from the key operations of <b>vision</b> <b>transformers</b> (ViTs), have demonstrated impressive performance in various <b>vision-based</b> <b>applications.</b> To address the issue of computational efficiency degradation in existing designs for supporting large-kernel <b>convolutions,</b> an FPGA-based inference accelerator is proposed for the efficient deployment of <b>CNNs</b> with arbitrary kernel sizes. Firstly, a Z-flow method is presented to optimize the computing data flow by maximizing data reuse opportunity. Besides, the proposed design, incorporating the kernel-segmentation (Kseg) scheme, enables extended support for large-kernel <b>convolutions,</b> significantly reducing the storage requirements for overlapped data. Moreover, based on the analysis of typical block structures in emerging <b>CNNs,</b> vertical-fused (VF) and horizontal-fused (HF) methods are developed to optimize <b>CNN</b> deployments from both computation and transmission perspectives. The proposed hardware accelerator, evaluated on Intel Arria 10 FPGA, achieves up to 3.91 times better DSP efficiency than prior art on the same network. Particularly, it demonstrates efficient support for large-kernel <b>CNNs,</b> achieving throughputs of 169.68 GOPS and 244.55 GOPS for RepLKNet-31 and PyConvResNet-50, respectively, both of which are implemented on hardware for the first time.</p></p class="citation"></blockquote><h2 id=csse-5>cs.SE (5)</h2><h3 id=15--224299-copilot-evaluation-harness-evaluating-llm-guided-software-programming-anisha-agarwal-et-al-2024>(1/5 | 224/299) Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming (Anisha Agarwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anisha Agarwal, Aaron Chan, Shubham Chandel, Jinu Jang, Shaun Miller, Roshanak Zilouchian Moghaddam, Yevhen Mohylevskyy, Neel Sundaresan, Michele Tufano. (2024)<br><strong>Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming</strong><br><button class=copy-to-clipboard title="Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 60<br>Keywords: GPT, GPT-3, LLaMA, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14261v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14261v1.pdf filename=2402.14261v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> into Development Environments (IDEs) has become a focal point in modern software development. <b>LLMs</b> such as OpenAI <b>GPT-3.5/4</b> and <b>Code</b> <b>Llama</b> offer the potential to significantly augment developer productivity by serving as intelligent, chat-driven programming assistants. However, utilizing <b>LLMs</b> out of the box is unlikely to be optimal for any given scenario. Rather, each system requires the <b>LLM</b> to be honed to its set of heuristics to ensure the best performance. In this paper, we introduce the Copilot evaluation harness: a set of data and tools for evaluating <b>LLM-guided</b> IDE interactions, covering various programming scenarios and languages. We propose our metrics as a more robust and information-dense evaluation than previous state of the art evaluation systems. We design and compute both static and execution based success metrics for scenarios encompassing a wide range of developer tasks, including <b>code</b> <b>generation</b> from natural language (generate), documentation generation from <b>code</b> <b>(doc),</b> test case generation (test), bug-fixing (fix), and workspace understanding and query resolution (workspace). These success metrics are designed to evaluate the performance of <b>LLMs</b> within a given IDE and its respective parameter space. Our learnings from evaluating three common <b>LLMs</b> using these metrics can inform the development and validation of future scenarios in <b>LLM</b> guided IDEs.</p></p class="citation"></blockquote><h3 id=25--225299-metmap-metamorphic-testing-for-detecting-false-vector-matching-problems-in-llm-augmented-generation-guanyu-wang-et-al-2024>(2/5 | 225/299) MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems in LLM Augmented Generation (Guanyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guanyu Wang, Yuekang Li, Yi Liu, Gelei Deng, Tianlin Li, Guosheng Xu, Yang Liu, Haoyu Wang, Kailong Wang. (2024)<br><strong>MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems in LLM Augmented Generation</strong><br><button class=copy-to-clipboard title="MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems in LLM Augmented Generation" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14480v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14480v1.pdf filename=2402.14480v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Augmented generation techniques such as <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> outputs with external knowledge and cached information. However, the integration of vector databases, which serve as a backbone for these augmentations, introduces critical challenges, particularly in ensuring accurate vector matching. False vector matching in these databases can significantly compromise the integrity and reliability of <b>LLM</b> outputs, leading to misinformation or erroneous responses. Despite the crucial impact of these issues, there is a notable research gap in methods to effectively detect and address false vector matches in <b>LLM-augmented</b> generation. This paper presents MeTMaP, a metamorphic testing framework developed to identify false vector matching in <b>LLM-augmented</b> generation systems. We derive eight metamorphic relations (MRs) from six NLP datasets, which form our method&rsquo;s core, based on the idea that semantically similar texts should match and dissimilar ones should not. MeTMaP uses these MRs to create sentence triplets for testing, simulating real-world <b>LLM</b> scenarios. Our evaluation of MeTMaP over 203 vector matching configurations, involving 29 embedding models and 7 distance metrics, uncovers significant inaccuracies. The results, showing a maximum accuracy of only 41.51% on our tests compared to the original datasets, emphasize the widespread issue of false matches in vector matching methods and the critical need for effective detection and mitigation in <b>LLM-augmented</b> applications.</p></p class="citation"></blockquote><h3 id=35--226299-opencodeinterpreter-integrating-code-generation-with-execution-and-refinement-tianyu-zheng-et-al-2024>(3/5 | 226/299) OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement (Tianyu Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, Xiang Yue. (2024)<br><strong>OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement</strong><br><button class=copy-to-clipboard title="OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-CL, cs-SE, cs.SE<br>Keyword Score: 43<br>Keywords: Benchmarking, GPT, GPT-4, Code Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14658v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14658v2.pdf filename=2402.14658v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The introduction of <b>large</b> <b>language</b> <b>models</b> has significantly advanced <b>code</b> <b>generation.</b> However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the <b>GPT-4</b> <b>Code</b> <b>Interpreter.</b> To address this, we introduce OpenCodeInterpreter, a family of open-source <b>code</b> <b>systems</b> designed for generating, executing, and iteratively refining <b>code.</b> <b>Supported</b> by <b>Code-Feedback,</b> <b>a</b> dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic <b>code</b> <b>refinement.</b> Our comprehensive evaluation of OpenCodeInterpreter across key <b>benchmarks</b> such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling <b>GPT-4&rsquo;s</b> 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized human feedback from <b>GPT-4.</b> OpenCodeInterpreter brings the gap between open-source <b>code</b> <b>generation</b> models and proprietary systems like <b>GPT-4</b> <b>Code</b> <b>Interpreter.</b></p></p class="citation"></blockquote><h3 id=45--227299-do-machines-and-humans-focus-on-similar-code-exploring-explainability-of-large-language-models-in-code-summarization-jiliang-li-et-al-2024>(4/5 | 227/299) Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization (Jiliang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiliang Li, Yifan Zhang, Zachary Karas, Collin McMillan, Kevin Leach, Yu Huang. (2024)<br><strong>Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization</strong><br><button class=copy-to-clipboard title="Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 35<br>Keywords: Black Box, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14182v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14182v1.pdf filename=2402.14182v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent language models have demonstrated proficiency in summarizing source code. However, as in many other domains of machine learning, language models of code lack sufficient explainability. Informally, we lack a formulaic or intuitive understanding of what and how models learn from code. Explainability of language models can be partially provided if, as the models learn to produce higher-quality code summaries, they also align in deeming the same code parts important as those identified by human programmers. In this paper, we report negative results from our investigation of explainability of language models in code <b>summarization</b> through the lens of human comprehension. We measure human focus on code using eye-tracking metrics such as fixation counts and duration in code <b>summarization</b> tasks. To approximate language model focus, we employ a state-of-the-art model-agnostic, <b>black-box,</b> <b>perturbation-based</b> approach, SHAP (SHapley Additive exPlanations), to identify which code tokens influence that generation of summaries. Using these settings, we find no statistically significant relationship between language models&rsquo; focus and human programmers&rsquo; attention. Furthermore, alignment between model and human foci in this setting does not seem to dictate the quality of the <b>LLM-generated</b> summaries. Our study highlights an inability to align human focus with SHAP-based model focus measures. This result calls for future investigation of multiple open questions for explainable language models for code <b>summarization</b> and software engineering tasks in general, including the training mechanisms of language models for code, whether there is an alignment between human and model attention on code, whether human attention can improve the development of language models, and what other model focus measures are appropriate for improving explainability.</p></p class="citation"></blockquote><h3 id=55--228299-repofuse-repository-level-code-completion-with-fused-dual-context-ming-liang-et-al-2024>(5/5 | 228/299) REPOFUSE: Repository-Level Code Completion with Fused Dual Context (Ming Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Liang, Xiaoheng Xie, Gehao Zhang, Xunjin Zheng, Peng Di, wei jiang, Hongwei Chen, Chengpeng Wang, Gang Fan. (2024)<br><strong>REPOFUSE: Repository-Level Code Completion with Fused Dual Context</strong><br><button class=copy-to-clipboard title="REPOFUSE: Repository-Level Code Completion with Fused Dual Context" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14323v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14323v2.pdf filename=2402.14323v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The success of language models in code assistance has spurred the proposal of repository-level code completion as a means to enhance prediction accuracy, utilizing the context from the entire codebase. However, this amplified context can inadvertently increase inference latency, potentially undermining the developer experience and deterring tool adoption - a challenge we termed the Context-Latency Conundrum. This paper introduces REPOFUSE, a pioneering solution designed to enhance repository-level code completion without the latency trade-off. REPOFUSE uniquely fuses two types of context: the analogy context, rooted in code analogies, and the rationale context, which encompasses in-depth semantic relationships. We propose a novel rank truncated generation (RTG) technique that efficiently condenses these contexts into <b>prompts</b> with restricted size. This enables REPOFUSE to deliver precise code completions while maintaining inference efficiency. Through testing with the CrossCodeEval suite, REPOFUSE has demonstrated a significant leap over existing models, achieving a 40.90% to 59.75% increase in exact match (EM) accuracy for code completions and a 26.8% enhancement in inference speed. Beyond experimental validation, REPOFUSE has been integrated into the workflow of a large enterprise, where it actively supports various coding tasks.</p></p class="citation"></blockquote><h2 id=csai-5>cs.AI (5)</h2><h3 id=15--229299-shm-traffic-drl-and-transfer-learning-based-uav-control-for-structural-health-monitoring-of-bridges-with-traffic-divija-swetha-gadiraju-et-al-2024>(1/5 | 229/299) SHM-Traffic: DRL and Transfer learning based UAV Control for Structural Health Monitoring of Bridges with Traffic (Divija Swetha Gadiraju et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Divija Swetha Gadiraju, Saeed Eftekhar Azam, Deepak Khazanchi. (2024)<br><strong>SHM-Traffic: DRL and Transfer learning based UAV Control for Structural Health Monitoring of Bridges with Traffic</strong><br><button class=copy-to-clipboard title="SHM-Traffic: DRL and Transfer learning based UAV Control for Structural Health Monitoring of Bridges with Traffic" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Reinforcement Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14757v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14757v1.pdf filename=2402.14757v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work focuses on using advanced techniques for structural health monitoring (SHM) for bridges with Traffic. We propose an approach using deep <b>reinforcement</b> <b>learning</b> (DRL)-based control for Unmanned Aerial Vehicle (UAV). Our approach conducts a concrete bridge deck survey while traffic is ongoing and detects cracks. The UAV performs the crack detection, and the location of cracks is initially unknown. We use two edge detection techniques. First, we use canny edge detection for crack detection. We also use a <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> for crack detection and compare it with canny edge detection. <b>Transfer</b> <b>learning</b> is applied using <b>CNN</b> with pre-trained weights obtained from a crack image dataset. This enables the model to adapt and improve its performance in identifying and localizing cracks. Proximal Policy Optimization (PPO) is applied for UAV control and bridge surveys. The experimentation across various scenarios is performed to evaluate the performance of the proposed methodology. Key metrics such as task completion time and reward convergence are observed to gauge the effectiveness of the approach. We observe that the Canny edge detector offers up to 40% lower task completion time, while the <b>CNN</b> excels in up to 12% better damage detection and 1.8 times better rewards.</p></p class="citation"></blockquote><h3 id=25--230299-automating-psychological-hypothesis-generation-with-ai-large-language-models-meet-causal-graph-song-tong-et-al-2024>(2/5 | 230/299) Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph (Song Tong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Song Tong, Kai Mao, Zhen Huang, Yukun Zhao, Kaiping Peng. (2024)<br><strong>Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph</strong><br><button class=copy-to-clipboard title="Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CY, cs.AI<br>Keyword Score: 28<br>Keywords: Graph, Knowledge Graph, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14424v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14424v1.pdf filename=2402.14424v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Leveraging the synergy between causal <b>knowledge</b> <b>graphs</b> and a <b>large</b> <b>language</b> <b>model</b> <b>(LLM),</b> our study introduces a groundbreaking approach for computational hypothesis generation in psychology. We analyzed 43,312 psychology articles using a <b>LLM</b> to extract causal relation pairs. This analysis produced a specialized causal <b>graph</b> for psychology. Applying link prediction algorithms, we generated 130 potential psychological hypotheses focusing on `well-being&rsquo;, then compared them against research ideas conceived by doctoral scholars and those produced solely by the <b>LLM.</b> Interestingly, our combined approach of a <b>LLM</b> and causal <b>graphs</b> mirrored the expert-level insights in terms of novelty, clearly surpassing the <b>LLM-only</b> hypotheses (t(59) = 3.34, p=0.007 and t(59) = 4.32, p&lt;0.001, respectively). This alignment was further corroborated using deep semantic analysis. Our results show that combining <b>LLM</b> with machine learning techniques such as causal <b>knowledge</b> <b>graphs</b> can revolutionize automated discovery in psychology, extracting novel insights from the extensive literature. This work stands at the crossroads of psychology and artificial intelligence, championing a new enriched paradigm for data-driven hypothesis generation in psychological research.</p></p class="citation"></blockquote><h3 id=35--231299-unleashing-the-power-of-imbalanced-modality-information-for-multi-modal-knowledge-graph-completion-yichi-zhang-et-al-2024>(3/5 | 231/299) Unleashing the Power of Imbalanced Modality Information for Multi-modal Knowledge Graph Completion (Yichi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yichi Zhang, Zhuo Chen, Lei Liang, Huajun Chen, Wen Zhang. (2024)<br><strong>Unleashing the Power of Imbalanced Modality Information for Multi-modal Knowledge Graph Completion</strong><br><button class=copy-to-clipboard title="Unleashing the Power of Imbalanced Modality Information for Multi-modal Knowledge Graph Completion" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs-MM, cs.AI<br>Keyword Score: 24<br>Keywords: Graph, Adversarial Learning, Benchmarking, Knowledge Graph, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15444v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15444v1.pdf filename=2402.15444v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> <b>knowledge</b> <b>graph</b> completion (MMKGC) aims to predict the missing triples in the <b>multi-modal</b> <b>knowledge</b> <b>graphs</b> by incorporating structural, visual, and textual information of entities into the discriminant models. The information from different modalities will work together to measure the triple plausibility. Existing MMKGC methods overlook the imbalance problem of modality information among entities, resulting in inadequate modal fusion and inefficient utilization of the raw modality information. To address the mentioned problems, we propose Adaptive <b>Multi-modal</b> Fusion and Modality <b>Adversarial</b> <b>Training</b> (AdaMF-MAT) to unleash the power of imbalanced modality information for MMKGC. AdaMF-MAT achieves <b>multi-modal</b> fusion with adaptive modality weights and further generates <b>adversarial</b> <b>samples</b> by modality-adversarial training to enhance the imbalanced modality information. Our approach is a co-design of the MMKGC model and training strategy which can outperform 19 recent MMKGC methods and achieve new state-of-the-art results on three public MMKGC <b>benchmarks.</b> Our code and data have been released at <a href=https://github.com/zjukg/AdaMF-MAT>https://github.com/zjukg/AdaMF-MAT</a>.</p></p class="citation"></blockquote><h3 id=45--232299-large-language-models-as-urban-residents-an-llm-agent-framework-for-personal-mobility-generation-jiawei-wang-et-al-2024>(4/5 | 232/299) Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation (Jiawei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Wang, Renhe Jiang, Chuang Yang, Zengqing Wu, Makoto Onizuka, Ryosuke Shibasaki, Chuan Xiao. (2024)<br><strong>Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation</strong><br><button class=copy-to-clipboard title="Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CY, cs-LG, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14744v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14744v1.pdf filename=2402.14744v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel approach using <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> integrated into an agent framework for flexible and efficient personal mobility generation. <b>LLMs</b> overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks. Our approach addresses the critical need to align <b>LLMs</b> with real-world urban mobility data, focusing on three research questions: aligning <b>LLMs</b> with rich activity data, developing reliable activity generation strategies, and exploring <b>LLM</b> applications in urban mobility. The key technical contribution is a novel <b>LLM</b> agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align <b>LLMs</b> with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. In experimental studies, comprehensive validation is performed using real-world data. This research marks the pioneering work of designing an <b>LLM</b> agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.</p></p class="citation"></blockquote><h3 id=55--233299-mentor-guiding-hierarchical-reinforcement-learning-with-human-feedback-and-dynamic-distance-constraint-xinglin-zhou-et-al-2024>(5/5 | 233/299) MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and Dynamic Distance Constraint (Xinglin Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinglin Zhou, Yifu Yuan, Shaofu Yang, Jianye Hao. (2024)<br><strong>MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and Dynamic Distance Constraint</strong><br><button class=copy-to-clipboard title="MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and Dynamic Distance Constraint" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-HC, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14244v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14244v1.pdf filename=2402.14244v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hierarchical <b>reinforcement</b> <b>learning</b> (HRL) provides a promising solution for complex tasks with sparse rewards of intelligent agents, which uses a hierarchical framework that divides tasks into subgoals and completes them sequentially. However, current methods struggle to find suitable subgoals for ensuring a stable learning process. Without additional guidance, it is impractical to rely solely on exploration or heuristics methods to determine subgoals in a large goal space. To address the issue, We propose a general hierarchical <b>reinforcement</b> <b>learning</b> framework incorporating human feedback and dynamic distance constraints (MENTOR). MENTOR acts as a &ldquo;mentor&rdquo;, incorporating human feedback into high-level policy learning, to find better subgoals. As for low-level policy, MENTOR designs a dual policy for exploration-exploitation decoupling respectively to stabilize the training. Furthermore, although humans can simply break down tasks into subgoals to guide the right learning direction, subgoals that are too difficult or too easy can still hinder downstream learning efficiency. We propose the Dynamic Distance Constraint (DDC) mechanism dynamically adjusting the space of optional subgoals. Thus MENTOR can generate subgoals matching the low-level policy learning process from easy to hard. Extensive experiments demonstrate that MENTOR uses a small amount of human feedback to achieve significant improvement in complex tasks with sparse rewards.</p></p class="citation"></blockquote><h2 id=quant-ph-3>quant-ph (3)</h2><h3 id=13--234299-quantum-circuit-optimization-with-alphatensor-francisco-j-r-ruiz-et-al-2024>(1/3 | 234/299) Quantum Circuit Optimization with AlphaTensor (Francisco J. R. Ruiz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francisco J. R. Ruiz, Tuomas Laakkonen, Johannes Bausch, Matej Balog, Mohammadamin Barekatain, Francisco J. H. Heras, Alexander Novikov, Nathan Fitzpatrick, Bernardino Romera-Paredes, John van de Wetering, Alhussein Fawzi, Konstantinos Meichanetzidis, Pushmeet Kohli. (2024)<br><strong>Quantum Circuit Optimization with AlphaTensor</strong><br><button class=copy-to-clipboard title="Quantum Circuit Optimization with AlphaTensor" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, quant-ph, quant-ph<br>Keyword Score: 43<br>Keywords: Benchmarking, Reinforcement Learning, Simulation, Simulator, Tensor Decomposition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14396v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14396v1.pdf filename=2402.14396v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A key challenge in realizing fault-tolerant quantum computers is circuit optimization. Focusing on the most expensive gates in fault-tolerant quantum computation (namely, the T gates), we address the problem of T-count optimization, i.e., minimizing the number of T gates that are needed to implement a given circuit. To achieve this, we develop AlphaTensor-Quantum, a method based on deep <b>reinforcement</b> <b>learning</b> that exploits the relationship between optimizing T-count and <b>tensor</b> <b>decomposition.</b> Unlike existing methods for T-count optimization, AlphaTensor-Quantum can incorporate domain-specific knowledge about quantum computation and leverage gadgets, which significantly reduces the T-count of the optimized circuits. AlphaTensor-Quantum outperforms the existing methods for T-count optimization on a set of arithmetic <b>benchmarks</b> (even when compared without making use of gadgets). Remarkably, it discovers an efficient algorithm akin to Karatsuba&rsquo;s method for multiplication in finite fields. AlphaTensor-Quantum also finds the best human-designed solutions for relevant arithmetic computations used in Shor&rsquo;s algorithm and for quantum chemistry <b>simulation,</b> thus demonstrating it can save hundreds of hours of research by optimizing relevant quantum circuits in a fully automated way.</p></p class="citation"></blockquote><h3 id=23--235299-quantum-markov-decision-processes-part-i-general-theory-approximations-and-classes-of-policies-naci-saldi-et-al-2024>(2/3 | 235/299) Quantum Markov Decision Processes Part I: General Theory, Approximations, and Classes of Policies (Naci Saldi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naci Saldi, Sina Sanjari, Serdar Yuksel. (2024)<br><strong>Quantum Markov Decision Processes Part I: General Theory, Approximations, and Classes of Policies</strong><br><button class=copy-to-clipboard title="Quantum Markov Decision Processes Part I: General Theory, Approximations, and Classes of Policies" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-SY, eess-SY, math-OC, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Discrete Time, Discrete Time, Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14649v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14649v1.pdf filename=2402.14649v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this two part article, the aim is to develop a quantum counterpart to classical Markov decision processes <b>(MDPs).</b> In Part I, we provide a very general formulation of quantum <b>MDPs</b> with state and action spaces in the quantum domain, quantum transitions, and cost functions. Once we formulate the quantum MDP (q-MDP), our focus shifts to establishing the verification theorem that proves the sufficiency of Markovian quantum control policies and provides a dynamic programming principle. Subsequently, a comparison is drawn between our q-MDP model and previously established quantum MDP models (referred to as QOMDPs) found in the literature. Furthermore, approximations of q-MDPs are obtained via finite-action models, which can be formulated as QOMDPs. Finally, classes of open-loop and closed-loop policies for q-MDPs are introduced, along with structural results for these policies. In summary, we present a novel quantum MDP model aiming to introduce a new framework, algorithms, and future research avenues. We believe that our approach will pave the way for a new research direction in <b>discrete-time</b> <b>quantum</b> control.</p></p class="citation"></blockquote><h3 id=33--236299-quantum-markov-decision-processes-part-ii-optimal-solutions-and-algorithms-naci-saldi-et-al-2024>(3/3 | 236/299) Quantum Markov Decision Processes Part II: Optimal Solutions and Algorithms (Naci Saldi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naci Saldi, Sina Sanjari, Serdar Yuksel. (2024)<br><strong>Quantum Markov Decision Processes Part II: Optimal Solutions and Algorithms</strong><br><button class=copy-to-clipboard title="Quantum Markov Decision Processes Part II: Optimal Solutions and Algorithms" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-SY, eess-SY, math-OC, quant-ph, quant-ph<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14651v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14651v1.pdf filename=2402.14651v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This two-part article aims to introduce a quantum analogue to classical Markov decision processes <b>(MDPs).</b> In Part II, building on the formulation of q-MDPs presented in Part I, our focus shifts to the development of algorithms for computing optimal policies and value functions of both open-loop and closed-loop policies. First, by using the duality between the dynamic programming and the semi-definite programming formulations of any q-MDP with open-loop policies, we establish an algorithm that enables us to efficiently compute optimal open-loop quantum policies and value functions. Then, dynamic programming and semi-definite programming formulations for closed-loop policies is established, where duality of these two formulations similarly enables the efficient computation of optimal closed-loop policies and value functions. Finally, given that any q-MDP can be approximated by q-MDPs with classical policies&ndash;potentially with higher-dimensional underlying Hilbert spaces than the original model&ndash;and since any classical policy is an element of the set of closed-loop policies, we conclude that any q-MDP can be approximated by q-MDPs with closed-loop policies having higher-dimensional Hilbert spaces.</p></p class="citation"></blockquote><h2 id=statml-6>stat.ML (6)</h2><h3 id=16--237299-in-context-learning-of-a-linear-transformer-block-benefits-of-the-mlp-component-and-one-step-gd-initialization-ruiqi-zhang-et-al-2024>(1/6 | 237/299) In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization (Ruiqi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruiqi Zhang, Jingfeng Wu, Peter L. Bartlett. (2024)<br><strong>In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization</strong><br><button class=copy-to-clipboard title="In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CL, cs-LG, stat-ML, stat.ML<br>Keyword Score: 40<br>Keywords: Transformer, In-context Learning, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14951v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14951v1.pdf filename=2402.14951v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the \emph{in-context learning} <b>(ICL)</b> ability of a \emph{Linear <b>Transformer</b> Block} (LTB) that combines a linear attention component and a linear multi-layer perceptron (MLP) component. For <b>ICL</b> of linear regression with a Gaussian prior and a \emph{non-zero mean}, we show that LTB can achieve nearly Bayes optimal <b>ICL</b> risk. In contrast, using only linear attention must incur an irreducible additive approximation error. Furthermore, we establish a correspondence between LTB and one-step gradient descent estimators with learnable initialization ($\mathsf{GD}\text{-}\mathbf{\beta}$), in the sense that every $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator can be implemented by an LTB estimator and every optimal LTB estimator that minimizes the in-class <b>ICL</b> risk is effectively a $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator. Finally, we show that $\mathsf{GD}\text{-}\mathbf{\beta}$ estimators can be efficiently optimized with gradient flow, despite a non-convex training objective. Our results reveal that LTB achieves <b>ICL</b> by implementing $\mathsf{GD}\text{-}\mathbf{\beta}$, and they highlight the role of MLP layers in reducing approximation error.</p></p class="citation"></blockquote><h3 id=26--238299-causal-imputation-for-counterfactual-scms-bridging-graphs-and-latent-factor-models-alvaro-ribot-et-al-2024>(2/6 | 238/299) Causal Imputation for Counterfactual SCMs: Bridging Graphs and Latent Factor Models (Alvaro Ribot et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alvaro Ribot, Chandler Squires, Caroline Uhler. (2024)<br><strong>Causal Imputation for Counterfactual SCMs: Bridging Graphs and Latent Factor Models</strong><br><button class=copy-to-clipboard title="Causal Imputation for Counterfactual SCMs: Bridging Graphs and Latent Factor Models" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 13<br>Keywords: Graph, Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14777v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14777v1.pdf filename=2402.14777v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the task of causal imputation, where we aim to predict the outcomes of some set of actions across a wide range of possible contexts. As a running example, we consider predicting how different drugs affect cells from different cell types. We study the index-only setting, where the actions and contexts are categorical variables with a finite number of possible values. Even in this simple setting, a practical challenge arises, since often only a small subset of possible action-context pairs have been studied. Thus, models must extrapolate to novel action-context pairs, which can be framed as a form of matrix completion with rows indexed by actions, columns indexed by contexts, and matrix entries corresponding to outcomes. We introduce a novel SCM-based model class, where the outcome is expressed as a <b>counterfactual,</b> actions are expressed as interventions on an instrumental variable, and contexts are defined based on the initial state of the system. We show that, under a linearity assumption, this setup induces a latent factor model over the matrix of outcomes, with an additional fixed effect term. To perform causal prediction based on this model class, we introduce simple extension to the Synthetic Interventions estimator (Agarwal et al., 2020). We evaluate several matrix completion approaches on the PRISM drug repurposing dataset, showing that our method outperforms all other considered matrix completion approaches.</p></p class="citation"></blockquote><h3 id=36--239299-smoothness-adaptive-hypothesis-transfer-learning-haotian-lin-et-al-2024>(3/6 | 239/299) Smoothness Adaptive Hypothesis Transfer Learning (Haotian Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haotian Lin, Matthew Reimherr. (2024)<br><strong>Smoothness Adaptive Hypothesis Transfer Learning</strong><br><button class=copy-to-clipboard title="Smoothness Adaptive Hypothesis Transfer Learning" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ME, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14966v1.pdf filename=2402.14966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many existing two-phase kernel-based hypothesis <b>transfer</b> <b>learning</b> algorithms employ the same kernel regularization across phases and rely on the known smoothness of functions to obtain optimality. Therefore, they fail to adapt to the varying and unknown smoothness between the target/source and their offset in practice. In this paper, we address these problems by proposing Smoothness Adaptive <b>Transfer</b> <b>Learning</b> (SATL), a two-phase kernel ridge regression(KRR)-based algorithm. We first prove that employing the misspecified fixed bandwidth Gaussian kernel in target-only KRR learning can achieve minimax optimality and derive an adaptive procedure to the unknown Sobolev smoothness. Leveraging these results, SATL employs Gaussian kernels in both phases so that the estimators can adapt to the unknown smoothness of the target/source and their offset function. We derive the minimax lower bound of the learning problem in excess risk and show that SATL enjoys a matching upper bound up to a logarithmic factor. The minimax convergence rate sheds light on the factors influencing <b>transfer</b> <b>dynamics</b> and demonstrates the superiority of SATL compared to non-transfer learning settings. While our main objective is a theoretical analysis, we also conduct several experiments to confirm our results.</p></p class="citation"></blockquote><h3 id=46--240299-on-the-performance-of-empirical-risk-minimization-with-smoothed-data-adam-block-et-al-2024>(4/6 | 240/299) On the Performance of Empirical Risk Minimization with Smoothed Data (Adam Block et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adam Block, Alexander Rakhlin, Abhishek Shetty. (2024)<br><strong>On the Performance of Empirical Risk Minimization with Smoothed Data</strong><br><button class=copy-to-clipboard title="On the Performance of Empirical Risk Minimization with Smoothed Data" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 5<br>Keywords: Square Loss<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14987v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14987v1.pdf filename=2402.14987v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In order to circumvent statistical and computational hardness results in sequential decision-making, recent work has considered smoothed online learning, where the distribution of data at each time is assumed to have bounded likeliehood ratio with respect to a base measure when conditioned on the history. While previous works have demonstrated the benefits of smoothness, they have either assumed that the base measure is known to the learner or have presented computationally inefficient algorithms applying only in special cases. This work investigates the more general setting where the base measure is \emph{unknown} to the learner, focusing in particular on the performance of Empirical Risk Minimization (ERM) with <b>square</b> <b>loss</b> when the data are well-specified and smooth. We show that in this setting, ERM is able to achieve sublinear error whenever a class is learnable with iid data; in particular, ERM achieves error scaling as $\tilde O( \sqrt{\mathrm{comp}(\mathcal F)\cdot T} )$, where $\mathrm{comp}(\mathcal F)$ is the statistical complexity of learning $\mathcal F$ with iid data. In so doing, we prove a novel norm comparison bound for smoothed data that comprises the first sharp norm comparison for dependent data applying to arbitrary, nonlinear function classes. We complement these results with a lower bound indicating that our analysis of ERM is essentially tight, establishing a separation in the performance of ERM between smoothed and iid data.</p></p class="citation"></blockquote><h3 id=56--241299-batch-and-match-black-box-variational-inference-with-a-score-based-divergence-diana-cai-et-al-2024>(5/6 | 241/299) Batch and match: black-box variational inference with a score-based divergence (Diana Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Diana Cai, Chirag Modi, Loucas Pillaud-Vivien, Charles C. Margossian, Robert M. Gower, David M. Blei, Lawrence K. Saul. (2024)<br><strong>Batch and match: black-box variational inference with a score-based divergence</strong><br><button class=copy-to-clipboard title="Batch and match: black-box variational inference with a score-based divergence" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-AI, cs-LG, stat-CO, stat-ML, stat.ML<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14758v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14758v1.pdf filename=2402.14758v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most leading implementations of <b>black-box</b> <b>variational</b> inference (BBVI) are based on optimizing a stochastic evidence lower bound (ELBO). But such approaches to BBVI often converge slowly due to the high variance of their gradient estimates. In this work, we propose batch and match (BaM), an alternative approach to BBVI based on a score-based divergence. Notably, this score-based divergence can be optimized by a closed-form proximal update for Gaussian variational families with full covariance matrices. We analyze the convergence of BaM when the target distribution is Gaussian, and we prove that in the limit of infinite batch size the variational parameter updates converge exponentially quickly to the target mean and covariance. We also evaluate the performance of BaM on Gaussian and non-Gaussian target distributions that arise from posterior inference in hierarchical and deep generative models. In these experiments, we find that BaM typically converges in fewer (and sometimes significantly fewer) gradient evaluations than leading implementations of BBVI based on ELBO maximization.</p></p class="citation"></blockquote><h3 id=66--242299-structure-agnostic-optimality-of-doubly-robust-learning-for-treatment-effect-estimation-jikai-jin-et-al-2024>(6/6 | 242/299) Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation (Jikai Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jikai Jin, Vasilis Syrgkanis. (2024)<br><strong>Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation</strong><br><button class=copy-to-clipboard title="Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, econ-EM, math-ST, stat-ME, stat-ML, stat-TH, stat.ML<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14264v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14264v1.pdf filename=2402.14264v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, recently also incorporating generic machine learning estimators, the statistical optimality of these methods has still remained an open area of investigation. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to <b>black-box</b> <b>estimators</b> that attain small errors; which is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as a <b>black-box</b> <b>sub-process.</b> Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATTE), as well as weighted variants of the former, which arise in policy evaluation.</p></p class="citation"></blockquote><h2 id=cscy-3>cs.CY (3)</h2><h3 id=13--243299-demographic-bias-of-expert-level-vision-language-foundation-models-in-medical-imaging-yuzhe-yang-et-al-2024>(1/3 | 243/299) Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging (Yuzhe Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuzhe Yang, Yujia Liu, Xin Liu, Avanti Gulhane, Domenico Mastrodicasa, Wei Wu, Edward J Wang, Dushyant W Sahani, Shwetak Patel. (2024)<br><strong>Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging</strong><br><button class=copy-to-clipboard title="Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CV, cs-CY, cs-LG, cs.CY<br>Keyword Score: 40<br>Keywords: Fairness, Foundation Model, Self-supervised Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14815v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14815v1.pdf filename=2402.14815v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances in artificial intelligence (AI) have achieved expert-level performance in medical imaging applications. Notably, <b>self-supervised</b> <b>vision-language</b> <b>foundation</b> <b>models</b> can detect a broad spectrum of pathologies without relying on explicit training annotations. However, it is crucial to ensure that these AI models do not mirror or amplify human biases, thereby disadvantaging historically marginalized groups such as females or Black patients. The manifestation of such biases could systematically delay essential medical care for certain patient subgroups. In this study, we investigate the algorithmic <b>fairness</b> of state-of-the-art <b>vision-language</b> <b>foundation</b> <b>models</b> in chest X-ray diagnosis across five globally-sourced datasets. Our findings reveal that compared to board-certified radiologists, these <b>foundation</b> <b>models</b> consistently underdiagnose marginalized groups, with even higher rates seen in intersectional subgroups, such as Black female patients. Such demographic biases present over a wide range of pathologies and demographic attributes. Further analysis of the model embedding uncovers its significant encoding of demographic information. Deploying AI systems with these biases in medical imaging can intensify pre-existing care disparities, posing potential challenges to equitable healthcare access and raising ethical questions about their clinical application.</p></p class="citation"></blockquote><h3 id=23--244299-filter-bubble-or-homogenization-disentangling-the-long-term-effects-of-recommendations-on-user-consumption-patterns-md-sanzeed-anwar-et-al-2024>(2/3 | 244/299) Filter Bubble or Homogenization? Disentangling the Long-Term Effects of Recommendations on User Consumption Patterns (Md Sanzeed Anwar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Sanzeed Anwar, Grant Schoenebeck, Paramveer S. Dhillon. (2024)<br><strong>Filter Bubble or Homogenization? Disentangling the Long-Term Effects of Recommendations on User Consumption Patterns</strong><br><button class=copy-to-clipboard title="Filter Bubble or Homogenization? Disentangling the Long-Term Effects of Recommendations on User Consumption Patterns" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs-IR, cs.CY<br>Keyword Score: 30<br>Keywords: Recommendation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15013v1.pdf filename=2402.15013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommendation</b> algorithms play a pivotal role in shaping our media choices, which makes it crucial to comprehend their long-term impact on user behavior. These algorithms are often linked to two critical outcomes: homogenization, wherein users consume similar content despite disparate underlying preferences, and the filter bubble effect, wherein individuals with differing preferences only consume content aligned with their preferences (without much overlap with other users). Prior research assumes a trade-off between homogenization and filter bubble effects and then shows that personalized <b>recommendations</b> mitigate filter bubbles by fostering homogenization. However, because of this assumption of a tradeoff between these two effects, prior work cannot develop a more nuanced view of how <b>recommendation</b> systems may independently impact homogenization and filter bubble effects. We develop a more refined definition of homogenization and the filter bubble effect by decomposing them into two key metrics: how different the average consumption is between users (inter-user diversity) and how varied an individual&rsquo;s consumption is (intra-user diversity). We then use a novel agent-based <b>simulation</b> framework that enables a holistic view of the impact of <b>recommendation</b> systems on homogenization and filter bubble effects. Our <b>simulations</b> show that traditional <b>recommendation</b> algorithms (based on past behavior) mainly reduce filter bubbles by affecting inter-user diversity without significantly impacting intra-user diversity. Building on these findings, we introduce two new <b>recommendation</b> algorithms that take a more nuanced approach by accounting for both types of diversity.</p></p class="citation"></blockquote><h3 id=33--245299-an-exploratory-analysis-of-covid-bot-vs-human-disinformation-dissemination-stemming-from-the-disinformation-dozen-on-telegram-lynnette-hui-xian-ng-et-al-2024>(3/3 | 245/299) An Exploratory Analysis of COVID Bot vs Human Disinformation Dissemination stemming from the Disinformation Dozen on Telegram (Lynnette Hui Xian Ng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lynnette Hui Xian Ng, Ian Kloo, Kathleen M. Carley. (2024)<br><strong>An Exploratory Analysis of COVID Bot vs Human Disinformation Dissemination stemming from the Disinformation Dozen on Telegram</strong><br><button class=copy-to-clipboard title="An Exploratory Analysis of COVID Bot vs Human Disinformation Dissemination stemming from the Disinformation Dozen on Telegram" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs-SI, cs.CY<br>Keyword Score: 10<br>Keywords: Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14203v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14203v1.pdf filename=2402.14203v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The COVID-19 pandemic of 2021 led to a worldwide health crisis that was accompanied by an infodemic. A group of 12 social media personalities, dubbed the ``Disinformation Dozen", were identified as key in spreading disinformation regarding the COVID-19 virus, treatments, and vaccines. This study focuses on the spread of disinformation propagated by this group on Telegram, a mobile messaging and social media platform. After segregating users into three groups &ndash; the Disinformation Dozen, bots, and humans &ndash;, we perform an investigation with a dataset of Telegram messages from January to June 2023, comparatively analyzing temporal, topical, and network features. We observe that the Disinformation Dozen are highly involved in the initial dissemination of disinformation but are not the main drivers of the propagation of disinformation. Bot users are extremely active in conversation threads, while human users are active propagators of information, disseminating posts between Telegram channels through the forwarding mechanism.</p></p class="citation"></blockquote><h2 id=csir-9>cs.IR (9)</h2><h3 id=19--246299-from-keywords-to-structured-summaries-streamlining-scholarly-knowledge-access-mahsa-shamsabadi-et-al-2024>(1/9 | 246/299) From Keywords to Structured Summaries: Streamlining Scholarly Knowledge Access (Mahsa Shamsabadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahsa Shamsabadi, Jennifer D&rsquo;Souza. (2024)<br><strong>From Keywords to Structured Summaries: Streamlining Scholarly Knowledge Access</strong><br><button class=copy-to-clipboard title="From Keywords to Structured Summaries: Streamlining Scholarly Knowledge Access" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-CL, cs-DL, cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Fine-tuning, Information Retrieval, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14622v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14622v1.pdf filename=2402.14622v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This short paper highlights the growing importance of <b>information</b> <b>retrieval</b> (IR) engines in the scientific community, addressing the inefficiency of traditional keyword-based search engines due to the rising volume of publications. The proposed solution involves structured records, underpinning advanced <b>information</b> <b>technology</b> (IT) tools, including visualization dashboards, to revolutionize how researchers access and filter articles, replacing the traditional text-heavy approach. This vision is exemplified through a proof of concept centered on the ``reproductive number estimate of infectious diseases&rsquo;&rsquo; research theme, using a <b>fine-tuned</b> <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to automate the creation of structured records to populate a backend database that now goes beyond keywords. The result is a next-generation IR method accessible at <a href=https://orkg.org/usecases/r0-estimates>https://orkg.org/usecases/r0-estimates</a>.</p></p class="citation"></blockquote><h3 id=29--247299-personalized-behavior-aware-transformer-for-multi-behavior-sequential-recommendation-jiajie-su-et-al-2024>(2/9 | 247/299) Personalized Behavior-Aware Transformer for Multi-Behavior Sequential Recommendation (Jiajie Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiajie Su, Chaochao Chen, Zibin Lin, Xi Li, Weiming Liu, Xiaolin Zheng. (2024)<br><strong>Personalized Behavior-Aware Transformer for Multi-Behavior Sequential Recommendation</strong><br><button class=copy-to-clipboard title="Personalized Behavior-Aware Transformer for Multi-Behavior Sequential Recommendation" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 33<br>Keywords: Benchmarking, Recommendation, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14473v1.pdf filename=2402.14473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sequential <b>Recommendation</b> (SR) captures users&rsquo; dynamic preferences by modeling how users transit among items. However, SR models that utilize only single type of behavior interaction data encounter performance degradation when the sequences are short. To tackle this problem, we focus on Multi-Behavior Sequential <b>Recommendation</b> (MBSR) in this paper, which aims to leverage time-evolving heterogeneous behavioral dependencies for better exploring users&rsquo; potential intents on the target behavior. Solving MBSR is challenging. On the one hand, users exhibit diverse multi-behavior patterns due to personal characteristics. On the other hand, there exists comprehensive co-influence between behavior correlations and item collaborations, the intensity of which is deeply affected by temporal factors. To tackle these challenges, we propose a Personalized Behavior-Aware <b>Transformer</b> framework (PBAT) for MBSR problem, which models personalized patterns and multifaceted sequential collaborations in a novel way to boost <b>recommendation</b> performance. First, PBAT develops a personalized behavior pattern generator in the representation layer, which extracts dynamic and discriminative behavior patterns for sequential learning. Second, PBAT reforms the <b>self-attention</b> layer with a behavior-aware collaboration extractor, which introduces a fused behavior-aware attention mechanism for incorporating both behavioral and temporal impacts into collaborative transitions. We conduct experiments on three <b>benchmark</b> datasets and the results demonstrate the effectiveness and interpretability of our framework. Our implementation code is released at <a href=https://github.com/TiliaceaeSU/PBAT>https://github.com/TiliaceaeSU/PBAT</a>.</p></p class="citation"></blockquote><h3 id=39--248299-scalable-and-provably-fair-exposure-control-for-large-scale-recommender-systems-riku-togashi-et-al-2024>(3/9 | 248/299) Scalable and Provably Fair Exposure Control for Large-Scale Recommender Systems (Riku Togashi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Riku Togashi, Kenshi Abe, Yuta Saito. (2024)<br><strong>Scalable and Provably Fair Exposure Control for Large-Scale Recommender Systems</strong><br><button class=copy-to-clipboard title="Scalable and Provably Fair Exposure Control for Large-Scale Recommender Systems" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Fairness, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14369v1.pdf filename=2402.14369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Typical <b>recommendation</b> and ranking methods aim to optimize the satisfaction of users, but they are often oblivious to their impact on the items (e.g., products, jobs, news, video) and their providers. However, there has been a growing understanding that the latter is crucial to consider for a wide range of applications, since it determines the utility of those being recommended. Prior approaches to <b>fairness-aware</b> <b>recommendation</b> optimize a regularized objective to balance user satisfaction and item <b>fairness</b> based on some notion such as exposure <b>fairness.</b> These existing methods have been shown to be effective in controlling <b>fairness,</b> however, most of them are computationally inefficient, limiting their applications to only unrealistically small-scale situations. This indeed implies that the literature does not yet provide a solution to enable a flexible control of exposure in the industry-scale <b>recommender</b> <b>systems</b> where millions of users and items exist. To enable a computationally efficient exposure control even for such large-scale systems, this work develops a scalable, fast, and fair method called \emph{\textbf{ex}posure-aware \textbf{ADMM} (\textbf{exADMM})}. exADMM is based on implicit alternating least squares (iALS), a conventional scalable algorithm for collaborative filtering, but optimizes a regularized objective to achieve a flexible control of accuracy-fairness tradeoff. A particular technical challenge in developing exADMM is the fact that the <b>fairness</b> regularizer destroys the separability of optimization subproblems for users and items, which is an essential property to ensure the scalability of iALS. Therefore, we develop a set of optimization tools to enable yet scalable <b>fairness</b> control with provable convergence guarantees as a basis of our algorithm.</p></p class="citation"></blockquote><h3 id=49--249299-genserp-large-language-models-for-whole-page-presentation-zhenning-zhang-et-al-2024>(4/9 | 249/299) GenSERP: Large Language Models for Whole Page Presentation (Zhenning Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenning Zhang, Yunan Zhang, Suyu Ge, Guangwei Weng, Mridu Narang, Xia Song, Saurabh Tiwary. (2024)<br><strong>GenSERP: Large Language Models for Whole Page Presentation</strong><br><button class=copy-to-clipboard title="GenSERP: Large Language Models for Whole Page Presentation" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 30<br>Keywords: Few-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14301v1.pdf filename=2402.14301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> brings an opportunity to minimize the effort in search engine result page (SERP) organization. In this paper, we propose GenSERP, a framework that leverages <b>LLMs</b> with vision in a <b>few-shot</b> setting to dynamically organize intermediate search results, including generated chat answers, website snippets, multimedia data, knowledge panels into a coherent SERP layout based on a user&rsquo;s query. Our approach has three main stages: (1) An information gathering phase where the <b>LLM</b> continuously orchestrates API tools to retrieve different types of items, and proposes candidate layouts based on the retrieved items, until it&rsquo;s confident enough to generate the final result. (2) An answer generation phase where the <b>LLM</b> populates the layouts with the retrieved content. In this phase, the <b>LLM</b> adaptively optimize the ranking of items and UX configurations of the SERP. Consequently, it assigns a location on the page to each item, along with the UX display details. (3) A scoring phase where an <b>LLM</b> with vision scores all the generated SERPs based on how likely it can satisfy the user. It then send the one with highest score to rendering. GenSERP features two generation paradigms. First, coarse-to-fine, which allow it to approach optimal layout in a more manageable way, (2) beam search, which give it a better chance to hit the optimal solution compared to greedy decoding. Offline experimental results on real-world data demonstrate how <b>LLMs</b> can contextually organize heterogeneous search results on-the-fly and provide a promising user experience.</p></p class="citation"></blockquote><h3 id=59--250299-transforming-norm-based-to-graph-based-spatial-representation-for-spatio-temporal-epidemiological-models-teddy-lazebnik-2024>(5/9 | 250/299) Transforming Norm-based To Graph-based Spatial Representation for Spatio-Temporal Epidemiological Models (Teddy Lazebnik, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Teddy Lazebnik. (2024)<br><strong>Transforming Norm-based To Graph-based Spatial Representation for Spatio-Temporal Epidemiological Models</strong><br><button class=copy-to-clipboard title="Transforming Norm-based To Graph-based Spatial Representation for Spatio-Temporal Epidemiological Models" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-NA, cs.IR, math-DS, math-NA<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14539v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14539v2.pdf filename=2402.14539v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pandemics, with their profound societal and economic impacts, pose significant threats to global health, mortality rates, economic stability, and political landscapes. In response to these challenges, numerous studies have employed spatio-temporal models to enhance our understanding and management of these complex phenomena. These spatio-temporal models can be roughly divided into two main spatial categories: norm-based and <b>graph-based.</b> Norm-based models are usually more accurate and easier to model but are more computationally intensive and require more data to fit. On the other hand, <b>graph-based</b> models are less accurate and harder to model but are less computationally intensive and require fewer data to fit. As such, ideally, one would like to use a <b>graph-based</b> model while preserving the representation accuracy obtained by the norm-based model. In this study, we explore the ability to transform from norm-based to <b>graph-based</b> spatial representation for these models. We first show no analytical mapping between the two exists, requiring one to use approximation numerical methods instead. We introduce a novel framework for this task together with twelve possible implementations using a wide range of heuristic optimization approaches. Our findings show that by leveraging agent-based <b>simulations</b> and heuristic algorithms for the <b>graph</b> node&rsquo;s location and population&rsquo;s spatial walk dynamics approximation one can use <b>graph-based</b> spatial representation without losing much of the model&rsquo;s accuracy and expressiveness. We investigate our framework for three real-world cases, achieving 94% accuracy preservation, on average. Moreover, an analysis of synthetic cases shows the proposed framework is relatively robust for changes in both spatial and temporal properties.</p></p class="citation"></blockquote><h3 id=69--251299-recommender-for-its-purpose-repeat-and-exploration-in-food-delivery-recommendations-jiayu-li-et-al-2024>(6/9 | 251/299) Recommender for Its Purpose: Repeat and Exploration in Food Delivery Recommendations (Jiayu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayu Li, Aixin Sun, Weizhi Ma, Peijie Sun, Min Zhang. (2024)<br><strong>Recommender for Its Purpose: Repeat and Exploration in Food Delivery Recommendations</strong><br><button class=copy-to-clipboard title="Recommender for Its Purpose: Repeat and Exploration in Food Delivery Recommendations" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14440v1.pdf filename=2402.14440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommender</b> <b>systems</b> have been widely used for various scenarios, such as e-commerce, news, and music, providing online contents to help and enrich users&rsquo; daily life. Different scenarios hold distinct and unique characteristics, calling for domain-specific investigations and corresponding designed <b>recommender</b> <b>systems.</b> Therefore, in this paper, we focus on food delivery <b>recommendations</b> to unveil unique features in this domain, where users order food online and enjoy their meals shortly after delivery. We first conduct an in-depth analysis on food delivery datasets. The analysis shows that repeat orders are prevalent for both users and stores, and situations&rsquo; differently influence repeat and exploration consumption in the food delivery <b>recommender</b> <b>systems.</b> Moreover, we revisit the ability of existing situation-aware methods for repeat and exploration <b>recommendations</b> respectively, and find them unable to effectively solve both tasks simultaneously. Based on the analysis and experiments, we have designed two separate <b>recommendation</b> models &ndash; ReRec for repeat orders and ExpRec for exploration orders; both are simple in their design and computation. We conduct experiments on three real-world food delivery datasets, and our proposed models outperform various types of baselines on repeat, exploration, and combined <b>recommendation</b> tasks. This paper emphasizes the importance of dedicated analyses and methods for domain-specific characteristics for the <b>recommender</b> <b>system</b> studies.</p></p class="citation"></blockquote><h3 id=79--252299-ensure-timeliness-and-accuracy-a-novel-sliding-window-data-stream-paradigm-for-live-streaming-recommendation-fengqi-liang-et-al-2024>(7/9 | 252/299) Ensure Timeliness and Accuracy: A Novel Sliding Window Data Stream Paradigm for Live Streaming Recommendation (Fengqi Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fengqi Liang, Baigong Zheng, Liqin Zhao, Guorui Zhou, Qian Wang, Yanan Niu. (2024)<br><strong>Ensure Timeliness and Accuracy: A Novel Sliding Window Data Stream Paradigm for Live Streaming Recommendation</strong><br><button class=copy-to-clipboard title="Ensure Timeliness and Accuracy: A Novel Sliding Window Data Stream Paradigm for Live Streaming Recommendation" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14399v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14399v1.pdf filename=2402.14399v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Live streaming <b>recommender</b> <b>system</b> is specifically designed to recommend real-time live streaming of interest to users. Due to the dynamic changes of live content, improving the timeliness of the live streaming <b>recommender</b> <b>system</b> is a critical problem. Intuitively, the timeliness of the data determines the upper bound of the timeliness that models can learn. However, none of the previous works addresses the timeliness problem of the live streaming <b>recommender</b> <b>system</b> from the perspective of data stream design. Employing the conventional fixed window data stream paradigm introduces a trade-off dilemma between labeling accuracy and timeliness. In this paper, we propose a new data stream design paradigm, dubbed Sliver, that addresses the timeliness and accuracy problem of labels by reducing the window size and implementing a sliding window correspondingly. Meanwhile, we propose a time-sensitive re-reco strategy reducing the latency between request and impression to improve the timeliness of the <b>recommendation</b> service and features by periodically requesting the <b>recommendation</b> service. To demonstrate the effectiveness of our approach, we conduct offline experiments on a multi-task live streaming dataset with labeling timestamps collected from the Kuaishou live streaming platform. Experimental results demonstrate that Sliver outperforms two fixed-window data streams with varying window sizes across all targets in four typical multi-task <b>recommendation</b> models. Furthermore, we deployed Sliver on the Kuaishou live streaming platform. Results of the online A/B test show a significant improvement in click-through rate (CTR), and new follow number (NFN), further validating the effectiveness of Sliver.</p></p class="citation"></blockquote><h3 id=89--253299-merrec-a-large-scale-multipurpose-mercari-dataset-for-consumer-to-consumer-recommendation-systems-lichi-li-et-al-2024>(8/9 | 253/299) MerRec: A Large-scale Multipurpose Mercari Dataset for Consumer-to-Consumer Recommendation Systems (Lichi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lichi Li, Zainul Abi Din, Zhen Tan, Sam London, Tianlong Chen, Ajay Daptardar. (2024)<br><strong>MerRec: A Large-scale Multipurpose Mercari Dataset for Consumer-to-Consumer Recommendation Systems</strong><br><button class=copy-to-clipboard title="MerRec: A Large-scale Multipurpose Mercari Dataset for Consumer-to-Consumer Recommendation Systems" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 13<br>Keywords: Benchmarking, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14230v1.pdf filename=2402.14230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the evolving e-commerce field, <b>recommendation</b> systems crucially shape user experience and engagement. The rise of Consumer-to-Consumer (C2C) <b>recommendation</b> systems, noted for their flexibility and ease of access for customer vendors, marks a significant trend. However, the academic focus remains largely on Business-to-Consumer (B2C) models, leaving a gap filled by the limited C2C <b>recommendation</b> datasets that lack in item attributes, user diversity, and scale. The intricacy of C2C <b>recommendation</b> systems is further accentuated by the dual roles users assume as both sellers and buyers, introducing a spectrum of less uniform and varied inputs. Addressing this, we introduce MerRec, the first large-scale dataset specifically for C2C <b>recommendations,</b> sourced from the Mercari e-commerce platform, covering millions of users and products over 6 months in 2023. MerRec not only includes standard features such as user_id, item_id, and session_id, but also unique elements like timestamped action types, product taxonomy, and textual product attributes, offering a comprehensive dataset for research. This dataset, extensively evaluated across six <b>recommendation</b> tasks, establishes a new <b>benchmark</b> for the development of advanced <b>recommendation</b> algorithms in real-world scenarios, bridging the gap between academia and industry and propelling the study of C2C <b>recommendations.</b></p></p class="citation"></blockquote><h3 id=99--254299-towards-efficient-pareto-optimal-utility-fairness-between-groups-in-repeated-rankings-phuong-dinh-mai-et-al-2024>(9/9 | 254/299) Towards Efficient Pareto-optimal Utility-Fairness between Groups in Repeated Rankings (Phuong Dinh Mai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Phuong Dinh Mai, Duc-Trong Le, Tuan-Anh Hoang, Dung D. Le. (2024)<br><strong>Towards Efficient Pareto-optimal Utility-Fairness between Groups in Repeated Rankings</strong><br><button class=copy-to-clipboard title="Towards Efficient Pareto-optimal Utility-Fairness between Groups in Repeated Rankings" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14305v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14305v1.pdf filename=2402.14305v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we tackle the problem of computing a sequence of rankings with the guarantee of the Pareto-optimal balance between (1) maximizing the utility of the consumers and (2) minimizing unfairness between producers of the items. Such a multi-objective optimization problem is typically solved using a combination of a scalarization method and linear programming on bi-stochastic matrices, representing the distribution of possible rankings of items. However, the above-mentioned approach relies on Birkhoff-von Neumann (BvN) decomposition, of which the computational complexity is $\mathcal{O}(n^5)$ with $n$ being the number of items, making it impractical for large-scale systems. To address this drawback, we introduce a novel approach to the above problem by using the Expohedron - a permutahedron whose points represent all achievable exposures of items. On the Expohedron, we profile the Pareto curve which captures the trade-off between group <b>fairness</b> and user utility by identifying a finite number of Pareto optimal solutions. We further propose an efficient method by relaxing our optimization problem on the Expohedron&rsquo;s circumscribed $n$-sphere, which significantly improve the running time. Moreover, the approximate Pareto curve is asymptotically close to the real Pareto optimal curve as the number of substantial solutions increases. Our methods are applicable with different ranking merits that are non-decreasing functions of item relevance. The effectiveness of our methods are validated through experiments on both synthetic and real-world datasets.</p></p class="citation"></blockquote><h2 id=csgt-5>cs.GT (5)</h2><h3 id=15--255299-optimal-mechanism-in-a-dynamic-stochastic-knapsack-environment-jihyeok-jung-et-al-2024>(1/5 | 255/299) Optimal Mechanism in a Dynamic Stochastic Knapsack Environment (Jihyeok Jung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jihyeok Jung, Chan-Oi Song, Deok-Joo Lee, Kiho Yoon. (2024)<br><strong>Optimal Mechanism in a Dynamic Stochastic Knapsack Environment</strong><br><button class=copy-to-clipboard title="Optimal Mechanism in a Dynamic Stochastic Knapsack Environment" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT, econ-GN, q-fin-EC<br>Keyword Score: 40<br>Keywords: Discrete Time, Discrete Time, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14269v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14269v1.pdf filename=2402.14269v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces an optimal mechanism in a dynamic stochastic knapsack environment. The model features a single seller who has a fixed quantity of a perfectly divisible item. Impatient buyers with a piece-wise linear utility function arrive randomly and they report the two-dimensional private information: marginal value and demanded quantity. We derive a revenue-maximizing dynamic mechanism in a finite <b>discrete</b> <b>time</b> framework that satisfies incentive compatibility, individual rationality, and feasibility conditions. It is achieved by characterizing buyers&rsquo; utility and deriving the Bellman equation. Moreover, we propose the essential penalty scheme for incentive compatibility, as well as the allocation and payment policies. Lastly, we propose algorithms to approximate the optimal policy, based on the Monte Carlo <b>simulation-based</b> regression method and <b>reinforcement</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=25--256299-stability-of-p2p-networks-under-greedy-peering-full-version-lucianna-kiffer-et-al-2024>(2/5 | 256/299) Stability of P2P Networks Under Greedy Peering (Full Version) (Lucianna Kiffer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucianna Kiffer, Rajmohan Rajaraman. (2024)<br><strong>Stability of P2P Networks Under Greedy Peering (Full Version)</strong><br><button class=copy-to-clipboard title="Stability of P2P Networks Under Greedy Peering (Full Version)" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-NI, cs.GT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14666v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14666v1.pdf filename=2402.14666v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Major cryptocurrency networks have relied on random peering choice rules for making connections in their peer-to-peer networks. Generally, these choices have good properties, particularly for open, permissionless networks. Random peering choices however do not take into account that some actors may choose to optimize who they connect to such that they are quicker to hear about information being propagated in the network. In this paper, we explore the dynamics of such greedy strategies. We study a model in which nodes select peers with the objective of minimizing their average distance to a designated subset of nodes in the network, and consider the impact of several factors including the peer selection process, degree constraints, and the size of the designated subset. The latter is particularly interesting in the context of blockchain networks as generally only a subset of nodes are the propagation source for content. We first analyze an idealized version of the game where each node has full knowledge of the current network and aims to select the $d$ best connections, and prove the existence of equilibria under various model assumptions. Since in reality nodes only have local knowledge based on their peers&rsquo; behavior, we also study a greedy protocol which runs in rounds, with each node replacing its worst-performing edge with a new random edge. We exactly characterize stability properties of networks that evolve with this peering rule and derive regimes where stability is possible and even inevitable. We also run extensive <b>simulations</b> with this peering rule examining both how the network evolves and how different network parameters affect the stability properties of the network. Our findings generally show that the only stable networks that arise from greedy peering choices are low-diameter and result in disparate performance for nodes in the network.</p></p class="citation"></blockquote><h3 id=35--257299-on-the-fairness-of-normalized-p-means-for-allocating-goods-and-chores-owen-eckart-et-al-2024>(3/5 | 257/299) On the Fairness of Normalized p-Means for Allocating Goods and Chores (Owen Eckart et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Owen Eckart, Alexandros Psomas, Paritosh Verma. (2024)<br><strong>On the Fairness of Normalized p-Means for Allocating Goods and Chores</strong><br><button class=copy-to-clipboard title="On the Fairness of Normalized p-Means for Allocating Goods and Chores" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14996v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14996v1.pdf filename=2402.14996v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Allocating items in a fair and economically efficient manner is a central problem in fair division. We study this problem for agents with additive preferences, when items are all goods or all chores, divisible or indivisible. The celebrated notion of Nash welfare is known to produce fair and efficient allocations for both divisible and indivisible goods; there is no known analogue for dividing chores. The Nash welfare objective belongs to a large, parameterized family of objectives called the p-mean welfare functions, which includes other notable members, like social welfare and egalitarian welfare. However, among the members of this family, only the Nash welfare produces fair allocations for goods. Incidentally, Nash welfare is also the only member that satisfies the axiom of scale invariance, which is crucially associated with its <b>fairness</b> properties. We define the class of &ldquo;normalized p-mean&rdquo; objectives, which imparts the missing key axiom of scale invariance to the p-mean family. Our results show that optimizing the normalized p-mean objectives produces fair and efficient allocations when the items are goods or chores, divisible or indivisible. For instance, the normalized p-means gives us an infinite class of objectives that produce (i) proportional and Pareto efficient allocations for divisible goods, (ii) approximately proportional and Pareto efficient allocations for divisible chores, (iii) EF1 and Pareto efficient allocations for indivisible goods for two agents, and (iv) EF1 and Pareto efficient allocations for indivisible chores for two agents.</p></p class="citation"></blockquote><h3 id=45--258299-on-truthful-item-acquiring-mechanisms-for-reward-maximization-liang-shan-et-al-2024>(4/5 | 258/299) On Truthful Item-Acquiring Mechanisms for Reward Maximization (Liang Shan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liang Shan, Shuo Zhang, Jie Zhang, Zihe Wang. (2024)<br><strong>On Truthful Item-Acquiring Mechanisms for Reward Maximization</strong><br><button class=copy-to-clipboard title="On Truthful Item-Acquiring Mechanisms for Reward Maximization" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14540v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14540v1.pdf filename=2402.14540v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this research, we study the problem that a collector acquires items from the owner based on the item qualities the owner declares and an independent appraiser&rsquo;s assessments. The owner is interested in maximizing the probability that the collector acquires the items and is the only one who knows the items&rsquo; factual quality. The appraiser performs her duties with impartiality, but her assessment may be subject to random noises, so it may not accurately reflect the factual quality of the items. The main challenge lies in devising mechanisms that <b>prompt</b> the owner to reveal accurate information, thereby optimizing the collector&rsquo;s expected reward. We consider the menu size of mechanisms as a measure of their practicability and study its impact on the attainable expected reward. For the single-item setting, we design optimal mechanisms with a monotone increasing menu size. Although the reward gap between the simplest and optimal mechanisms is bounded, we show that simple mechanisms with a small menu size cannot ensure any positive fraction of the optimal reward of mechanisms with a larger menu size. For the multi-item setting, we show that an ordinal mechanism that only takes the owner&rsquo;s ordering of the items as input is not incentive-compatible. We then propose a set of Union mechanisms that combine single-item mechanisms. Moreover, we run experiments to examine these mechanisms&rsquo; robustness against the independent appraiser&rsquo;s assessment accuracy and the items&rsquo; acquiring rate.</p></p class="citation"></blockquote><h3 id=55--259299-tight-inapproximability-of-nash-equilibria-in-public-goods-games-jérémi-do-dinh-et-al-2024>(5/5 | 259/299) Tight Inapproximability of Nash Equilibria in Public Goods Games (Jérémi Do Dinh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jérémi Do Dinh, Alexandros Hollender. (2024)<br><strong>Tight Inapproximability of Nash Equilibria in Public Goods Games</strong><br><button class=copy-to-clipboard title="Tight Inapproximability of Nash Equilibria in Public Goods Games" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-CC, cs-GT, cs.GT<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14198v1.pdf filename=2402.14198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study public goods games, a type of game where every player has to decide whether or not to produce a good which is public, i.e., neighboring players can also benefit from it. Specifically, we consider a setting where the good is indivisible and where the neighborhood structure is represented by a directed <b>graph,</b> with the players being the nodes. Papadimitriou and Peng (2023) recently showed that in this setting computing mixed Nash equilibria is PPAD-hard, and that this remains the case even for $\varepsilon$-well-supported approximate equilibria for some sufficiently small constant $\varepsilon$. In this work, we strengthen this inapproximability result by showing that the problem remains PPAD-hard for any non-trivial approximation parameter $\varepsilon$.</p></p class="citation"></blockquote><h2 id=cshc-6>cs.HC (6)</h2><h3 id=16--260299-multimodal-healthcare-ai-identifying-and-designing-clinically-relevant-vision-language-applications-for-radiology-nur-yildirim-et-al-2024>(1/6 | 260/299) Multimodal Healthcare AI: Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology (Nur Yildirim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nur Yildirim, Hannah Richardson, Maria T. Wetscherek, Junaid Bajwa, Joseph Jacob, Mark A. Pinnock, Stephen Harris, Daniel Coelho de Castro, Shruthi Bannur, Stephanie L. Hyland, Pratik Ghosh, Mercy Ranjit, Kenza Bouzid, Anton Schwaighofer, Fernando Pérez-García, Harshita Sharma, Ozan Oktay, Matthew Lungren, Javier Alvarez-Valle, Aditya Nori, Anja Thieme. (2024)<br><strong>Multimodal Healthcare AI: Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology</strong><br><button class=copy-to-clipboard title="Multimodal Healthcare AI: Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14252v1.pdf filename=2402.14252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in AI combine <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with vision encoders that bring forward unprecedented technical capabilities to leverage for a wide range of healthcare applications. Focusing on the domain of radiology, <b>vision-language</b> models (VLMs) achieve good performance results for tasks such as generating radiology findings based on a patient&rsquo;s medical image, or answering visual questions (e.g., &lsquo;Where are the nodules in this chest X-ray?&rsquo;). However, the clinical utility of potential applications of these capabilities is currently underexplored. We engaged in an iterative, multidisciplinary design process to envision clinically relevant VLM interactions, and co-designed four VLM use concepts: Draft Report Generation, Augmented Report Review, Visual Search and Querying, and Patient Imaging History Highlights. We studied these concepts with 13 radiologists and clinicians who assessed the VLM concepts as valuable, yet articulated many design considerations. Reflecting on our findings, we discuss implications for integrating VLM capabilities in radiology, and for healthcare AI more generally.</p></p class="citation"></blockquote><h3 id=26--261299-ai-augmented-brainwriting-investigating-the-use-of-llms-in-group-ideation-orit-shaer-et-al-2024>(2/6 | 261/299) AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation (Orit Shaer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Orit Shaer, Angelora Cooper, Osnat Mokryn, Andrew L. Kun, Hagit Ben Shoshan. (2024)<br><strong>AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation</strong><br><button class=copy-to-clipboard title="AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: H-5-2; J-4, cs-AI, cs-CY, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Generative AI, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14978v1.pdf filename=2402.14978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing availability of <b>generative</b> <b>AI</b> technologies such as <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has significant implications for creative work. This paper explores twofold aspects of integrating <b>LLMs</b> into the creative process - the divergence stage of idea generation, and the convergence stage of evaluation and selection of ideas. We devised a collaborative group-AI Brainwriting ideation framework, which incorporated an <b>LLM</b> as an enhancement into the group ideation process, and evaluated the idea generation process and the resulted solution space. To assess the potential of using <b>LLMs</b> in the idea evaluation process, we design an evaluation engine and compared it to idea ratings assigned by three expert and six novice evaluators. Our findings suggest that integrating <b>LLM</b> in Brainwriting could enhance both the ideation process and its outcome. We also provide evidence that <b>LLMs</b> can support idea evaluation. We conclude by discussing implications for HCI education and practice.</p></p class="citation"></blockquote><h3 id=36--262299-wizard-of-oz-experimentation-for-language-technology-applications-challenges-and-tools-stephan-schlögl-et-al-2024>(3/6 | 262/299) Wizard of Oz Experimentation for Language Technology Applications: Challenges and Tools (Stephan Schlögl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephan Schlögl, Gavin Doherty, Saturnino Luz. (2024)<br><strong>Wizard of Oz Experimentation for Language Technology Applications: Challenges and Tools</strong><br><button class=copy-to-clipboard title="Wizard of Oz Experimentation for Language Technology Applications: Challenges and Tools" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Automatic Speech Recognition, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14563v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14563v1.pdf filename=2402.14563v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wizard of OZ (WOZ) is a well-established method for simulating the functionality and user experience of future systems. Using a human wizard to mimic certain operations of a potential system is particularly useful in situations where extensive engineering effort would otherwise be needed to explore the design possibilities offered by such operations. The WOZ method has been widely used in connection with <b>speech</b> <b>and</b> language technologies, but advances in sensor technology and pattern recognition as well as new application areas such as human-robot interaction have made it increasingly relevant to the design of a wider range of interactive systems. In such cases achieving acceptable performance at the user interface level often hinges on resource intensive improvements such as domain tuning, which are better done once the overall design is relatively stable. While WOZ is recognised as a valuable prototyping technique, surprisingly little effort has been put into exploring it from a methodological point of view. Starting from a survey of the literature, this paper presents a systematic investigation and analysis of the design space for WOZ for language technology applications, and proposes a generic architecture for tool support that supports the integration of components for <b>speech</b> <b>recognition</b> and synthesis as well as for <b>machine</b> <b>translation.</b> This architecture is instantiated in WebWOZ - a new web-based open-source WOZ prototyping platform. The viability of generic support is explored empirically through a series of evaluations. Researchers from a variety of backgrounds were able to create experiments, independent of their previous experience with WOZ. The approach was further validated through a number of real experiments, which also helped to identify a number of possibilities for additional support, and flagged potential issues relating to consistency in Wizard performance.</p></p class="citation"></blockquote><h3 id=46--263299-saharaline-a-collective-social-support-intervention-for-teachers-in-low-income-indian-schools-rama-adithya-varanasi-et-al-2024>(4/6 | 263/299) Saharaline: A Collective Social Support Intervention for Teachers in Low-Income Indian Schools (Rama Adithya Varanasi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rama Adithya Varanasi, Aditya Vashistha, Nicola Dell. (2024)<br><strong>Saharaline: A Collective Social Support Intervention for Teachers in Low-Income Indian Schools</strong><br><button class=copy-to-clipboard title="Saharaline: A Collective Social Support Intervention for Teachers in Low-Income Indian Schools" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Low-Resource, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14292v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14292v1.pdf filename=2402.14292v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents Saharaline, an intervention designed to provide collective social support for teachers in low-income schools. Implemented as a WhatsApp-based helpline, Saharaline enables teachers to reach out for personalized, long-term assistance with a wide range of problems and stressors, including pedagogical, emotional, and technological challenges. Depending on the support needed, teachers&rsquo; requests are routed to appropriate domain experts &ndash; staff employed by educational non-profit organizations who understand teachers&rsquo; on-the-ground realities &ndash; who offer localized and contextualized assistance. Via a three-month exploratory deployment with 28 teachers in India, we show how Saharaline&rsquo;s design enabled a collective of diverse education experts to craft and deliver localized solutions that teachers could incorporate into their practice. We conclude by reflecting on the efficacy of our intervention in <b>low-resource</b> work contexts and provide <b>recommendations</b> to enhance collective social support interventions similar to Saharaline.</p></p class="citation"></blockquote><h3 id=56--264299-gazetrak-exploring-acoustic-based-eye-tracking-on-a-glass-frame-ke-li-et-al-2024>(5/6 | 264/299) GazeTrak: Exploring Acoustic-based Eye Tracking on a Glass Frame (Ke Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ke Li, Ruidong Zhang, Boao Chen, Siyuan Chen, Sicheng Yin, Saif Mahmud, Qikang Liang, François Guimbretière, Cheng Zhang. (2024)<br><strong>GazeTrak: Exploring Acoustic-based Eye Tracking on a Glass Frame</strong><br><button class=copy-to-clipboard title="GazeTrak: Exploring Acoustic-based Eye Tracking on a Glass Frame" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14634v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14634v2.pdf filename=2402.14634v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present GazeTrak, the first acoustic-based eye tracking system on glasses. Our system only needs one speaker and four microphones attached to each side of the glasses. These acoustic sensors capture the formations of the eyeballs and the surrounding areas by emitting encoded inaudible sound towards eyeballs and receiving the reflected signals. These reflected signals are further processed to calculate the echo profiles, which are fed to a customized deep learning pipeline to continuously infer the gaze position. In a user study with 20 participants, GazeTrak achieves an accuracy of 3.6{\deg} within the same remounting session and 4.9{\deg} across different sessions with a refreshing rate of 83.3 Hz and a power signature of 287.9 mW. Furthermore, we report the performance of our gaze tracking system fully implemented on an MCU with a low-power <b>CNN</b> accelerator (MAX78002). In this configuration, the system runs at up to 83.3 Hz and has a total power signature of 95.4 mW with a 30 Hz FPS.</p></p class="citation"></blockquote><h3 id=66--265299-make-interaction-situated-designing-user-acceptable-interaction-for-situated-visualization-in-public-environments-qian-zhu-et-al-2024>(6/6 | 265/299) Make Interaction Situated: Designing User Acceptable Interaction for Situated Visualization in Public Environments (Qian Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qian Zhu, Zhuo Wang, Wei Zeng, Tong Wai, Weiyue Lin, Xiaojuan Ma. (2024)<br><strong>Make Interaction Situated: Designing User Acceptable Interaction for Situated Visualization in Public Environments</strong><br><button class=copy-to-clipboard title="Make Interaction Situated: Designing User Acceptable Interaction for Situated Visualization in Public Environments" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14251v1.pdf filename=2402.14251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Situated visualization blends data into the real world to fulfill individuals&rsquo; contextual information needs. However, interacting with situated visualization in public environments faces challenges posed by user acceptance and contextual constraints. To explore appropriate interaction design, we first conduct a formative study to identify user needs for data and interaction. Informed by the findings, we <b>summarize</b> appropriate interaction modalities with eye-based, hand-based and spatially-aware object interaction for situated visualization in public environments. Then, through an iterative design process with six users, we explore and implement interactive techniques for activating and analyzing with situated visualization. To assess the effectiveness and acceptance of these interactions, we integrate them into an AR prototype and conduct a within-subjects study in public scenarios using conventional hand-only interactions as the baseline. The results show that participants preferred our prototype over the baseline, attributing their preference to the interactions being more acceptable, flexible, and practical in public.</p></p class="citation"></blockquote><h2 id=eesssy-6>eess.SY (6)</h2><h3 id=16--266299-mr-arl-model-reference-adaptive-reinforcement-learning-for-robustly-stable-on-policy-data-driven-lqr-marco-borghesi-et-al-2024>(1/6 | 266/299) MR-ARL: Model Reference Adaptive Reinforcement Learning for Robustly Stable On-Policy Data-Driven LQR (Marco Borghesi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Borghesi, Alessandro Bosso, Giuseppe Notarstefano. (2024)<br><strong>MR-ARL: Model Reference Adaptive Reinforcement Learning for Robustly Stable On-Policy Data-Driven LQR</strong><br><button class=copy-to-clipboard title="MR-ARL: Model Reference Adaptive Reinforcement Learning for Robustly Stable On-Policy Data-Driven LQR" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14483v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14483v1.pdf filename=2402.14483v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article introduces a novel framework for data-driven linear quadratic regulator (LQR) design. First, we introduce a <b>reinforcement</b> <b>learning</b> paradigm for on-policy data-driven LQR, where exploration and exploitation are simultaneously performed while guaranteeing robust stability of the whole closed-loop system encompassing the plant and the control/learning dynamics. Then, we propose Model Reference Adaptive <b>Reinforcement</b> <b>Learning</b> (MR-ARL), a control architecture integrating tools from <b>reinforcement</b> <b>learning</b> and model reference adaptive control. The approach stands on a variable reference model containing the currently identified value function. Then, an adaptive stabilizer is used to ensure convergence of the applied policy to the optimal one, convergence of the plant to the optimal reference model, and overall robust closed-loop stability. The proposed framework provides theoretical robustness certificates against real-world perturbations such as measurement noise, plant nonlinearities, or slowly varying parameters. The effectiveness of the proposed architecture is validated via realistic numerical <b>simulations.</b></p></p class="citation"></blockquote><h3 id=26--267299-using-hybrid-system-dynamics-and-discrete-event-simulations-to-identify-high-leverage-targets-for-process-improvement-in-a-skill-based-organizational-structure-eric-enos-et-al-2024>(2/6 | 267/299) Using Hybrid System Dynamics and Discrete Event Simulations to Identify High Leverage Targets for Process Improvement in a Skill-based Organizational Structure (Eric Enos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric Enos, Daniel Herber. (2024)<br><strong>Using Hybrid System Dynamics and Discrete Event Simulations to Identify High Leverage Targets for Process Improvement in a Skill-based Organizational Structure</strong><br><button class=copy-to-clipboard title="Using Hybrid System Dynamics and Discrete Event Simulations to Identify High Leverage Targets for Process Improvement in a Skill-based Organizational Structure" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14768v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14768v1.pdf filename=2402.14768v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper is based on a case study of an IT organization in a large, US-based healthcare provider, and develops simluation models to identify areas for performance improvement. These organizations are often grouped into departments by technical skill and support both operational work (tickets) and project work (tasks) of various priorities. From a practical standpoint, resource managers and staff regularly manage all work as queued and assign / complete it based on the priorities of the day. Using project and operational metrics from the case study organization, the hybrid model using both system dynamics and discrete event <b>simulation</b> developed through this research depicts the flow of work through a skill-based team as well as many of the key factors that influence that workflow, both positive and negative. Experience indicates that the interaction between project and operational work &ndash; as well as between teams with differing skills &ndash; entangles work queues and wait times within those queues in a way that rapidly scales in complexity as the number of interacting individuals and teams increases. Results from model <b>simulation</b> bear out this intuition. Scaling the models to accommodate multiple teams is a topic of future research.</p></p class="citation"></blockquote><h3 id=36--268299-run-time-assurance-for-simultaneous-constraint-satisfaction-during-spacecraft-attitude-maneuvering-cassie-kay-mcquinn-et-al-2024>(3/6 | 268/299) Run Time Assurance for Simultaneous Constraint Satisfaction During Spacecraft Attitude Maneuvering (Cassie-Kay McQuinn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cassie-Kay McQuinn, Kyle Dunlap, Nathaniel Hamilton, Jabari Wilson, Kerianne L. Hobbs. (2024)<br><strong>Run Time Assurance for Simultaneous Constraint Satisfaction During Spacecraft Attitude Maneuvering</strong><br><button class=copy-to-clipboard title="Run Time Assurance for Simultaneous Constraint Satisfaction During Spacecraft Attitude Maneuvering" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14723v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14723v1.pdf filename=2402.14723v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A fundamental capability for On-orbit Servicing, Assembly, and Manufacturing (OSAM) is inspection of the vehicle to be serviced, or the structure being assembled. This research assumes autonomous slewing to maintain situational awareness of multiple vehicles operating in close proximity where several safety constraints must be satisfied. A variety of techniques may be used as the primary controller. The focus of this research is developing Run Time Assurance (RTA) filters that monitor system behavior and the output of the primary controller to enforce safety constraint satisfaction. Specifically, this research explores combining a subset of the constraints into an Active Set Invariance Filter (ASIF) RTA defined using control barrier functions. This method is minimally invasive to the primary control by minimizing deviation from the desired control output of the primary controller, while simultaneously enforcing all safety constraints. The RTA is designed to ensure the spacecraft maintains attitude requirements for communication and data transfer with a ground station during scheduled communication windows, adheres to conical attitude keep out zones, limits thermally unfavorable attitude duration, maintains attitude requirements for sufficient power generation, ensures maneuvers are below threshold to cause structural damage, ensures maximum angular velocity is below limits to maintain ability to respond quickly to new slewing commands, and conserves actuator use to prevent wear when possible. Slack variables are introduced into the ASIF controller to prioritize safety constraints when a solution to all safety constraints is infeasible. Monte Carlo <b>simulation</b> results as well as plots of example cases are shown and evaluated for a three degree of freedom spacecraft with reaction wheel attitude control.</p></p class="citation"></blockquote><h3 id=46--269299-strong-arm-dynamic-latch-comparators-design-and-analyses-on-cad-platform-kasi-bandla-et-al-2024>(4/6 | 269/299) Strong-ARM Dynamic Latch Comparators: Design and Analyses on CAD Platform (Kasi Bandla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kasi Bandla, Dipankar Pal. (2024)<br><strong>Strong-ARM Dynamic Latch Comparators: Design and Analyses on CAD Platform</strong><br><button class=copy-to-clipboard title="Strong-ARM Dynamic Latch Comparators: Design and Analyses on CAD Platform" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14519v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14519v1.pdf filename=2402.14519v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Strong-ARM Dynamic Latch Comparators are widely used in high-speed analog-to-digital converters (ADCs), sense amplifiers in memory, RFID applications, and data receivers. This paper presents different methods to improve the performance of Strong-Arm latch-based comparators. The comparator&rsquo;s significant features such as power dissipation, propagation delay, offset voltage, clock feedthrough, area, and kickback noises are discussed and compared with state-of-the-art candidate topologies. <b>Simulation</b> results show that the new comparator topologies of Strong-ARM Dynamic Latch proposed by these authors gave the best results. The proposed designs are tested. The <b>simulations</b> are carried out using UMC 180nm double metal, double poly standard CMOS process technology, for a 100 MHz clock, at 1.8V supply-rail on the Cadence Virtuoso EDA platform.</p></p class="citation"></blockquote><h3 id=56--270299-closed-loop-data-enabled-predictive-control-and-its-equivalence-with-closed-loop-subspace-predictive-control-rogier-dinkla-et-al-2024>(5/6 | 270/299) Closed-loop Data-Enabled Predictive Control and its equivalence with Closed-loop Subspace Predictive Control (Rogier Dinkla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rogier Dinkla, Sebastiaan Mulders, Tom Oomen, Jan-Willem van Wingerden. (2024)<br><strong>Closed-loop Data-Enabled Predictive Control and its equivalence with Closed-loop Subspace Predictive Control</strong><br><button class=copy-to-clipboard title="Closed-loop Data-Enabled Predictive Control and its equivalence with Closed-loop Subspace Predictive Control" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14374v1.pdf filename=2402.14374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Factors like improved data availability and increasing system complexity have sparked interest in data-driven predictive control (DDPC) methods like Data-enabled Predictive Control (DeePC). However, closed-loop identification bias arises in the presence of noise, which reduces the effectiveness of obtained control policies. In this paper we propose Closed-loop Data-enabled Predictive Control (CL-DeePC), a framework that unifies different approaches to address this challenge. To this end, CL-DeePC incorporates instrumental variables (IVs) to synthesize and sequentially apply consistent single or multi-step-ahead predictors. Furthermore, a computationally efficient CL-DeePC implementation is developed that reveals an equivalence with Closed-loop Subspace Predictive Control (CL-SPC). Compared to DeePC, CL-DeePC <b>simulations</b> demonstrate superior reference tracking, with a sensitivity study finding a 48% lower susceptibility to noise-induced reference tracking performance degradation.</p></p class="citation"></blockquote><h3 id=66--271299-parking-of-connected-automated-vehicles-vehicle-control-parking-assignment-and-multi-agent-simulation-xu-shen-et-al-2024>(6/6 | 271/299) Parking of Connected Automated Vehicles: Vehicle Control, Parking Assignment, and Multi-agent Simulation (Xu Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Shen, Yongkeun Choi, Alex Wong, Francesco Borrelli, Scott Moura, Soomin Woo. (2024)<br><strong>Parking of Connected Automated Vehicles: Vehicle Control, Parking Assignment, and Multi-agent Simulation</strong><br><button class=copy-to-clipboard title="Parking of Connected Automated Vehicles: Vehicle Control, Parking Assignment, and Multi-agent Simulation" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14183v1.pdf filename=2402.14183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel approach to optimize the parking efficiency for fleets of Connected and Automated Vehicles (CAVs). We present a novel multi-vehicle parking simulator, equipped with hierarchical path planning and collision avoidance capabilities for individual CAVs. The simulator is designed to capture the key decision-making processes in parking, from low-level vehicle control to high-level parking assignment, and it enables the effective assessment of parking strategies for large fleets of ground vehicles. We formulate and compare different strategic parking spot assignments to minimize a collective cost. While the proposed framework is designed to optimize various objective functions, we choose the total parking time for the experiment, as it is closely related to the reduction of vehicles&rsquo; energy consumption and greenhouse gas emissions. We validate the effectiveness of the proposed strategies through empirical evaluation against a dataset of real-world parking lot dynamics, realizing a substantial reduction in parking time by up to 43.8%. This improvement is attributed to the synergistic benefits of driving automation, the utilization of shared infrastructure state data, the exclusion of pedestrian traffic, and the real-time computation of optimal parking spot allocation.</p></p class="citation"></blockquote><h2 id=csni-2>cs.NI (2)</h2><h3 id=12--272299-what-comes-after-optical-bypass-network-a-study-on-optical-computing-enabled-network-dao-thanh-hai-2024>(1/2 | 272/299) What comes after optical-bypass network? A study on optical-computing-enabled network (Dao Thanh Hai, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dao Thanh Hai. (2024)<br><strong>What comes after optical-bypass network? A study on optical-computing-enabled network</strong><br><button class=copy-to-clipboard title="What comes after optical-bypass network? A study on optical-computing-enabled network" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14970v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14970v1.pdf filename=2402.14970v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A new architectural paradigm, named, optical-computing-enabled network, is proposed as a potential evolution of the currently used optical-bypass framework. The main idea is to leverage the optical computing capabilities performed on transitional lightpaths at intermediate nodes and such proposal reverses the conventional wisdom in optical-bypass network, that is, separating in-transit lightpaths in avoidance of unwanted interference. In optical-computing-enabled network, the optical nodes are therefore upgraded from conventional functions of add-drop and cross-connect to include optical computing / processing capabilities. This is enabled by exploiting the superposition of in-transit lightpaths for computing purposes to achieve greater capacity efficiency. While traditional network design and planning algorithms have been well-developed for optical-bypass framework in which the routing and resource allocation is dedicated to each optical channel (lightpath), more complicated problems arise in optical-computing-enabled architecture as a consequence of intricate interaction between optical channels and hence resulting into the establishment of the so-called integrated / computed lightpaths. This necessitates for a different framework of network design and planning to maximize the impact of optical computing opportunities. In highlighting this critical point, a detailed case study exploiting the optical aggregation operation to re-design the optical core network is investigated in this paper. Numerical results obtained from extensive <b>simulations</b> on the COST239 network are presented to quantify the efficacy of optical-computing-enabled approach versus the conventional optical-bypass-enabled one.</p></p class="citation"></blockquote><h3 id=22--273299-joint-ap-ue-association-and-power-factor-optimization-for-distributed-massive-mimo-mohd-saif-ali-khan-et-al-2024>(2/2 | 273/299) Joint AP-UE Association and Power Factor Optimization for Distributed Massive MIMO (Mohd Saif Ali Khan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohd Saif Ali Khan, Samar Agnihotri, Karthik R. M. (2024)<br><strong>Joint AP-UE Association and Power Factor Optimization for Distributed Massive MIMO</strong><br><button class=copy-to-clipboard title="Joint AP-UE Association and Power Factor Optimization for Distributed Massive MIMO" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-IT, cs-NI, cs.NI, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14693v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14693v2.pdf filename=2402.14693v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The uplink sum-throughput of distributed massive multiple-input-multiple-output (mMIMO) networks depends majorly on Access point (AP)-User Equipment (UE) association and power control. The AP-UE association and power control both are important problems in their own right in distributed mMIMO networks to improve scalability and reduce front-haul load of the network, and to enhance the system performance by mitigating the interference and boosting the desired signals, respectively. Unlike previous studies, which focused primarily on addressing these two problems separately, this work addresses the uplink sum-throughput maximization problem in distributed mMIMO networks by solving the joint AP-UE association and power control problem, while maintaining Quality-of-Service (QoS) requirements for each UE. To improve scalability, we present an l1-penalty function that delicately balances the trade-off between spectral efficiency (SE) and front-haul signaling load. Our proposed methodology leverages fractional programming, Lagrangian dual formation, and penalty functions to provide an elegant and effective iterative solution with guaranteed convergence. Extensive numerical <b>simulations</b> validate the efficacy of the proposed technique for maximizing sum-throughput while considering the joint AP-UE association and power control problem, demonstrating its superiority over approaches that address these problems individually. Furthermore, the results show that the introduced penalty function can help us effectively control the maximum front-haul load.</p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=12--274299-robust-mass-lumping-and-outlier-removal-strategies-in-isogeometric-analysis-yannis-voet-et-al-2024>(1/2 | 274/299) Robust mass lumping and outlier removal strategies in isogeometric analysis (Yannis Voet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yannis Voet, Espen Sande, Annalisa Buffa. (2024)<br><strong>Robust mass lumping and outlier removal strategies in isogeometric analysis</strong><br><button class=copy-to-clipboard title="Robust mass lumping and outlier removal strategies in isogeometric analysis" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65M60, 65F15, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14956v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14956v1.pdf filename=2402.14956v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mass lumping techniques are commonly employed in explicit time integration schemes for problems in structural dynamics and both avoid solving costly linear systems with the consistent mass matrix and increase the critical time step. In isogeometric analysis, the critical time step is constrained by so-called &ldquo;outlier&rdquo; frequencies, representing the inaccurate high frequency part of the spectrum. Removing or dampening these high frequencies is paramount for fast explicit solution techniques. In this work, we propose robust mass lumping and outlier removal techniques for nontrivial geometries, including multipatch and trimmed geometries. Our lumping strategies provably do not deteriorate (and often improve) the CFL condition of the original problem and are combined with deflation techniques to remove persistent outlier frequencies. Numerical experiments reveal the advantages of the method, especially for <b>simulations</b> covering large time spans where they may halve the number of iterations with little or no effect on the numerical solution.</p></p class="citation"></blockquote><h3 id=22--275299-on-schrödingerization-based-quantum-algorithms-for-linear-dynamical-systems-with-inhomogeneous-terms-shi-jin-et-al-2024>(2/2 | 275/299) On Schrödingerization based quantum algorithms for linear dynamical systems with inhomogeneous terms (Shi Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shi Jin, Nana Liu, Chuwen Ma. (2024)<br><strong>On Schrödingerization based quantum algorithms for linear dynamical systems with inhomogeneous terms</strong><br><button class=copy-to-clipboard title="On Schrödingerization based quantum algorithms for linear dynamical systems with inhomogeneous terms" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14696v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14696v2.pdf filename=2402.14696v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We analyze the Schr"odingerisation method for quantum <b>simulation</b> of a general class of non-unitary dynamics with inhomogeneous source terms. The Schr"odingerisation technique, introduced in \cite{JLY22a,JLY23}, transforms any linear ordinary and partial differential equations with non-unitary dynamics into a system under unitary dynamics via a warped phase transition that maps the equations into a higher dimension, making them suitable for quantum <b>simulation.</b> This technique can also be applied to these equations with inhomogeneous terms modeling source or forcing terms or boundary and interface conditions, and discrete dynamical systems such as iterative methods in numerical linear algebra, through extra equations in the system. Difficulty airses with the presense of inhomogeneous terms since it can change the stability of the original system. In this paper, we systematically study&ndash;both theoretically and numerically&ndash;the important issue of recovering the original variables from the Schr"odingerized equations, even when the evolution operator contains unstable modes. We show that even with unstable modes, one can still construct a stable scheme, yet to recover the original variable one needs to use suitable data in the extended space. We analyze and compare both the discrete and continuous Fourier transforms used in the extended dimension, and derive corresponding error estimates, which allows one to use the more appropriate transform for specific equations. We also provide a smoother initialization for the Schrod"odingerized system to gain higher order accuracy in the extended space. We homogenize the inhomogeneous terms with a stretch transformation, making it easier to recover the original variable. Our recovering technique also provides a simple and generic framework to solve general ill-posed problems in a computationally stable way.</p></p class="citation"></blockquote><h2 id=csdc-2>cs.DC (2)</h2><h3 id=12--276299-toward-scalable-docker-based-emulations-of-blockchain-networks-for-research-and-development-diego-pennino-et-al-2024>(1/2 | 276/299) Toward Scalable Docker-Based Emulations of Blockchain Networks for Research and Development (Diego Pennino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Diego Pennino, Maurizio Pizzonia. (2024)<br><strong>Toward Scalable Docker-Based Emulations of Blockchain Networks for Research and Development</strong><br><button class=copy-to-clipboard title="Toward Scalable Docker-Based Emulations of Blockchain Networks for Research and Development" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-PF, cs.DC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14610v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14610v1.pdf filename=2402.14610v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Blockchain, like any other complex technology, needs a strong testing methodology to support its evolution in both research and development contexts. Setting up meaningful tests for permissionless blockchain technology is a notoriously complex task for several reasons: software is complex, large number of nodes are involved, network is non ideal, etc. Developers usually adopt small virtual laboratories or costly real devnets, based on real software. Researchers usually prefer <b>simulations</b> of a large number of nodes, based on simplified models. In this paper, we aim to obtain the advantages of both approaches, i.e., performing large, realistic, inexpensive, and flexible experiments, using real blockchain software within a virtual environment. To do that, we tackle the challenge of running large blockchain networks in a single physical machine, leveraging Linux and Docker. We analyze a number of problems that arise when large blockchain networks are emulated and we provide technical solutions for all of them. Finally, we describe two experiences of emulating fairly large blockchain networks on a single machine, adopting both research oriented and production oriented software, and involving up to more than 3000 containers.</p></p class="citation"></blockquote><h3 id=22--277299-towards-singular-optimality-in-the-presence-of-local-initial-knowledge-hongyan-ji-et-al-2024>(2/2 | 277/299) Towards singular optimality in the presence of local initial knowledge (Hongyan Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyan Ji, Sriram V. Pemmaraju. (2024)<br><strong>Towards singular optimality in the presence of local initial knowledge</strong><br><button class=copy-to-clipboard title="Towards singular optimality in the presence of local initial knowledge" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14221v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14221v2.pdf filename=2402.14221v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Knowledge Till rho CONGEST model is a variant of the classical CONGEST model of distributed computing in which each vertex v has initial knowledge of the radius-rho ball centered at v. The most commonly studied variants of the CONGEST model are KT0 CONGEST in which nodes initially know nothing about their neighbors and KT1 CONGEST in which nodes initially know the IDs of all their neighbors. It has been shown that having access to neighbors&rsquo; IDs (as in the KT1 CONGEST model) can substantially reduce the message complexity of algorithms for fundamental problems such as BROADCAST and MST. For example, King, Kutten, and Thorup (PODC 2015) show how to construct an MST using just Otilde(n) messages in the KT1 CONGEST model, whereas there is an Omega(m) message lower bound for MST in the KT0 CONGEST model. Building on this result, Gmyr and Pandurangen (DISC 2018) present a family of distributed randomized algorithms for various global problems that exhibit a trade-off between message and round complexity. These algorithms are based on constructing a sparse, spanning subgraph called a danner. Specifically, given a <b>graph</b> G and any delta in [0,1], their algorithm constructs (with high probability) a danner that has diameter Otilde(D + n^{1-delta}) and Otilde(min{m,n^{1+delta}}) edges in Otilde(n^{1-delta}) rounds while using Otilde(min{m,n^{1+\delta}}) messages, where n, m, and D are the number of nodes, edges, and the diameter of G, respectively. In the main result of this paper, we show that if we assume the KT2 CONGEST model, it is possible to substantially improve the time-message trade-off in constructing a danner. Specifically, we show in the KT2 CONGEST model, how to construct a danner that has diameter Otilde(D + n^{1-2delta}) and Otilde(min{m,n^{1+delta}}) edges in Otilde(n^{1-2delta}) rounds while using Otilde(min{m,n^{1+\delta}}) messages for any delta in [0,1/2].</p></p class="citation"></blockquote><h2 id=mathds-1>math.DS (1)</h2><h3 id=11--278299-shifts-on-the-lamplighter-group-laurent-bartholdi-et-al-2024>(1/1 | 278/299) Shifts on the lamplighter group (Laurent Bartholdi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Laurent Bartholdi, Ville Salo. (2024)<br><strong>Shifts on the lamplighter group</strong><br><button class=copy-to-clipboard title="Shifts on the lamplighter group" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.DS<br>Categories: cs-LO, math-DS, math-GR, math.DS<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14508v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14508v1.pdf filename=2402.14508v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We prove that the lamplighter group admits strongly aperiodic SFTs, has undecidable tiling problem, and the entropies of its SFTs are exactly the upper semicomputable nonnegative real numbers, and some other results. These results follow from two relatively general <b>simulation</b> theorems, which show that for a large class of effective subshifts on the sea-level subgroup, their induction to the lamplighter group is sofic; and the pullback of every effective Cantor system on the integers admits an SFT cover. We exhibit a concrete strongly aperiodic set with $1488$ tetrahedra. We show that metabelian Baumslag-Solitar groups are intersimulable with lamplighter groups, and thus we obtain the same characterization for their entropies.</p></p class="citation"></blockquote><h2 id=cssi-1>cs.SI (1)</h2><h3 id=11--279299-human-machine-social-systems-milena-tsvetkova-et-al-2024>(1/1 | 279/299) Human-machine social systems (Milena Tsvetkova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Milena Tsvetkova, Taha Yasseri, Niccolo Pescetelli, Tobias Werner. (2024)<br><strong>Human-machine social systems</strong><br><button class=copy-to-clipboard title="Human-machine social systems" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: A-1; C-2-4; H-1-2; J-4; K-4-0; K-6-0, cs-CY, cs-HC, cs-SI, cs.SI, physics-soc-ph<br>Keyword Score: 20<br>Keywords: Generative AI, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14410v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14410v1.pdf filename=2402.14410v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>From fake accounts on social media and <b>generative-AI</b> <b>bots</b> such as <b>ChatGPT</b> to high-frequency trading algorithms on financial markets and self-driving vehicles on the streets, robots, bots, and algorithms are proliferating and permeating our communication channels, social interactions, economic transactions, and transportation arteries. Networks of multiple interdependent and interacting humans and autonomous machines constitute complex adaptive social systems where the collective outcomes cannot be simply deduced from either human or machine behavior alone. Under this paradigm, we review recent experimental, theoretical, and observational research from across a range of disciplines - robotics, human-computer interaction, web science, complexity science, computational social science, finance, economics, political science, social psychology, and sociology. We identify general dynamics and patterns in situations of competition, coordination, cooperation, contagion, and collective decision-making, and contextualize them in four prominent existing human-machine communities: high-frequency trading markets, the social media platform formerly known as Twitter, the open-collaboration encyclopedia Wikipedia, and the news aggregation and discussion community Reddit. We conclude with suggestions for the research, design, and governance of human-machine social systems, which are necessary to reduce misinformation, prevent financial crashes, improve road safety, overcome labor market disruptions, and enable a better human future.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--280299-open-meshed-anatomy-towards-a-comprehensive-finite-element-hexahedral-mesh-derived-from-open-atlases-andy-trung-huynh-et-al-2024>(1/1 | 280/299) Open Meshed Anatomy: Towards a comprehensive finite element hexahedral mesh derived from open atlases (Andy Trung Huynh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andy Trung Huynh, Benjamin Zwick, Michael Halle, Adam Wittek, Karol Miller. (2024)<br><strong>Open Meshed Anatomy: Towards a comprehensive finite element hexahedral mesh derived from open atlases</strong><br><button class=copy-to-clipboard title="Open Meshed Anatomy: Towards a comprehensive finite element hexahedral mesh derived from open atlases" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14303v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14303v1.pdf filename=2402.14303v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computational <b>simulations</b> using methods such as the finite element (FE) method rely on high-quality meshes for achieving accurate results. This study introduces a method for creating a high-quality hexahedral mesh using the Open Anatomy Project&rsquo;s brain atlas. Our atlas-based FE hexahedral mesh of the brain mitigates potential inaccuracies and uncertainties due to segmentation - a process that often requires input of an inexperienced analyst. It accomplishes this by leveraging existing segmentation from the atlas. We further extend the mesh&rsquo;s usability by forming a two-way correspondence between the atlas and mesh. This feature facilitates property assignment for computational <b>simulations</b> and enhances result analysis within an anatomical context. We demonstrate the application of the mesh by solving the electroencephalography (EEG) forward problem. Our method simplifies the mesh creation process, reducing time and effort, and provides a more comprehensive and contextually enriched visualisation of <b>simulation</b> outcomes.</p></p class="citation"></blockquote><h2 id=eessas-2>eess.AS (2)</h2><h3 id=12--281299-sicrn-advancing-speech-enhancement-through-state-space-model-and-inplace-convolution-techniques-changjiang-zhao-et-al-2024>(1/2 | 281/299) SICRN: Advancing Speech Enhancement through State Space Model and Inplace Convolution Techniques (Changjiang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changjiang Zhao, Shulin He, Xueliang Zhang. (2024)<br><strong>SICRN: Advancing Speech Enhancement through State Space Model and Inplace Convolution Techniques</strong><br><button class=copy-to-clipboard title="SICRN: Advancing Speech Enhancement through State Space Model and Inplace Convolution Techniques" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 20<br>Keywords: Convolution, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14225v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14225v1.pdf filename=2402.14225v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Speech enhancement aims to improve speech quality and intelligibility, especially in noisy environments where background noise degrades speech signals. Currently, deep learning methods achieve great success in speech enhancement, e.g. the representative <b>convolutional</b> <b>recurrent</b> <b>neural</b> <b>network</b> (CRN) and its variants. However, CRN typically employs consecutive downsampling and upsampling <b>convolution</b> for frequency modeling, which destroys the inherent structure of the signal over frequency. Additionally, <b>convolutional</b> layers lacks of temporal modelling abilities. To address these issues, we propose an innovative module combing a State space model and Inplace <b>Convolution</b> (SIC), and to replace the conventional <b>convolution</b> in CRN, called SICRN. Specifically, a dual-path multidimensional State space model captures the global frequencies dependency and long-term temporal dependencies. Meanwhile, the 2D-inplace <b>convolution</b> is used to capture the local structure, which abandons the downsampling and upsampling. Systematic evaluations on the public INTERSPEECH 2020 DNS challenge dataset demonstrate SICRN&rsquo;s efficacy. Compared to strong baselines, SICRN achieves performance close to state-of-the-art while having advantages in model parameters, computations, and algorithmic delay. The proposed SICRN shows great promise for improved speech enhancement.</p></p class="citation"></blockquote><h3 id=22--282299-periodgrad-towards-pitch-controllable-neural-vocoder-based-on-a-diffusion-probabilistic-model-yukiya-hono-et-al-2024>(2/2 | 282/299) PeriodGrad: Towards Pitch-Controllable Neural Vocoder Based on a Diffusion Probabilistic Model (Yukiya Hono et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yukiya Hono, Kei Hashimoto, Yoshihiko Nankaku, Keiichi Tokuda. (2024)<br><strong>PeriodGrad: Towards Pitch-Controllable Neural Vocoder Based on a Diffusion Probabilistic Model</strong><br><button class=copy-to-clipboard title="PeriodGrad: Towards Pitch-Controllable Neural Vocoder Based on a Diffusion Probabilistic Model" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-LG, cs-SD, eess-AS, eess-SP, eess.AS<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14692v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14692v1.pdf filename=2402.14692v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a neural vocoder based on a denoising diffusion <b>probabilistic</b> <b>model</b> (DDPM) incorporating explicit periodic signals as auxiliary conditioning signals. Recently, DDPM-based neural vocoders have gained prominence as non-autoregressive models that can generate high-quality waveforms. The neural vocoders based on DDPM have the advantage of training with a simple time-domain loss. In practical applications, such as singing voice synthesis, there is a demand for neural vocoders to generate high-fidelity speech waveforms with flexible pitch control. However, conventional DDPM-based neural vocoders struggle to generate speech waveforms under such conditions. Our proposed model aims to accurately capture the periodic structure of speech waveforms by incorporating explicit periodic signals. Experimental results show that our model improves sound quality and provides better pitch control than conventional DDPM-based neural vocoders.</p></p class="citation"></blockquote><h2 id=q-bionc-1>q-bio.NC (1)</h2><h3 id=11--283299-contrastive-learning-of-shared-spatiotemporal-eeg-representations-across-individuals-for-naturalistic-neuroscience-xinke-shen-et-al-2024>(1/1 | 283/299) Contrastive Learning of Shared Spatiotemporal EEG Representations Across Individuals for Naturalistic Neuroscience (Xinke Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinke Shen, Lingyi Tao, Xuyang Chen, Sen Song, Quanying Liu, Dan Zhang. (2024)<br><strong>Contrastive Learning of Shared Spatiotemporal EEG Representations Across Individuals for Naturalistic Neuroscience</strong><br><button class=copy-to-clipboard title="Contrastive Learning of Shared Spatiotemporal EEG Representations Across Individuals for Naturalistic Neuroscience" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.NC<br>Categories: cs-LG, eess-SP, q-bio-NC, q-bio.NC<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14213v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14213v1.pdf filename=2402.14213v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural representations induced by naturalistic stimuli offer insights into how humans respond to peripheral stimuli in daily life. The key to understanding the general neural mechanisms underlying naturalistic stimuli processing involves aligning neural activities across individuals and extracting inter-subject shared neural representations. Targeting the Electroencephalogram (EEG) technique, known for its rich spatial and temporal information, this study presents a general framework for <b>Contrastive</b> <b>Learning</b> of Shared SpatioTemporal EEG Representations across individuals (CL-SSTER). Harnessing the representational capabilities of <b>contrastive</b> <b>learning,</b> CL-SSTER utilizes a neural network to maximize the similarity of EEG representations across individuals for identical stimuli, contrasting with those for varied stimuli. The network employed spatial and temporal <b>convolutions</b> to simultaneously learn the spatial and temporal patterns inherent in EEG. The versatility of CL-SSTER was demonstrated on three EEG datasets, including a synthetic dataset, a speech audio EEG dataset, and an emotional video EEG dataset. CL-SSTER attained the highest inter-subject correlation (ISC) values compared to the state-of-the-art ISC methods. The latent representations generated by CL-SSTER exhibited reliable spatiotemporal EEG patterns, which can be explained by specific aspects of the stimuli. CL-SSTER serves as an interpretable and scalable foundational framework for the identification of inter-subject shared neural representations in the realm of naturalistic neuroscience.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--284299-compression-robust-synthetic-speech-detection-using-patched-spectrogram-transformer-amit-kumar-singh-yadav-et-al-2024>(1/2 | 284/299) Compression Robust Synthetic Speech Detection Using Patched Spectrogram Transformer (Amit Kumar Singh Yadav et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amit Kumar Singh Yadav, Ziyue Xiang, Kratika Bhagtani, Paolo Bestagini, Stefano Tubaro, Edward J. Delp. (2024)<br><strong>Compression Robust Synthetic Speech Detection Using Patched Spectrogram Transformer</strong><br><button class=copy-to-clipboard title="Compression Robust Synthetic Speech Detection Using Patched Spectrogram Transformer" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CV, cs-LG, cs-SD, cs.SD, eess-AS, eess-SP<br>Keyword Score: 20<br>Keywords: Out-of-distribution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14205v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14205v1.pdf filename=2402.14205v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many deep learning synthetic speech generation tools are readily available. The use of synthetic speech has caused financial fraud, impersonation of people, and misinformation to spread. For this reason forensic methods that can detect synthetic speech have been proposed. Existing methods often overfit on one dataset and their performance reduces substantially in practical scenarios such as detecting synthetic speech shared on social platforms. In this paper we propose, Patched Spectrogram Synthetic Speech Detection <b>Transformer</b> (PS3DT), a synthetic speech detector that converts a time domain speech signal to a mel-spectrogram and processes it in patches using a <b>transformer</b> neural network. We evaluate the detection performance of PS3DT on ASVspoof2019 dataset. Our experiments show that PS3DT performs well on ASVspoof2019 dataset compared to other approaches using spectrogram for synthetic speech detection. We also investigate generalization performance of PS3DT on In-the-Wild dataset. PS3DT generalizes well than several existing methods on detecting synthetic speech from an <b>out-of-distribution</b> dataset. We also evaluate robustness of PS3DT to detect telephone quality synthetic speech and synthetic speech shared on social platforms (compressed speech). PS3DT is robust to compression and can detect telephone quality synthetic speech better than several existing methods.</p></p class="citation"></blockquote><h3 id=22--285299-symbolic-music-generation-with-non-differentiable-rule-guided-diffusion-yujia-huang-et-al-2024>(2/2 | 285/299) Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion (Yujia Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yujia Huang, Adishree Ghatare, Yuanzhe Liu, Ziniu Hu, Qinsheng Zhang, Chandramouli S Sastry, Siddharth Gururani, Sageev Oore, Yisong Yue. (2024)<br><strong>Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion</strong><br><button class=copy-to-clipboard title="Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14285v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14285v2.pdf filename=2402.14285v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of symbolic music generation (e.g., generating piano rolls), with a technical focus on non-differentiable rule guidance. Musical rules are often expressed in symbolic form on note characteristics, such as note density or chord progression, many of which are non-differentiable which pose a challenge when using them for guided <b>diffusion.</b> <b>We</b> propose Stochastic Control Guidance (SCG), a novel guidance method that only requires forward evaluation of rule functions that can work with pre-trained <b>diffusion</b> <b>models</b> in a plug-and-play way, thus achieving training-free guidance for non-differentiable rules for the first time. Additionally, we introduce a latent <b>diffusion</b> <b>architecture</b> for symbolic music generation with high time resolution, which can be composed with SCG in a plug-and-play fashion. Compared to standard strong baselines in symbolic music generation, this framework demonstrates marked advancements in music quality and rule-based controllability, outperforming current state-of-the-art generators in a variety of settings. For detailed demonstrations, code and model checkpoints, please visit our project website: <a href=https://scg-rule-guided-music.github.io/>https://scg-rule-guided-music.github.io/</a>.</p></p class="citation"></blockquote><h2 id=cspf-1>cs.PF (1)</h2><h3 id=11--286299-cesasme-and-staticdeps-static-detection-of-memory-carried-dependencies-for-code-analyzers-théophile-bastian-et-al-2024>(1/1 | 286/299) CesASMe and Staticdeps: static detection of memory-carried dependencies for code analyzers (Théophile Bastian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Théophile Bastian, Hugo Pompougnac, Alban Dutilleul, Fabrice Rastello. (2024)<br><strong>CesASMe and Staticdeps: static detection of memory-carried dependencies for code analyzers</strong><br><button class=copy-to-clipboard title="CesASMe and Staticdeps: static detection of memory-carried dependencies for code analyzers" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PF<br>Categories: cs-PF, cs.PF<br>Keyword Score: 13<br>Keywords: Benchmarking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14567v1.pdf filename=2402.14567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A variety of code analyzers, such as IACA, uiCA, llvm-mca or Ithemal, strive to statically predict the throughput of a computation kernel. Each analyzer is based on its own simplified CPU model <b>reasoning</b> at the scale of a basic block. Facing this diversity, evaluating their strengths and weaknesses is important to guide both their usage and their enhancement. We present CesASMe, a fully-tooled solution to evaluate code analyzers on C-level <b>benchmarks</b> composed of a <b>benchmark</b> derivation procedure that feeds an evaluation harness. We conclude that memory-carried data dependencies are a major source of imprecision for these tools. We tackle this issue with staticdeps, a static analyzer extracting memory-carried data dependencies, including across loop iterations, from an assembly basic block. We integrate its output to uiCA, a state-of-the-art code analyzer, to evaluate staticdeps&rsquo; impact on a code analyzer&rsquo;s precision through CesASMe.</p></p class="citation"></blockquote><h2 id=csit-2>cs.IT (2)</h2><h3 id=12--287299-efficient-unbiased-sparsification-leighton-barnes-et-al-2024>(1/2 | 287/299) Efficient Unbiased Sparsification (Leighton Barnes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leighton Barnes, Timothy Chow, Emma Cohen, Keith Frankston, Benjamin Howard, Fred Kochman, Daniel Scheinerman, Jeffrey VanderKam. (2024)<br><strong>Efficient Unbiased Sparsification</strong><br><button class=copy-to-clipboard title="Efficient Unbiased Sparsification" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-LG, cs.IT, math-IT, math-ST, stat-TH<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14925v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14925v1.pdf filename=2402.14925v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An unbiased $m$-sparsification of a vector $p\in \mathbb{R}^n$ is a random vector $Q\in \mathbb{R}^n$ with mean $p$ that has at most $m&lt;n$ nonzero coordinates. Unbiased sparsification compresses the original vector without introducing bias; it arises in various contexts, such as in <b>federated</b> <b>learning</b> and sampling sparse probability distributions. Ideally, unbiased sparsification should also minimize the expected value of a divergence function $\mathsf{Div}(Q,p)$ that measures how far away $Q$ is from the original $p$. If $Q$ is optimal in this sense, then we call it efficient. Our main results describe efficient unbiased sparsifications for divergences that are either permutation-invariant or additively separable. Surprisingly, the characterization for permutation-invariant divergences is robust to the choice of divergence function, in the sense that our class of optimal $Q$ for squared Euclidean distance coincides with our class of optimal $Q$ for Kullback-Leibler divergence, or indeed any of a wide variety of divergences.</p></p class="citation"></blockquote><h3 id=22--288299-efficient-solvers-for-wyner-common-information-with-application-to-multi-modal-clustering-teng-hui-huang-et-al-2024>(2/2 | 288/299) Efficient Solvers for Wyner Common Information with Application to Multi-Modal Clustering (Teng-Hui Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Teng-Hui Huang, Hesham El Gamal. (2024)<br><strong>Efficient Solvers for Wyner Common Information with Application to Multi-Modal Clustering</strong><br><button class=copy-to-clipboard title="Efficient Solvers for Wyner Common Information with Application to Multi-Modal Clustering" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 6<br>Keywords: Clustering, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14266v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14266v1.pdf filename=2402.14266v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose two novel extensions of the Wyner common information optimization problem. Each relaxes one fundamental constraints in Wyner&rsquo;s formulation. The \textit{Variational Wyner Common Information} relaxes the matching constraint to the known distribution while imposing conditional independence to the feasible solution set. We derive a tight surrogate upper bound of the obtained unconstrained Lagrangian via the theory of variational inference, which can be minimized efficiently. Our solver caters to problems where conditional independence holds with significantly reduced computation complexity; On the other hand, the \textit{Bipartite Wyner Common Information} relaxes the conditional independence constraint whereas the matching condition is enforced on the feasible set. By leveraging the difference-of-convex structure of the formulated optimization problem, we show that our solver is resilient to conditional dependent sources. Both solvers are provably convergent (local stationary points), and empirically, they obtain more accurate solutions to Wyner&rsquo;s formulation with substantially less runtime. Moreover, them can be extended to unknown distribution settings by parameterizing the common randomness as a member of the exponential family of distributions. Our approaches apply to <b>multi-modal</b> <b>clustering</b> problems, where multiple modalities of observations come from the same cluster. Empirically, our solvers outperform the state-of-the-art <b>multi-modal</b> <b>clustering</b> algorithms with significantly improved performance.</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--289299-machine-checked-categorical-diagrammatic-reasoning-benoît-guillemet-et-al-2024>(1/1 | 289/299) Machine-Checked Categorical Diagrammatic Reasoning (Benoît Guillemet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benoît Guillemet, Assia Mahboubi, Matthieu Piquerez. (2024)<br><strong>Machine-Checked Categorical Diagrammatic Reasoning</strong><br><button class=copy-to-clipboard title="Machine-Checked Categorical Diagrammatic Reasoning" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: F-4-1; G-4, cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14485v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14485v3.pdf filename=2402.14485v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper describes a formal proof library, developed using the Coq proof assistant, designed to assist users in writing correct diagrammatic proofs, for 1-categories. This library proposes a deep-embedded, domain-specific formal language, which features dedicated proof commands to automate the synthesis, and the verification, of the technical parts often eluded in the literature.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--290299-model-based-reinforcement-learning-control-of-reaction-diffusion-problems-christina-schenk-et-al-2024>(1/1 | 290/299) Model-Based Reinforcement Learning Control of Reaction-Diffusion Problems (Christina Schenk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christina Schenk, Aditya Vasudevan, Maciej Haranczyk, Ignacio Romero. (2024)<br><strong>Model-Based Reinforcement Learning Control of Reaction-Diffusion Problems</strong><br><button class=copy-to-clipboard title="Model-Based Reinforcement Learning Control of Reaction-Diffusion Problems" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, cs-SY, eess-SY, math-DS, math-MP, math-OC, math-ph, math.OC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14446v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14446v1.pdf filename=2402.14446v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mathematical and computational tools have proven to be reliable in decision-making processes. In recent times, in particular, machine learning-based methods are becoming increasingly popular as advanced support tools. When dealing with control problems, <b>reinforcement</b> <b>learning</b> has been applied to decision-making in several applications, most notably in games. The success of these methods in finding solutions to complex problems motivates the exploration of new areas where they can be employed to overcome current difficulties. In this paper, we explore the use of automatic control strategies to initial boundary value problems in thermal and disease transport. Specifically, in this work, we adapt an existing <b>reinforcement</b> <b>learning</b> algorithm using a stochastic policy gradient method and we introduce two novel reward functions to drive the flow of the transported field. The new model-based framework exploits the interactions between a reaction-diffusion model and the modified agent. The results show that certain controls can be implemented successfully in these applications, although model simplifications had to be assumed.</p></p class="citation"></blockquote><h2 id=csds-3>cs.DS (3)</h2><h3 id=13--291299-parallel-approximate-maximum-flows-in-near-linear-work-and-polylogarithmic-depth-arpit-agarwal-et-al-2024>(1/3 | 291/299) Parallel Approximate Maximum Flows in Near-Linear Work and Polylogarithmic Depth (Arpit Agarwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arpit Agarwal, Sanjeev Khanna, Huan Li, Prathamesh Patil, Chen Wang, Nathan White, Peilin Zhong. (2024)<br><strong>Parallel Approximate Maximum Flows in Near-Linear Work and Polylogarithmic Depth</strong><br><button class=copy-to-clipboard title="Parallel Approximate Maximum Flows in Near-Linear Work and Polylogarithmic Depth" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14950v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14950v1.pdf filename=2402.14950v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a parallel algorithm for the $(1-\epsilon)$-approximate maximum flow problem in capacitated, undirected <b>graphs</b> with $n$ vertices and $m$ edges, achieving $O(\epsilon^{-3}\text{polylog} n)$ depth and $O(m \epsilon^{-3} \text{polylog} n)$ work in the PRAM model. Although near-linear time sequential algorithms for this problem have been known for almost a decade, no parallel algorithms that simultaneously achieved polylogarithmic depth and near-linear work were known. At the heart of our result is a polylogarithmic depth, near-linear work recursive algorithm for computing congestion approximators. Our algorithm involves a recursive step to obtain a low-quality congestion approximator followed by a &ldquo;boosting&rdquo; step to improve its quality which prevents a multiplicative blow-up in error. Similar to Peng [SODA'16], our boosting step builds upon the hierarchical decomposition scheme of R"acke, Shah, and T"aubig [SODA'14]. A direct implementation of this approach, however, leads only to an algorithm with $n^{o(1)}$ depth and $m^{1+o(1)}$ work. To get around this, we introduce a new hierarchical decomposition scheme, in which we only need to solve maximum flows on subgraphs obtained by contracting vertices, as opposed to vertex-induced subgraphs used in R"acke, Shah, and T"aubig [SODA'14]. In particular, we are able to directly extract congestion approximators for the subgraphs from a congestion approximator for the entire <b>graph,</b> thereby avoiding additional recursion on those subgraphs. Along the way, we also develop a parallel flow-decomposition algorithm that is crucial to achieving polylogarithmic depth and may be of independent interest.</p></p class="citation"></blockquote><h3 id=23--292299-hitting-meets-packing-how-hard-can-it-be-jacob-focke-et-al-2024>(2/3 | 292/299) Hitting Meets Packing: How Hard Can it Be? (Jacob Focke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacob Focke, Fabian Frei, Shaohua Li, Dániel Marx, Philipp Schepper, Roohani Sharma, Karol Węgrzycki. (2024)<br><strong>Hitting Meets Packing: How Hard Can it Be?</strong><br><button class=copy-to-clipboard title="Hitting Meets Packing: How Hard Can it Be?" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14927v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14927v1.pdf filename=2402.14927v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a general family of problems that form a common generalization of classic hitting (also referred to as covering or transversal) and packing problems. An instance of X-HitPack asks: Can removing k (deletable) vertices of a <b>graph</b> G prevent us from packing $\ell$ vertex-disjoint objects of type X? This problem captures a spectrum of problems with standard hitting and packing on opposite ends. Our main motivating question is whether the combination X-HitPack can be significantly harder than these two base problems. Already for a particular choice of X, this question can be posed for many different complexity notions, leading to a large, so-far unexplored domain in the intersection of the areas of hitting and packing problems. On a high-level, we present two case studies: (1) X being all cycles, and (2) X being all copies of a fixed <b>graph</b> H. In each, we explore the classical complexity, as well as the parameterized complexity with the natural parameters k+l and treewidth. We observe that the combined problem can be drastically harder than the base problems: for cycles or for H being a connected <b>graph</b> with at least 3 vertices, the problem is \Sigma_2^P-complete and requires double-exponential dependence on the treewidth of the <b>graph</b> (assuming the Exponential-Time Hypothesis). In contrast, the combined problem admits qualitatively similar running times as the base problems in some cases, although significant novel ideas are required. For example, for X being all cycles, we establish a 2^poly(k+l)n^O(1) algorithm using an involved branching method. Also, for X being all edges (i.e., H = K_2; this combines Vertex Cover and Maximum Matching) the problem can be solved in time 2^\poly(tw)n^O(1) on <b>graphs</b> of treewidth tw. The key step enabling this running time relies on a combinatorial bound obtained from an algebraic (linear delta-matroid) representation of possible matchings.</p></p class="citation"></blockquote><h3 id=33--293299-parameterized-complexity-of-finding-dissimilar-shortest-paths-ryo-funayama-et-al-2024>(3/3 | 293/299) Parameterized Complexity of Finding Dissimilar Shortest Paths (Ryo Funayama et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryo Funayama, Yasuaki Kobayashi, Takeaki Uno. (2024)<br><strong>Parameterized Complexity of Finding Dissimilar Shortest Paths</strong><br><button class=copy-to-clipboard title="Parameterized Complexity of Finding Dissimilar Shortest Paths" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14376v1.pdf filename=2402.14376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of finding ``dissimilar&rsquo;&rsquo; $k$ shortest paths from $s$ to $t$ in an edge-weighted directed <b>graph</b> $D$, where the dissimilarity is measured by the minimum pairwise Hamming distances between these paths. More formally, given an edge-weighted directed <b>graph</b> $D = (V, A)$, two specified vertices $s, t \in V$, and integers $d, k$, the goal of Dissimilar Shortest Paths is to decide whether $D$ has $k$ shortest paths $P_1, \dots, P_k$ from $s$ to $t$ such that $|A(P_i) \mathbin{\triangle} A(P_j)| \ge d$ for distinct $P_i$ and $P_j$. We design a deterministic algorithm to solve Dissimilar Shortest Paths with running time $2^{O(3^kdk^2)}n^{O(1)}$, that is, Dissimilar Shortest Paths is fixed-parameter tractable parameterized by $k + d$. To complement this positive result, we show that Dissimilar Shortest Paths is W[1]-hard when parameterized by only $k$ and paraNP-hard parameterized by $d$.</p></p class="citation"></blockquote><h2 id=cscc-1>cs.CC (1)</h2><h3 id=11--294299-on-the-communication-complexity-of-finding-a-king-in-a-tournament-nikhil-s-mande-et-al-2024>(1/1 | 294/299) On the communication complexity of finding a king in a tournament (Nikhil S. Mande et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikhil S. Mande, Manaswi Paraashar, Swagato Sanyal, Nitin Saurabh. (2024)<br><strong>On the communication complexity of finding a king in a tournament</strong><br><button class=copy-to-clipboard title="On the communication complexity of finding a king in a tournament" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs.CC, quant-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14751v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14751v1.pdf filename=2402.14751v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A tournament is a complete directed <b>graph.</b> A king in a tournament is a vertex v such that every other vertex is reachable from v via a path of length at most 2. It is well known that every tournament has at least one king, one of which is a maximum out-degree vertex. The tasks of finding a king, a maximum out-degree vertex and a source in a tournament has been relatively well studied in the context of query complexity. We study the communication complexity of these tasks, where the edges are partitioned between two players. The following are our main results for n-vertex tournaments: 1) The deterministic communication complexity of finding whether a source exists is tilde{Theta}(log^2 n). 2) The deterministic and randomized communication complexities of finding a king are Theta(n). The quantum communication complexity is tilde{Theta}(sqrt{n}). 3) The deterministic, randomized and quantum communication complexities of finding a maximum out-degree vertex are Theta(n log n), tilde{Theta}(n) and tilde{Theta}(sqrt{n}), respectively. Our upper bounds hold for all partitions of edges, and the lower bounds for a specific partition of the edges. To show the first bullet above, we show, perhaps surprisingly, that finding a source in a tournament is equivalent to the well-studied Clique vs. Independent Set (CIS) problem on undirected <b>graphs.</b> Our bounds for finding a source then follow from known bounds on the complexity of the CIS problem. In view of this equivalence, we can view the task of finding a king in a tournament to be a natural generalization of CIS. One of our lower bounds uses a fooling-set based argument, and all our other lower bounds follow from carefully-constructed reductions from Set-Disjointness.</p></p class="citation"></blockquote><h2 id=mathco-2>math.CO (2)</h2><h3 id=12--295299-new-scattered-linearized-quadrinomials-valentino-smaldore-et-al-2024>(1/2 | 295/299) New scattered linearized quadrinomials (Valentino Smaldore et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Valentino Smaldore, Corrado Zanella, Ferdinando Zullo. (2024)<br><strong>New scattered linearized quadrinomials</strong><br><button class=copy-to-clipboard title="New scattered linearized quadrinomials" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 11T71 11T06 94B05, cs-IT, math-CO, math-IT, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14742v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14742v2.pdf filename=2402.14742v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Let $1&lt;t&lt;n$ be integers, where $t$ is a divisor of $n$. An R-$q^t$-partially scattered polynomial is a $\mathbb F_q$-linearized polynomial $f$ in $\mathbb F_{q^n}[X]$ that satisfies the condition that for all $x,y\in\mathbb F_{q^n}^<em>$ such that $x/y\in\mathbb F_{q^t}$, if $f(x)/x=f(y)/y$, then $x/y\in\mathbb F_q$; $f$ is called scattered if this implication holds for all $x,y\in\mathbb F_{q^n}^</em>$. Two polynomials in $\mathbb F_{q^n}[X]$ are said to be equivalent if their <b>graphs</b> are in the same orbit under the action of the group $\Gamma L(2,q^n)$. For $n>8$ only three families of scattered polynomials in $\mathbb F_{q^n}[X]$ are known: $(i)$~monomials of pseudoregulus type, $(ii)$~binomials of Lunardon-Polverino type, and $(iii)$~a family of quadrinomials defined in [1,10] and extended in [8,13]. In this paper we prove that the polynomial $\varphi_{m,q^J}=X^{q^{J(t-1)}}+X^{q^{J(2t-1)}}+m(X^{q^J}-X^{q^{J(t+1)}})\in\mathbb F_{q^{2t}}[X]$, $q$ odd, $t\ge3$ is R-$q^t$-partially scattered for every value of $m\in\mathbb F_{q^t}^*$ and $J$ coprime with $2t$. Moreover, for every $t>4$ and $q>5$ there exist values of $m$ for which $\varphi_{m,q}$ is scattered and new with respect to the polynomials mentioned in $(i)$, $(ii)$ and $(iii)$ above. The related linear sets are of $\Gamma L$-class at least two.</p></p class="citation"></blockquote><h3 id=22--296299-the-expansion-of-half-integral-polytopes-jean-cardinal-et-al-2024>(2/2 | 296/299) The expansion of half-integral polytopes (Jean Cardinal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jean Cardinal, Lionel Pournin. (2024)<br><strong>The expansion of half-integral polytopes</strong><br><button class=copy-to-clipboard title="The expansion of half-integral polytopes" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-CG, cs-DM, math-CO, math-MG, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14343v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14343v1.pdf filename=2402.14343v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The expansion of a polytope is an important parameter for the analysis of the random walks on its <b>graph.</b> A conjecture of Mihai and Vazirani states that all $0/1$-polytopes have expansion at least 1. We show that the generalization to half-integral polytopes does not hold by constructing $d$-dimensional half-integral polytopes whose expansion decreases exponentially fast with $d$. We also prove that the expansion of half-integral zonotopes is uniformly bounded away from $0$. As an intermediate result, we show that half-integral zonotopes are always graphical.</p></p class="citation"></blockquote><h2 id=cscg-2>cs.CG (2)</h2><h3 id=12--297299-embeddings-and-near-neighbor-searching-with-constant-additive-error-for-hyperbolic-spaces-eunku-park-et-al-2024>(1/2 | 297/299) Embeddings and near-neighbor searching with constant additive error for hyperbolic spaces (Eunku Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eunku Park, Antoine Vigneron. (2024)<br><strong>Embeddings and near-neighbor searching with constant additive error for hyperbolic spaces</strong><br><button class=copy-to-clipboard title="Embeddings and near-neighbor searching with constant additive error for hyperbolic spaces" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs.CG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14604v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14604v1.pdf filename=2402.14604v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We give an embedding of the Poincar'e halfspace $H^D$ into a discrete metric space based on a binary tiling of $H^D$, with additive distortion $O(\log D)$. It yields the following results. We show that any subset $P$ of $n$ points in $H^D$ can be embedded into a <b>graph-metric</b> with $2^{O(D)}n$ vertices and edges, and with additive distortion $O(\log D)$. We also show how to construct, for any $k$, an $O(k\log D)$-purely additive spanner of $P$ with $2^{O(D)}n$ Steiner vertices and $2^{O(D)}n \cdot \lambda_k(n)$ edges, where $\lambda_k(n)$ is the $k$th-row inverse Ackermann function. Finally, we present a data structure for approximate near-neighbor searching in $H^D$, with construction time $2^{O(D)}n\log n$, query time $2^{O(D)}\log n$ and additive error $O(\log D)$. These constructions can be done in $2^{O(D)}n \log n$ time.</p></p class="citation"></blockquote><h3 id=22--298299-on-k-plane-insertion-into-plane-drawings-julia-katheder-et-al-2024>(2/2 | 298/299) On $k$-Plane Insertion into Plane Drawings (Julia Katheder et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julia Katheder, Philipp Kindermann, Fabian Klute, Irene Parada, Ignaz Rutter. (2024)<br><strong>On $k$-Plane Insertion into Plane Drawings</strong><br><button class=copy-to-clipboard title="On $k$-Plane Insertion into Plane Drawings" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs.CG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14552v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14552v1.pdf filename=2402.14552v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the $k$-Plane Insertion into Plane drawing ($k$-PIP) problem: given a plane drawing of a planar <b>graph</b> $G$ and a set of edges $F$, insert the edges in $F$ into the drawing such that the resulting drawing is $k$-plane. In this paper, we focus on the $1$-PIP scenario. We present a linear-time algorithm for the case that $G$ is a triangulation, while proving NP-completeness for the case that $G$ is biconnected and $F$ forms a path or a matching.</p></p class="citation"></blockquote><h2 id=q-biobm-1>q-bio.BM (1)</h2><h3 id=11--299299-structure-based-drug-design-via-3d-molecular-generative-pre-training-and-sampling-yuwei-yang-et-al-2024>(1/1 | 299/299) Structure-Based Drug Design via 3D Molecular Generative Pre-training and Sampling (Yuwei Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuwei Yang, Siqi Ouyang, Xueyu Hu, Meihua Dang, Mingyue Zheng, Hao Zhou, Lei Li. (2024)<br><strong>Structure-Based Drug Design via 3D Molecular Generative Pre-training and Sampling</strong><br><button class=copy-to-clipboard title="Structure-Based Drug Design via 3D Molecular Generative Pre-training and Sampling" index=299>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-299 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14315v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14315v1.pdf filename=2402.14315v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Structure-based drug design aims at generating high affinity ligands with prior knowledge of 3D target structures. Existing methods either use conditional generative model to learn the distribution of 3D ligands given target binding sites, or iteratively modify molecules to optimize a structure-based activity estimator. The former is highly constrained by data quantity and quality, which leaves optimization-based approaches more promising in practical scenario. However, existing optimization-based approaches choose to edit molecules in 2D space, and use molecular docking to estimate the activity using docking predicted 3D target-ligand complexes. The misalignment between the action space and the objective hinders the performance of these models, especially for those employ deep learning for acceleration. In this work, we propose MolEdit3D to combine 3D molecular generation with optimization frameworks. We develop a novel 3D <b>graph</b> editing model to generate molecules using fragments, and pre-train this model on abundant 3D ligands for learning target-independent properties. Then we employ a target-guided self-learning strategy to improve target-related properties using self-sampled molecules. MolEdit3D achieves state-of-the-art performance on majority of the evaluation metrics, and demonstrate strong capability of capturing both target-dependent and -independent properties.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.23</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.25</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#csro-15>cs.RO (15)</a><ul><li><a href=#115--1299-roboscript-code-generation-for-free-form-manipulation-tasks-across-real-and-simulation-junting-chen-et-al-2024>(1/15 | 1/299) RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation (Junting Chen et al., 2024)</a></li><li><a href=#215--2299-reinforcement-learning-with-elastic-time-steps-dong-wang-et-al-2024>(2/15 | 2/299) Reinforcement Learning with Elastic Time Steps (Dong Wang et al., 2024)</a></li><li><a href=#315--3299-path-planning-based-on-2d-object-bounding-box-yanliang-huang-et-al-2024>(3/15 | 3/299) Path Planning based on 2D Object Bounding-box (Yanliang Huang et al., 2024)</a></li><li><a href=#415--4299-practice-makes-perfect-planning-to-learn-skill-parameter-policies-nishanth-kumar-et-al-2024>(4/15 | 4/299) Practice Makes Perfect: Planning to Learn Skill Parameter Policies (Nishanth Kumar et al., 2024)</a></li><li><a href=#515--5299-radarmoseve-a-spatial-temporal-transformer-network-for-radar-only-moving-object-segmentation-and-ego-velocity-estimation-changsong-pang-et-al-2024>(5/15 | 5/299) RadarMOSEVE: A Spatial-Temporal Transformer Network for Radar-Only Moving Object Segmentation and Ego-Velocity Estimation (Changsong Pang et al., 2024)</a></li><li><a href=#615--6299-we-choose-to-go-to-space-agent-driven-human-and-multi-robot-collaboration-in-microgravity-miao-xin-et-al-2024>(6/15 | 6/299) We Choose to Go to Space: Agent-driven Human and Multi-Robot Collaboration in Microgravity (Miao Xin et al., 2024)</a></li><li><a href=#715--7299-towards-diverse-behaviors-a-benchmark-for-imitation-learning-with-human-demonstrations-xiaogang-jia-et-al-2024>(7/15 | 7/299) Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations (Xiaogang Jia et al., 2024)</a></li><li><a href=#815--8299-enhancing-robotic-manipulation-with-ai-feedback-from-multimodal-large-language-models-jinyi-liu-et-al-2024>(8/15 | 8/299) Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models (Jinyi Liu et al., 2024)</a></li><li><a href=#915--9299-autonomy-oriented-digital-twins-for-real2sim2real-autoware-deployment-chinmay-vilas-samak-et-al-2024>(9/15 | 9/299) Autonomy Oriented Digital Twins for Real2Sim2Real Autoware Deployment (Chinmay Vilas Samak et al., 2024)</a></li><li><a href=#1015--10299-a-collision-aware-cable-grasping-method-in-cluttered-environment-lei-zhang-et-al-2024>(10/15 | 10/299) A Collision-Aware Cable Grasping Method in Cluttered Environment (Lei Zhang et al., 2024)</a></li><li><a href=#1115--11299-ground-fusion-a-low-cost-ground-slam-system-robust-to-corner-cases-jie-yin-et-al-2024>(11/15 | 11/299) Ground-Fusion: A Low-cost Ground SLAM System Robust to Corner Cases (Jie Yin et al., 2024)</a></li><li><a href=#1215--12299-vision-language-navigation-with-embodied-intelligence-a-survey-peng-gao-et-al-2024>(12/15 | 12/299) Vision-Language Navigation with Embodied Intelligence: A Survey (Peng Gao et al., 2024)</a></li><li><a href=#1315--13299-cyberdemo-augmenting-simulated-human-demonstration-for-real-world-dexterous-manipulation-jun-wang-et-al-2024>(13/15 | 13/299) CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation (Jun Wang et al., 2024)</a></li><li><a href=#1415--14299-transformable-gaussian-reward-function-for-socially-aware-navigation-with-deep-reinforcement-learning-jinyeob-kim-et-al-2024>(14/15 | 14/299) Transformable Gaussian Reward Function for Socially-Aware Navigation with Deep Reinforcement Learning (Jinyeob Kim et al., 2024)</a></li><li><a href=#1515--15299-transition-state-clustering-for-interaction-segmentation-and-learning-fabian-hahne-et-al-2024>(15/15 | 15/299) Transition State Clustering for Interaction Segmentation and Learning (Fabian Hahne et al., 2024)</a></li></ul></li><li><a href=#cscl-87>cs.CL (87)</a><ul><li><a href=#187--16299-tokenization-counts-the-impact-of-tokenization-on-arithmetic-in-frontier-llms-aaditya-k-singh-et-al-2024>(1/87 | 16/299) Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs (Aaditya K. Singh et al., 2024)</a></li><li><a href=#287--17299-can-large-language-models-detect-misinformation-in-scientific-news-reporting-yupeng-cao-et-al-2024>(2/87 | 17/299) Can Large Language Models Detect Misinformation in Scientific News Reporting? (Yupeng Cao et al., 2024)</a></li><li><a href=#387--18299-unintended-impacts-of-llm-alignment-on-global-representation-michael-j-ryan-et-al-2024>(3/87 | 18/299) Unintended Impacts of LLM Alignment on Global Representation (Michael J. Ryan et al., 2024)</a></li><li><a href=#487--19299-leveraging-large-language-models-for-concept-graph-recovery-and-question-answering-in-nlp-education-rui-yang-et-al-2024>(4/87 | 19/299) Leveraging Large Language Models for Concept Graph Recovery and Question Answering in NLP Education (Rui Yang et al., 2024)</a></li><li><a href=#587--20299-hint-before-solving-prompting-guiding-llms-to-effectively-utilize-encoded-knowledge-jinlan-fu-et-al-2024>(5/87 | 20/299) Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge (Jinlan Fu et al., 2024)</a></li><li><a href=#687--21299-is-chatgpt-the-future-of-causal-text-mining-a-comprehensive-evaluation-and-analysis-takehiro-takayanagi-et-al-2024>(6/87 | 21/299) Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis (Takehiro Takayanagi et al., 2024)</a></li><li><a href=#787--22299-on-the-tip-of-the-tongue-analyzing-conceptual-representation-in-large-language-models-with-reverse-dictionary-probe-ningyu-xu-et-al-2024>(7/87 | 22/299) On the Tip of the Tongue: Analyzing Conceptual Representation in Large Language Models with Reverse-Dictionary Probe (Ningyu Xu et al., 2024)</a></li><li><a href=#887--23299-rethinking-scientific-summarization-evaluation-grounding-explainable-metrics-on-facet-aware-benchmark-xiuying-chen-et-al-2024>(8/87 | 23/299) Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark (Xiuying Chen et al., 2024)</a></li><li><a href=#987--24299-assessing-generalization-capability-of-text-ranking-models-in-polish-sławomir-dadas-et-al-2024>(9/87 | 24/299) Assessing generalization capability of text ranking models in Polish (Sławomir Dadas et al., 2024)</a></li><li><a href=#1087--25299-llm-da-data-augmentation-via-large-language-models-for-few-shot-named-entity-recognition-junjie-ye-et-al-2024>(10/87 | 25/299) LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition (Junjie Ye et al., 2024)</a></li><li><a href=#1187--26299-whose-llm-is-it-anyway-linguistic-comparison-and-llm-attribution-for-gpt-35-gpt-4-and-bard-ariel-rosenfeld-et-al-2024>(11/87 | 26/299) Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard (Ariel Rosenfeld et al., 2024)</a></li><li><a href=#1287--27299-noise-bert-a-unified-perturbation-robust-framework-with-noise-alignment-pre-training-for-noisy-slot-filling-task-jinxu-zhao-et-al-2024>(12/87 | 27/299) Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment Pre-training for Noisy Slot Filling Task (Jinxu Zhao et al., 2024)</a></li><li><a href=#1387--28299-small-language-model-is-a-good-guide-for-large-language-model-in-chinese-entity-relation-extraction-xuemei-tang-et-al-2024>(13/87 | 28/299) Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction (Xuemei Tang et al., 2024)</a></li><li><a href=#1487--29299-rule-or-story-which-is-a-better-commonsense-expression-for-talking-with-large-language-models-ning-bian-et-al-2024>(14/87 | 29/299) Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models? (Ning Bian et al., 2024)</a></li><li><a href=#1587--30299-mitigating-biases-of-large-language-models-in-stance-detection-with-calibration-ang-li-et-al-2024>(15/87 | 30/299) Mitigating Biases of Large Language Models in Stance Detection with Calibration (Ang Li et al., 2024)</a></li><li><a href=#1687--31299-learning-to-reduce-optimal-representations-of-structured-data-in-prompting-large-language-models-younghun-lee-et-al-2024>(16/87 | 31/299) Learning to Reduce: Optimal Representations of Structured Data in Prompting Large Language Models (Younghun Lee et al., 2024)</a></li><li><a href=#1787--32299-conceptmath-a-bilingual-concept-wise-benchmark-for-measuring-mathematical-reasoning-of-large-language-models-yanan-wu-et-al-2024>(17/87 | 32/299) ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models (Yanan Wu et al., 2024)</a></li><li><a href=#1887--33299-enhancing-systematic-decompositional-natural-language-inference-using-informal-logic-nathaniel-weir-et-al-2024>(18/87 | 33/299) Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic (Nathaniel Weir et al., 2024)</a></li><li><a href=#1987--34299-zero-shot-cross-lingual-transfer-in-instruction-tuning-of-large-language-model-nadezhda-chirkova-et-al-2024>(19/87 | 34/299) Zero-shot cross-lingual transfer in instruction tuning of large language model (Nadezhda Chirkova et al., 2024)</a></li><li><a href=#2087--35299-annotation-and-classification-of-relevant-clauses-in-terms-and-conditions-contracts-pietro-giovanni-bizzaro-et-al-2024>(20/87 | 35/299) Annotation and Classification of Relevant Clauses in Terms-and-Conditions Contracts (Pietro Giovanni Bizzaro et al., 2024)</a></li><li><a href=#2187--36299-transferring-bert-capabilities-from-high-resource-to-low-resource-languages-using-vocabulary-matching-piotr-rybak-2024>(21/87 | 36/299) Transferring BERT Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching (Piotr Rybak, 2024)</a></li><li><a href=#2287--37299-instraug-automatic-instruction-augmentation-for-multimodal-instruction-fine-tuning-wei-han-et-al-2024>(22/87 | 37/299) INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction Fine-tuning (Wei Han et al., 2024)</a></li><li><a href=#2387--38299-enhancing-temporal-knowledge-graph-forecasting-with-large-language-models-via-chain-of-history-reasoning-yuwei-xia-et-al-2024>(23/87 | 38/299) Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning (Yuwei Xia et al., 2024)</a></li><li><a href=#2487--39299-gate-x-e--a-challenge-set-for-gender-fair-translations-from-weakly-gendered-languages-spencer-rarrick-et-al-2024>(24/87 | 39/299) GATE X-E : A Challenge Set for Gender-Fair Translations from Weakly-Gendered Languages (Spencer Rarrick et al., 2024)</a></li><li><a href=#2587--40299-iepile-unearthing-large-scale-schema-based-information-extraction-corpus-honghao-gui-et-al-2024>(25/87 | 40/299) IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus (Honghao Gui et al., 2024)</a></li><li><a href=#2687--41299-data-augmentation-is-dead-long-live-data-augmentation-frédéric-piedboeuf-et-al-2024>(26/87 | 41/299) Data Augmentation is Dead, Long Live Data Augmentation (Frédéric Piedboeuf et al., 2024)</a></li><li><a href=#2787--42299-towards-unified-task-embeddings-across-multiple-models-bridging-the-gap-for-prompt-based-large-language-models-and-beyond-xinyu-wang-et-al-2024>(27/87 | 42/299) Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond (Xinyu Wang et al., 2024)</a></li><li><a href=#2887--43299-malaysian-english-news-decoded-a-linguistic-resource-for-named-entity-and-relation-extraction-mohan-raj-chanthran-et-al-2024>(28/87 | 43/299) Malaysian English News Decoded: A Linguistic Resource for Named Entity and Relation Extraction (Mohan Raj Chanthran et al., 2024)</a></li><li><a href=#2987--44299-my-answer-is-c-first-token-probabilities-do-not-match-text-answers-in-instruction-tuned-language-models-xinpeng-wang-et-al-2024>(29/87 | 44/299) &lsquo;My Answer is C&rsquo;: First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models (Xinpeng Wang et al., 2024)</a></li><li><a href=#3087--45299-word-sequence-entropy-towards-uncertainty-estimation-in-free-form-medical-question-answering-applications-and-beyond-zhiyuan-wang-et-al-2024>(30/87 | 45/299) Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond (Zhiyuan Wang et al., 2024)</a></li><li><a href=#3187--46299-eagle-ethical-dataset-given-from-real-interactions-masahiro-kaneko-et-al-2024>(31/87 | 46/299) Eagle: Ethical Dataset Given from Real Interactions (Masahiro Kaneko et al., 2024)</a></li><li><a href=#3287--47299-towards-understanding-counseling-conversations-domain-knowledge-and-large-language-models-younghun-lee-et-al-2024>(32/87 | 47/299) Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models (Younghun Lee et al., 2024)</a></li><li><a href=#3387--48299-palo-a-polyglot-large-multimodal-model-for-5b-people-muhammad-maaz-et-al-2024>(33/87 | 48/299) PALO: A Polyglot Large Multimodal Model for 5B People (Muhammad Maaz et al., 2024)</a></li><li><a href=#3487--49299-tinybenchmarks-evaluating-llms-with-fewer-examples-felipe-maia-polo-et-al-2024>(34/87 | 49/299) tinyBenchmarks: evaluating LLMs with fewer examples (Felipe Maia Polo et al., 2024)</a></li><li><a href=#3587--50299-mt-bench-101-a-fine-grained-benchmark-for-evaluating-large-language-models-in-multi-turn-dialogues-ge-bai-et-al-2024>(35/87 | 50/299) MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues (Ge Bai et al., 2024)</a></li><li><a href=#3687--51299-an-llm-enhanced-adversarial-editing-system-for-lexical-simplification-keren-tan-et-al-2024>(36/87 | 51/299) An LLM-Enhanced Adversarial Editing System for Lexical Simplification (Keren Tan et al., 2024)</a></li><li><a href=#3787--52299-triad-a-framework-leveraging-a-multi-role-llm-based-agent-to-solve-knowledge-base-question-answering-chang-zong-et-al-2024>(37/87 | 52/299) Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering (Chang Zong et al., 2024)</a></li><li><a href=#3887--53299-divide-or-conquer-which-part-should-you-distill-your-llm-zhuofeng-wu-et-al-2024>(38/87 | 53/299) Divide-or-Conquer? Which Part Should You Distill Your LLM? (Zhuofeng Wu et al., 2024)</a></li><li><a href=#3987--54299-fine-tuning-enhances-existing-mechanisms-a-case-study-on-entity-tracking-nikhil-prakash-et-al-2024>(39/87 | 54/299) Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking (Nikhil Prakash et al., 2024)</a></li><li><a href=#4087--55299-identifying-multiple-personalities-in-large-language-models-with-external-evaluation-xiaoyang-song-et-al-2024>(40/87 | 55/299) Identifying Multiple Personalities in Large Language Models with External Evaluation (Xiaoyang Song et al., 2024)</a></li><li><a href=#4187--56299-compass-computational-mapping-of-patient-therapist-alliance-strategies-with-language-modeling-baihan-lin-et-al-2024>(41/87 | 56/299) COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling (Baihan Lin et al., 2024)</a></li><li><a href=#4287--57299-ufo-a-unified-and-flexible-framework-for-evaluating-factuality-of-large-language-models-zhaoheng-huang-et-al-2024>(42/87 | 57/299) UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models (Zhaoheng Huang et al., 2024)</a></li><li><a href=#4387--58299-middleware-for-llms-tools-are-instrumental-for-language-agents-in-complex-environments-yu-gu-et-al-2024>(43/87 | 58/299) Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments (Yu Gu et al., 2024)</a></li><li><a href=#4487--59299-llms-with-industrial-lens-deciphering-the-challenges-and-prospects----a-survey-ashok-urlana-et-al-2024>(44/87 | 59/299) LLMs with Industrial Lens: Deciphering the Challenges and Prospects &ndash; A Survey (Ashok Urlana et al., 2024)</a></li><li><a href=#4587--60299-does-the-generator-mind-its-contexts-an-analysis-of-generative-model-faithfulness-under-context-transfer-xinshuo-hu-et-al-2024>(45/87 | 60/299) Does the Generator Mind its Contexts? An Analysis of Generative Model Faithfulness under Context Transfer (Xinshuo Hu et al., 2024)</a></li><li><a href=#4687--61299-do-llms-implicitly-determine-the-suitable-text-difficulty-for-users-seiji-gobara-et-al-2024>(46/87 | 61/299) Do LLMs Implicitly Determine the Suitable Text Difficulty for Users? (Seiji Gobara et al., 2024)</a></li><li><a href=#4787--62299-kocosa-korean-context-aware-sarcasm-detection-dataset-yumin-kim-et-al-2024>(47/87 | 62/299) KoCoSa: Korean Context-aware Sarcasm Detection Dataset (Yumin Kim et al., 2024)</a></li><li><a href=#4887--63299-qsnail-a-questionnaire-dataset-for-sequential-question-generation-yan-lei-et-al-2024>(48/87 | 63/299) Qsnail: A Questionnaire Dataset for Sequential Question Generation (Yan Lei et al., 2024)</a></li><li><a href=#4987--64299-content-conditional-debiasing-for-fair-text-embedding-wenlong-deng-et-al-2024>(49/87 | 64/299) Content Conditional Debiasing for Fair Text Embedding (Wenlong Deng et al., 2024)</a></li><li><a href=#5087--65299-criticbench-benchmarking-llms-for-critique-correct-reasoning-zicheng-lin-et-al-2024>(50/87 | 65/299) CriticBench: Benchmarking LLMs for Critique-Correct Reasoning (Zicheng Lin et al., 2024)</a></li><li><a href=#5187--66299-2d-matryoshka-sentence-embeddings-xianming-li-et-al-2024>(51/87 | 66/299) 2D Matryoshka Sentence Embeddings (Xianming Li et al., 2024)</a></li><li><a href=#5287--67299-re-examine-distantly-supervised-ner-a-new-benchmark-and-a-simple-approach-yuepei-li-et-al-2024>(52/87 | 67/299) Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach (Yuepei Li et al., 2024)</a></li><li><a href=#5387--68299-instructir-a-benchmark-for-instruction-following-of-information-retrieval-models-hanseok-oh-et-al-2024>(53/87 | 68/299) INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval Models (Hanseok Oh et al., 2024)</a></li><li><a href=#5487--69299-genception-evaluate-multimodal-llms-with-unlabeled-unimodal-data-lele-cao-et-al-2024>(54/87 | 69/299) GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data (Lele Cao et al., 2024)</a></li><li><a href=#5587--70299-commvqa-situating-visual-question-answering-in-communicative-contexts-nandita-shankar-naik-et-al-2024>(55/87 | 70/299) CommVQA: Situating Visual Question Answering in Communicative Contexts (Nandita Shankar Naik et al., 2024)</a></li><li><a href=#5687--71299-multils-a-multi-task-lexical-simplification-framework-kai-north-et-al-2024>(56/87 | 71/299) MultiLS: A Multi-task Lexical Simplification Framework (Kai North et al., 2024)</a></li><li><a href=#5787--72299-mirror-a-multiple-perspective-self-reflection-method-for-knowledge-rich-reasoning-hanqi-yan-et-al-2024>(57/87 | 72/299) Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning (Hanqi Yan et al., 2024)</a></li><li><a href=#5887--73299-relayattention-for-efficient-large-language-model-serving-with-long-system-prompts-lei-zhu-et-al-2024>(58/87 | 73/299) RelayAttention for Efficient Large Language Model Serving with Long System Prompts (Lei Zhu et al., 2024)</a></li><li><a href=#5987--74299-not-all-experts-are-equal-efficient-expert-pruning-and-skipping-for-mixture-of-experts-large-language-models-xudong-lu-et-al-2024>(59/87 | 74/299) Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models (Xudong Lu et al., 2024)</a></li><li><a href=#6087--75299-dependency-annotation-of-ottoman-turkish-with-multilingual-bert-şaziye-betül-özateş-et-al-2024>(60/87 | 75/299) Dependency Annotation of Ottoman Turkish with Multilingual BERT (Şaziye Betül Özateş et al., 2024)</a></li><li><a href=#6187--76299-chain-of-thought-unfaithfulness-as-disguised-accuracy-oliver-bentham-et-al-2024>(61/87 | 76/299) Chain-of-Thought Unfaithfulness as Disguised Accuracy (Oliver Bentham et al., 2024)</a></li><li><a href=#6287--77299-efficient-and-effective-vocabulary-expansion-towards-multilingual-large-language-models-seungduk-kim-et-al-2024>(62/87 | 77/299) Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models (Seungduk Kim et al., 2024)</a></li><li><a href=#6387--78299-two-counterexamples-to-textittokenization-and-the-noiseless-channel-marco-cognetta-et-al-2024>(63/87 | 78/299) Two Counterexamples to \textit{Tokenization and the Noiseless Channel} (Marco Cognetta et al., 2024)</a></li><li><a href=#6487--79299-should-we-respect-llms-a-cross-lingual-study-on-the-influence-of-prompt-politeness-on-llm-performance-ziqi-yin-et-al-2024>(64/87 | 79/299) Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance (Ziqi Yin et al., 2024)</a></li><li><a href=#6587--80299-understanding-and-patching-compositional-reasoning-in-llms-zhaoyi-li-et-al-2024>(65/87 | 80/299) Understanding and Patching Compositional Reasoning in LLMs (Zhaoyi Li et al., 2024)</a></li><li><a href=#6687--81299-cev-lm-controlled-edit-vector-language-model-for-shaping-natural-language-generations-samraj-moorjani-et-al-2024>(66/87 | 81/299) CEV-LM: Controlled Edit Vector Language Model for Shaping Natural Language Generations (Samraj Moorjani et al., 2024)</a></li><li><a href=#6787--82299-can-language-models-act-as-knowledge-bases-at-scale-qiyuan-he-et-al-2024>(67/87 | 82/299) Can Language Models Act as Knowledge Bases at Scale? (Qiyuan He et al., 2024)</a></li><li><a href=#6887--83299-multi-modal-stance-detection-new-datasets-and-model-bin-liang-et-al-2024>(68/87 | 83/299) Multi-modal Stance Detection: New Datasets and Model (Bin Liang et al., 2024)</a></li><li><a href=#6987--84299-is-cognition-and-action-consistent-or-not-investigating-large-language-models-personality-yiming-ai-et-al-2024>(69/87 | 84/299) Is Cognition and Action Consistent or Not: Investigating Large Language Model&rsquo;s Personality (Yiming Ai et al., 2024)</a></li><li><a href=#7087--85299-balanced-data-sampling-for-language-model-training-with-clustering-yunfan-shao-et-al-2024>(70/87 | 85/299) Balanced Data Sampling for Language Model Training with Clustering (Yunfan Shao et al., 2024)</a></li><li><a href=#7187--86299-vygotsky-distance-measure-for-benchmark-task-similarity-maxim-k-surkov-et-al-2024>(71/87 | 86/299) Vygotsky Distance: Measure for Benchmark Task Similarity (Maxim K. Surkov et al., 2024)</a></li><li><a href=#7287--87299-cobias-contextual-reliability-in-bias-assessment-priyanshul-govil-et-al-2024>(72/87 | 87/299) COBIAS: Contextual Reliability in Bias Assessment (Priyanshul Govil et al., 2024)</a></li><li><a href=#7387--88299-a-usage-centric-take-on-intent-understanding-in-e-commerce-wendi-zhou-et-al-2024>(73/87 | 88/299) A Usage-centric Take on Intent Understanding in E-Commerce (Wendi Zhou et al., 2024)</a></li><li><a href=#7487--89299-ar-spider-text-to-sql-in-arabic-saleh-almohaimeed-et-al-2024>(74/87 | 89/299) Ar-Spider: Text-to-SQL in Arabic (Saleh Almohaimeed et al., 2024)</a></li><li><a href=#7587--90299-how-important-is-tokenization-in-french-medical-masked-language-models-yanis-labrak-et-al-2024>(75/87 | 90/299) How Important Is Tokenization in French Medical Masked Language Models? (Yanis Labrak et al., 2024)</a></li><li><a href=#7687--91299-unveiling-linguistic-regions-in-large-language-models-zhihao-zhang-et-al-2024>(76/87 | 91/299) Unveiling Linguistic Regions in Large Language Models (Zhihao Zhang et al., 2024)</a></li><li><a href=#7787--92299-novi-jezički-modeli-za-srpski-jezik-mihailo-škorić-2024>(77/87 | 92/299) Novi jezički modeli za srpski jezik (Mihailo Škorić, 2024)</a></li><li><a href=#7887--93299-aura-natural-language-reasoning-for-aleatoric-uncertainty-in-rationales-hazel-kim-2024>(78/87 | 93/299) AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales (Hazel Kim, 2024)</a></li><li><a href=#7987--94299-mitigating-the-linguistic-gap-with-phonemic-representations-for-robust-multilingual-language-understanding-haeji-jung-et-al-2024>(79/87 | 94/299) Mitigating the Linguistic Gap with Phonemic Representations for Robust Multilingual Language Understanding (Haeji Jung et al., 2024)</a></li><li><a href=#8087--95299-llmbind-a-unified-modality-task-integration-framework-bin-zhu-et-al-2024>(80/87 | 95/299) LLMBind: A Unified Modality-Task Integration Framework (Bin Zhu et al., 2024)</a></li><li><a href=#8187--96299-scaling-efficient-llms-b-n-kausik-2024>(81/87 | 96/299) Scaling Efficient LLMs (B. N. Kausik, 2024)</a></li><li><a href=#8287--97299-inffeed-influence-functions-as-a-feedback-to-improve-the-performance-of-subjective-tasks-somnath-banerjee-et-al-2024>(82/87 | 97/299) InfFeed: Influence Functions as a Feedback to Improve the Performance of Subjective Tasks (Somnath Banerjee et al., 2024)</a></li><li><a href=#8387--98299-domain-generalization-via-causal-adjustment-for-cross-domain-sentiment-analysis-siyin-wang-et-al-2024>(83/87 | 98/299) Domain Generalization via Causal Adjustment for Cross-Domain Sentiment Analysis (Siyin Wang et al., 2024)</a></li><li><a href=#8487--99299-daisy-tts-simulating-wider-spectrum-of-emotions-via-prosody-embedding-decomposition-rendi-chevi-et-al-2024>(84/87 | 99/299) Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding Decomposition (Rendi Chevi et al., 2024)</a></li><li><a href=#8587--100299-nlas-multi-a-multilingual-corpus-of-automatically-generated-natural-language-argumentation-schemes-ramon-ruiz-dolz-et-al-2024>(85/87 | 100/299) NLAS-multi: A Multilingual Corpus of Automatically Generated Natural Language Argumentation Schemes (Ramon Ruiz-Dolz et al., 2024)</a></li><li><a href=#8687--101299-assisting-in-writing-wikipedia-like-articles-from-scratch-with-large-language-models-yijia-shao-et-al-2024>(86/87 | 101/299) Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models (Yijia Shao et al., 2024)</a></li><li><a href=#8787--102299-less-is-more-mitigating-multimodal-hallucination-from-an-eos-decision-perspective-zihao-yue-et-al-2024>(87/87 | 102/299) Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective (Zihao Yue et al., 2024)</a></li></ul></li><li><a href=#cslg-60>cs.LG (60)</a><ul><li><a href=#160--103299-copr-continual-human-preference-learning-via-optimal-policy-regularization-han-zhang-et-al-2024>(1/60 | 103/299) COPR: Continual Human Preference Learning via Optimal Policy Regularization (Han Zhang et al., 2024)</a></li><li><a href=#260--104299-generalizing-reward-modeling-for-out-of-distribution-preference-learning-chen-jia-2024>(2/60 | 104/299) Generalizing Reward Modeling for Out-of-Distribution Preference Learning (Chen Jia, 2024)</a></li><li><a href=#360--105299-efficient-data-selection-employing-semantic-similarity-based-graph-structures-for-model-training-roxana-petcu-et-al-2024>(3/60 | 105/299) Efficient data selection employing Semantic Similarity-based Graph Structures for model training (Roxana Petcu et al., 2024)</a></li><li><a href=#460--106299-link-prediction-under-heterophily-a-physics-inspired-graph-neural-network-approach-andrea-giuseppe-di-francesco-et-al-2024>(4/60 | 106/299) Link Prediction under Heterophily: A Physics-Inspired Graph Neural Network Approach (Andrea Giuseppe Di Francesco et al., 2024)</a></li><li><a href=#560--107299-how-transformers-learn-causal-structure-with-gradient-descent-eshaan-nichani-et-al-2024>(5/60 | 107/299) How Transformers Learn Causal Structure with Gradient Descent (Eshaan Nichani et al., 2024)</a></li><li><a href=#660--108299-practical-insights-into-knowledge-distillation-for-pre-trained-models-norah-alballa-et-al-2024>(6/60 | 108/299) Practical Insights into Knowledge Distillation for Pre-Trained Models (Norah Alballa et al., 2024)</a></li><li><a href=#760--109299-back-to-basics-revisiting-reinforce-style-optimization-for-learning-from-human-feedback-in-llms-arash-ahmadian-et-al-2024>(7/60 | 109/299) Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs (Arash Ahmadian et al., 2024)</a></li><li><a href=#860--110299-q-probe-a-lightweight-approach-to-reward-maximization-for-language-models-kenneth-li-et-al-2024>(8/60 | 110/299) Q-Probe: A Lightweight Approach to Reward Maximization for Language Models (Kenneth Li et al., 2024)</a></li><li><a href=#960--111299-fedcqa-answering-complex-queries-on-multi-source-knowledge-graphs-via-federated-learning-qi-hu-et-al-2024>(9/60 | 111/299) FedCQA: Answering Complex Queries on Multi-Source Knowledge Graphs via Federated Learning (Qi Hu et al., 2024)</a></li><li><a href=#1060--112299-rethinking-invariance-regularization-in-adversarial-training-to-improve-robustness-accuracy-trade-off-futa-waseda-et-al-2024>(10/60 | 112/299) Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off (Futa Waseda et al., 2024)</a></li><li><a href=#1160--113299-data-science-with-llms-and-interpretable-models-sebastian-bordt-et-al-2024>(11/60 | 113/299) Data Science with LLMs and Interpretable Models (Sebastian Bordt et al., 2024)</a></li><li><a href=#1260--114299-large-scale-actionless-video-pre-training-via-discrete-diffusion-for-efficient-policy-learning-haoran-he-et-al-2024>(12/60 | 114/299) Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning (Haoran He et al., 2024)</a></li><li><a href=#1360--115299-generative-adversarial-network-with-soft-dynamic-time-warping-and-parallel-reconstruction-for-energy-time-series-anomaly-detection-hardik-prabhu-et-al-2024>(13/60 | 115/299) Generative Adversarial Network with Soft-Dynamic Time Warping and Parallel Reconstruction for Energy Time Series Anomaly Detection (Hardik Prabhu et al., 2024)</a></li><li><a href=#1460--116299-estimating-unknown-population-sizes-using-the-hypergeometric-distribution-liam-hodgson-et-al-2024>(14/60 | 116/299) Estimating Unknown Population Sizes Using the Hypergeometric Distribution (Liam Hodgson et al., 2024)</a></li><li><a href=#1560--117299-mobilellm-optimizing-sub-billion-parameter-language-models-for-on-device-use-cases-zechun-liu-et-al-2024>(15/60 | 117/299) MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases (Zechun Liu et al., 2024)</a></li><li><a href=#1660--118299-self-guided-masked-autoencoders-for-domain-agnostic-self-supervised-learning-johnathan-xie-et-al-2024>(16/60 | 118/299) Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning (Johnathan Xie et al., 2024)</a></li><li><a href=#1760--119299-cat-gnn-enhancing-credit-card-fraud-detection-via-causal-temporal-graph-neural-networks-yifan-duan-et-al-2024>(17/60 | 119/299) CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks (Yifan Duan et al., 2024)</a></li><li><a href=#1860--120299-robust-training-of-federated-models-with-extremely-label-deficiency-yonggang-zhang-et-al-2024>(18/60 | 120/299) Robust Training of Federated Models with Extremely Label Deficiency (Yonggang Zhang et al., 2024)</a></li><li><a href=#1960--121299-take-the-bull-by-the-horns-hard-sample-reweighted-continual-training-improves-llm-generalization-xuxi-chen-et-al-2024>(19/60 | 121/299) Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization (Xuxi Chen et al., 2024)</a></li><li><a href=#2060--122299-towards-few-shot-adaptation-of-foundation-models-via-multitask-finetuning-zhuoyan-xu-et-al-2024>(20/60 | 122/299) Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning (Zhuoyan Xu et al., 2024)</a></li><li><a href=#2160--123299-prompting-a-pretrained-transformer-can-be-a-universal-approximator-aleksandar-petrov-et-al-2024>(21/60 | 123/299) Prompting a Pretrained Transformer Can Be a Universal Approximator (Aleksandar Petrov et al., 2024)</a></li><li><a href=#2260--124299-clifford-steerable-convolutional-neural-networks-maksim-zhdanov-et-al-2024>(22/60 | 124/299) Clifford-Steerable Convolutional Neural Networks (Maksim Zhdanov et al., 2024)</a></li><li><a href=#2360--125299-opentab-advancing-large-language-models-as-open-domain-table-reasoners-kezhi-kong-et-al-2024>(23/60 | 125/299) OpenTab: Advancing Large Language Models as Open-domain Table Reasoners (Kezhi Kong et al., 2024)</a></li><li><a href=#2460--126299-applying-reinforcement-learning-to-optimize-traffic-light-cycles-seungah-son-et-al-2024>(24/60 | 126/299) Applying Reinforcement Learning to Optimize Traffic Light Cycles (Seungah Son et al., 2024)</a></li><li><a href=#2560--127299-quaternion-recurrent-neural-network-with-real-time-recurrent-learning-and-maximum-correntropy-criterion-pauline-bourigault-et-al-2024>(25/60 | 127/299) Quaternion recurrent neural network with real-time recurrent learning and maximum correntropy criterion (Pauline Bourigault et al., 2024)</a></li><li><a href=#2660--128299-betail-behavior-transformer-adversarial-imitation-learning-from-human-racing-gameplay-catherine-weaver-et-al-2024>(26/60 | 128/299) BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human Racing Gameplay (Catherine Weaver et al., 2024)</a></li><li><a href=#2760--129299-graph-parsing-networks-yunchong-song-et-al-2024>(27/60 | 129/299) Graph Parsing Networks (Yunchong Song et al., 2024)</a></li><li><a href=#2860--130299-securing-transactions-a-hybrid-dependable-ensemble-machine-learning-model-using-iht-lr-and-grid-search-md-alamin-talukder-et-al-2024>(28/60 | 130/299) Securing Transactions: A Hybrid Dependable Ensemble Machine Learning Model using IHT-LR and Grid Search (Md. Alamin Talukder et al., 2024)</a></li><li><a href=#2960--131299-probabilistically-sound-beam-search-with-masked-language-models-charlie-cowen-breen-et-al-2024>(29/60 | 131/299) Probabilistically-sound beam search with masked language models (Charlie Cowen-Breen et al., 2024)</a></li><li><a href=#3060--132299-consistency-guided-temperature-scaling-using-style-and-content-information-for-out-of-domain-calibration-wonjeong-choi-et-al-2024>(30/60 | 132/299) Consistency-Guided Temperature Scaling Using Style and Content Information for Out-of-Domain Calibration (Wonjeong Choi et al., 2024)</a></li><li><a href=#3160--133299-optimizing-language-models-for-human-preferences-is-a-causal-inference-problem-victoria-lin-et-al-2024>(31/60 | 133/299) Optimizing Language Models for Human Preferences is a Causal Inference Problem (Victoria Lin et al., 2024)</a></li><li><a href=#3260--134299-federated-fairness-without-access-to-sensitive-groups-afroditi-papadaki-et-al-2024>(32/60 | 134/299) Federated Fairness without Access to Sensitive Groups (Afroditi Papadaki et al., 2024)</a></li><li><a href=#3360--135299-bandits-with-abstention-under-expert-advice-stephen-pasteris-et-al-2024>(33/60 | 135/299) Bandits with Abstention under Expert Advice (Stephen Pasteris et al., 2024)</a></li><li><a href=#3460--136299-dyngma-a-robust-approach-for-learning-stochastic-differential-equations-from-data-aiqing-zhu-et-al-2024>(34/60 | 136/299) DynGMA: a robust approach for learning stochastic differential equations from data (Aiqing Zhu et al., 2024)</a></li><li><a href=#3560--137299-global-safe-sequential-learning-via-efficient-knowledge-transfer-cen-you-li-et-al-2024>(35/60 | 137/299) Global Safe Sequential Learning via Efficient Knowledge Transfer (Cen-You Li et al., 2024)</a></li><li><a href=#3660--138299-representation-learning-for-frequent-subgraph-mining-rex-ying-et-al-2024>(36/60 | 138/299) Representation Learning for Frequent Subgraph Mining (Rex Ying et al., 2024)</a></li><li><a href=#3760--139299-rao-blackwellising-bayesian-causal-inference-christian-toth-et-al-2024>(37/60 | 139/299) Rao-Blackwellising Bayesian Causal Inference (Christian Toth et al., 2024)</a></li><li><a href=#3860--140299-stable-neural-stochastic-differential-equations-in-analyzing-irregular-time-series-data-yongkyung-oh-et-al-2024>(38/60 | 140/299) Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data (YongKyung Oh et al., 2024)</a></li><li><a href=#3960--141299-boosting-gets-full-attention-for-relational-learning-mathieu-guillame-bert-et-al-2024>(39/60 | 141/299) Boosting gets full Attention for Relational Learning (Mathieu Guillame-Bert et al., 2024)</a></li><li><a href=#4060--142299-imbalanced-data-clustering-using-equilibrium-k-means-yudong-he-2024>(40/60 | 142/299) Imbalanced Data Clustering using Equilibrium K-Means (Yudong He, 2024)</a></li><li><a href=#4160--143299-comparing-graph-transformers-via-positional-encodings-mitchell-black-et-al-2024>(41/60 | 143/299) Comparing Graph Transformers via Positional Encodings (Mitchell Black et al., 2024)</a></li><li><a href=#4260--144299-privacy-enhancing-collaborative-information-sharing-through-federated-learning----a-case-of-the-insurance-industry-panyi-dong-et-al-2024>(42/60 | 144/299) Privacy-Enhancing Collaborative Information Sharing through Federated Learning &ndash; A Case of the Insurance Industry (Panyi Dong et al., 2024)</a></li><li><a href=#4360--145299-enhancing-power-quality-event-classification-with-ai-transformer-models-ahmad-mohammad-saber-et-al-2024>(43/60 | 145/299) Enhancing Power Quality Event Classification with AI Transformer Models (Ahmad Mohammad Saber et al., 2024)</a></li><li><a href=#4460--146299-on-the-curses-of-future-and-history-in-future-dependent-value-functions-for-off-policy-evaluation-yuheng-zhang-et-al-2024>(44/60 | 146/299) On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation (Yuheng Zhang et al., 2024)</a></li><li><a href=#4560--147299-bayesian-off-policy-evaluation-and-learning-for-large-action-spaces-imad-aouali-et-al-2024>(45/60 | 147/299) Bayesian Off-Policy Evaluation and Learning for Large Action Spaces (Imad Aouali et al., 2024)</a></li><li><a href=#4660--148299-federated-learning-on-transcriptomic-data-model-quality-and-performance-trade-offs-anika-hannemann-et-al-2024>(46/60 | 148/299) Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs (Anika Hannemann et al., 2024)</a></li><li><a href=#4760--149299-towards-automated-causal-discovery-a-case-study-on-5g-telecommunication-data-konstantina-biza-et-al-2024>(47/60 | 149/299) Towards Automated Causal Discovery: a case study on 5G telecommunication data (Konstantina Biza et al., 2024)</a></li><li><a href=#4860--150299-text-me-the-data-generating-ground-pressure-sequence-from-textual-descriptions-for-har-lala-shakti-swarup-ray-et-al-2024>(48/60 | 150/299) Text me the data: Generating Ground Pressure Sequence from Textual Descriptions for HAR (Lala Shakti Swarup Ray et al., 2024)</a></li><li><a href=#4960--151299-hyperfast-instant-classification-for-tabular-data-david-bonet-et-al-2024>(49/60 | 151/299) HyperFast: Instant Classification for Tabular Data (David Bonet et al., 2024)</a></li><li><a href=#5060--152299-deep-generative-model-based-synthesis-of-four-bar-linkage-mechanisms-with-target-conditions-sumin-lee-et-al-2024>(50/60 | 152/299) Deep Generative Model-based Synthesis of Four-bar Linkage Mechanisms with Target Conditions (Sumin Lee et al., 2024)</a></li><li><a href=#5160--153299-reconstruction-based-anomaly-localization-via-knowledge-informed-self-training-cheng-qian-et-al-2024>(51/60 | 153/299) Reconstruction-Based Anomaly Localization via Knowledge-Informed Self-Training (Cheng Qian et al., 2024)</a></li><li><a href=#5260--154299-automated-design-and-optimization-of-distributed-filtering-circuits-via-reinforcement-learning-peng-gao-et-al-2024>(52/60 | 154/299) Automated Design and Optimization of Distributed Filtering Circuits via Reinforcement Learning (Peng Gao et al., 2024)</a></li><li><a href=#5360--155299-diversity-aware-ensembling-of-language-models-based-on-topological-data-analysis-polina-proskura-et-al-2024>(53/60 | 155/299) Diversity-Aware Ensembling of Language Models Based on Topological Data Analysis (Polina Proskura et al., 2024)</a></li><li><a href=#5460--156299-tinyllava-a-framework-of-small-scale-large-multimodal-models-baichuan-zhou-et-al-2024>(54/60 | 156/299) TinyLLaVA: A Framework of Small-scale Large Multimodal Models (Baichuan Zhou et al., 2024)</a></li><li><a href=#5560--157299-omnipred-language-models-as-universal-regressors-xingyou-song-et-al-2024>(55/60 | 157/299) OmniPred: Language Models as Universal Regressors (Xingyou Song et al., 2024)</a></li><li><a href=#5660--158299-latrend-a-framework-for-clustering-longitudinal-data-niek-den-teuling-et-al-2024>(56/60 | 158/299) latrend: A Framework for Clustering Longitudinal Data (Niek Den Teuling et al., 2024)</a></li><li><a href=#5760--159299-ace--off-policy-actor-critic-with-causality-aware-entropy-regularization-tianying-ji-et-al-2024>(57/60 | 159/299) ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization (Tianying Ji et al., 2024)</a></li><li><a href=#5860--160299-from-large-to-small-datasets-size-generalization-for-clustering-algorithm-selection-vaggos-chatziafratis-et-al-2024>(58/60 | 160/299) From Large to Small Datasets: Size Generalization for Clustering Algorithm Selection (Vaggos Chatziafratis et al., 2024)</a></li><li><a href=#5960--161299-high-arity-pac-learning-via-exchangeability-leonardo-n-coregliano-et-al-2024>(59/60 | 161/299) High-arity PAC learning via exchangeability (Leonardo N. Coregliano et al., 2024)</a></li><li><a href=#6060--162299-a-hierarchical-decomposition-for-explaining-ml-performance-discrepancies-jean-feng-et-al-2024>(60/60 | 162/299) A hierarchical decomposition for explaining ML performance discrepancies (Jean Feng et al., 2024)</a></li></ul></li><li><a href=#cscr-6>cs.CR (6)</a><ul><li><a href=#16--163299-mitigating-fine-tuning-jailbreak-attack-with-backdoor-enhanced-alignment-jiongxiao-wang-et-al-2024>(1/6 | 163/299) Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment (Jiongxiao Wang et al., 2024)</a></li><li><a href=#26--164299-watermarking-makes-language-models-radioactive-tom-sander-et-al-2024>(2/6 | 164/299) Watermarking Makes Language Models Radioactive (Tom Sander et al., 2024)</a></li><li><a href=#36--165299-mudjacking-patching-backdoor-vulnerabilities-in-foundation-models-hongbin-liu-et-al-2024>(3/6 | 165/299) Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models (Hongbin Liu et al., 2024)</a></li><li><a href=#46--166299-double-i-watermark-protecting-model-copyright-for-llm-fine-tuning-shen-li-et-al-2024>(4/6 | 166/299) Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning (Shen Li et al., 2024)</a></li><li><a href=#56--167299-bionib-blockchain-based-iot-using-novelty-index-in-bridge-health-monitoring-divija-swetha-gadiraju-et-al-2024>(5/6 | 167/299) BIONIB: Blockchain-based IoT using Novelty Index in Bridge Health Monitoring (Divija Swetha Gadiraju et al., 2024)</a></li><li><a href=#66--168299-a-new-hope-contextual-privacy-policies-for-mobile-applications-and-an-approach-toward-automated-generation-shidong-pan-et-al-2024>(6/6 | 168/299) {A New Hope}: Contextual Privacy Policies for Mobile Applications and An Approach Toward Automated Generation (Shidong Pan et al., 2024)</a></li></ul></li><li><a href=#cscv-50>cs.CV (50)</a><ul><li><a href=#150--169299-visual-hallucinations-of-multi-modal-large-language-models-wen-huang-et-al-2024>(1/50 | 169/299) Visual Hallucinations of Multi-modal Large Language Models (Wen Huang et al., 2024)</a></li><li><a href=#250--170299-unsupervised-domain-adaptation-within-deep-foundation-latent-spaces-dmitry-kangin-et-al-2024>(2/50 | 170/299) Unsupervised Domain Adaptation within Deep Foundation Latent Spaces (Dmitry Kangin et al., 2024)</a></li><li><a href=#350--171299-zero-shot-pediatric-tuberculosis-detection-in-chest-x-rays-using-self-supervised-learning-daniel-capellán-martín-et-al-2024>(3/50 | 171/299) Zero-Shot Pediatric Tuberculosis Detection in Chest X-Rays using Self-Supervised Learning (Daniel Capellán-Martín et al., 2024)</a></li><li><a href=#450--172299-clce-an-approach-to-refining-cross-entropy-and-contrastive-learning-for-optimized-learning-fusion-zijun-long-et-al-2024>(4/50 | 172/299) CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for Optimized Learning Fusion (Zijun Long et al., 2024)</a></li><li><a href=#550--173299-uncertainty-aware-evaluation-for-vision-language-models-vasily-kostumov-et-al-2024>(5/50 | 173/299) Uncertainty-Aware Evaluation for Vision-Language Models (Vasily Kostumov et al., 2024)</a></li><li><a href=#650--174299-self-supervised-visualisation-of-medical-image-datasets-ifeoma-veronica-nwabufo-et-al-2024>(6/50 | 174/299) Self-supervised Visualisation of Medical Image Datasets (Ifeoma Veronica Nwabufo et al., 2024)</a></li><li><a href=#750--175299-weaksam-segment-anything-meets-weakly-supervised-instance-level-recognition-lianghui-zhu-et-al-2024>(7/50 | 175/299) WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition (Lianghui Zhu et al., 2024)</a></li><li><a href=#850--176299-subobject-level-image-tokenization-delong-chen-et-al-2024>(8/50 | 176/299) Subobject-level Image Tokenization (Delong Chen et al., 2024)</a></li><li><a href=#950--177299-typographic-text-generation-with-off-the-shelf-diffusion-model-khaytze-peong-et-al-2024>(9/50 | 177/299) Typographic Text Generation with Off-the-Shelf Diffusion Model (KhayTze Peong et al., 2024)</a></li><li><a href=#1050--178299-a-simple-framework-uniting-visual-in-context-learning-with-masked-image-modeling-to-improve-ultrasound-segmentation-yuyue-zhou-et-al-2024>(10/50 | 178/299) A Simple Framework Uniting Visual In-context Learning with Masked Image Modeling to Improve Ultrasound Segmentation (Yuyue Zhou et al., 2024)</a></li><li><a href=#1150--179299-modeling-3d-infant-kinetics-using-adaptive-graph-convolutional-networks-daniel-holmberg-et-al-2024>(11/50 | 179/299) Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks (Daniel Holmberg et al., 2024)</a></li><li><a href=#1250--180299-overcoming-dimensional-collapse-in-self-supervised-contrastive-learning-for-medical-image-segmentation-jamshid-hassanpour-et-al-2024>(12/50 | 180/299) Overcoming Dimensional Collapse in Self-supervised Contrastive Learning for Medical Image Segmentation (Jamshid Hassanpour et al., 2024)</a></li><li><a href=#1350--181299-tie-kd-teacher-independent-and-explainable-knowledge-distillation-for-monocular-depth-estimation-sangwon-choi-et-al-2024>(13/50 | 181/299) TIE-KD: Teacher-Independent and Explainable Knowledge Distillation for Monocular Depth Estimation (Sangwon Choi et al., 2024)</a></li><li><a href=#1450--182299-mvd2-efficient-multiview-3d-reconstruction-for-multiview-diffusion-xin-yang-zheng-et-al-2024>(14/50 | 182/299) MVD$^2$: Efficient Multiview 3D Reconstruction for Multiview Diffusion (Xin-Yang Zheng et al., 2024)</a></li><li><a href=#1550--183299-hint-high-quality-inpainting-transformer-with-mask-aware-encoding-and-enhanced-attention-shuang-chen-et-al-2024>(15/50 | 183/299) HINT: High-quality INPainting Transformer with Mask-Aware Encoding and Enhanced Attention (Shuang Chen et al., 2024)</a></li><li><a href=#1650--184299-stop-reasoning-when-multimodal-llms-with-chain-of-thought-reasoning-meets-adversarial-images-zefeng-wang-et-al-2024>(16/50 | 184/299) Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images (Zefeng Wang et al., 2024)</a></li><li><a href=#1750--185299-consolidating-attention-features-for-multi-view-image-editing-or-patashnik-et-al-2024>(17/50 | 185/299) Consolidating Attention Features for Multi-view Image Editing (Or Patashnik et al., 2024)</a></li><li><a href=#1850--186299-clove-encoding-compositional-language-in-contrastive-vision-language-models-santiago-castro-et-al-2024>(18/50 | 186/299) CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models (Santiago Castro et al., 2024)</a></li><li><a href=#1950--187299-multi-hmr-multi-person-whole-body-human-mesh-recovery-in-a-single-shot-fabien-baradel-et-al-2024>(19/50 | 187/299) Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot (Fabien Baradel et al., 2024)</a></li><li><a href=#2050--188299-s2former-or-single-stage-bimodal-transformer-for-scene-graph-generation-in-or-jialun-pei-et-al-2024>(20/50 | 188/299) S^2Former-OR: Single-Stage Bimodal Transformer for Scene Graph Generation in OR (Jialun Pei et al., 2024)</a></li><li><a href=#2150--189299-the-common-stability-mechanism-behind-most-self-supervised-learning-approaches-abhishek-jha-et-al-2024>(21/50 | 189/299) The Common Stability Mechanism behind most Self-Supervised Learning Approaches (Abhishek Jha et al., 2024)</a></li><li><a href=#2250--190299-two-stage-cytopathological-image-synthesis-for-augmenting-cervical-abnormality-screening-zhenrong-shen-et-al-2024>(22/50 | 190/299) Two-stage Cytopathological Image Synthesis for Augmenting Cervical Abnormality Screening (Zhenrong Shen et al., 2024)</a></li><li><a href=#2350--191299-measuring-multimodal-mathematical-reasoning-with-math-vision-dataset-ke-wang-et-al-2024>(23/50 | 191/299) Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset (Ke Wang et al., 2024)</a></li><li><a href=#2450--192299-a-landmark-aware-visual-navigation-dataset-faith-johnson-et-al-2024>(24/50 | 192/299) A Landmark-Aware Visual Navigation Dataset (Faith Johnson et al., 2024)</a></li><li><a href=#2550--193299-dualfocus-integrating-macro-and-micro-perspectives-in-multi-modal-large-language-models-yuhang-cao-et-al-2024>(25/50 | 193/299) DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models (Yuhang Cao et al., 2024)</a></li><li><a href=#2650--194299-framenerf-a-simple-and-efficient-framework-for-few-shot-novel-view-synthesis-yan-xing-et-al-2024>(26/50 | 194/299) FrameNeRF: A Simple and Efficient Framework for Few-shot Novel View Synthesis (Yan Xing et al., 2024)</a></li><li><a href=#2750--195299-cameras-as-rays-pose-estimation-via-ray-diffusion-jason-y-zhang-et-al-2024>(27/50 | 195/299) Cameras as Rays: Pose Estimation via Ray Diffusion (Jason Y. Zhang et al., 2024)</a></li><li><a href=#2850--196299-customize-a-video-one-shot-motion-customization-of-text-to-video-diffusion-models-yixuan-ren-et-al-2024>(28/50 | 196/299) Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models (Yixuan Ren et al., 2024)</a></li><li><a href=#2950--197299-high-speed-detector-for-low-powered-devices-in-aerial-grasping-ashish-kumar-et-al-2024>(29/50 | 197/299) High-Speed Detector For Low-Powered Devices In Aerial Grasping (Ashish Kumar et al., 2024)</a></li><li><a href=#3050--198299-debiasing-text-to-image-diffusion-models-ruifei-he-et-al-2024>(30/50 | 198/299) Debiasing Text-to-Image Diffusion Models (Ruifei He et al., 2024)</a></li><li><a href=#3150--199299-reimagining-anomalies-what-if-anomalies-were-normal-philipp-liznerski-et-al-2024>(31/50 | 199/299) Reimagining Anomalies: What If Anomalies Were Normal? (Philipp Liznerski et al., 2024)</a></li><li><a href=#3250--200299-diffusion-model-based-visual-compensation-guidance-and-visual-difference-analysis-for-no-reference-image-quality-assessment-zhaoyang-wang-et-al-2024>(32/50 | 200/299) Diffusion Model Based Visual Compensation Guidance and Visual Difference Analysis for No-Reference Image Quality Assessment (Zhaoyang Wang et al., 2024)</a></li><li><a href=#3350--201299-learning-to-kern----set-wise-estimation-of-optimal-letter-space-kei-nakatsuru-et-al-2024>(33/50 | 201/299) Learning to Kern &ndash; Set-wise Estimation of Optimal Letter Space (Kei Nakatsuru et al., 2024)</a></li><li><a href=#3450--202299-yolo-tla-an-efficient-and-lightweight-small-object-detection-model-based-on-yolov5-peng-gao-et-al-2024>(34/50 | 202/299) YOLO-TLA: An Efficient and Lightweight Small Object Detection Model based on YOLOv5 (Peng Gao et al., 2024)</a></li><li><a href=#3550--203299-a-self-supervised-pressure-map-human-keypoint-detection-approch-optimizing-generalization-and-computational-efficiency-across-datasets-chengzhang-yu-et-al-2024>(35/50 | 203/299) A Self-supervised Pressure Map human keypoint Detection Approch: Optimizing Generalization and Computational Efficiency Across Datasets (Chengzhang Yu et al., 2024)</a></li><li><a href=#3650--204299-snap-video-scaled-spatiotemporal-transformers-for-text-to-video-synthesis-willi-menapace-et-al-2024>(36/50 | 204/299) Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis (Willi Menapace et al., 2024)</a></li><li><a href=#3750--205299-a-transformer-model-for-boundary-detection-in-continuous-sign-language-razieh-rastgoo-et-al-2024>(37/50 | 205/299) A Transformer Model for Boundary Detection in Continuous Sign Language (Razieh Rastgoo et al., 2024)</a></li><li><a href=#3850--206299-quadruplet-loss-for-improving-the-robustness-to-face-morphing-attacks-iurii-medvedev-et-al-2024>(38/50 | 206/299) Quadruplet Loss For Improving the Robustness to Face Morphing Attacks (Iurii Medvedev et al., 2024)</a></li><li><a href=#3950--207299-towards-seamless-adaptation-of-pre-trained-models-for-visual-place-recognition-feng-lu-et-al-2024>(39/50 | 207/299) Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition (Feng Lu et al., 2024)</a></li><li><a href=#4050--208299-gradual-residuals-alignment-a-dual-stream-framework-for-gan-inversion-and-image-attribute-editing-hao-li-et-al-2024>(40/50 | 208/299) Gradual Residuals Alignment: A Dual-Stream Framework for GAN Inversion and Image Attribute Editing (Hao Li et al., 2024)</a></li><li><a href=#4150--209299-gam-depth-self-supervised-indoor-depth-estimation-leveraging-a-gradient-aware-mask-and-semantic-constraints-anqi-cheng-et-al-2024>(41/50 | 209/299) GAM-Depth: Self-Supervised Indoor Depth Estimation Leveraging a Gradient-Aware Mask and Semantic Constraints (Anqi Cheng et al., 2024)</a></li><li><a href=#4250--210299-font-style-interpolation-with-diffusion-models-tetta-kondo-et-al-2024>(42/50 | 210/299) Font Style Interpolation with Diffusion Models (Tetta Kondo et al., 2024)</a></li><li><a href=#4350--211299-swin3d-effective-multi-source-pretraining-for-3d-indoor-scene-understanding-yu-qi-yang-et-al-2024>(43/50 | 211/299) Swin3D++: Effective Multi-Source Pretraining for 3D Indoor Scene Understanding (Yu-Qi Yang et al., 2024)</a></li><li><a href=#4450--212299-mip-grid-anti-aliased-grid-representations-for-neural-radiance-fields-seungtae-nam-et-al-2024>(44/50 | 212/299) Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields (Seungtae Nam et al., 2024)</a></li><li><a href=#4550--213299-distributed-radiance-fields-for-edge-video-compression-and-metaverse-integration-in-autonomous-driving-eugen-šlapak-et-al-2024>(45/50 | 213/299) Distributed Radiance Fields for Edge Video Compression and Metaverse Integration in Autonomous Driving (Eugen Šlapak et al., 2024)</a></li><li><a href=#4650--214299-nerf-det-incorporating-semantic-cues-and-perspective-aware-depth-supervision-for-indoor-multi-view-3d-detection-chenxi-huang-et-al-2024>(46/50 | 214/299) NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth Supervision for Indoor Multi-View 3D Detection (Chenxi Huang et al., 2024)</a></li><li><a href=#4750--215299-taylorgrid-towards-fast-and-high-quality-implicit-field-learning-via-direct-taylor-based-grid-optimization-renyi-mao-et-al-2024>(47/50 | 215/299) TaylorGrid: Towards Fast and High-Quality Implicit Field Learning via Direct Taylor-based Grid Optimization (Renyi Mao et al., 2024)</a></li><li><a href=#4850--216299-geneoh-diffusion-towards-generalizable-hand-object-interaction-denoising-via-denoising-diffusion-xueyi-liu-et-al-2024>(48/50 | 216/299) GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion (Xueyi Liu et al., 2024)</a></li><li><a href=#4950--217299-ccpa-long-term-person-re-identification-via-contrastive-clothing-and-pose-augmentation-vuong-d-nguyen-et-al-2024>(49/50 | 217/299) CCPA: Long-term Person Re-Identification via Contrastive Clothing and Pose Augmentation (Vuong D. Nguyen et al., 2024)</a></li><li><a href=#5050--218299-semantic-image-synthesis-with-unconditional-generator-jungwoo-chae-et-al-2024>(50/50 | 218/299) Semantic Image Synthesis with Unconditional Generator (Jungwoo Chae et al., 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#11--219299-a-decision-language-model-dlm-for-dynamic-restless-multi-armed-bandit-tasks-in-public-health-nikhil-behari-et-al-2024>(1/1 | 219/299) A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health (Nikhil Behari et al., 2024)</a></li></ul></li><li><a href=#eessiv-3>eess.IV (3)</a><ul><li><a href=#13--220299-dicom----diverse-concept-modeling-towards-enhancing-generalizability-in-chest-x-ray-studies-abhieet-parida-et-al-2024>(1/3 | 220/299) DiCoM &ndash; Diverse Concept Modeling towards Enhancing Generalizability in Chest X-Ray Studies (Abhieet Parida et al., 2024)</a></li><li><a href=#23--221299-uncertainty-driven-and-adversarial-calibration-learning-for-epicardial-adipose-tissue-segmentation-kai-zhao-et-al-2024>(2/3 | 221/299) Uncertainty-driven and Adversarial Calibration Learning for Epicardial Adipose Tissue Segmentation (Kai Zhao et al., 2024)</a></li><li><a href=#33--222299-towards-spatially-lucid-ai-classification-in-non-euclidean-space-an-application-for-mxif-oncology-data-majid-farhadloo-et-al-2024>(3/3 | 222/299) Towards Spatially-Lucid AI Classification in Non-Euclidean Space: An Application for MxIF Oncology Data (Majid Farhadloo et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--223299-an-fpga-based-accelerator-enabling-efficient-support-for-cnns-with-arbitrary-kernel-sizes-miaoxin-wang-et-al-2024>(1/1 | 223/299) An FPGA-Based Accelerator Enabling Efficient Support for CNNs with Arbitrary Kernel Sizes (Miaoxin Wang et al., 2024)</a></li></ul></li><li><a href=#csse-5>cs.SE (5)</a><ul><li><a href=#15--224299-copilot-evaluation-harness-evaluating-llm-guided-software-programming-anisha-agarwal-et-al-2024>(1/5 | 224/299) Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming (Anisha Agarwal et al., 2024)</a></li><li><a href=#25--225299-metmap-metamorphic-testing-for-detecting-false-vector-matching-problems-in-llm-augmented-generation-guanyu-wang-et-al-2024>(2/5 | 225/299) MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems in LLM Augmented Generation (Guanyu Wang et al., 2024)</a></li><li><a href=#35--226299-opencodeinterpreter-integrating-code-generation-with-execution-and-refinement-tianyu-zheng-et-al-2024>(3/5 | 226/299) OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement (Tianyu Zheng et al., 2024)</a></li><li><a href=#45--227299-do-machines-and-humans-focus-on-similar-code-exploring-explainability-of-large-language-models-in-code-summarization-jiliang-li-et-al-2024>(4/5 | 227/299) Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization (Jiliang Li et al., 2024)</a></li><li><a href=#55--228299-repofuse-repository-level-code-completion-with-fused-dual-context-ming-liang-et-al-2024>(5/5 | 228/299) REPOFUSE: Repository-Level Code Completion with Fused Dual Context (Ming Liang et al., 2024)</a></li></ul></li><li><a href=#csai-5>cs.AI (5)</a><ul><li><a href=#15--229299-shm-traffic-drl-and-transfer-learning-based-uav-control-for-structural-health-monitoring-of-bridges-with-traffic-divija-swetha-gadiraju-et-al-2024>(1/5 | 229/299) SHM-Traffic: DRL and Transfer learning based UAV Control for Structural Health Monitoring of Bridges with Traffic (Divija Swetha Gadiraju et al., 2024)</a></li><li><a href=#25--230299-automating-psychological-hypothesis-generation-with-ai-large-language-models-meet-causal-graph-song-tong-et-al-2024>(2/5 | 230/299) Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph (Song Tong et al., 2024)</a></li><li><a href=#35--231299-unleashing-the-power-of-imbalanced-modality-information-for-multi-modal-knowledge-graph-completion-yichi-zhang-et-al-2024>(3/5 | 231/299) Unleashing the Power of Imbalanced Modality Information for Multi-modal Knowledge Graph Completion (Yichi Zhang et al., 2024)</a></li><li><a href=#45--232299-large-language-models-as-urban-residents-an-llm-agent-framework-for-personal-mobility-generation-jiawei-wang-et-al-2024>(4/5 | 232/299) Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation (Jiawei Wang et al., 2024)</a></li><li><a href=#55--233299-mentor-guiding-hierarchical-reinforcement-learning-with-human-feedback-and-dynamic-distance-constraint-xinglin-zhou-et-al-2024>(5/5 | 233/299) MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and Dynamic Distance Constraint (Xinglin Zhou et al., 2024)</a></li></ul></li><li><a href=#quant-ph-3>quant-ph (3)</a><ul><li><a href=#13--234299-quantum-circuit-optimization-with-alphatensor-francisco-j-r-ruiz-et-al-2024>(1/3 | 234/299) Quantum Circuit Optimization with AlphaTensor (Francisco J. R. Ruiz et al., 2024)</a></li><li><a href=#23--235299-quantum-markov-decision-processes-part-i-general-theory-approximations-and-classes-of-policies-naci-saldi-et-al-2024>(2/3 | 235/299) Quantum Markov Decision Processes Part I: General Theory, Approximations, and Classes of Policies (Naci Saldi et al., 2024)</a></li><li><a href=#33--236299-quantum-markov-decision-processes-part-ii-optimal-solutions-and-algorithms-naci-saldi-et-al-2024>(3/3 | 236/299) Quantum Markov Decision Processes Part II: Optimal Solutions and Algorithms (Naci Saldi et al., 2024)</a></li></ul></li><li><a href=#statml-6>stat.ML (6)</a><ul><li><a href=#16--237299-in-context-learning-of-a-linear-transformer-block-benefits-of-the-mlp-component-and-one-step-gd-initialization-ruiqi-zhang-et-al-2024>(1/6 | 237/299) In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization (Ruiqi Zhang et al., 2024)</a></li><li><a href=#26--238299-causal-imputation-for-counterfactual-scms-bridging-graphs-and-latent-factor-models-alvaro-ribot-et-al-2024>(2/6 | 238/299) Causal Imputation for Counterfactual SCMs: Bridging Graphs and Latent Factor Models (Alvaro Ribot et al., 2024)</a></li><li><a href=#36--239299-smoothness-adaptive-hypothesis-transfer-learning-haotian-lin-et-al-2024>(3/6 | 239/299) Smoothness Adaptive Hypothesis Transfer Learning (Haotian Lin et al., 2024)</a></li><li><a href=#46--240299-on-the-performance-of-empirical-risk-minimization-with-smoothed-data-adam-block-et-al-2024>(4/6 | 240/299) On the Performance of Empirical Risk Minimization with Smoothed Data (Adam Block et al., 2024)</a></li><li><a href=#56--241299-batch-and-match-black-box-variational-inference-with-a-score-based-divergence-diana-cai-et-al-2024>(5/6 | 241/299) Batch and match: black-box variational inference with a score-based divergence (Diana Cai et al., 2024)</a></li><li><a href=#66--242299-structure-agnostic-optimality-of-doubly-robust-learning-for-treatment-effect-estimation-jikai-jin-et-al-2024>(6/6 | 242/299) Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation (Jikai Jin et al., 2024)</a></li></ul></li><li><a href=#cscy-3>cs.CY (3)</a><ul><li><a href=#13--243299-demographic-bias-of-expert-level-vision-language-foundation-models-in-medical-imaging-yuzhe-yang-et-al-2024>(1/3 | 243/299) Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging (Yuzhe Yang et al., 2024)</a></li><li><a href=#23--244299-filter-bubble-or-homogenization-disentangling-the-long-term-effects-of-recommendations-on-user-consumption-patterns-md-sanzeed-anwar-et-al-2024>(2/3 | 244/299) Filter Bubble or Homogenization? Disentangling the Long-Term Effects of Recommendations on User Consumption Patterns (Md Sanzeed Anwar et al., 2024)</a></li><li><a href=#33--245299-an-exploratory-analysis-of-covid-bot-vs-human-disinformation-dissemination-stemming-from-the-disinformation-dozen-on-telegram-lynnette-hui-xian-ng-et-al-2024>(3/3 | 245/299) An Exploratory Analysis of COVID Bot vs Human Disinformation Dissemination stemming from the Disinformation Dozen on Telegram (Lynnette Hui Xian Ng et al., 2024)</a></li></ul></li><li><a href=#csir-9>cs.IR (9)</a><ul><li><a href=#19--246299-from-keywords-to-structured-summaries-streamlining-scholarly-knowledge-access-mahsa-shamsabadi-et-al-2024>(1/9 | 246/299) From Keywords to Structured Summaries: Streamlining Scholarly Knowledge Access (Mahsa Shamsabadi et al., 2024)</a></li><li><a href=#29--247299-personalized-behavior-aware-transformer-for-multi-behavior-sequential-recommendation-jiajie-su-et-al-2024>(2/9 | 247/299) Personalized Behavior-Aware Transformer for Multi-Behavior Sequential Recommendation (Jiajie Su et al., 2024)</a></li><li><a href=#39--248299-scalable-and-provably-fair-exposure-control-for-large-scale-recommender-systems-riku-togashi-et-al-2024>(3/9 | 248/299) Scalable and Provably Fair Exposure Control for Large-Scale Recommender Systems (Riku Togashi et al., 2024)</a></li><li><a href=#49--249299-genserp-large-language-models-for-whole-page-presentation-zhenning-zhang-et-al-2024>(4/9 | 249/299) GenSERP: Large Language Models for Whole Page Presentation (Zhenning Zhang et al., 2024)</a></li><li><a href=#59--250299-transforming-norm-based-to-graph-based-spatial-representation-for-spatio-temporal-epidemiological-models-teddy-lazebnik-2024>(5/9 | 250/299) Transforming Norm-based To Graph-based Spatial Representation for Spatio-Temporal Epidemiological Models (Teddy Lazebnik, 2024)</a></li><li><a href=#69--251299-recommender-for-its-purpose-repeat-and-exploration-in-food-delivery-recommendations-jiayu-li-et-al-2024>(6/9 | 251/299) Recommender for Its Purpose: Repeat and Exploration in Food Delivery Recommendations (Jiayu Li et al., 2024)</a></li><li><a href=#79--252299-ensure-timeliness-and-accuracy-a-novel-sliding-window-data-stream-paradigm-for-live-streaming-recommendation-fengqi-liang-et-al-2024>(7/9 | 252/299) Ensure Timeliness and Accuracy: A Novel Sliding Window Data Stream Paradigm for Live Streaming Recommendation (Fengqi Liang et al., 2024)</a></li><li><a href=#89--253299-merrec-a-large-scale-multipurpose-mercari-dataset-for-consumer-to-consumer-recommendation-systems-lichi-li-et-al-2024>(8/9 | 253/299) MerRec: A Large-scale Multipurpose Mercari Dataset for Consumer-to-Consumer Recommendation Systems (Lichi Li et al., 2024)</a></li><li><a href=#99--254299-towards-efficient-pareto-optimal-utility-fairness-between-groups-in-repeated-rankings-phuong-dinh-mai-et-al-2024>(9/9 | 254/299) Towards Efficient Pareto-optimal Utility-Fairness between Groups in Repeated Rankings (Phuong Dinh Mai et al., 2024)</a></li></ul></li><li><a href=#csgt-5>cs.GT (5)</a><ul><li><a href=#15--255299-optimal-mechanism-in-a-dynamic-stochastic-knapsack-environment-jihyeok-jung-et-al-2024>(1/5 | 255/299) Optimal Mechanism in a Dynamic Stochastic Knapsack Environment (Jihyeok Jung et al., 2024)</a></li><li><a href=#25--256299-stability-of-p2p-networks-under-greedy-peering-full-version-lucianna-kiffer-et-al-2024>(2/5 | 256/299) Stability of P2P Networks Under Greedy Peering (Full Version) (Lucianna Kiffer et al., 2024)</a></li><li><a href=#35--257299-on-the-fairness-of-normalized-p-means-for-allocating-goods-and-chores-owen-eckart-et-al-2024>(3/5 | 257/299) On the Fairness of Normalized p-Means for Allocating Goods and Chores (Owen Eckart et al., 2024)</a></li><li><a href=#45--258299-on-truthful-item-acquiring-mechanisms-for-reward-maximization-liang-shan-et-al-2024>(4/5 | 258/299) On Truthful Item-Acquiring Mechanisms for Reward Maximization (Liang Shan et al., 2024)</a></li><li><a href=#55--259299-tight-inapproximability-of-nash-equilibria-in-public-goods-games-jérémi-do-dinh-et-al-2024>(5/5 | 259/299) Tight Inapproximability of Nash Equilibria in Public Goods Games (Jérémi Do Dinh et al., 2024)</a></li></ul></li><li><a href=#cshc-6>cs.HC (6)</a><ul><li><a href=#16--260299-multimodal-healthcare-ai-identifying-and-designing-clinically-relevant-vision-language-applications-for-radiology-nur-yildirim-et-al-2024>(1/6 | 260/299) Multimodal Healthcare AI: Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology (Nur Yildirim et al., 2024)</a></li><li><a href=#26--261299-ai-augmented-brainwriting-investigating-the-use-of-llms-in-group-ideation-orit-shaer-et-al-2024>(2/6 | 261/299) AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation (Orit Shaer et al., 2024)</a></li><li><a href=#36--262299-wizard-of-oz-experimentation-for-language-technology-applications-challenges-and-tools-stephan-schlögl-et-al-2024>(3/6 | 262/299) Wizard of Oz Experimentation for Language Technology Applications: Challenges and Tools (Stephan Schlögl et al., 2024)</a></li><li><a href=#46--263299-saharaline-a-collective-social-support-intervention-for-teachers-in-low-income-indian-schools-rama-adithya-varanasi-et-al-2024>(4/6 | 263/299) Saharaline: A Collective Social Support Intervention for Teachers in Low-Income Indian Schools (Rama Adithya Varanasi et al., 2024)</a></li><li><a href=#56--264299-gazetrak-exploring-acoustic-based-eye-tracking-on-a-glass-frame-ke-li-et-al-2024>(5/6 | 264/299) GazeTrak: Exploring Acoustic-based Eye Tracking on a Glass Frame (Ke Li et al., 2024)</a></li><li><a href=#66--265299-make-interaction-situated-designing-user-acceptable-interaction-for-situated-visualization-in-public-environments-qian-zhu-et-al-2024>(6/6 | 265/299) Make Interaction Situated: Designing User Acceptable Interaction for Situated Visualization in Public Environments (Qian Zhu et al., 2024)</a></li></ul></li><li><a href=#eesssy-6>eess.SY (6)</a><ul><li><a href=#16--266299-mr-arl-model-reference-adaptive-reinforcement-learning-for-robustly-stable-on-policy-data-driven-lqr-marco-borghesi-et-al-2024>(1/6 | 266/299) MR-ARL: Model Reference Adaptive Reinforcement Learning for Robustly Stable On-Policy Data-Driven LQR (Marco Borghesi et al., 2024)</a></li><li><a href=#26--267299-using-hybrid-system-dynamics-and-discrete-event-simulations-to-identify-high-leverage-targets-for-process-improvement-in-a-skill-based-organizational-structure-eric-enos-et-al-2024>(2/6 | 267/299) Using Hybrid System Dynamics and Discrete Event Simulations to Identify High Leverage Targets for Process Improvement in a Skill-based Organizational Structure (Eric Enos et al., 2024)</a></li><li><a href=#36--268299-run-time-assurance-for-simultaneous-constraint-satisfaction-during-spacecraft-attitude-maneuvering-cassie-kay-mcquinn-et-al-2024>(3/6 | 268/299) Run Time Assurance for Simultaneous Constraint Satisfaction During Spacecraft Attitude Maneuvering (Cassie-Kay McQuinn et al., 2024)</a></li><li><a href=#46--269299-strong-arm-dynamic-latch-comparators-design-and-analyses-on-cad-platform-kasi-bandla-et-al-2024>(4/6 | 269/299) Strong-ARM Dynamic Latch Comparators: Design and Analyses on CAD Platform (Kasi Bandla et al., 2024)</a></li><li><a href=#56--270299-closed-loop-data-enabled-predictive-control-and-its-equivalence-with-closed-loop-subspace-predictive-control-rogier-dinkla-et-al-2024>(5/6 | 270/299) Closed-loop Data-Enabled Predictive Control and its equivalence with Closed-loop Subspace Predictive Control (Rogier Dinkla et al., 2024)</a></li><li><a href=#66--271299-parking-of-connected-automated-vehicles-vehicle-control-parking-assignment-and-multi-agent-simulation-xu-shen-et-al-2024>(6/6 | 271/299) Parking of Connected Automated Vehicles: Vehicle Control, Parking Assignment, and Multi-agent Simulation (Xu Shen et al., 2024)</a></li></ul></li><li><a href=#csni-2>cs.NI (2)</a><ul><li><a href=#12--272299-what-comes-after-optical-bypass-network-a-study-on-optical-computing-enabled-network-dao-thanh-hai-2024>(1/2 | 272/299) What comes after optical-bypass network? A study on optical-computing-enabled network (Dao Thanh Hai, 2024)</a></li><li><a href=#22--273299-joint-ap-ue-association-and-power-factor-optimization-for-distributed-massive-mimo-mohd-saif-ali-khan-et-al-2024>(2/2 | 273/299) Joint AP-UE Association and Power Factor Optimization for Distributed Massive MIMO (Mohd Saif Ali Khan et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#12--274299-robust-mass-lumping-and-outlier-removal-strategies-in-isogeometric-analysis-yannis-voet-et-al-2024>(1/2 | 274/299) Robust mass lumping and outlier removal strategies in isogeometric analysis (Yannis Voet et al., 2024)</a></li><li><a href=#22--275299-on-schrödingerization-based-quantum-algorithms-for-linear-dynamical-systems-with-inhomogeneous-terms-shi-jin-et-al-2024>(2/2 | 275/299) On Schrödingerization based quantum algorithms for linear dynamical systems with inhomogeneous terms (Shi Jin et al., 2024)</a></li></ul></li><li><a href=#csdc-2>cs.DC (2)</a><ul><li><a href=#12--276299-toward-scalable-docker-based-emulations-of-blockchain-networks-for-research-and-development-diego-pennino-et-al-2024>(1/2 | 276/299) Toward Scalable Docker-Based Emulations of Blockchain Networks for Research and Development (Diego Pennino et al., 2024)</a></li><li><a href=#22--277299-towards-singular-optimality-in-the-presence-of-local-initial-knowledge-hongyan-ji-et-al-2024>(2/2 | 277/299) Towards singular optimality in the presence of local initial knowledge (Hongyan Ji et al., 2024)</a></li></ul></li><li><a href=#mathds-1>math.DS (1)</a><ul><li><a href=#11--278299-shifts-on-the-lamplighter-group-laurent-bartholdi-et-al-2024>(1/1 | 278/299) Shifts on the lamplighter group (Laurent Bartholdi et al., 2024)</a></li></ul></li><li><a href=#cssi-1>cs.SI (1)</a><ul><li><a href=#11--279299-human-machine-social-systems-milena-tsvetkova-et-al-2024>(1/1 | 279/299) Human-machine social systems (Milena Tsvetkova et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--280299-open-meshed-anatomy-towards-a-comprehensive-finite-element-hexahedral-mesh-derived-from-open-atlases-andy-trung-huynh-et-al-2024>(1/1 | 280/299) Open Meshed Anatomy: Towards a comprehensive finite element hexahedral mesh derived from open atlases (Andy Trung Huynh et al., 2024)</a></li></ul></li><li><a href=#eessas-2>eess.AS (2)</a><ul><li><a href=#12--281299-sicrn-advancing-speech-enhancement-through-state-space-model-and-inplace-convolution-techniques-changjiang-zhao-et-al-2024>(1/2 | 281/299) SICRN: Advancing Speech Enhancement through State Space Model and Inplace Convolution Techniques (Changjiang Zhao et al., 2024)</a></li><li><a href=#22--282299-periodgrad-towards-pitch-controllable-neural-vocoder-based-on-a-diffusion-probabilistic-model-yukiya-hono-et-al-2024>(2/2 | 282/299) PeriodGrad: Towards Pitch-Controllable Neural Vocoder Based on a Diffusion Probabilistic Model (Yukiya Hono et al., 2024)</a></li></ul></li><li><a href=#q-bionc-1>q-bio.NC (1)</a><ul><li><a href=#11--283299-contrastive-learning-of-shared-spatiotemporal-eeg-representations-across-individuals-for-naturalistic-neuroscience-xinke-shen-et-al-2024>(1/1 | 283/299) Contrastive Learning of Shared Spatiotemporal EEG Representations Across Individuals for Naturalistic Neuroscience (Xinke Shen et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--284299-compression-robust-synthetic-speech-detection-using-patched-spectrogram-transformer-amit-kumar-singh-yadav-et-al-2024>(1/2 | 284/299) Compression Robust Synthetic Speech Detection Using Patched Spectrogram Transformer (Amit Kumar Singh Yadav et al., 2024)</a></li><li><a href=#22--285299-symbolic-music-generation-with-non-differentiable-rule-guided-diffusion-yujia-huang-et-al-2024>(2/2 | 285/299) Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion (Yujia Huang et al., 2024)</a></li></ul></li><li><a href=#cspf-1>cs.PF (1)</a><ul><li><a href=#11--286299-cesasme-and-staticdeps-static-detection-of-memory-carried-dependencies-for-code-analyzers-théophile-bastian-et-al-2024>(1/1 | 286/299) CesASMe and Staticdeps: static detection of memory-carried dependencies for code analyzers (Théophile Bastian et al., 2024)</a></li></ul></li><li><a href=#csit-2>cs.IT (2)</a><ul><li><a href=#12--287299-efficient-unbiased-sparsification-leighton-barnes-et-al-2024>(1/2 | 287/299) Efficient Unbiased Sparsification (Leighton Barnes et al., 2024)</a></li><li><a href=#22--288299-efficient-solvers-for-wyner-common-information-with-application-to-multi-modal-clustering-teng-hui-huang-et-al-2024>(2/2 | 288/299) Efficient Solvers for Wyner Common Information with Application to Multi-Modal Clustering (Teng-Hui Huang et al., 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--289299-machine-checked-categorical-diagrammatic-reasoning-benoît-guillemet-et-al-2024>(1/1 | 289/299) Machine-Checked Categorical Diagrammatic Reasoning (Benoît Guillemet et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--290299-model-based-reinforcement-learning-control-of-reaction-diffusion-problems-christina-schenk-et-al-2024>(1/1 | 290/299) Model-Based Reinforcement Learning Control of Reaction-Diffusion Problems (Christina Schenk et al., 2024)</a></li></ul></li><li><a href=#csds-3>cs.DS (3)</a><ul><li><a href=#13--291299-parallel-approximate-maximum-flows-in-near-linear-work-and-polylogarithmic-depth-arpit-agarwal-et-al-2024>(1/3 | 291/299) Parallel Approximate Maximum Flows in Near-Linear Work and Polylogarithmic Depth (Arpit Agarwal et al., 2024)</a></li><li><a href=#23--292299-hitting-meets-packing-how-hard-can-it-be-jacob-focke-et-al-2024>(2/3 | 292/299) Hitting Meets Packing: How Hard Can it Be? (Jacob Focke et al., 2024)</a></li><li><a href=#33--293299-parameterized-complexity-of-finding-dissimilar-shortest-paths-ryo-funayama-et-al-2024>(3/3 | 293/299) Parameterized Complexity of Finding Dissimilar Shortest Paths (Ryo Funayama et al., 2024)</a></li></ul></li><li><a href=#cscc-1>cs.CC (1)</a><ul><li><a href=#11--294299-on-the-communication-complexity-of-finding-a-king-in-a-tournament-nikhil-s-mande-et-al-2024>(1/1 | 294/299) On the communication complexity of finding a king in a tournament (Nikhil S. Mande et al., 2024)</a></li></ul></li><li><a href=#mathco-2>math.CO (2)</a><ul><li><a href=#12--295299-new-scattered-linearized-quadrinomials-valentino-smaldore-et-al-2024>(1/2 | 295/299) New scattered linearized quadrinomials (Valentino Smaldore et al., 2024)</a></li><li><a href=#22--296299-the-expansion-of-half-integral-polytopes-jean-cardinal-et-al-2024>(2/2 | 296/299) The expansion of half-integral polytopes (Jean Cardinal et al., 2024)</a></li></ul></li><li><a href=#cscg-2>cs.CG (2)</a><ul><li><a href=#12--297299-embeddings-and-near-neighbor-searching-with-constant-additive-error-for-hyperbolic-spaces-eunku-park-et-al-2024>(1/2 | 297/299) Embeddings and near-neighbor searching with constant additive error for hyperbolic spaces (Eunku Park et al., 2024)</a></li><li><a href=#22--298299-on-k-plane-insertion-into-plane-drawings-julia-katheder-et-al-2024>(2/2 | 298/299) On $k$-Plane Insertion into Plane Drawings (Julia Katheder et al., 2024)</a></li></ul></li><li><a href=#q-biobm-1>q-bio.BM (1)</a><ul><li><a href=#11--299299-structure-based-drug-design-via-3d-molecular-generative-pre-training-and-sampling-yuwei-yang-et-al-2024>(1/1 | 299/299) Structure-Based Drug Design via 3D Molecular Generative Pre-training and Sampling (Yuwei Yang et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>