<!doctype html><html><head><title>arXiv @ 2024.02.14</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.14"><meta property="og:description" content="Primary Categories astro-ph.IM (1) cs.AI (22) cs.AR (3) cs.CG (1) cs.CL (39) cs.CR (9) cs.CV (27) cs.CY (4) cs.DC (5) cs.DM (1) cs.DS (3) cs.GT (2) cs.HC (6) cs.IR (8) cs.IT (2) cs.LG (62) cs.MM (2) cs.NE (2) cs.RO (8) cs.SD (2) cs.SE (5) cs.SI (4) eess.AS (3) eess.IV (6) eess.SP (2) eess.SY (5) hep-ph (1) math.CO (1) math.OC (2) math.ST (1) physics.ao-ph (1) physics.comp-ph (1) q-bio.GN (1) q-bio.QM (1) quant-ph (2) stat."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240214000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-14T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-14T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.14"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240214000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Wednesday, Feb 14, 2024</p></div><div class=title><h1>arXiv @ 2024.02.14</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#astro-phim-1>astro-ph.IM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#csai-22>cs.AI (22)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#csar-3>cs.AR (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#cscl-39>cs.CL (39)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#cscr-9>cs.CR (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#cscv-27>cs.CV (27)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#cscy-4>cs.CY (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#csdc-5>cs.DC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#csdm-1>cs.DM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#csds-3>cs.DS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#csgt-2>cs.GT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#cshc-6>cs.HC (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#csir-8>cs.IR (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#csit-2>cs.IT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#cslg-62>cs.LG (62)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#csmm-2>cs.MM (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#csne-2>cs.NE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#csro-8>cs.RO (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#csse-5>cs.SE (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#cssi-4>cs.SI (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#eessas-3>eess.AS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#eessiv-6>eess.IV (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#eesssp-2>eess.SP (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#eesssy-5>eess.SY (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#hep-ph-1>hep-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#mathoc-2>math.OC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#mathst-1>math.ST (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#physicsao-ph-1>physics.ao-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#physicscomp-ph-1>physics.comp-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#q-biogn-1>q-bio.GN (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#quant-ph-2>quant-ph (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/#statml-7>stat.ML (7)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Adversarial Attack</td><td></td><td></td><td>1</td><td>3</td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Aspect-based Sentiment Analysis</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td>2</td><td>1</td></tr><tr><td>Automatic Evaluation</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>7</td><td></td><td></td></tr><tr><td>BERT</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td></td><td>4</td></tr><tr><td>Benchmarking</td><td>4</td><td>7</td><td>6</td><td>8</td></tr><tr><td>ChatGPT</td><td>1</td><td>1</td><td></td><td>1</td></tr><tr><td>Clustering</td><td>1</td><td>1</td><td></td><td>3</td></tr><tr><td>Code Generation</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Common-sense Reasoning</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Continual Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Contrastive Learning</td><td>1</td><td>2</td><td></td><td>1</td></tr><tr><td>Convolution</td><td>1</td><td></td><td>3</td><td>7</td></tr><tr><td>Convolutional Neural Network</td><td>1</td><td></td><td>5</td><td>7</td></tr><tr><td>Counter-factual</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Curriculum Learning</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Data Augmentation</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Dependency Parsing</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Distributional Reinforcement Learning</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Domain Adaptation</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Edge Prediction</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Fact Verification</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fairness</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Fake News Detection</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td></td><td>4</td></tr><tr><td>Few-shot</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Fine-tuning</td><td>2</td><td>14</td><td>4</td><td>13</td></tr><tr><td>Foundation Model</td><td>1</td><td></td><td></td><td>2</td></tr><tr><td>GPT</td><td>5</td><td>6</td><td></td><td></td></tr><tr><td>GPT-3</td><td>2</td><td>4</td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>2</td><td>4</td><td></td><td></td></tr><tr><td>GPT-4</td><td>3</td><td>5</td><td></td><td></td></tr><tr><td>Generative AI</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Graph</td><td>2</td><td>1</td><td>1</td><td>17</td></tr><tr><td>Graph Classification</td><td></td><td></td><td></td><td>2</td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Graph Neural Network</td><td>2</td><td></td><td></td><td>8</td></tr><tr><td>GraphMAE</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Grounding</td><td></td><td></td><td></td><td>2</td></tr><tr><td>In-context Learning</td><td>2</td><td>5</td><td></td><td>8</td></tr><tr><td>Instruction Following</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Instruction Tuning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Knowledge Graph</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Knowledge Transfer</td><td></td><td>1</td><td></td><td></td></tr><tr><td>LLaMA</td><td></td><td>3</td><td></td><td></td></tr><tr><td>LSTM</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Language Generation</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>21</td><td>39</td><td>3</td><td>21</td></tr><tr><td>Low-Resource</td><td></td><td>2</td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Masked Language Model</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Mathematical Reasoning</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Message-Passing</td><td></td><td></td><td></td><td>3</td></tr><tr><td>Mistral</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Model Compression</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Multi-modal</td><td>2</td><td>2</td><td>8</td><td>2</td></tr><tr><td>Multiple Instance Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Mutual Information</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Natural Language Explanation</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Neural Machine Translation</td><td></td><td>9</td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td></td><td>2</td></tr><tr><td>Object Detection</td><td></td><td></td><td>3</td><td>1</td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Optical Character Recognition</td><td></td><td></td><td>2</td><td>1</td></tr><tr><td>Out-of-distribution</td><td>1</td><td></td><td>2</td><td>1</td></tr><tr><td>PaLM</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Perplexity</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>1</td><td>5</td><td></td><td></td></tr><tr><td>Prompt</td><td>2</td><td>7</td><td></td><td>3</td></tr><tr><td>Pruning</td><td>1</td><td>1</td><td>2</td><td>2</td></tr><tr><td>Question Answering</td><td>1</td><td>3</td><td>3</td><td>1</td></tr><tr><td>Reasoning</td><td>4</td><td>4</td><td></td><td>3</td></tr><tr><td>Recommendation</td><td>2</td><td>1</td><td></td><td>1</td></tr><tr><td>Reconstruction Loss</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Reinforcement Learning</td><td>1</td><td>1</td><td></td><td>9</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Rerank</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>3</td><td></td><td></td><td>3</td></tr><tr><td>Scaling Law</td><td></td><td></td><td></td><td>3</td></tr><tr><td>Self-Attention</td><td></td><td>1</td><td></td><td>3</td></tr><tr><td>Self-supervised Learning</td><td></td><td>1</td><td>1</td><td>2</td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Sentiment Analysis</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>2</td><td>1</td><td>1</td><td>3</td></tr><tr><td>Simulator</td><td>2</td><td>1</td><td>1</td><td>3</td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td>3</td></tr><tr><td>Style Transfer</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Summarization</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Supervised Learning</td><td></td><td>3</td><td>3</td><td>8</td></tr><tr><td>Text Classification</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Text Generation</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Text Understanding</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Text-to-speech</td><td></td><td></td><td></td><td>2</td></tr><tr><td>Tokenization</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Topic Model</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Topic Modeling</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Transformer</td><td>1</td><td>2</td><td>3</td><td>12</td></tr><tr><td>Unsupervised Learning</td><td></td><td>2</td><td>3</td><td>1</td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>4</td><td>2</td></tr><tr><td>Vision-and-Language</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Word Embedding</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Yolo</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Zero-shot</td><td>2</td><td>4</td><td>2</td><td>2</td></tr><tr><td>Zero-shot Learning</td><td>2</td><td></td><td></td><td></td></tr><tr><td>falcon</td><td>1</td><td></td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cslg-62>cs.LG (62)</h2><h3 id=162--1252-g-retriever-retrieval-augmented-generation-for-textual-graph-understanding-and-question-answering-xiaoxin-he-et-al-2024>(1/62 | 1/252) G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering (Xiaoxin He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, Bryan Hooi. (2024)<br><strong>G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering</strong><br><button class=copy-to-clipboard title="G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 141<br>Keywords: Graph Classification, Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Fine-tuning, Knowledge Graph, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Common-sense Reasoning, Question Answering, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07630v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07630v1.pdf filename=2402.07630v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a <b>graph</b> <b>with</b> <b>textual</b> attributes, we enable users to `chat with their <b>graph&rsquo;:</b> <b>that</b> <b>is,</b> to ask <b>questions</b> <b>about</b> the <b>graph</b> <b>using</b> <b>a</b> conversational interface. In response to a user&rsquo;s <b>questions,</b> <b>our</b> method provides textual replies and highlights the relevant parts of the <b>graph.</b> <b>While</b> <b>existing</b> works integrate <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> in various ways, they mostly focus on either conventional <b>graph</b> <b>tasks</b> <b>(such</b> as node, edge, and <b>graph</b> <b>classification),</b> <b>or</b> on answering simple <b>graph</b> <b>queries</b> <b>on</b> small or synthetic <b>graphs.</b> <b>In</b> <b>contrast,</b> we develop a flexible <b>question-answering</b> <b>framework</b> targeting real-world textual <b>graphs,</b> <b>applicable</b> <b>to</b> multiple applications including scene <b>graph</b> <b>understanding,</b> <b>common</b> sense <b>reasoning,</b> and <b>knowledge</b> <b>graph</b> <b>reasoning.</b> <b>Toward</b> this goal, we first develop our <b>Graph</b> <b>Question</b> <b>Answering</b> (GraphQA) <b>benchmark</b> with data collected from different tasks. Then, we propose our G-Retriever approach, which integrates the strengths of <b>GNNs,</b> <b>LLMs,</b> and <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG),</b> and can be <b>fine-tuned</b> to enhance <b>graph</b> <b>understanding</b> <b>via</b> soft <b>prompting.</b> To resist hallucination and to allow for textual <b>graphs</b> <b>that</b> <b>greatly</b> exceed the <b>LLM&rsquo;s</b> context window size, G-Retriever performs <b>RAG</b> over a <b>graph</b> <b>by</b> <b>formulating</b> this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual <b>graph</b> <b>tasks</b> <b>from</b> multiple domains, scales well with larger <b>graph</b> <b>sizes,</b> <b>and</b> resists hallucination. (Our codes and datasets are available at: <a href=https://github.com/XiaoxinHe/G-Retriever>https://github.com/XiaoxinHe/G-Retriever</a>.)</p></p class="citation"></blockquote><h3 id=262--2252-transaxx-efficient-transformers-with-approximate-computing-dimitrios-danopoulos-et-al-2024>(2/62 | 2/252) TransAxx: Efficient Transformers with Approximate Computing (Dimitrios Danopoulos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dimitrios Danopoulos, Georgios Zervakis, Dimitrios Soudris, Jörg Henkel. (2024)<br><strong>TransAxx: Efficient Transformers with Approximate Computing</strong><br><button class=copy-to-clipboard title="TransAxx: Efficient Transformers with Approximate Computing" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AR, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Convolutional Neural Network, Fine-tuning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07545v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07545v1.pdf filename=2402.07545v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision</b> <b>Transformer</b> (ViT) models which were recently introduced by the <b>transformer</b> architecture have shown to be very competitive and often become a popular alternative to <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs).</b> However, the high computational requirements of these models limit their practical applicability especially on low-power devices. Current state-of-the-art employs approximate multipliers to address the highly increased compute demands of DNN accelerators but no prior research has explored their use on ViT models. In this work we propose TransAxx, a framework based on the popular PyTorch library that enables fast inherent support for approximate arithmetic to seamlessly evaluate the impact of approximate computing on DNNs such as ViT models. Using TransAxx we analyze the sensitivity of <b>transformer</b> models on the ImageNet dataset to approximate multiplications and perform approximate-aware <b>finetuning</b> to regain accuracy. Furthermore, we propose a methodology to generate approximate accelerators for ViT models. Our approach uses a Monte Carlo Tree Search (MCTS) algorithm to efficiently search the space of possible configurations using a hardware-driven hand-crafted policy. Our evaluation demonstrates the efficacy of our methodology in achieving significant trade-offs between accuracy and power, resulting in substantial gains without compromising on performance.</p></p class="citation"></blockquote><h3 id=362--3252-grounding-data-science-code-generation-with-input-output-specifications-yeming-wen-et-al-2024>(3/62 | 3/252) Grounding Data Science Code Generation with Input-Output Specifications (Yeming Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yeming Wen, Pengcheng Yin, Kensen Shi, Henryk Michalewski, Swarat Chaudhuri, Alex Polozov. (2024)<br><strong>Grounding Data Science Code Generation with Input-Output Specifications</strong><br><button class=copy-to-clipboard title="Grounding Data Science Code Generation with Input-Output Specifications" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-PL, cs-SE, cs.LG<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, Code Generation, Grounding, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08073v1.pdf filename=2402.08073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have recently demonstrated a remarkable ability to generate <b>code</b> <b>from</b> natural language (NL) <b>prompts.</b> However, in the real world, NL is often too ambiguous to capture the true intent behind programming problems, requiring additional input-output (I/O) specifications. Unfortunately, <b>LLMs</b> can have difficulty aligning their outputs with both the NL <b>prompt</b> and the I/O specification. In this paper, we give a way to mitigate this issue in the context of data science programming, where tasks require explicit I/O specifications for clarity. Specifically, we propose GIFT4Code, a novel approach for the instruction <b>fine-tuning</b> of <b>LLMs</b> with respect to I/O specifications. Our method leverages synthetic data produced by the <b>LLM</b> itself and utilizes execution-derived feedback as a key learning signal. This feedback, in the form of program I/O specifications, is provided to the <b>LLM</b> to facilitate instruction <b>fine-tuning.</b> We evaluated our approach on two challenging data science <b>benchmarks,</b> Arcade and DS-1000. The results demonstrate a significant improvement in the <b>LLM&rsquo;s</b> ability to generate <b>code</b> <b>that</b> is not only executable but also accurately aligned with user specifications, substantially improving the quality of <b>code</b> <b>generation</b> for complex data science tasks.</p></p class="citation"></blockquote><h3 id=462--4252-base-tts-lessons-from-building-a-billion-parameter-text-to-speech-model-on-100k-hours-of-data-mateusz-łajszczak-et-al-2024>(4/62 | 4/252) BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data (Mateusz Łajszczak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mateusz Łajszczak, Guillermo Cámbara, Yang Li, Fatih Beyhan, Arent van Korlaar, Fan Yang, Arnaud Joly, Álvaro Martín-Cortinas, Ammar Abbas, Adam Michalski, Alexis Moinet, Sri Karlapati, Ewa Muszyńska, Haohan Guo, Bartosz Putrycz, Soledad López Gambino, Kayeon Yoo, Elena Sokolova, Thomas Drugman. (2024)<br><strong>BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data</strong><br><button class=copy-to-clipboard title="BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG, eess-AS<br>Keyword Score: 60<br>Keywords: Convolution, Transformer, Text-to-speech, Text-to-speech, Tokenization, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08093v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08093v2.pdf filename=2402.08093v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a <b>text-to-speech</b> <b>(TTS)</b> model called BASE <b>TTS,</b> which stands for $\textbf{B}$ig $\textbf{A}$daptive $\textbf{S}$treamable <b>TTS</b> with $\textbf{E}$mergent abilities. BASE <b>TTS</b> is the largest <b>TTS</b> model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive <b>Transformer</b> that converts raw texts into discrete codes (&ldquo;speechcodes&rdquo;) followed by a <b>convolution-based</b> decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech <b>tokenization</b> technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported &ldquo;emergent abilities&rdquo; of <b>large</b> <b>language</b> <b>models</b> when trained on increasing volume of data, we show that BASE <b>TTS</b> variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for <b>text-to-speech.</b> We showcase state-of-the-art naturalness of BASE <b>TTS</b> by evaluating against baselines that include publicly available <b>large-scale</b> <b>text-to-speech</b> <b>systems:</b> YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at <a href=https://amazon-ltts-paper.com/>https://amazon-ltts-paper.com/</a>.</p></p class="citation"></blockquote><h3 id=562--5252-differentially-private-zeroth-order-methods-for-scalable-large-language-model-finetuning-z-liu-et-al-2024>(5/62 | 5/252) Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning (Z Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Z Liu, J Lou, W Bao, Z Qin, K Ren. (2024)<br><strong>Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning</strong><br><button class=copy-to-clipboard title="Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Fine-tuning, Pruning, Stochastic Gradient Descent, Large Language Model, Large Language Model, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07818v1.pdf filename=2402.07818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Finetuning</b> on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained <b>LLMs</b> for various downstream tasks. Due to the popularity of <b>LLMs</b> <b>finetuning</b> and its accompanying privacy concerns, differentially private (DP) <b>finetuning</b> of pretrained <b>LLMs</b> has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP <b>LLM</b> <b>finetuning</b> methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based <b>finetuning</b> methods are unfortunately limited by the inherent inefficiency of <b>SGD.</b> In this paper, we investigate the potential of DP zeroth-order methods for <b>LLM</b> pretraining, which avoids the scalability bottleneck of <b>SGD</b> by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replacement for <b>SGD,</b> this paper presents a comprehensive study both theoretically and empirically. First, we propose the stagewise DP zeroth-order method that dynamically schedules key hyperparameters. This design is grounded on the synergy between DP random perturbation and the gradient approximation error of the zeroth-order method, and its effect on <b>finetuning</b> trajectory. Second, we further enhance the scalability by reducing the trainable parameters that are identified by repurposing a data-free <b>pruning</b> technique requiring no additional data or extra privacy budget. We provide theoretical analysis for both proposed methods. We conduct extensive empirical analysis on both encoder-only <b>masked</b> <b>language</b> <b>model</b> and decoder-only autoregressive language model, achieving impressive results in terms of scalability and utility.</p></p class="citation"></blockquote><h3 id=662--6252-text-centric-alignment-for-multi-modality-learning-yun-da-tsai-et-al-2024>(6/62 | 6/252) Text-centric Alignment for Multi-Modality Learning (Yun-Da Tsai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yun-Da Tsai, Ting-Yu Yen, Pei-Fu Guo, Zhe-Yan Li, Shou-De Lin. (2024)<br><strong>Text-centric Alignment for Multi-Modality Learning</strong><br><button class=copy-to-clipboard title="Text-centric Alignment for Multi-Modality Learning" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 56<br>Keywords: Foundation Model, Multi-modal, Multi-modal, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08086v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08086v1.pdf filename=2402.08086v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research paper addresses the challenge of modality mismatch in <b>multimodal</b> learning, where the modalities available during inference differ from those available at training. We propose the Text-centric Alignment for Multi-Modality Learning (TAMML) approach, an innovative method that utilizes <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with <b>in-context</b> <b>learning</b> and <b>foundation</b> <b>models</b> to enhance the generalizability of <b>multimodal</b> systems under these conditions. By leveraging the unique properties of text as a unified semantic space, TAMML demonstrates significant improvements in handling unseen, diverse, and unpredictable modality combinations. TAMML not only adapts to varying modalities but also maintains robust performance, showcasing the potential of <b>foundation</b> <b>models</b> in overcoming the limitations of traditional fixed-modality frameworks in embedding representations. This study contributes to the field by offering a flexible, effective solution for real-world applications where modality availability is dynamic and uncertain.</p></p class="citation"></blockquote><h3 id=762--7252-message-detouring-a-simple-yet-effective-cycle-representation-for-expressive-graph-learning-ziquan-wei-et-al-2024>(7/62 | 7/252) Message Detouring: A Simple Yet Effective Cycle Representation for Expressive Graph Learning (Ziquan Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziquan Wei, Tingting Dan, Guorong Wu. (2024)<br><strong>Message Detouring: A Simple Yet Effective Cycle Representation for Expressive Graph Learning</strong><br><button class=copy-to-clipboard title="Message Detouring: A Simple Yet Effective Cycle Representation for Expressive Graph Learning" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CG, cs-LG, cs.LG<br>Keyword Score: 56<br>Keywords: Message-Passing, Edge Prediction, Graph Classification, Node Classification, Graph, Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08085v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08085v1.pdf filename=2402.08085v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>learning</b> is crucial in the fields of bioinformatics, social networks, and chemicals. Although high-order graphlets, such as cycles, are critical to achieving an informative <b>graph</b> <b>representation</b> for <b>node</b> <b>classification,</b> <b>edge</b> <b>prediction,</b> and <b>graph</b> <b>recognition,</b> modeling high-order topological characteristics poses significant computational challenges, restricting its widespread applications in machine learning. To address this limitation, we introduce the concept of \textit{message detouring} to hierarchically characterize cycle representation throughout the entire <b>graph,</b> <b>which</b> capitalizes on the contrast between the shortest and longest pathways within a range of local topologies associated with each <b>graph</b> <b>node.</b> <b>The</b> topological feature representations derived from our message detouring landscape demonstrate comparable expressive power to high-order \textit{Weisfeiler-Lehman} (WL) tests but much less computational demands. In addition to the integration with <b>graph</b> <b>kernel</b> and message passing neural networks, we present a novel message detouring neural network, which uses <b>Transformer</b> backbone to integrate cycle representations across <b>nodes</b> <b>and</b> <b>edges.</b> <b>Aside</b> from theoretical results, experimental results on expressiveness, <b>graph</b> <b>classification,</b> and <b>node</b> <b>classification</b> show message detouring can significantly outperform current counterpart approaches on various <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=862--8252-ugmae-a-unified-framework-for-graph-masked-autoencoders-yijun-tian-et-al-2024>(8/62 | 8/252) UGMAE: A Unified Framework for Graph Masked Autoencoders (Yijun Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijun Tian, Chuxu Zhang, Ziyi Kou, Zheyuan Liu, Xiangliang Zhang, Nitesh V. Chawla. (2024)<br><strong>UGMAE: A Unified Framework for Graph Masked Autoencoders</strong><br><button class=copy-to-clipboard title="UGMAE: A Unified Framework for Graph Masked Autoencoders" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: GraphMAE, Graph, Autoencoder, Reconstruction Loss, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08023v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08023v1.pdf filename=2402.08023v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative <b>self-supervised</b> <b>learning</b> on <b>graphs,</b> <b>particularly</b> <b>graph</b> <b>masked</b> <b>autoencoders,</b> has emerged as a popular learning paradigm and demonstrated its efficacy in handling non-Euclidean data. However, several remaining issues limit the capability of existing methods: 1) the disregard of uneven node significance in masking, 2) the underutilization of holistic <b>graph</b> <b>information,</b> <b>3)</b> the ignorance of semantic knowledge in the representation space due to the exclusive use of <b>reconstruction</b> <b>loss</b> in the output space, and 4) the unstable <b>reconstructions</b> <b>caused</b> by the large volume of masked contents. In light of this, we propose UGMAE, a unified framework for <b>graph</b> <b>masked</b> <b>autoencoders</b> to address these issues from the perspectives of adaptivity, integrity, complementarity, and consistency. Specifically, we first develop an adaptive feature mask generator to account for the unique significance of nodes and sample informative masks (adaptivity). We then design a ranking-based structure <b>reconstruction</b> <b>objective</b> joint with feature <b>reconstruction</b> <b>to</b> capture holistic <b>graph</b> <b>information</b> <b>and</b> emphasize the topological proximity between neighbors (integrity). After that, we present a bootstrapping-based similarity module to encode the high-level semantic knowledge in the representation space, complementary to the low-level <b>reconstruction</b> <b>in</b> the output space (complementarity). Finally, we build a consistency assurance module to provide <b>reconstruction</b> <b>objectives</b> with extra stabilized consistency targets (consistency). Extensive experiments demonstrate that UGMAE outperforms both contrastive and generative state-of-the-art baselines on several tasks across multiple datasets.</p></p class="citation"></blockquote><h3 id=962--9252-active-preference-learning-for-large-language-models-william-muldrew-et-al-2024>(9/62 | 9/252) Active Preference Learning for Large Language Models (William Muldrew et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>William Muldrew, Peter Hayes, Mingtian Zhang, David Barber. (2024)<br><strong>Active Preference Learning for Large Language Models</strong><br><button class=copy-to-clipboard title="Active Preference Learning for Large Language Models" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Active Learning, Fine-tuning, Reinforcement Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08114v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08114v1.pdf filename=2402.08114v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> become more capable, <b>fine-tuning</b> techniques for aligning with human intent are increasingly important. A key consideration for aligning these models is how to most effectively use human resources, or model resources in the case where <b>LLMs</b> themselves are used as oracles. <b>Reinforcement</b> <b>learning</b> from Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique, but is complex and often unstable. Direct Preference Optimization (DPO) has recently been proposed as a simpler and more stable alternative. In this work, we develop an <b>active</b> <b>learning</b> strategy for DPO to make better use of preference labels. We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO. We demonstrate how our approach improves both the rate of learning and final performance of <b>fine-tuning</b> on pairwise preference data.</p></p class="citation"></blockquote><h3 id=1062--10252-empowering-federated-learning-for-massive-models-with-nvidia-flare-holger-r-roth-et-al-2024>(10/62 | 10/252) Empowering Federated Learning for Massive Models with NVIDIA FLARE (Holger R. Roth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Holger R. Roth, Ziyue Xu, Yuan-Ting Hsieh, Adithya Renduchintala, Isaac Yang, Zhihong Zhang, Yuhong Wen, Sean Yang, Kevin Lu, Kristopher Kersten, Camir Ricketts, Daguang Xu, Chester Chen, Yan Cheng, Andrew Feng. (2024)<br><strong>Empowering Federated Learning for Massive Models with NVIDIA FLARE</strong><br><button class=copy-to-clipboard title="Empowering Federated Learning for Massive Models with NVIDIA FLARE" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Federated Learning, Fine-tuning, Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07792v1.pdf filename=2402.07792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the ever-evolving landscape of artificial intelligence (AI) and <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> handling and leveraging data effectively has become a critical challenge. Most state-of-the-art machine learning algorithms are data-centric. However, as the lifeblood of model performance, necessary data cannot always be centralized due to various factors such as privacy, regulation, geopolitics, copyright issues, and the sheer effort required to move vast datasets. In this paper, we explore how <b>federated</b> <b>learning</b> enabled by NVIDIA FLARE can address these challenges with easy and scalable integration capabilities, enabling parameter-efficient and full <b>supervised</b> <b>fine-tuning</b> of <b>LLMs</b> for natural language processing and biopharmaceutical applications to enhance their accuracy and robustness.</p></p class="citation"></blockquote><h3 id=1162--11252-foundational-inference-models-for-dynamical-systems-patrick-seifner-et-al-2024>(11/62 | 11/252) Foundational Inference Models for Dynamical Systems (Patrick Seifner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Patrick Seifner, Kostadin Cvejoski, Ramses J. Sanchez. (2024)<br><strong>Foundational Inference Models for Dynamical Systems</strong><br><button class=copy-to-clipboard title="Foundational Inference Models for Dynamical Systems" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-DS<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, Supervised Learning, Supervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07594v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07594v1.pdf filename=2402.07594v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ordinary differential equations (ODEs) underlie dynamical systems which serve as models for a vast number of natural and social phenomena. Yet inferring the ODE that best describes a set of noisy observations on one such phenomenon can be remarkably challenging, and the models available to achieve it tend to be highly specialized and complex too. In this work we propose a novel <b>supervised</b> <b>learning</b> framework for <b>zero-shot</b> inference of ODEs from noisy data. We first generate large datasets of one-dimensional ODEs, by sampling distributions over the space of initial conditions, and the space of vector fields defining them. We then learn neural maps between noisy observations on the solutions of these equations, and their corresponding initial condition and vector fields. The resulting models, which we call foundational inference models (FIM), can be (i) copied and matched along the time dimension to increase their resolution; and (ii) copied and composed to build inference models of any dimensionality, without the need of any <b>finetuning.</b> We use FIM to model both ground-truth dynamical systems of different dimensionalities and empirical time series data in a <b>zero-shot</b> fashion, and outperform state-of-the-art models which are <b>finetuned</b> to these systems. Our (pretrained) FIMs are available online</p></p class="citation"></blockquote><h3 id=1262--12252-only-the-curve-shape-matters-training-foundation-models-for-zero-shot-multivariate-time-series-forecasting-through-next-curve-shape-prediction-cheng-feng-et-al-2024>(12/62 | 12/252) Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction (Cheng Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Feng, Long Huang, Denis Krompass. (2024)<br><strong>Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction</strong><br><button class=copy-to-clipboard title="Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Foundation Model, Supervised Learning, Zero-shot, Transformer, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07570v1.pdf filename=2402.07570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present General Time <b>Transformer</b> (GTT), an encoder-only style <b>foundation</b> <b>model</b> for <b>zero-shot</b> multivariate time series forecasting. GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse domains. In our proposed framework, the task of multivariate time series forecasting is formulated as a channel-wise next curve shape prediction problem, where each time series sample is represented as a sequence of non-overlapping curve shapes with a unified numerical magnitude. GTT is trained to predict the next curve shape based on a window of past curve shapes in a channel-wise manner. Experimental results demonstrate that GTT exhibits superior <b>zero-shot</b> multivariate forecasting capabilities on unseen time series datasets, even surpassing state-of-the-art <b>supervised</b> baselines. Additionally, we investigate the impact of varying GTT model parameters and training dataset scales, observing that the <b>scaling</b> <b>law</b> also holds in the context of <b>zero-shot</b> multivariate time series forecasting.</p></p class="citation"></blockquote><h3 id=1362--13252-bayesian-federated-learning-via-expectation-maximization-and-turbo-deep-approximate-message-passing-wei-xu-et-al-2024>(13/62 | 13/252) Bayesian Federated Learning Via Expectation Maximization and Turbo Deep Approximate Message Passing (Wei Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Xu, An Liu, Yiting Zhang, Vincent Lau. (2024)<br><strong>Bayesian Federated Learning Via Expectation Maximization and Turbo Deep Approximate Message Passing</strong><br><button class=copy-to-clipboard title="Bayesian Federated Learning Via Expectation Maximization and Turbo Deep Approximate Message Passing" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Message-Passing, Federated Learning, Model Compression, Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07366v1.pdf filename=2402.07366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) is a machine learning paradigm where the clients possess decentralized training data and the central server handles aggregation and scheduling. Typically, FL algorithms involve clients training their local <b>models</b> <b>using</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD),</b> which carries drawbacks such as slow convergence and being prone to getting stuck in suboptimal solutions. In this work, we propose a message passing based Bayesian <b>federated</b> <b>learning</b> (BFL) framework to avoid these drawbacks.Specifically, we formulate the problem of deep neural network (DNN) learning and compression and as a sparse Bayesian inference problem, in which group sparse prior is employed to achieve structured <b>model</b> <b>compression.</b> Then, we propose an efficient BFL algorithm called EMTDAMP, where expectation maximization (EM) and turbo deep approximate message passing (TDAMP) are combined to achieve distributed learning and compression. The central server aggregates local posterior distributions to update global posterior distributions and update hyperparameters based on EM to accelerate convergence. The clients perform TDAMP to achieve efficient approximate message passing over DNN with joint prior distribution. We detail the application of EMTDAMP to Boston housing price prediction and handwriting recognition, and present extensive numerical results to demonstrate the advantages of EMTDAMP.</p></p class="citation"></blockquote><h3 id=1462--14252-universal-link-predictor-by-in-context-learning-on-graphs-kaiwen-dong-et-al-2024>(14/62 | 14/252) Universal Link Predictor By In-Context Learning on Graphs (Kaiwen Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiwen Dong, Haitao Mao, Zhichun Guo, Nitesh V. Chawla. (2024)<br><strong>Universal Link Predictor By In-Context Learning on Graphs</strong><br><button class=copy-to-clipboard title="Universal Link Predictor By In-Context Learning on Graphs" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph, Fine-tuning, In-context Learning, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07738v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07738v2.pdf filename=2402.07738v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Link prediction is a crucial task in <b>graph</b> machine learning, where the goal is to infer missing or future links within a <b>graph.</b> Traditional approaches leverage heuristic methods based on widely observed connectivity patterns, offering broad applicability and generalizability without the need for model training. Despite their utility, these methods are limited by their reliance on human-derived heuristics and lack the adaptability of data-driven approaches. Conversely, parametric link predictors excel in automatically learning the connectivity patterns from data and achieving state-of-the-art but fail short to directly transfer across different <b>graphs.</b> Instead, it requires the cost of extensive training and hyperparameter optimization to adapt to the target <b>graph.</b> In this work, we introduce the Universal Link Predictor (UniLP), a novel model that combines the generalizability of heuristic approaches with the pattern learning capabilities of parametric models. UniLP is designed to autonomously identify connectivity patterns across diverse <b>graphs,</b> ready for immediate application to any unseen <b>graph</b> dataset without targeted training. We address the challenge of conflicting connectivity patterns-arising from the unique distributions of different <b>graphs-through</b> the implementation of <b>In-context</b> <b>Learning</b> <b>(ICL).</b> This approach allows UniLP to dynamically adjust to various target <b>graphs</b> based on contextual demonstrations, thereby avoiding negative transfer. Through rigorous experimentation, we demonstrate UniLP&rsquo;s effectiveness in adapting to new, unseen <b>graphs</b> at test time, showcasing its ability to perform comparably or even outperform parametric models that have been <b>finetuned</b> for specific datasets. Our findings highlight UniLP&rsquo;s potential to set a new standard in link prediction, combining the strengths of heuristic and parametric methods in a single, versatile framework.</p></p class="citation"></blockquote><h3 id=1562--15252-assessing-generalization-for-subpopulation-representative-modeling-via-in-context-learning-gabriel-simmons-et-al-2024>(15/62 | 15/252) Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning (Gabriel Simmons et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriel Simmons, Vladislav Savinov. (2024)<br><strong>Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning</strong><br><button class=copy-to-clipboard title="Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CY, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Benchmarking, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07368v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07368v1.pdf filename=2402.07368v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study evaluates the ability of <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)-based</b> Subpopulation Representative Models (SRMs) to generalize from empirical data, utilizing <b>in-context</b> <b>learning</b> with data from the 2016 and 2020 American National Election Studies. We explore generalization across response variables and demographic subgroups. While conditioning with empirical data improves performance on the whole, the benefit of <b>in-context</b> <b>learning</b> varies considerably across demographics, sometimes hurting performance for one demographic while helping performance for others. The inequitable benefits of <b>in-context</b> <b>learning</b> for SRM present a challenge for practitioners implementing SRMs, and for decision-makers who might come to rely on them. Our work highlights a need for fine-grained <b>benchmarks</b> captured from diverse subpopulations that test not only fidelity but generalization.</p></p class="citation"></blockquote><h3 id=1662--16252-on-the-resurgence-of-recurrent-models-for-long-sequences----survey-and-research-opportunities-in-the-transformer-era-matteo-tiezzi-et-al-2024>(16/62 | 16/252) On the Resurgence of Recurrent Models for Long Sequences &ndash; Survey and Research Opportunities in the Transformer Era (Matteo Tiezzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matteo Tiezzi, Michele Casoni, Alessandro Betti, Tommaso Guidi, Marco Gori, Stefano Melacci. (2024)<br><strong>On the Resurgence of Recurrent Models for Long Sequences &ndash; Survey and Research Opportunities in the Transformer Era</strong><br><button class=copy-to-clipboard title="On the Resurgence of Recurrent Models for Long Sequences -- Survey and Research Opportunities in the Transformer Era" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Recurrent Neural Network, Transformer, Large Language Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08132v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08132v2.pdf filename=2402.08132v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A longstanding challenge for the Machine Learning community is the one of developing models that are capable of processing and learning from very long sequences of data. The outstanding results of <b>Transformers-based</b> networks (e.g., <b>Large</b> <b>Language</b> <b>Models)</b> promotes the idea of parallel attention as the key to succeed in such a challenge, obfuscating the role of classic sequential processing of <b>Recurrent</b> <b>Models.</b> <b>However,</b> in the last few years, researchers who were concerned by the quadratic complexity of <b>self-attention</b> have been proposing a novel wave of neural models, which gets the best from the two worlds, i.e., <b>Transformers</b> and <b>Recurrent</b> <b>Nets.</b> <b>Meanwhile,</b> Deep Space-State Models emerged as robust approaches to function approximation over time, thus opening a new perspective in learning from sequential data, followed by many people in the field and exploited to implement a special class of (linear) <b>Recurrent</b> <b>Neural</b> <b>Networks.</b> This survey is aimed at providing an overview of these trends framed under the unifying umbrella of Recurrence. Moreover, it emphasizes novel research opportunities that become prominent when abandoning the idea of processing long sequences whose length is known-in-advance for the more realistic setting of potentially infinite-length sequences, thus intersecting the field of lifelong-online learning from streamed data.</p></p class="citation"></blockquote><h3 id=1762--17252-policy-improvement-using-language-feedback-models-victor-zhong-et-al-2024>(17/62 | 17/252) Policy Improvement using Language Feedback Models (Victor Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor Zhong, Dipendra Misra, Xingdi Yuan, Marc-Alexandre Côté. (2024)<br><strong>Policy Improvement using Language Feedback Models</strong><br><button class=copy-to-clipboard title="Policy Improvement using Language Feedback Models" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Grounding, Instruction Following, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07876v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07876v2.pdf filename=2402.07876v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the <b>instruction</b> <b>-</b> for imitation learning in <b>instruction</b> <b>following.</b> To train LFMs, we obtain feedback from <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language <b>grounding</b> environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using <b>LLMs</b> as experts to directly predict actions, when controlling for the number of <b>LLM</b> output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.</p></p class="citation"></blockquote><h3 id=1862--18252-fourier-circuits-in-neural-networks-unlocking-the-potential-of-large-language-models-in-mathematical-reasoning-and-modular-arithmetic-jiuxiang-gu-et-al-2024>(18/62 | 18/252) Fourier Circuits in Neural Networks: Unlocking the Potential of Large Language Models in Mathematical Reasoning and Modular Arithmetic (Jiuxiang Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiuxiang Gu, Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Tianyi Zhou. (2024)<br><strong>Fourier Circuits in Neural Networks: Unlocking the Potential of Large Language Models in Mathematical Reasoning and Modular Arithmetic</strong><br><button class=copy-to-clipboard title="Fourier Circuits in Neural Networks: Unlocking the Potential of Large Language Models in Mathematical Reasoning and Modular Arithmetic" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 40<br>Keywords: Transformer, Mathematical Reasoning, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09469v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09469v1.pdf filename=2402.09469v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the evolving landscape of machine learning, a pivotal challenge lies in deciphering the internal representations harnessed by neural networks and <b>Transformers.</b> Building on recent progress toward comprehending how networks execute distinct target functions, our study embarks on an exploration of the underlying reasons behind networks adopting specific computational strategies. We direct our focus to the complex algebraic learning task of modular addition involving $k$ inputs. Our research presents a thorough analytical characterization of the features learned by stylized one-hidden layer neural networks and one-layer <b>Transformers</b> in addressing this task. A cornerstone of our theoretical framework is the elucidation of how the principle of margin maximization shapes the features adopted by one-hidden layer neural networks. Let $p$ denote the modulus, $D_p$ denote the dataset of modular arithmetic with $k$ inputs and $m$ denote the network width. We demonstrate that a neuron count of $ m \geq 2^{2k-2} \cdot (p-1) $, these networks attain a maximum $ L_{2,k+1} $-margin on the dataset $ D_p $. Furthermore, we establish that each hidden-layer neuron aligns with a specific Fourier spectrum, integral to solving modular addition problems. By correlating our findings with the empirical observations of similar studies, we contribute to a deeper comprehension of the intrinsic computational mechanisms of neural networks. Furthermore, we observe similar computational mechanisms in the attention matrix of the <b>Transformer.</b> This research stands as a significant stride in unraveling their operation complexities, particularly in the realm of complex algebraic tasks.</p></p class="citation"></blockquote><h3 id=1962--19252-accuracy-of-textfooler-black-box-adversarial-attacks-on-01-loss-sign-activation-neural-network-ensemble-yunzhe-xue-et-al-2024>(19/62 | 19/252) Accuracy of TextFooler black box adversarial attacks on 01 loss sign activation neural network ensemble (Yunzhe Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunzhe Xue, Usman Roshan. (2024)<br><strong>Accuracy of TextFooler black box adversarial attacks on 01 loss sign activation neural network ensemble</strong><br><button class=copy-to-clipboard title="Accuracy of TextFooler black box adversarial attacks on 01 loss sign activation neural network ensemble" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Text Classification, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07347v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07347v1.pdf filename=2402.07347v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work has shown the defense of 01 loss sign activation neural networks against image classification <b>adversarial</b> <b>attacks.</b> A public challenge to attack the models on CIFAR10 dataset remains undefeated. We ask the following question in this study: are 01 loss sign activation neural networks hard to deceive with a popular black box <b>text</b> <b>adversarial</b> <b>attack</b> program called TextFooler? We study this question on four popular <b>text</b> <b>classification</b> datasets: IMDB reviews, Yelp reviews, MR sentiment classification, and AG news classification. We find that our 01 loss sign activation network is much harder to attack with TextFooler compared to sigmoid activation cross entropy and binary neural networks. We also study a 01 loss sign activation <b>convolutional</b> <b>neural</b> <b>network</b> with a novel global pooling step specific to sign activation networks. With this new variation we see a significant gain in <b>adversarial</b> <b>accuracy</b> rendering TextFooler practically useless against it. We make our code freely available at \url{https://github.com/zero-one-loss/wordcnn01} and \url{https://github.com/xyzacademic/mlp01example}. Our work here suggests that 01 loss sign activation networks could be further developed to create fool proof models against <b>text</b> <b>adversarial</b> <b>attacks.</b></p></p class="citation"></blockquote><h3 id=2062--20252-an-investigation-into-using-unsupervised-metrics-to-optimise-gnns-for-node-clustering-william-leeney-et-al-2024>(20/62 | 20/252) An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering (William Leeney et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>William Leeney, Ryan McConville. (2024)<br><strong>An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering</strong><br><button class=copy-to-clipboard title="An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 36<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Clustering, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07845v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07845v1.pdf filename=2402.07845v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> can be trained to detect communities within a <b>graph</b> <b>by</b> <b>learning</b> from the duality of feature and connectivity information. Currently, the common approach for optimisation of <b>GNNs</b> is to use comparisons to ground-truth for hyperparameter tuning and model selection. In this work, we show that nodes can be clustered into communities with <b>GNNs</b> by solely optimising for modularity, without any comparison to ground-truth. Although modularity is a <b>graph</b> <b>partitioning</b> <b>quality</b> metric, we show that this can be used to optimise <b>GNNs</b> that also encode features without a drop in performance. We take it a step further and also study whether the <b>unsupervised</b> metric performance can predict ground-truth performance. To investigate why modularity can be used to optimise <b>GNNs,</b> we design synthetic experiments that show the limitations of this approach. The synthetic <b>graphs</b> <b>are</b> <b>created</b> to highlight current capabilities in distinct, random and zero information space partitions in attributed <b>graphs.</b> <b>We</b> <b>conclude</b> that modularity can be used for hyperparameter optimisation and model selection on real-world datasets as well as being a suitable proxy for predicting ground-truth performance, however, <b>GNNs</b> fail to balance the information duality when the spaces contain conflicting signals.</p></p class="citation"></blockquote><h3 id=2162--21252-clustertabnet-supervised-clustering-method-for-table-detection-and-table-structure-recognition-marek-polewczyk-et-al-2024>(21/62 | 21/252) ClusterTabNet: Supervised clustering method for table detection and table structure recognition (Marek Polewczyk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marek Polewczyk, Marco Spinaci. (2024)<br><strong>ClusterTabNet: Supervised clustering method for table detection and table structure recognition</strong><br><button class=copy-to-clipboard title="ClusterTabNet: Supervised clustering method for table detection and table structure recognition" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 36<br>Keywords: Optical Character Recognition, Graph, Clustering, Supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07502v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07502v1.pdf filename=2402.07502v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel deep-learning-based method to cluster words in documents which we apply to detect and recognize tables given the <b>OCR</b> output. We interpret table structure bottom-up as a <b>graph</b> of relations between pairs of words (belonging to the same row, column, header, as well as to the same table) and use a <b>transformer</b> encoder model to predict its adjacency matrix. We demonstrate the performance of our method on the PubTables-1M dataset as well as PubTabNet and FinTabNet datasets. Compared to the current state-of-the-art detection methods such as DETR and Faster R-CNN, our method achieves similar or better accuracy, while requiring a significantly smaller model.</p></p class="citation"></blockquote><h3 id=2262--22252-a-competition-winning-deep-reinforcement-learning-agent-in-microrts-scott-goodfriend-2024>(22/62 | 22/252) A Competition Winning Deep Reinforcement Learning Agent in microRTS (Scott Goodfriend, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Scott Goodfriend. (2024)<br><strong>A Competition Winning Deep Reinforcement Learning Agent in microRTS</strong><br><button class=copy-to-clipboard title="A Competition Winning Deep Reinforcement Learning Agent in microRTS" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Reinforcement Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08112v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08112v1.pdf filename=2402.08112v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scripted agents have predominantly won the five previous iterations of the IEEE microRTS ($\mu$RTS) competitions hosted at CIG and CoG. Despite Deep <b>Reinforcement</b> <b>Learning</b> (DRL) algorithms making significant strides in real-time strategy (RTS) games, their adoption in this primarily academic competition has been limited due to the considerable training resources required and the complexity inherent in creating and debugging such agents. RAISocketAI is the first DRL agent to win the IEEE microRTS competition. In a <b>benchmark</b> without performance constraints, RAISocketAI regularly defeated the two prior competition winners. This first competition-winning DRL submission can be a <b>benchmark</b> for future microRTS competitions and a starting point for future DRL research. Iteratively <b>fine-tuning</b> the base policy and <b>transfer</b> <b>learning</b> to specific maps were critical to RAISocketAI&rsquo;s winning performance. These strategies can be used to economically train future DRL agents. Further work in Imitation Learning using Behavior Cloning and <b>fine-tuning</b> these models with DRL has proven promising as an efficient way to bootstrap models with demonstrated, competitive behaviors.</p></p class="citation"></blockquote><h3 id=2362--23252-netinfof-framework-measuring-and-exploiting-network-usable-information-meng-chieh-lee-et-al-2024>(23/62 | 23/252) NetInfoF Framework: Measuring and Exploiting Network Usable Information (Meng-Chieh Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meng-Chieh Lee, Haiyang Yu, Jian Zhang, Vassilis N. Ioannidis, Xiang Song, Soji Adeshina, Da Zheng, Christos Faloutsos. (2024)<br><strong>NetInfoF Framework: Measuring and Exploiting Network Usable Information</strong><br><button class=copy-to-clipboard title="NetInfoF Framework: Measuring and Exploiting Network Usable Information" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 33<br>Keywords: Node Classification, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07999v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07999v1.pdf filename=2402.07999v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a <b>node-attributed</b> <b>graph,</b> <b>and</b> <b>a</b> <b>graph</b> <b>task</b> <b>(link</b> prediction or <b>node</b> <b>classification),</b> can we tell if a <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> will perform well? More specifically, do the <b>graph</b> <b>structure</b> <b>and</b> the <b>node</b> <b>features</b> carry enough usable information for the task? Our goals are (1) to develop a fast tool to measure how much information is in the <b>graph</b> <b>structure</b> <b>and</b> in the <b>node</b> <b>features,</b> and (2) to exploit the information to solve the task, if there is enough. We propose NetInfoF, a framework including NetInfoF_Probe and NetInfoF_Act, for the measurement and the exploitation of network usable information (NUI), respectively. Given a <b>graph</b> <b>data,</b> <b>NetInfoF_Probe</b> measures NUI without any model training, and NetInfoF_Act solves link prediction and <b>node</b> <b>classification,</b> while two modules share the same backbone. In summary, NetInfoF has following notable advantages: (a) General, handling both link prediction and <b>node</b> <b>classification;</b> (b) Principled, with theoretical guarantee and closed-form solution; (c) Effective, thanks to the proposed adjustment to <b>node</b> <b>similarity;</b> (d) Scalable, scaling linearly with the input size. In our carefully designed synthetic datasets, NetInfoF correctly identifies the ground truth of NUI and is the only method being robust to all <b>graph</b> <b>scenarios.</b> <b>Applied</b> on real-world datasets, NetInfoF wins in 11 out of 12 times on link prediction compared to general <b>GNN</b> baselines.</p></p class="citation"></blockquote><h3 id=2462--24252-towards-an-understanding-of-stepwise-inference-in-transformers-a-synthetic-graph-navigation-model-mikail-khona-et-al-2024>(24/62 | 24/252) Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model (Mikail Khona et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mikail Khona, Maya Okawa, Jan Hula, Rahul Ramesh, Kento Nishi, Robert Dick, Ekdeep Singh Lubana, Hidenori Tanaka. (2024)<br><strong>Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model</strong><br><button class=copy-to-clipboard title="Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Transformer, Reasoning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07757v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07757v1.pdf filename=2402.07757v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. Despite the significant gain in performance achieved via these protocols, the underlying mechanisms of stepwise inference have remained elusive. To address this, we propose to study autoregressive <b>Transformer</b> models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a <b>graph</b> navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the <b>graph.</b> Despite is simplicity, we find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference <b>reasoning</b> gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy tradeoff in model generations as sampling temperature varies; (iii) a simplicity bias in the model&rsquo;s output; and (iv) compositional generalization and a primacy bias with <b>in-context</b> exemplars. Overall, our work introduces a grounded, synthetic framework for studying stepwise inference and offers mechanistic hypotheses that can lay the foundation for a deeper understanding of this phenomenon.</p></p class="citation"></blockquote><h3 id=2562--25252-unveiling-group-specific-distributed-concept-drift-a-fairness-imperative-in-federated-learning-teresa-salazar-et-al-2024>(25/62 | 25/252) Unveiling Group-Specific Distributed Concept Drift: A Fairness Imperative in Federated Learning (Teresa Salazar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Teresa Salazar, João Gama, Helder Araújo, Pedro Henriques Abreu. (2024)<br><strong>Unveiling Group-Specific Distributed Concept Drift: A Fairness Imperative in Federated Learning</strong><br><button class=copy-to-clipboard title="Unveiling Group-Specific Distributed Concept Drift: A Fairness Imperative in Federated Learning" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T01, I-2-m, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Clustering, Fairness, Federated Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07586v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07586v1.pdf filename=2402.07586v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the evolving field of machine learning, ensuring <b>fairness</b> has become a critical concern, <b>prompting</b> the development of algorithms designed to mitigate discriminatory outcomes in decision-making processes. However, achieving <b>fairness</b> in the presence of group-specific concept drift remains an unexplored frontier, and our research represents pioneering efforts in this regard. Group-specific concept drift refers to situations where one group experiences concept drift over time while another does not, leading to a decrease in <b>fairness</b> even if accuracy remains fairly stable. Within the framework of <b>federated</b> <b>learning,</b> where clients collaboratively train models, its distributed nature further amplifies these challenges since each client can experience group-specific concept drift independently while still sharing the same underlying concept, creating a complex and dynamic environment for maintaining <b>fairness.</b> One of the significant contributions of our research is the formalization and introduction of the problem of group-specific concept drift and its distributed counterpart, shedding light on its critical importance in the realm of <b>fairness.</b> In addition, leveraging insights from prior research, we adapt an existing distributed concept drift adaptation algorithm to tackle group-specific distributed concept drift which utilizes a multi-model approach, a local group-specific drift detection mechanism, and continuous <b>clustering</b> of models over time. The findings from our experiments highlight the importance of addressing group-specific concept drift and its distributed counterpart to advance <b>fairness</b> in machine learning.</p></p class="citation"></blockquote><h3 id=2662--26252-one-train-for-two-tasks-an-encrypted-traffic-classification-framework-using-supervised-contrastive-learning-haozhen-zhang-et-al-2024>(26/62 | 26/252) One Train for Two Tasks: An Encrypted Traffic Classification Framework Using Supervised Contrastive Learning (Haozhen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haozhen Zhang, Xi Xiao, Le Yu, Qing Li, Zhen Ling, Ye Zhang. (2024)<br><strong>One Train for Two Tasks: An Encrypted Traffic Classification Framework Using Supervised Contrastive Learning</strong><br><button class=copy-to-clipboard title="One Train for Two Tasks: An Encrypted Traffic Classification Framework Using Supervised Contrastive Learning" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Contrastive Learning, Data Augmentation, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07501v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07501v1.pdf filename=2402.07501v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As network security receives widespread attention, encrypted traffic classification has become the current research focus. However, existing methods conduct traffic classification without sufficiently considering the common characteristics between <b>data</b> <b>samples,</b> leading to suboptimal performance. Moreover, they train the packet-level and flow-level classification tasks independently, which is redundant because the packet representations learned in the packet-level task can be exploited by the flow-level task. Therefore, in this paper, we propose an effective model named a <b>Contrastive</b> <b>Learning</b> Enhanced Temporal Fusion Encoder (CLE-TFE). In particular, we utilize <b>supervised</b> <b>contrastive</b> <b>learning</b> to enhance the packet-level and flow-level representations and perform <b>graph</b> <b>data</b> <b>augmentation</b> on the byte-level traffic <b>graph</b> so that the fine-grained semantic-invariant characteristics between bytes can be captured through <b>contrastive</b> <b>learning.</b> We also propose cross-level multi-task learning, which simultaneously accomplishes the packet-level and flow-level classification tasks in the same model with one training. Further experiments show that CLE-TFE achieves the best overall performance on the two tasks, while its computational overhead (i.e., floating point operations, FLOPs) is only about 1/14 of the pre-trained model (e.g., ET-BERT). We release the code at <a href=https://github.com/ViktorAxelsen/CLE-TFE>https://github.com/ViktorAxelsen/CLE-TFE</a></p></p class="citation"></blockquote><h3 id=2762--27252-topological-safeguard-for-evasion-attack-interpreting-the-neural-networks-behavior-xabier-echeberria-barrio-et-al-2024>(27/62 | 27/252) Topological safeguard for evasion attack interpreting the neural networks&rsquo; behavior (Xabier Echeberria-Barrio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xabier Echeberria-Barrio, Amaia Gil-Lerchundi, Iñigo Mendialdua, Raul Orduna-Urrutia. (2024)<br><strong>Topological safeguard for evasion attack interpreting the neural networks&rsquo; behavior</strong><br><button class=copy-to-clipboard title="Topological safeguard for evasion attack interpreting the neural networks' behavior" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07480v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07480v2.pdf filename=2402.07480v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the last years, Deep Learning technology has been proposed in different fields, bringing many advances in each of them, but identifying new threats in these solutions regarding cybersecurity. Those implemented models have brought several vulnerabilities associated with Deep Learning technology. Moreover, those allow taking advantage of the implemented model, obtaining private information, and even modifying the model&rsquo;s decision-making. Therefore, interest in studying those vulnerabilities/attacks and designing defenses to avoid or fight them is gaining prominence among researchers. In particular, the widely known evasion attack is being analyzed by researchers; thus, several defenses to avoid such a threat can be found in the literature. Since the presentation of the L-BFG algorithm, this threat concerns the research community. However, it continues developing new and ingenious countermeasures since there is no perfect defense for all the known evasion algorithms. In this work, a novel detector of evasion attacks is developed. It focuses on the information of the activations of the neurons given by the model when an input sample is injected. Moreover, it puts attention to the topology of the targeted deep learning model to analyze the activations according to which neurons are connecting. This approach has been decided because the literature shows that the targeted model&rsquo;s topology contains essential information about if the evasion attack occurs. For this purpose, a huge data preprocessing is required to introduce all this information in the detector, which uses the <b>Graph</b> <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(GCN)</b> technology. Thus, it understands the topology of the target model, obtaining promising results and improving the outcomes presented in the literature related to similar defenses.</p></p class="citation"></blockquote><h3 id=2862--28252-implicit-bias-of-policy-gradient-in-linear-quadratic-control-extrapolation-to-unseen-initial-states-noam-razin-et-al-2024>(28/62 | 28/252) Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States (Noam Razin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Noam Razin, Yotam Alexander, Edo Cohen-Karlik, Raja Giryes, Amir Globerson, Nadav Cohen. (2024)<br><strong>Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States</strong><br><button class=copy-to-clipboard title="Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SY, cs.LG, eess-SY, stat-ML<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07875v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07875v1.pdf filename=2402.07875v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In modern machine learning, models can often fit training data in numerous ways, some of which perform well on unseen (test) data, while others do not. Remarkably, in such cases gradient descent frequently exhibits an implicit bias that leads to excellent performance on unseen data. This implicit bias was extensively studied in <b>supervised</b> <b>learning,</b> but is far less understood in optimal control <b>(reinforcement</b> <b>learning).</b> There, learning a controller applied to a system via gradient descent is known as policy gradient, and a question of prime importance is the extent to which a learned controller extrapolates to unseen initial states. This paper theoretically studies the implicit bias of policy gradient in terms of extrapolation to unseen initial states. Focusing on the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the extent of extrapolation depends on the degree of exploration induced by the system when commencing from initial states included in training. Experiments corroborate our theory, and demonstrate its conclusions on problems beyond LQR, where systems are non-linear and controllers are neural networks. We hypothesize that real-world optimal control may be greatly improved by developing methods for informed selection of initial states to train on.</p></p class="citation"></blockquote><h3 id=2962--29252-scaling-laws-for-fine-grained-mixture-of-experts-jakub-krajewski-et-al-2024>(29/62 | 29/252) Scaling Laws for Fine-Grained Mixture of Experts (Jakub Krajewski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pióro, Michał Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Król, Tomasz Odrzygóźdź, Piotr Sankowski, Marek Cygan, Sebastian Jaszczur. (2024)<br><strong>Scaling Laws for Fine-Grained Mixture of Experts</strong><br><button class=copy-to-clipboard title="Scaling Laws for Fine-Grained Mixture of Experts" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Transformer, Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07871v1.pdf filename=2402.07871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of <b>Large</b> <b>Language</b> <b>Models.</b> In this work, we analyze their <b>scaling</b> <b>properties,</b> incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish <b>scaling</b> <b>laws</b> for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense <b>Transformers</b> but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.</p></p class="citation"></blockquote><h3 id=3062--30252-lora-drop-efficient-lora-parameter-pruning-based-on-output-evaluation-hongyun-zhou-et-al-2024>(30/62 | 30/252) LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation (Hongyun Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyun Zhou, Xiangyu Lu, Wang Xu, Conghui Zhu, Tiejun Zhao. (2024)<br><strong>LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation</strong><br><button class=copy-to-clipboard title="LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Pruning, Natural Language Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07721v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07721v1.pdf filename=2402.07721v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to <b>fine-tune</b> the pre-trained model under limited computing resources. But it still faces challenges of resource consumption when scaling up to larger models. Previous studies employ <b>pruning</b> techniques by evaluating the importance of LoRA parameters for different layers to address the problem. However, these efforts only analyzed parameter features to evaluate their importance. Indeed, the output of LoRA related to the parameters and data is the factor that directly impacts the frozen model. To this end, we propose LoRA-drop which evaluates the importance of the parameters by analyzing the LoRA output. We retain LoRA for important layers and the LoRA of the other layers share the same parameters. Abundant experiments on NLU and <b>NLG</b> tasks demonstrate the effectiveness of LoRA-drop.</p></p class="citation"></blockquote><h3 id=3162--31252-model-collapse-demystified-the-case-of-regression-elvis-dohmatob-et-al-2024>(31/62 | 31/252) Model Collapse Demystified: The Case of Regression (Elvis Dohmatob et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elvis Dohmatob, Yunzhen Feng, Julia Kempe. (2024)<br><strong>Model Collapse Demystified: The Case of Regression</strong><br><button class=copy-to-clipboard title="Model Collapse Demystified: The Case of Regression" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07712v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07712v1.pdf filename=2402.07712v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the era of <b>large</b> <b>language</b> <b>models</b> like <b>ChatGPT,</b> the phenomenon of &ldquo;model collapse&rdquo; refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses. In this work, we study this phenomenon in the simplified setting of kernel regression and obtain results which show a clear crossover between where the model can cope with fake data, and a regime where the model&rsquo;s performance completely collapses. Under polynomial decaying spectral and source conditions, we obtain modified <b>scaling</b> <b>laws</b> which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.</p></p class="citation"></blockquote><h3 id=3262--32252-optimization-of-sparse-convolution-for-3d-point-cloud-on-gpus-with-cuda-chester-luo-et-al-2024>(32/62 | 32/252) Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA (Chester Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chester Luo, Kevin Lai. (2024)<br><strong>Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA</strong><br><button class=copy-to-clipboard title="Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07710v1.pdf filename=2402.07710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, there has been a significant increase in the utilization of deep learning methods, particularly <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs),</b> which have emerged as the dominant approach in various domains that involve structured grid data, such as picture analysis and processing. Nevertheless, the exponential growth in the utilization of LiDAR and 3D sensors across many domains has resulted in an increased need for the analysis of 3D point clouds. The utilization of 3D point clouds is crucial in various applications, including object recognition and segmentation, as they offer a spatial depiction of things within a three-dimensional environment. In contrast to photos, point clouds exhibit sparsity and lack a regular grid, hence posing distinct processing and computational issues.</p></p class="citation"></blockquote><h3 id=3362--33252-sourcerer-sample-based-maximum-entropy-source-distribution-estimation-julius-vetter-et-al-2024>(33/62 | 33/252) Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation (Julius Vetter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julius Vetter, Guy Moss, Cornelius Schröder, Richard Gao, Jakob H. Macke. (2024)<br><strong>Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation</strong><br><button class=copy-to-clipboard title="Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07808v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07808v1.pdf filename=2402.07808v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scientific modeling applications often require estimating a distribution of parameters consistent with a dataset of observations - an inference task also known as source distribution estimation. This problem can be ill-posed, however, since many different source distributions might produce the same distribution of data-consistent <b>simulations.</b> To make a principled choice among many equally valid sources, we propose an approach which targets the maximum entropy distribution, i.e., prioritizes retaining as much uncertainty as possible. Our method is purely sample-based - leveraging the Sliced-Wasserstein distance to measure the discrepancy between the dataset and <b>simulations</b> - and thus suitable for simulators with intractable likelihoods. We <b>benchmark</b> our method on several tasks, and show that it can recover source distributions with substantially higher entropy without sacrificing the fidelity of the <b>simulations.</b> Finally, to demonstrate the utility of our approach, we infer source distributions for parameters of the Hodgkin-Huxley neuron model from experimental datasets with thousands of measurements. In summary, we propose a principled framework for inferring unique source distributions of scientific simulator parameters while retaining as much uncertainty as possible.</p></p class="citation"></blockquote><h3 id=3462--34252-weisfeiler-leman-at-the-margin-when-more-expressivity-matters-billy-j-franks-et-al-2024>(34/62 | 34/252) Weisfeiler-Leman at the margin: When more expressivity matters (Billy J. Franks et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Billy J. Franks, Christopher Morris, Ameya Velingker, Floris Geerts. (2024)<br><strong>Weisfeiler-Leman at the margin: When more expressivity matters</strong><br><button class=copy-to-clipboard title="Weisfeiler-Leman at the margin: When more expressivity matters" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DM, cs-LG, cs-NE, cs.LG, stat-ML<br>Keyword Score: 23<br>Keywords: Message-Passing, Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07568v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07568v1.pdf filename=2402.07568v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Weisfeiler-Leman algorithm ($1$-WL) is a well-studied heuristic for the <b>graph</b> <b>isomorphism</b> <b>problem.</b> Recently, the algorithm has played a prominent role in understanding the expressive power of <b>message-passing</b> <b>graph</b> <b>neural</b> <b>networks</b> (MPNNs) and being effective as a <b>graph</b> <b>kernel.</b> <b>Despite</b> its success, $1$-WL faces challenges in distinguishing non-isomorphic <b>graphs,</b> <b>leading</b> <b>to</b> the development of more expressive MPNN and kernel architectures. However, the relationship between enhanced expressivity and improved generalization performance remains unclear. Here, we show that an architecture&rsquo;s expressivity offers limited insights into its generalization performance when viewed through <b>graph</b> <b>isomorphism.</b> <b>Moreover,</b> we focus on augmenting $1$-WL and MPNNs with subgraph information and employ classical margin theory to investigate the conditions under which an architecture&rsquo;s increased expressivity aligns with improved generalization performance. In addition, we show that gradient flow pushes the MPNN&rsquo;s weights toward the maximum margin solution. Further, we introduce variations of expressive $1$-WL-based kernel and MPNN architectures with provable generalization properties. Our empirical study confirms the validity of our theoretical findings.</p></p class="citation"></blockquote><h3 id=3562--35252-understanding-deep-learning-defenses-against-adversarial-examples-through-visualizations-for-dynamic-risk-assessment-xabier-echeberria-barrio-et-al-2024>(35/62 | 35/252) Understanding Deep Learning defenses Against Adversarial Examples Through Visualizations for Dynamic Risk Assessment (Xabier Echeberria-Barrio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xabier Echeberria-Barrio, Amaia Gil-Lerchundi, Jon Egana-Zubia, Raul Orduna-Urrutia. (2024)<br><strong>Understanding Deep Learning defenses Against Adversarial Examples Through Visualizations for Dynamic Risk Assessment</strong><br><button class=copy-to-clipboard title="Understanding Deep Learning defenses Against Adversarial Examples Through Visualizations for Dynamic Risk Assessment" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Adversarial Learning, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07496v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07496v1.pdf filename=2402.07496v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, Deep Neural Network models have been developed in different fields, where they have brought many advances. However, they have also started to be used in tasks where risk is critical. A misdiagnosis of these models can lead to serious accidents or even death. This concern has led to an interest among researchers to study possible attacks on these models, discovering a long list of vulnerabilities, from which every model should be defended. The <b>adversarial</b> <b>example</b> attack is a widely known attack among researchers, who have developed several defenses to avoid such a threat. However, these defenses are as opaque as a deep neural network model, how they work is still unknown. This is why visualizing how they change the behavior of the target model is interesting in order to understand more precisely how the performance of the defended model is being modified. For this work, some defenses, against <b>adversarial</b> <b>example</b> attack, have been selected in order to visualize the behavior modification of each of them in the defended model. <b>Adversarial</b> <b>training,</b> dimensionality reduction and prediction similarity were the selected defenses, which have been developed using a model composed by <b>convolution</b> neural network layers and dense neural network layers. In each defense, the behavior of the original model has been compared with the behavior of the defended model, representing the target model by a <b>graph</b> in a visualization.</p></p class="citation"></blockquote><h3 id=3662--36252-contextual-multinomial-logit-bandits-with-general-value-functions-mengxiao-zhang-et-al-2024>(36/62 | 36/252) Contextual Multinomial Logit Bandits with General Value Functions (Mengxiao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengxiao Zhang, Haipeng Luo. (2024)<br><strong>Contextual Multinomial Logit Bandits with General Value Functions</strong><br><button class=copy-to-clipboard title="Contextual Multinomial Logit Bandits with General Value Functions" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08126v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08126v1.pdf filename=2402.08126v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contextual multinomial logit (MNL) <b>bandits</b> capture many real-world assortment <b>recommendation</b> problems such as online retailing/advertising. However, prior work has only considered (generalized) linear value functions, which greatly limits its applicability. Motivated by this fact, in this work, we consider contextual MNL <b>bandits</b> with a general value function class that contains the ground truth, borrowing ideas from a recent trend of studies on contextual <b>bandits.</b> Specifically, we consider both the stochastic and the adversarial settings, and propose a suite of algorithms, each with different computation-regret trade-off. When applied to the linear case, our results not only are the first ones with no dependence on a certain problem-dependent constant that can be exponentially large, but also enjoy other advantages such as computational efficiency, dimension-free regret bounds, or the ability to handle completely adversarial contexts and rewards.</p></p class="citation"></blockquote><h3 id=3762--37252-which-pretrain-samples-to-rehearse-when-finetuning-pretrained-models-andrew-bai-et-al-2024>(37/62 | 37/252) Which Pretrain Samples to Rehearse when Finetuning Pretrained Models? (Andrew Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Bai, Chih-Kuan Yeh, Cho-Jui Hsieh, Ankur Taly. (2024)<br><strong>Which Pretrain Samples to Rehearse when Finetuning Pretrained Models?</strong><br><button class=copy-to-clipboard title="Which Pretrain Samples to Rehearse when Finetuning Pretrained Models?" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fine-tuning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08096v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08096v1.pdf filename=2402.08096v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> pretrained foundational models on specific tasks is now the de facto approach for text and vision tasks. A known pitfall of this approach is the forgetting of pretraining knowledge that happens during <b>finetuning.</b> Rehearsing samples randomly from the pretrain dataset is a common approach to alleviate such forgetting. However, we find that random mixing unintentionally includes samples which are not (yet) forgotten or unlearnable by the model. We propose a novel sampling scheme, mix-cd, that identifies and prioritizes samples that actually face forgetting, which we call collateral damage. Since directly identifying collateral damage samples is computationally expensive, we propose a procedure to estimate the distribution of such samples by tracking the statistics of <b>finetuned</b> samples. Our approach is lightweight, easy to implement, and can be seamlessly integrated into existing models, offering an effective means to retain pretrain performance without additional computational costs.</p></p class="citation"></blockquote><h3 id=3862--38252-avoiding-catastrophe-in-continuous-spaces-by-asking-for-help-benjamin-plaut-et-al-2024>(38/62 | 38/252) Avoiding Catastrophe in Continuous Spaces by Asking for Help (Benjamin Plaut et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Plaut, Hanlin Zhu, Stuart Russell. (2024)<br><strong>Avoiding Catastrophe in Continuous Spaces by Asking for Help</strong><br><button class=copy-to-clipboard title="Avoiding Catastrophe in Continuous Spaces by Asking for Help" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08062v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08062v1.pdf filename=2402.08062v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most <b>reinforcement</b> <b>learning</b> algorithms with formal regret guarantees assume all mistakes are reversible and rely on essentially trying all possible options. This approach leads to poor outcomes when some mistakes are irreparable or even catastrophic. We propose a variant of the contextual <b>bandit</b> problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff each round represents the chance of avoiding catastrophe that round, and try to maximize the product of payoffs (the overall chance of avoiding catastrophe). To give the agent some chance of success, we allow a limited number of queries to a mentor and assume a Lipschitz continuous payoff function. We present an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows, assuming a continuous 1D state space and a relatively &ldquo;simple&rdquo; payoff function. We also provide a matching lower bound: without the simplicity assumption: any algorithm either constantly asks for help or is nearly guaranteed to cause catastrophe. Finally, we identify the key obstacle to generalizing our algorithm to a multi-dimensional state space.</p></p class="citation"></blockquote><h3 id=3962--39252-which-frequencies-do-cnns-need-emergent-bottleneck-structure-in-feature-learning-yuxiao-wen-et-al-2024>(39/62 | 39/252) Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning (Yuxiao Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxiao Wen, Arthur Jacot. (2024)<br><strong>Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning</strong><br><button class=copy-to-clipboard title="Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08010v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08010v1.pdf filename=2402.08010v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We describe the emergence of a <b>Convolution</b> Bottleneck (CBN) structure in <b>CNNs,</b> where the network uses its first few layers to transform the input representation into a representation that is supported only along a few frequencies and channels, before using the last few layers to map back to the outputs. We define the CBN rank, which describes the number and type of frequencies that are kept inside the bottleneck, and partially prove that the parameter norm required to represent a function $f$ scales as depth times the CBN rank $f$. We also show that the parameter norm depends at next order on the regularity of $f$. We show that any network with almost optimal parameter norm will exhibit a CBN structure in both the weights and - under the assumption that the network is stable under large learning rate - the activations, which motivates the common practice of down-sampling; and we verify that the CBN results still hold with down-sampling. Finally we use the CBN structure to interpret the functions learned by <b>CNNs</b> on a number of tasks.</p></p class="citation"></blockquote><h3 id=4062--40252-fast-factorizable-attention-for-speeding-up-transformers-armin-gerami-et-al-2024>(40/62 | 40/252) FAST: Factorizable Attention for Speeding up Transformers (Armin Gerami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Armin Gerami, Monte Hoover, Pranav S. Dulepet, Ramani Duraiswami. (2024)<br><strong>FAST: Factorizable Attention for Speeding up Transformers</strong><br><button class=copy-to-clipboard title="FAST: Factorizable Attention for Speeding up Transformers" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07901v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07901v1.pdf filename=2402.07901v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by the factorization inherent in the original fast multipole method and the improved fast Gauss transform we introduce a factorable form of attention that operates efficiently in high dimensions. This approach reduces the computational and memory complexity of the attention mechanism in <b>transformers</b> from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our work presents a linearly scaled attention mechanism that maintains the full representation of the attention matrix without compromising on sparsification and incorporates the all-to-all relationship between tokens. We explore the properties of our new attention metric and conduct tests in various standard settings. Results indicate that our attention mechanism has a robust performance and holds significant promise for diverse applications where <b>self-attention</b> is used.</p></p class="citation"></blockquote><h3 id=4162--41252-comparing-skill-of-historical-rainfall-data-based-monsoon-rainfall-prediction-in-india-with-ncep-nwp-forecasts-apoorva-narula-et-al-2024>(41/62 | 41/252) Comparing skill of historical rainfall data based monsoon rainfall prediction in India with NCEP-NWP forecasts (Apoorva Narula et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Apoorva Narula, Aastha Jain, Jatin Batra, Sandeep Juneja. (2024)<br><strong>Comparing skill of historical rainfall data based monsoon rainfall prediction in India with NCEP-NWP forecasts</strong><br><button class=copy-to-clipboard title="Comparing skill of historical rainfall data based monsoon rainfall prediction in India with NCEP-NWP forecasts" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: LSTM, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07851v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07851v1.pdf filename=2402.07851v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this draft we consider the problem of forecasting rainfall across India during the four monsoon months, one day as well as three days in advance. We train neural networks using historical daily gridded precipitation data for India obtained from IMD for the time period $1901- 2022$, at a spatial resolution of $1^{\circ} \times 1^{\circ}$. This is compared with the numerical weather prediction (NWP) forecasts obtained from NCEP (National Centre for Environmental Prediction) available for the period 2011-2022. We conduct a detailed country wide analysis and separately analyze some of the most populated cities in India. Our conclusion is that forecasts obtained by applying deep learning to historical rainfall data are more accurate compared to NWP forecasts as well as predictions based on persistence. On average, compared to our predictions, forecasts from NCEP-NWP model have about 34% higher error for a single day prediction, and over 68% higher error for a three day prediction. Similarly, persistence estimates report a 29% higher error in a single day forecast, and over 54% error in a three day forecast. We further observe that data up to 20 days in the past is useful in reducing errors of one and three day forecasts, when a <b>transformer</b> based learning architecture, and to a lesser extent when an <b>LSTM</b> is used. A key conclusion suggested by our preliminary analysis is that NWP forecasts can be substantially improved upon through more and diverse data relevant to monsoon prediction combined with carefully selected neural network architecture.</p></p class="citation"></blockquote><h3 id=4262--42252-tighter-bounds-on-the-information-bottleneck-with-application-to-deep-learning-nir-weingarten-et-al-2024>(42/62 | 42/252) Tighter Bounds on the Information Bottleneck with Application to Deep Learning (Nir Weingarten et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nir Weingarten, Zohar Yakhini, Moshe Butman, Ran Gilad-Bachrach. (2024)<br><strong>Tighter Bounds on the Information Bottleneck with Application to Deep Learning</strong><br><button class=copy-to-clipboard title="Tighter Bounds on the Information Bottleneck with Application to Deep Learning" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 94A08, 94A10, 94A11, 68T06, 62B04, 62B08, I-2; E-4; I-4; I-7, cs-AI, cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 20<br>Keywords: Mutual Information, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07639v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07639v1.pdf filename=2402.07639v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Neural Nets (DNNs) learn latent representations induced by their downstream task, objective function, and other parameters. The quality of the learned representations impacts the DNN&rsquo;s generalization ability and the coherence of the emerging latent space. The Information Bottleneck (IB) provides a hypothetically optimal framework for data modeling, yet it is often intractable. Recent efforts combined DNNs with the IB by applying VAE-inspired variational methods to approximate bounds on <b>mutual</b> <b>information,</b> resulting in improved robustness to <b>adversarial</b> <b>attacks.</b> This work introduces a new and tighter variational bound for the IB, improving performance of previous IB-inspired DNNs. These advancements strengthen the case for the IB and its variational approximations as a data modeling framework, and provide a simple method to significantly enhance the <b>adversarial</b> <b>robustness</b> of classifier DNNs.</p></p class="citation"></blockquote><h3 id=4362--43252-near-minimax-optimal-distributional-reinforcement-learning-with-a-generative-model-mark-rowland-et-al-2024>(43/62 | 43/252) Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model (Mark Rowland et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mark Rowland, Li Kevin Wenliang, Rémi Munos, Clare Lyle, Yunhao Tang, Will Dabney. (2024)<br><strong>Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model</strong><br><button class=copy-to-clipboard title="Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Distributional Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07598v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07598v1.pdf filename=2402.07598v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a new algorithm for model-based <b>distributional</b> <b>reinforcement</b> <b>learning</b> (RL), and prove that it is minimax-optimal for approximating return distributions with a generative model (up to logarithmic factors), resolving an open question of Zhang et al. (2023). Our analysis provides new theoretical results on categorical approaches to <b>distributional</b> <b>RL,</b> <b>and</b> also introduces a new <b>distributional</b> <b>Bellman</b> <b>equation,</b> the stochastic categorical CDF Bellman equation, which we expect to be of independent interest. We also provide an experimental study comparing several model-based <b>distributional</b> <b>RL</b> <b>algorithms,</b> with several takeaways for practitioners.</p></p class="citation"></blockquote><h3 id=4462--44252-rolling-diffusion-models-david-ruhe-et-al-2024>(44/62 | 44/252) Rolling Diffusion Models (David Ruhe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Ruhe, Jonathan Heek, Tim Salimans, Emiel Hoogeboom. (2024)<br><strong>Rolling Diffusion Models</strong><br><button class=copy-to-clipboard title="Rolling Diffusion Models" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09470v1.pdf filename=2402.09470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffusion models have recently been increasingly applied to temporal data such as video, fluid mechanics <b>simulations,</b> or climate data. These methods generally treat subsequent frames equally regarding the amount of noise in the diffusion process. This paper explores Rolling Diffusion: a new approach that uses a sliding window denoising process. It ensures that the diffusion process progressively corrupts through time by assigning more noise to frames that appear later in a sequence, reflecting greater uncertainty about the future as the generation process unfolds. Empirically, we show that when the temporal dynamics are complex, Rolling Diffusion is superior to standard diffusion. In particular, this result is demonstrated in a video prediction task using the Kinetics-600 video dataset and in a chaotic fluid dynamics forecasting experiment.</p></p class="citation"></blockquote><h3 id=4562--45252-score-based-physics-informed-neural-networks-for-high-dimensional-fokker-planck-equations-zheyuan-hu-et-al-2024>(45/62 | 45/252) Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck Equations (Zheyuan Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheyuan Hu, Zhongqiang Zhang, George Em Karniadakis, Kenji Kawaguchi. (2024)<br><strong>Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck Equations</strong><br><button class=copy-to-clipboard title="Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck Equations" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 14J60, cs-AI, cs-LG, cs-NA, cs.LG, math-DS, math-NA, stat-ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07465v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07465v1.pdf filename=2402.07465v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Fokker-Planck (FP) equation is a foundational PDE in stochastic processes. However, curse of dimensionality (CoD) poses challenge when dealing with high-dimensional FP PDEs. Although Monte Carlo and vanilla Physics-Informed Neural Networks (PINNs) have shown the potential to tackle CoD, both methods exhibit numerical errors in high dimensions when dealing with the probability density function (PDF) associated with Brownian motion. The point-wise PDF values tend to decrease exponentially as dimension increases, surpassing the precision of numerical <b>simulations</b> and resulting in substantial errors. Moreover, due to its massive sampling, Monte Carlo fails to offer fast sampling. Modeling the logarithm likelihood (LL) via vanilla PINNs transforms the FP equation into a difficult HJB equation, whose error grows rapidly with dimension. To this end, we propose a novel approach utilizing a score-based solver to fit the score function in SDEs. The score function, defined as the gradient of the LL, plays a fundamental role in inferring LL and PDF and enables fast SDE sampling. Three fitting methods, Score Matching (SM), Sliced SM (SSM), and Score-PINN, are introduced. The proposed score-based SDE solver operates in two stages: first, employing SM, SSM, or Score-PINN to acquire the score; and second, solving the LL via an ODE using the obtained score. Comparative evaluations across these methods showcase varying trade-offs. The proposed method is evaluated across diverse SDEs, including anisotropic OU processes, geometric Brownian, and Brownian with varying eigenspace. We also test various distributions, including Gaussian, Log-normal, Laplace, and Cauchy. The numerical results demonstrate the score-based SDE solver&rsquo;s stability, speed, and performance across different settings, solidifying its potential as a solution to CoD for high-dimensional FP equations.</p></p class="citation"></blockquote><h3 id=4662--46252-the-io-complexity-of-attention-or-how-optimal-is-flash-attention-barna-saha-et-al-2024>(46/62 | 46/252) The I/O Complexity of Attention, or How Optimal is Flash Attention? (Barna Saha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Barna Saha, Christopher Ye. (2024)<br><strong>The I/O Complexity of Attention, or How Optimal is Flash Attention?</strong><br><button class=copy-to-clipboard title="The I/O Complexity of Attention, or How Optimal is Flash Attention?" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CC, cs-DS, cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07443v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07443v1.pdf filename=2402.07443v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-attention</b> is at the heart of the popular <b>Transformer</b> architecture, yet suffers from quadratic time and memory complexity. The breakthrough FlashAttention algorithm revealed I/O complexity as the true bottleneck in scaling <b>Transformers.</b> Given two levels of memory hierarchy, a fast cache (e.g. GPU on-chip SRAM) and a slow memory (e.g. GPU high-bandwidth memory), the I/O complexity measures the number of accesses to memory. FlashAttention computes attention using $\frac{N^2d^2}{M}$ I/O operations where $N$ is the dimension of the attention matrix, $d$ the head-dimension and $M$ the cache size. However, is this I/O complexity optimal? The known lower bound only rules out an I/O complexity of $o(Nd)$ when $M=\Theta(Nd)$, since the output that needs to be written to slow memory is $\Omega(Nd)$. This leads to the main question of our work: Is FlashAttention I/O optimal for all values of $M$? We resolve the above question in its full generality by showing an I/O complexity lower bound that matches the upper bound provided by FlashAttention for any values of $M \geq d^2$ within any constant factors. Further, we give a better algorithm with lower I/O complexity for $M &lt; d^2$, and show that it is optimal as well. Moreover, our lower bounds do not rely on using combinatorial matrix multiplication for computing the attention matrix. We show even if one uses fast matrix multiplication, the above I/O complexity bounds cannot be improved. We do so by introducing a new communication complexity protocol for matrix compression, and connecting communication complexity to I/O complexity. To the best of our knowledge, this is the first work to establish a connection between communication complexity and I/O complexity, and we believe this connection could be of independent interest and will find many more applications in proving I/O complexity lower bounds in the future.</p></p class="citation"></blockquote><h3 id=4762--47252-measurement-scheduling-for-icu-patients-with-offline-reinforcement-learning-zongliang-ji-et-al-2024>(47/62 | 47/252) Measurement Scheduling for ICU Patients with Offline Reinforcement Learning (Zongliang Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zongliang Ji, Anna Goldenberg, Rahul G. Krishnan. (2024)<br><strong>Measurement Scheduling for ICU Patients with Offline Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Measurement Scheduling for ICU Patients with Offline Reinforcement Learning" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07344v1.pdf filename=2402.07344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scheduling laboratory tests for ICU patients presents a significant challenge. Studies show that 20-40% of lab tests ordered in the ICU are redundant and could be eliminated without compromising patient safety. Prior work has leveraged <b>offline</b> <b>reinforcement</b> <b>learning</b> <b>(Offline-RL)</b> <b>to</b> <b>find</b> optimal policies for ordering lab tests based on patient information. However, new ICU patient datasets have since been released, and various advancements have been made in <b>Offline-RL</b> <b>methods.</b> <b>In</b> this study, we first introduce a preprocessing pipeline for the newly-released MIMIC-IV dataset geared toward time-series tasks. We then explore the efficacy of state-of-the-art <b>Offline-RL</b> <b>methods</b> <b>in</b> identifying better policies for ICU patient lab test scheduling. Besides assessing methodological performance, we also discuss the overall suitability and practicality of using <b>Offline-RL</b> <b>frameworks</b> <b>for</b> scheduling laboratory tests in ICU settings.</p></p class="citation"></blockquote><h3 id=4862--48252-efficient-contextual-bandits-with-uninformed-feedback-graphs-mengxiao-zhang-et-al-2024>(48/62 | 48/252) Efficient Contextual Bandits with Uninformed Feedback Graphs (Mengxiao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengxiao Zhang, Yuheng Zhang, Haipeng Luo, Paul Mineiro. (2024)<br><strong>Efficient Contextual Bandits with Uninformed Feedback Graphs</strong><br><button class=copy-to-clipboard title="Efficient Contextual Bandits with Uninformed Feedback Graphs" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08127v1.pdf filename=2402.08127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Bandits</b> with feedback <b>graphs</b> are powerful online learning models that interpolate between the full information and classic <b>bandit</b> problems, capturing many real-life applications. A recent work by Zhang et al. (2023) studies the contextual version of this problem and proposes an efficient and optimal algorithm via a reduction to online regression. However, their algorithm crucially relies on seeing the feedback <b>graph</b> before making each decision, while in many applications, the feedback <b>graph</b> is uninformed, meaning that it is either only revealed after the learner makes her decision or even never fully revealed at all. This work develops the first contextual algorithm for such uninformed settings, via an efficient reduction to online regression over both the losses and the <b>graphs.</b> Importantly, we show that it is critical to learn the <b>graphs</b> using log loss instead of squared loss to obtain favorable regret guarantees. We also demonstrate the empirical effectiveness of our algorithm on a bidding application using both synthetic and real-world data.</p></p class="citation"></blockquote><h3 id=4962--49252-hypo-hyperspherical-out-of-distribution-generalization-haoyue-bai-et-al-2024>(49/62 | 49/252) HYPO: Hyperspherical Out-of-Distribution Generalization (Haoyue Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyue Bai, Yifei Ming, Julian Katz-Samuels, Yixuan Li. (2024)<br><strong>HYPO: Hyperspherical Out-of-Distribution Generalization</strong><br><button class=copy-to-clipboard title="HYPO: Hyperspherical Out-of-Distribution Generalization" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07785v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07785v1.pdf filename=2402.07785v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Out-of-distribution</b> (OOD) generalization is critical for machine learning models deployed in the real world. However, achieving this can be fundamentally challenging, as it requires the ability to learn invariant features across different domains or environments. In this paper, we propose a novel framework HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant representations in a hyperspherical space. In particular, our hyperspherical learning algorithm is guided by intra-class variation and inter-class separation principles &ndash; ensuring that features from the same class (across different training domains) are closely aligned with their class prototypes, while different class prototypes are maximally separated. We further provide theoretical justifications on how our prototypical learning objective improves the OOD generalization bound. Through extensive experiments on challenging OOD <b>benchmarks,</b> we demonstrate that our approach outperforms competitive baselines and achieves superior performance. Code is available at <a href=https://github.com/deeplearning-wisc/hypo>https://github.com/deeplearning-wisc/hypo</a>.</p></p class="citation"></blockquote><h3 id=5062--50252-differentially-private-decentralized-learning-with-random-walks-edwige-cyffers-et-al-2024>(50/62 | 50/252) Differentially Private Decentralized Learning with Random Walks (Edwige Cyffers et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Edwige Cyffers, Aurélien Bellet, Jalaj Upadhyay. (2024)<br><strong>Differentially Private Decentralized Learning with Random Walks</strong><br><button class=copy-to-clipboard title="Differentially Private Decentralized Learning with Random Walks" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07471v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07471v1.pdf filename=2402.07471v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The popularity of <b>federated</b> <b>learning</b> comes from the possibility of better scalability and the ability for participants to keep control of their data, improving data security and sovereignty. Unfortunately, sharing model updates also creates a new privacy attack surface. In this work, we characterize the privacy guarantees of decentralized learning with random walk algorithms, where a model is updated by traveling from one node to another along the edges of a communication <b>graph.</b> Using a recent variant of differential privacy tailored to the study of decentralized algorithms, namely Pairwise Network Differential Privacy, we derive closed-form expressions for the privacy loss between each pair of nodes where the impact of the communication topology is captured by <b>graph</b> theoretic quantities. Our results further reveal that random walk algorithms tends to yield better privacy guarantees than gossip algorithms for nodes close from each other. We supplement our theoretical results with empirical evaluation on synthetic and real-world <b>graphs</b> and datasets.</p></p class="citation"></blockquote><h3 id=5162--51252-conditional-generative-models-are-sufficient-to-sample-from-any-causal-effect-estimand-md-musfiqur-rahman-et-al-2024>(51/62 | 51/252) Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand (Md Musfiqur Rahman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Musfiqur Rahman, Matt Jordan, Murat Kocaoglu. (2024)<br><strong>Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand</strong><br><button class=copy-to-clipboard title="Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ME, stat-ML<br>Keyword Score: 13<br>Keywords: MNIST, Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07419v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07419v1.pdf filename=2402.07419v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Causal inference from observational data has recently found many applications in machine learning. While sound and complete algorithms exist to compute causal effects, many of these algorithms require explicit access to conditional likelihoods over the observational distribution, which is difficult to estimate in the high-dimensional regime, such as with images. To alleviate this issue, researchers have approached the problem by simulating causal relations with neural models and obtained impressive results. However, none of these existing approaches can be applied to generic scenarios such as causal <b>graphs</b> on image data with latent confounders, or obtain conditional interventional samples. In this paper, we show that any identifiable causal effect given an arbitrary causal <b>graph</b> can be computed through push-forward computations of conditional generative models. Based on this result, we devise a diffusion-based approach to sample from any (conditional) interventional distribution on image data. To showcase our algorithm&rsquo;s performance, we conduct experiments on a Colored <b>MNIST</b> dataset having both the treatment ($X$) and the target variables ($Y$) as images and obtain interventional samples from $P(y|do(x))$. As an application of our algorithm, we evaluate two large conditional generative models that are pre-trained on the CelebA dataset by analyzing the strength of spurious correlations and the level of disentanglement they achieve.</p></p class="citation"></blockquote><h3 id=5262--52252-random-geometric-graph-alignment-with-graph-neural-networks-suqi-liu-et-al-2024>(52/62 | 52/252) Random Geometric Graph Alignment with Graph Neural Networks (Suqi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suqi Liu, Morgane Austern. (2024)<br><strong>Random Geometric Graph Alignment with Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Random Geometric Graph Alignment with Graph Neural Networks" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IT, cs-LG, cs-SI, cs.LG, math-IT, math-PR, math-ST, stat-ML, stat-TH<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07340v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07340v1.pdf filename=2402.07340v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We characterize the performance of <b>graph</b> <b>neural</b> <b>networks</b> for <b>graph</b> <b>alignment</b> <b>problems</b> in the presence of vertex feature information. More specifically, given two <b>graphs</b> <b>that</b> <b>are</b> independent perturbations of a single random geometric <b>graph</b> <b>with</b> <b>noisy</b> sparse features, the task is to recover an unknown one-to-one mapping between the vertices of the two <b>graphs.</b> <b>We</b> <b>show</b> under certain conditions on the sparsity and noise level of the feature vectors, a carefully designed one-layer <b>graph</b> <b>neural</b> <b>network</b> can with high probability recover the correct alignment between the vertices with the help of the <b>graph</b> <b>structure.</b> <b>We</b> also prove that our conditions on the noise level are tight up to logarithmic factors. Finally we compare the performance of the <b>graph</b> <b>neural</b> <b>network</b> to directly solving an assignment problem on the noisy vertex features. We demonstrate that when the noise level is at least constant this direct matching fails to have perfect recovery while the <b>graph</b> <b>neural</b> <b>network</b> can tolerate noise level growing as fast as a power of the size of the <b>graph.</b></p></p class="citation"></blockquote><h3 id=5362--53252-leveraging-digital-cousins-for-ensemble-q-learning-in-large-scale-wireless-networks-talha-bozkus-et-al-2024>(53/62 | 53/252) Leveraging Digital Cousins for Ensemble Q-Learning in Large-Scale Wireless Networks (Talha Bozkus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Talha Bozkus, Urbashi Mitra. (2024)<br><strong>Leveraging Digital Cousins for Ensemble Q-Learning in Large-Scale Wireless Networks</strong><br><button class=copy-to-clipboard title="Leveraging Digital Cousins for Ensemble Q-Learning in Large-Scale Wireless Networks" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NI, cs.LG, eess-SP<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08022v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08022v1.pdf filename=2402.08022v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optimizing large-scale wireless networks, including optimal resource management, power allocation, and throughput maximization, is inherently challenging due to their non-observable system dynamics and heterogeneous and complex nature. Herein, a novel ensemble Q-learning algorithm that addresses the performance and complexity challenges of the traditional Q-learning algorithm for optimizing wireless networks is presented. Ensemble learning with synthetic Markov Decision Processes is tailored to wireless networks via new models for approximating large state-space observable wireless networks. In particular, digital cousins are proposed as an extension of the traditional digital twin concept wherein multiple Q-learning algorithms on multiple synthetic Markovian environments are run in parallel and their outputs are fused into a single Q-function. Convergence analyses of key statistics and Q-functions and derivations of upper bounds on the estimation bias and variance are provided. Numerical results across a variety of real-world wireless networks show that the proposed algorithm can achieve up to 50% less average policy error with up to 40% less runtime complexity than the state-of-the-art <b>reinforcement</b> <b>learning</b> algorithms. It is also shown that theoretical results properly predict trends in the experimental results.</p></p class="citation"></blockquote><h3 id=5462--54252-one-for-many-counterfactual-explanations-by-column-generation-andrea-lodi-et-al-2024>(54/62 | 54/252) One-for-many Counterfactual Explanations by Column Generation (Andrea Lodi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Lodi, Jasone Ramírez-Ayerbe. (2024)<br><strong>One-for-many Counterfactual Explanations by Column Generation</strong><br><button class=copy-to-clipboard title="One-for-many Counterfactual Explanations by Column Generation" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09473v1.pdf filename=2402.09473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider the problem of generating a set of <b>counterfactual</b> explanations for a group of instances, with the one-for-many allocation rule, where one explanation is allocated to a subgroup of the instances. For the first time, we solve the problem of minimizing the number of explanations needed to explain all the instances, while considering sparsity by limiting the number of features allowed to be changed collectively in each explanation. A novel column generation framework is developed to efficiently search for the explanations. Our framework can be applied to any black-box classifier, like neural networks. Compared with a simple adaptation of a mixed-integer programming formulation from the literature, the column generation framework dominates in terms of scalability, computational performance and quality of the solutions.</p></p class="citation"></blockquote><h3 id=5562--55252-accelerated-smoothing-a-scalable-approach-to-randomized-smoothing-devansh-bhardwaj-et-al-2024>(55/62 | 55/252) Accelerated Smoothing: A Scalable Approach to Randomized Smoothing (Devansh Bhardwaj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Devansh Bhardwaj, Kshitiz Kaushik, Sarthak Gupta. (2024)<br><strong>Accelerated Smoothing: A Scalable Approach to Randomized Smoothing</strong><br><button class=copy-to-clipboard title="Accelerated Smoothing: A Scalable Approach to Randomized Smoothing" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07498v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07498v1.pdf filename=2402.07498v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Randomized smoothing has emerged as a potent certifiable defense against <b>adversarial</b> <b>attacks</b> by employing smoothing noises from specific distributions to ensure the robustness of a smoothed classifier. However, the utilization of Monte Carlo sampling in this process introduces a compute-intensive element, which constrains the practicality of randomized smoothing on a larger scale. To address this limitation, we propose a novel approach that replaces Monte Carlo sampling with the training of a surrogate neural network. Through extensive experimentation in various settings, we demonstrate the efficacy of our approach in approximating the smoothed classifier with remarkable precision. Furthermore, we demonstrate that our approach significantly accelerates the robust radius certification process, providing nearly $600$X improvement in computation time, overcoming the computational bottlenecks associated with traditional randomized smoothing.</p></p class="citation"></blockquote><h3 id=5662--56252-score-based-diffusion-models-via-stochastic-differential-equations----a-technical-tutorial-wenpin-tang-et-al-2024>(56/62 | 56/252) Score-based Diffusion Models via Stochastic Differential Equations &ndash; a Technical Tutorial (Wenpin Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenpin Tang, Hanyang Zhao. (2024)<br><strong>Score-based Diffusion Models via Stochastic Differential Equations &ndash; a Technical Tutorial</strong><br><button class=copy-to-clipboard title="Score-based Diffusion Models via Stochastic Differential Equations -- a Technical Tutorial" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-HO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07487v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07487v1.pdf filename=2402.07487v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This is an expository article on the score-based diffusion models, with a particular focus on the formulation via stochastic differential equations (SDE). After a gentle introduction, we discuss the two pillars in the diffusion modeling &ndash; sampling and score matching, which encompass the SDE/ODE sampling, score matching efficiency, the consistency model, and <b>reinforcement</b> <b>learning.</b> Short proofs are given to illustrate the main idea of the stated results. The article is primarily for introducing the beginners to the field, and practitioners may also find some analysis useful in designing new models or algorithms.</p></p class="citation"></blockquote><h3 id=5762--57252-bandit-feedback-online-multiclass-classification-variants-and-tradeoffs-yuval-filmus-et-al-2024>(57/62 | 57/252) Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs (Yuval Filmus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuval Filmus, Steve Hanneke, Idan Mehalel, Shay Moran. (2024)<br><strong>Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs</strong><br><button class=copy-to-clipboard title="Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07453v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07453v1.pdf filename=2402.07453v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Consider the domain of multiclass classification within the adversarial online setting. What is the price of relying on <b>bandit</b> feedback as opposed to full information? To what extent can an adaptive adversary amplify the loss compared to an oblivious one? To what extent can a randomized learner reduce the loss compared to a deterministic one? We study these questions in the mistake bound model and provide nearly tight answers. We demonstrate that the optimal mistake bound under <b>bandit</b> feedback is at most $O(k)$ times higher than the optimal mistake bound in the full information case, where $k$ represents the number of labels. This bound is tight and provides an answer to an open question previously posed and studied by Daniely and Helbertal [&lsquo;13] and by Long [&lsquo;17, &lsquo;20], who focused on deterministic learners. Moreover, we present nearly optimal bounds of $\tilde{\Theta}(k)$ on the gap between randomized and deterministic learners, as well as between adaptive and oblivious adversaries in the <b>bandit</b> feedback setting. This stands in contrast to the full information scenario, where adaptive and oblivious adversaries are equivalent, and the gap in mistake bounds between randomized and deterministic learners is a constant multiplicative factor of $2$. In addition, our results imply that in some cases the optimal randomized mistake bound is approximately the square-root of its deterministic parallel. Previous results show that this is essentially the smallest it can get.</p></p class="citation"></blockquote><h3 id=5862--58252-context-aware-multi-model-object-detection-for-diversely-heterogeneous-compute-systems-justin-davis-et-al-2024>(58/62 | 58/252) Context-aware Multi-Model Object Detection for Diversely Heterogeneous Compute Systems (Justin Davis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Justin Davis, Mehmet E. Belviranli. (2024)<br><strong>Context-aware Multi-Model Object Detection for Diversely Heterogeneous Compute Systems</strong><br><button class=copy-to-clipboard title="Context-aware Multi-Model Object Detection for Diversely Heterogeneous Compute Systems" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs-RO, cs.LG<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07415v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07415v1.pdf filename=2402.07415v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, deep neural networks (DNNs) have gained widespread adoption for continuous mobile <b>object</b> <b>detection</b> (OD) tasks, particularly in autonomous systems. However, a prevalent issue in their deployment is the one-size-fits-all approach, where a single DNN is used, resulting in inefficient utilization of computational resources. This inefficiency is particularly detrimental in energy-constrained systems, as it degrades overall system efficiency. We identify that, the contextual information embedded in the input data stream (e.g. the frames in the camera feed that the OD models are run on) could be exploited to allow a more efficient multi-model-based OD process. In this paper, we propose SHIFT which continuously selects from a variety of DNN-based OD models depending on the dynamically changing contextual information and computational constraints. During this selection, SHIFT uniquely considers multi-accelerator execution to better optimize the energy-efficiency while satisfying the latency constraints. Our proposed methodology results in improvements of up to 7.5x in energy usage and 2.8x in latency compared to state-of-the-art GPU-based single model OD approaches.</p></p class="citation"></blockquote><h3 id=5962--59252-auxiliary-reward-generation-with-transition-distance-representation-learning-siyuan-li-et-al-2024>(59/62 | 59/252) Auxiliary Reward Generation with Transition Distance Representation Learning (Siyuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyuan Li, Shijie Han, Yingnan Zhao, By Liang, Peng Liu. (2024)<br><strong>Auxiliary Reward Generation with Transition Distance Representation Learning</strong><br><button class=copy-to-clipboard title="Auxiliary Reward Generation with Transition Distance Representation Learning" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07412v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07412v1.pdf filename=2402.07412v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL) has shown its strength in challenging sequential decision-making problems. The reward function in RL is crucial to the learning performance, as it serves as a measure of the task completion degree. In real-world problems, the rewards are predominantly human-designed, which requires laborious tuning, and is easily affected by human cognitive biases. To achieve automatic auxiliary reward generation, we propose a novel representation learning approach that can measure the ``transition distance&rsquo;&rsquo; between states. Building upon these representations, we introduce an auxiliary reward generation technique for both single-task and skill-chaining scenarios without the need for human knowledge. The proposed approach is evaluated in a wide range of manipulation tasks. The experiment results demonstrate the effectiveness of measuring the transition distance between states and the induced improvement by auxiliary rewards, which not only promotes better learning efficiency but also increases convergent stability.</p></p class="citation"></blockquote><h3 id=6062--60252-data-distribution-based-curriculum-learning-shonal-chaudhry-et-al-2024>(60/62 | 60/252) Data Distribution-based Curriculum Learning (Shonal Chaudhry et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shonal Chaudhry, Anuraganand Sharma. (2024)<br><strong>Data Distribution-based Curriculum Learning</strong><br><button class=copy-to-clipboard title="Data Distribution-based Curriculum Learning" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Curriculum Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07352v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07352v1.pdf filename=2402.07352v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The order of training samples can have a significant impact on the performance of a classifier. <b>Curriculum</b> <b>learning</b> is a method of ordering training samples from easy to hard. This paper proposes the novel idea of a <b>curriculum</b> <b>learning</b> approach called Data Distribution-based <b>Curriculum</b> <b>Learning</b> (DDCL). DDCL uses the data distribution of a dataset to build a <b>curriculum</b> <b>based</b> on the order of samples. Two types of scoring methods known as DDCL (Density) and DDCL (Point) are used to score training samples thus determining their training order. DDCL (Density) uses the sample density to assign scores while DDCL (Point) utilises the Euclidean distance for scoring. We evaluate the proposed DDCL approach by conducting experiments on multiple datasets using a neural network, support vector machine and random forest classifier. Evaluation results show that the application of DDCL improves the average classification accuracy for all datasets compared to standard evaluation without any <b>curriculum.</b> <b>Moreover,</b> analysis of the error losses for a single training epoch reveals that convergence is faster when using DDCL over the no <b>curriculum</b> <b>method.</b></p></p class="citation"></blockquote><h3 id=6162--61252-learning-cartesian-product-graphs-with-laplacian-constraints-changhao-shi-et-al-2024>(61/62 | 61/252) Learning Cartesian Product Graphs with Laplacian Constraints (Changhao Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changhao Shi, Gal Mishne. (2024)<br><strong>Learning Cartesian Product Graphs with Laplacian Constraints</strong><br><button class=copy-to-clipboard title="Learning Cartesian Product Graphs with Laplacian Constraints" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08105v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08105v1.pdf filename=2402.08105v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> Laplacian learning, also known as network topology inference, is a problem of great interest to multiple communities. In Gaussian graphical models (GM), <b>graph</b> learning amounts to endowing covariance selection with the Laplacian structure. In <b>graph</b> signal processing (GSP), it is essential to infer the unobserved <b>graph</b> from the outputs of a filtering system. In this paper, we study the problem of learning Cartesian product <b>graphs</b> under Laplacian constraints. The Cartesian <b>graph</b> product is a natural way for modeling higher-order conditional dependencies and is also the key for generalizing GSP to multi-way tensors. We establish statistical consistency for the penalized maximum likelihood estimation (MLE) of a Cartesian product Laplacian, and propose an efficient algorithm to solve the problem. We also extend our method for efficient joint <b>graph</b> learning and imputation in the presence of structural missing values. Experiments on synthetic and real-world datasets demonstrate that our method is superior to previous GSP and GM methods.</p></p class="citation"></blockquote><h3 id=6262--62252-boundary-exploration-for-bayesian-optimization-with-unknown-physical-constraints-yunsheng-tian-et-al-2024>(62/62 | 62/252) Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints (Yunsheng Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunsheng Tian, Ane Zuniga, Xinwei Zhang, Johannes P. Dürholt, Payel Das, Jie Chen, Wojciech Matusik, Mina Konaković Luković. (2024)<br><strong>Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints</strong><br><button class=copy-to-clipboard title="Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07692v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07692v1.pdf filename=2402.07692v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bayesian optimization has been successfully applied to optimize black-box functions where the number of evaluations is severely limited. However, in many real-world applications, it is hard or impossible to know in advance which designs are feasible due to some physical or system limitations. These issues lead to an even more challenging problem of optimizing an unknown function with unknown constraints. In this paper, we observe that in such scenarios optimal solution typically lies on the boundary between feasible and infeasible regions of the design space, making it considerably more difficult than that with interior optima. Inspired by this observation, we propose BE-CBO, a new Bayesian optimization method that efficiently explores the boundary between feasible and infeasible designs. To identify the boundary, we learn the constraints with an ensemble of neural networks that outperform the standard Gaussian Processes for capturing complex boundaries. Our method demonstrates superior performance against state-of-the-art methods through comprehensive experiments on synthetic and real-world <b>benchmarks.</b></p></p class="citation"></blockquote><h2 id=cscl-39>cs.CL (39)</h2><h3 id=139--63252-addressing-cognitive-bias-in-medical-language-models-samuel-schmidgall-et-al-2024>(1/39 | 63/252) Addressing cognitive bias in medical language models (Samuel Schmidgall et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Schmidgall, Carl Harris, Ime Essien, Daniel Olshvang, Tawsifur Rahman, Ji Woong Kim, Rojin Ziaei, Jason Eshraghian, Peter Abadir, Rama Chellappa. (2024)<br><strong>Addressing cognitive bias in medical language models</strong><br><button class=copy-to-clipboard title="Addressing cognitive bias in medical language models" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs.CL<br>Keyword Score: 103<br>Keywords: Benchmarking, Simulation, Simulator, GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, PaLM, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08113v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08113v2.pdf filename=2402.08113v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> into the medical field has gained significant attention due to their promising accuracy in simulated clinical decision-making settings. However, clinical decision-making is more complex than <b>simulations</b> because physicians&rsquo; decisions are shaped by many factors, including the presence of cognitive bias. However, the degree to which <b>LLMs</b> are susceptible to the same cognitive biases that affect human clinicians remains unexplored. Our hypothesis posits that when <b>LLMs</b> are confronted with clinical questions containing cognitive biases, they will yield significantly less accurate responses compared to the same questions presented without such biases. In this study, we developed BiasMedQA, a novel <b>benchmark</b> for evaluating cognitive biases in <b>LLMs</b> applied to medical tasks. Using BiasMedQA we evaluated six <b>LLMs,</b> namely <b>GPT-4,</b> Mixtral-8x70B, <b>GPT-3.5,</b> <b>PaLM-2,</b> <b>Llama</b> 2 70B-chat, and the medically specialized PMC <b>Llama</b> 13B. We tested these models on 1,273 questions from the US Medical Licensing Exam (USMLE) Steps 1, 2, and 3, modified to replicate common clinically-relevant cognitive biases. Our analysis revealed varying effects for biases on these <b>LLMs,</b> with <b>GPT-4</b> standing out for its resilience to bias, in contrast to <b>Llama</b> 2 70B-chat and PMC <b>Llama</b> 13B, which were disproportionately affected by cognitive bias. Our findings highlight the critical need for bias mitigation in the development of medical <b>LLMs,</b> pointing towards safer and more reliable applications in healthcare.</p></p class="citation"></blockquote><h3 id=239--64252-large-language-models-ad-referendum-how-good-are-they-at-machine-translation-in-the-legal-domain-vicent-briva-iglesias-et-al-2024>(2/39 | 64/252) Large Language Models &lsquo;Ad Referendum&rsquo;: How Good Are They at Machine Translation in the Legal Domain? (Vicent Briva-Iglesias et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vicent Briva-Iglesias, Joao Lucas Cavalheiro Camargo, Gokhan Dogru. (2024)<br><strong>Large Language Models &lsquo;Ad Referendum&rsquo;: How Good Are They at Machine Translation in the Legal Domain?</strong><br><button class=copy-to-clipboard title="Large Language Models 'Ad Referendum': How Good Are They at Machine Translation in the Legal Domain?" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Automatic Evaluation, GPT, GPT-4, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07681v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07681v1.pdf filename=2402.07681v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study evaluates the <b>machine</b> <b>translation</b> <b>(MT)</b> quality of two state-of-the-art <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> against a tradition-al <b>neural</b> <b>machine</b> <b>translation</b> <b>(NMT)</b> system across four language pairs in the legal domain. It combines <b>automatic</b> <b>evaluation</b> met-rics (AEMs) and human evaluation (HE) by professional transla-tors to assess translation ranking, fluency and adequacy. The re-sults indicate that while Google Translate generally outperforms <b>LLMs</b> in AEMs, human evaluators rate <b>LLMs,</b> especially <b>GPT-4,</b> comparably or slightly better in terms of producing contextually adequate and fluent translations. This discrepancy suggests <b>LLMs&rsquo;</b> potential in handling specialized legal terminology and context, highlighting the importance of human evaluation methods in assessing <b>MT</b> quality. The study underscores the evolving capabil-ities of <b>LLMs</b> in specialized domains and calls for reevaluation of traditional AEMs to better capture the nuances of <b>LLM-generated</b> translations.</p></p class="citation"></blockquote><h3 id=339--65252-suppressing-pink-elephants-with-direct-principle-feedback-louis-castricato-et-al-2024>(3/39 | 65/252) Suppressing Pink Elephants with Direct Principle Feedback (Louis Castricato et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Louis Castricato, Nathan Lile, Suraj Anand, Hailey Schoelkopf, Siddharth Verma, Stella Biderman. (2024)<br><strong>Suppressing Pink Elephants with Direct Principle Feedback</strong><br><button class=copy-to-clipboard title="Suppressing Pink Elephants with Direct Principle Feedback" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Fine-tuning, Reinforcement Learning from Human Feedback, GPT, GPT-4, LLaMA, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07896v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07896v2.pdf filename=2402.07896v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing methods for controlling language models, such as <b>RLHF</b> and Constitutional AI, involve determining which <b>LLM</b> behaviors are desirable and training them into a language model. However, in many cases, it is desirable for <b>LLMs</b> to be controllable at inference time, so that they can be used in multiple contexts with diverse needs. We illustrate this with the Pink Elephant Problem: instructing an <b>LLM</b> to avoid discussing a certain entity (a <code>Pink Elephant''), and instead discuss a preferred entity (</code>Grey Elephant&rsquo;&rsquo;). We apply a novel simplification of Constitutional AI, Direct Principle Feedback, which skips the ranking of responses and uses DPO directly on critiques and revisions. Our results show that after DPF <b>fine-tuning</b> on our synthetic Pink Elephants dataset, our 13B <b>fine-tuned</b> <b>LLaMA</b> 2 model significantly outperforms <b>Llama-2-13B-Chat</b> and a <b>prompted</b> baseline, and performs as well as <b>GPT-4</b> in on our curated test set assessing the Pink Elephant Problem.</p></p class="citation"></blockquote><h3 id=439--66252-lissard-long-and-simple-sequential-reasoning-datasets-mirelle-bueno-et-al-2024>(4/39 | 66/252) Lissard: Long and Simple Sequential Reasoning Datasets (Mirelle Bueno et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mirelle Bueno, Roberto Lotufo, Rodrigo Nogueira. (2024)<br><strong>Lissard: Long and Simple Sequential Reasoning Datasets</strong><br><button class=copy-to-clipboard title="Lissard: Long and Simple Sequential Reasoning Datasets" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, GPT, GPT-3, GPT-3.5, GPT-4, Mistral, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07859v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07859v1.pdf filename=2402.07859v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training. For example, state-of-the-art <b>LLMs</b> can find common items in two lists with up to 20 items but fail when lists have 80 items. In this paper, we introduce Lissard, a <b>benchmark</b> comprising seven tasks whose goal is to assess the ability of models to process and generate wide-range sequence lengths, requiring repetitive procedural execution. Our evaluation of open-source <b>(Mistral-7B</b> and Mixtral-8x7B) and proprietary models <b>(GPT-3.5</b> and <b>GPT-4)</b> show a consistent decline in performance across all models as the complexity of the sequence increases. The datasets and code are available at <a href=https://github.com/unicamp-dl/Lissard>https://github.com/unicamp-dl/Lissard</a></p></p class="citation"></blockquote><h3 id=539--67252-investigating-the-impact-of-data-contamination-of-large-language-models-in-text-to-sql-translation-federico-ranaldi-et-al-2024>(5/39 | 67/252) Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation (Federico Ranaldi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Federico Ranaldi, Elena Sofia Ruzzetti, Dario Onorati, Leonardo Ranaldi, Cristina Giannone, Andrea Favalli, Raniero Romagnoli, Fabio Massimo Zanzotto. (2024)<br><strong>Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation</strong><br><button class=copy-to-clipboard title="Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Zero-shot, GPT, GPT-3, GPT-3.5, Instruction Following, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08100v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08100v1.pdf filename=2402.08100v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding textual description to generate code seems to be an achieved capability of <b>instruction-following</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in <b>zero-shot</b> scenario. However, there is a severe possibility that this translation ability may be influenced by having seen target textual descriptions and the related code. This effect is known as Data Contamination. In this study, we investigate the impact of Data Contamination on the performance of <b>GPT-3.5</b> in the Text-to-SQL code-generating tasks. Hence, we introduce a novel method to detect Data Contamination in <b>GPTs</b> and examine <b>GPT-3.5&rsquo;s</b> Text-to-SQL performances using the known Spider Dataset and our new unfamiliar dataset Termite. Furthermore, we analyze <b>GPT-3.5&rsquo;s</b> efficacy on databases with modified information via an adversarial table disconnection (ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of information from the database. Our results indicate a significant performance drop in <b>GPT-3.5</b> on the unfamiliar Termite dataset, even with ATD modifications, highlighting the effect of Data Contamination on <b>LLMs</b> in Text-to-SQL translation tasks.</p></p class="citation"></blockquote><h3 id=639--68252-the-sound-of-healthcare-improving-medical-transcription-asr-accuracy-with-large-language-models-ayo-adedeji-et-al-2024>(6/39 | 68/252) The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models (Ayo Adedeji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayo Adedeji, Sarita Joshi, Brendan Doohan. (2024)<br><strong>The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models</strong><br><button class=copy-to-clipboard title="The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 70<br>Keywords: Zero-shot, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07658v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07658v1.pdf filename=2402.07658v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving landscape of medical documentation, transcribing clinical dialogues accurately is increasingly paramount. This study explores the potential of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to enhance the accuracy of <b>Automatic</b> <b>Speech</b> <b>Recognition</b> <b>(ASR)</b> systems in medical transcription. Utilizing the PriMock57 dataset, which encompasses a diverse range of primary care consultations, we apply advanced <b>LLMs</b> to refine <b>ASR-generated</b> transcripts. Our research is multifaceted, focusing on improvements in general Word Error Rate (WER), Medical Concept WER (MC-WER) for the accurate transcription of essential medical terms, and speaker diarization accuracy. Additionally, we assess the role of <b>LLM</b> post-processing in improving semantic textual similarity, thereby preserving the contextual integrity of clinical dialogues. Through a series of experiments, we compare the efficacy of <b>zero-shot</b> and Chain-of-Thought (CoT) <b>prompting</b> techniques in enhancing diarization and correction accuracy. Our findings demonstrate that <b>LLMs,</b> particularly through CoT <b>prompting,</b> not only improve the diarization accuracy of existing <b>ASR</b> systems but also achieve state-of-the-art performance in this domain. This improvement extends to more accurately capturing medical concepts and enhancing the overall semantic coherence of the transcribed dialogues. These findings illustrate the dual role of <b>LLMs</b> in augmenting <b>ASR</b> outputs and independently excelling in transcription tasks, holding significant promise for transforming medical <b>ASR</b> systems and leading to more accurate and reliable patient records in healthcare settings.</p></p class="citation"></blockquote><h3 id=739--69252-can-llms-produce-faithful-explanations-for-fact-checking-towards-faithful-explainable-fact-checking-via-multi-agent-debate-kyungha-kim-et-al-2024>(7/39 | 69/252) Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate (Kyungha Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyungha Kim, Sangyun Lee, Kung-Hsiang Huang, Hou Pong Chan, Manling Li, Heng Ji. (2024)<br><strong>Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate</strong><br><button class=copy-to-clipboard title="Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Zero-shot, Fact Verification, Natural Language Explanation, Text Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07401v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07401v1.pdf filename=2402.07401v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fact-checking</b> <b>research</b> has extensively explored verification but less so the generation of <b>natural-language</b> <b>explanations,</b> <b>crucial</b> for user trust. While <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> excel in <b>text</b> <b>generation,</b> their capability for producing faithful explanations in <b>fact-checking</b> <b>remains</b> underexamined. Our study investigates <b>LLMs&rsquo;</b> ability to generate such explanations, finding that <b>zero-shot</b> <b>prompts</b> often result in unfaithfulness. To address these challenges, we propose the Multi-Agent Debate Refinement (MADR) framework, leveraging multiple <b>LLMs</b> as agents with diverse roles in an iterative refining process aimed at enhancing faithfulness in generated explanations. MADR ensures that the final explanation undergoes rigorous validation, significantly reducing the likelihood of unfaithful elements and aligning closely with the provided evidence. Experimental results demonstrate that MADR significantly improves the faithfulness of <b>LLM-generated</b> explanations to the evidence, advancing the credibility and trustworthiness of these explanations.</p></p class="citation"></blockquote><h3 id=839--70252-dólares-or-dollars-unraveling-the-bilingual-prowess-of-financial-llms-between-spanish-and-english-xiao-zhang-et-al-2024>(8/39 | 70/252) Dólares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English (Xiao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Zhang, Ruoyu Xiang, Chenhan Yuan, Duanyu Feng, Weiguang Han, Alejandro Lopez-Lira, Xiao-Yang Liu, Sophia Ananiadou, Min Peng, Jimin Huang, Qianqian Xie. (2024)<br><strong>Dólares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English</strong><br><button class=copy-to-clipboard title="Dólares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, GPT, GPT-4, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07405v1.pdf filename=2402.07405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite Spanish&rsquo;s pivotal role in the global finance industry, a pronounced gap exists in Spanish financial natural language processing (NLP) and application studies compared to English, especially in the era of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> To bridge this gap, we unveil Tois'on de Oro, the first bilingual framework that establishes <b>instruction</b> <b>datasets,</b> <b>finetuned</b> <b>LLMs,</b> and evaluation <b>benchmark</b> for financial <b>LLMs</b> in Spanish joint with English. We construct a rigorously curated bilingual <b>instruction</b> <b>dataset</b> including over 144K Spanish and English samples from 15 datasets covering 7 tasks. Harnessing this, we introduce FinMA-ES, an <b>LLM</b> designed for bilingual financial applications. We evaluate our model and existing <b>LLMs</b> using FLARE-ES, the first comprehensive bilingual evaluation <b>benchmark</b> with 21 datasets covering 9 tasks. The FLARE-ES <b>benchmark</b> results reveal a significant multilingual performance gap and bias in existing <b>LLMs.</b> FinMA-ES models surpass SOTA <b>LLMs</b> such as <b>GPT-4</b> in Spanish financial tasks, due to strategic <b>instruction</b> <b>tuning</b> and leveraging data from diverse linguistic resources, highlighting the positive impact of cross-linguistic transfer. All our datasets, models, and <b>benchmarks</b> have been released.</p></p class="citation"></blockquote><h3 id=939--71252-chain-of-layer-iteratively-prompting-large-language-models-for-taxonomy-induction-from-limited-examples-qingkai-zeng-et-al-2024>(9/39 | 71/252) Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy Induction from Limited Examples (Qingkai Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingkai Zeng, Yuyang Bai, Zhaoxuan Tan, Shangbin Feng, Zhenwen Liang, Zhihan Zhang, Meng Jiang. (2024)<br><strong>Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy Induction from Limited Examples</strong><br><button class=copy-to-clipboard title="Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy Induction from Limited Examples" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Recommendation, Question Answering, In-context Learning, In-context Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07386v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07386v1.pdf filename=2402.07386v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic taxonomy induction is crucial for web search, <b>recommendation</b> systems, and <b>question</b> <b>answering.</b> Manual curation of taxonomies is expensive in terms of human effort, making automatic taxonomy construction highly desirable. In this work, we introduce Chain-of-Layer which is an <b>in-context</b> <b>learning</b> framework designed to induct taxonomies from a given set of entities. Chain-of-Layer breaks down the task into selecting relevant candidate entities in each layer and gradually building the taxonomy from top to bottom. To minimize errors, we introduce the Ensemble-based Ranking Filter to reduce the hallucinated content generated at each iteration. Through extensive experiments, we demonstrate that Chain-of-Layer achieves state-of-the-art performance on four real-world <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1039--72252-enhancing-amharic-llama-integrating-task-specific-and-generative-datasets-israel-abebe-azime-et-al-2024>(10/39 | 72/252) Enhancing Amharic-LLaMA: Integrating Task Specific and Generative Datasets (Israel Abebe Azime et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Israel Abebe Azime, Mitiku Yohannes Fuge, Atnafu Lambebo Tonja, Tadesse Destaw Belay, Aman Kassahun Wassie, Eyasu Shiferaw Jada, Yonas Chanie, Walelign Tewabe Sewunetie, Seid Muhie Yimam. (2024)<br><strong>Enhancing Amharic-LLaMA: Integrating Task Specific and Generative Datasets</strong><br><button class=copy-to-clipboard title="Enhancing Amharic-LLaMA: Integrating Task Specific and Generative Datasets" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Fine-tuning, Low-Resource, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08015v1.pdf filename=2402.08015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have received a lot of attention in natural language processing (NLP) research because of their exceptional performance in understanding and generating human languages. However, <b>low-resource</b> languages are left behind due to the unavailability of resources. In this work, we focus on enhancing the <b>LLaMA-2-Amharic</b> model by integrating task-specific and generative datasets to improve language model performance for Amharic. We compile an Amharic instruction <b>fine-tuning</b> dataset and <b>fine-tuned</b> <b>LLaMA-2-Amharic</b> model. The <b>fine-tuned</b> model shows promising results in different NLP tasks. We open-source our dataset creation pipeline, instruction datasets, trained models, and evaluation outputs to promote language-specific studies on these models.</p></p class="citation"></blockquote><h3 id=1139--73252-injecting-wiktionary-to-improve-token-level-contextual-representations-using-contrastive-learning-anna-mosolova-et-al-2024>(11/39 | 73/252) Injecting Wiktionary to improve token-level contextual representations using contrastive learning (Anna Mosolova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna Mosolova, Marie Candito, Carlos Ramisch. (2024)<br><strong>Injecting Wiktionary to improve token-level contextual representations using contrastive learning</strong><br><button class=copy-to-clipboard title="Injecting Wiktionary to improve token-level contextual representations using contrastive learning" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Contrastive Learning, Fine-tuning, Unsupervised Learning, Pre-trained Language Model, Pre-trained Language Model, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07817v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07817v1.pdf filename=2402.07817v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While static <b>word</b> <b>embeddings</b> are blind to context, for lexical semantics tasks context is rather too present in contextual <b>word</b> <b>embeddings,</b> vectors of same-meaning occurrences being too different (Ethayarajh, 2019). <b>Fine-tuning</b> <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> using <b>contrastive</b> <b>learning</b> was proposed, leveraging automatically self-augmented examples (Liu et al., 2021b). In this paper, we investigate how to inject a lexicon as an alternative source of supervision, using the English Wiktionary. We also test how dimensionality reduction impacts the resulting contextual <b>word</b> <b>embeddings.</b> We evaluate our approach on the <b>Word-In-Context</b> <b>(WiC)</b> task, in the <b>unsupervised</b> setting (not using the training set). We achieve new SoTA result on the original WiC test set. We also propose two new WiC test sets for which we show that our <b>fine-tuning</b> method achieves substantial improvements. We also observe improvements, although modest, for the semantic frame induction task. Although we experimented on English to allow comparison with related work, our method is adaptable to the many languages for which large Wiktionaries exist.</p></p class="citation"></blockquote><h3 id=1239--74252-automathtext-autonomous-data-selection-with-language-models-for-mathematical-texts-yifan-zhang-et-al-2024>(12/39 | 74/252) AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts (Yifan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Zhang, Yifan Luo, Yang Yuan, Andrew Chi-Chih Yao. (2024)<br><strong>AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts</strong><br><button class=copy-to-clipboard title="AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Supervised Learning, Zero-shot, Mistral, Mathematical Reasoning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07625v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07625v1.pdf filename=2402.07625v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To improve language models&rsquo; proficiency in <b>mathematical</b> <b>reasoning</b> via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional <b>supervised</b> <b>fine-tuning</b> or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as <b>zero-shot</b> verifiers to autonomously evaluate and select high-quality <b>mathematical</b> <b>content,</b> and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter <b>Mistral</b> language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing models&rsquo; <b>mathematical</b> <b>reasoning</b> capabilities. The AutoMathText dataset is available at <a href=https://huggingface.co/datasets/math-ai/AutoMathText>https://huggingface.co/datasets/math-ai/AutoMathText</a>. The code is available at <a href=https://github.com/yifanzhang-pro/AutoMathText>https://github.com/yifanzhang-pro/AutoMathText</a>.</p></p class="citation"></blockquote><h3 id=1339--75252-anchor-based-large-language-models-jianhui-pang-et-al-2024>(13/39 | 75/252) Anchor-based Large Language Models (Jianhui Pang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianhui Pang, Fanghua Ye, Derek F. Wong, Longyue Wang. (2024)<br><strong>Anchor-based Large Language Models</strong><br><button class=copy-to-clipboard title="Anchor-based Large Language Models" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Transformer, Question Answering, Large Language Model, Large Language Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07616v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07616v2.pdf filename=2402.07616v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> predominantly employ decoder-only <b>transformer</b> architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these <b>LLMs</b> require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces Anchor-based <b>LLMs</b> (AnLLMs), which utilize an innovative anchor-based <b>self-attention</b> network (AnSAN) and also an anchor-based inference strategy. This approach enables <b>LLMs</b> to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments on <b>question-answering</b> <b>benchmarks</b> reveal that AnLLMs maintain similar accuracy levels while achieving up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the substantial enhancements of AnLLMs employing the AnSAN technique in resource utilization and computational efficiency underscore their potential for practical <b>LLM</b> applications.</p></p class="citation"></blockquote><h3 id=1439--76252-pushing-the-limit-of-llm-capacity-for-text-classification-yazhou-zhang-et-al-2024>(14/39 | 76/252) Pushing The Limit of LLM Capacity for Text Classification (Yazhou Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yazhou Zhang, Mengyao Wang, Chenyu Ren, Qiuchi Li, Prayag Tiwari, Benyou Wang, Jing Qin. (2024)<br><strong>Pushing The Limit of LLM Capacity for Text Classification</strong><br><button class=copy-to-clipboard title="Pushing The Limit of LLM Capacity for Text Classification" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Text Classification, Large Language Model, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07470v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07470v2.pdf filename=2402.07470v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The value of <b>text</b> <b>classification&rsquo;s</b> future research has encountered challenges and uncertainties, due to the extraordinary efficacy demonstrated by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> across numerous downstream NLP tasks. In this era of open-ended language modeling, where task boundaries are gradually fading, an urgent question emerges: have we made significant advances in <b>text</b> <b>classification</b> under the full benefit of LLMs? To answer this question, we propose RGPT, an adaptive boosting framework tailored to produce a specialized <b>text</b> <b>classification</b> <b>LLM</b> by recurrently ensembling a pool of strong base learners. The base learners are constructed by adaptively adjusting the distribution of training samples and iteratively <b>fine-tuning</b> <b>LLMs</b> with them. Such base learners are then ensembled to be a specialized <b>text</b> <b>classification</b> <b>LLM,</b> by recurrently incorporating the historical predictions from the previous learners. Through a comprehensive empirical comparison, we show that RGPT significantly outperforms 8 SOTA <b>PLMs</b> and 7 SOTA <b>LLMs</b> on four <b>benchmarks</b> by 1.36% on average. Further evaluation experiments show a clear surpassing of RGPT over human classification.</p></p class="citation"></blockquote><h3 id=1539--77252-large-language-models-as-agents-in-two-player-games-yang-liu-et-al-2024>(15/39 | 77/252) Large Language Models as Agents in Two-Player Games (Yang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Liu, Peng Sun, Hang Li. (2024)<br><strong>Large Language Models as Agents in Two-Player Games</strong><br><button class=copy-to-clipboard title="Large Language Models as Agents in Two-Player Games" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Reinforcement Learning, Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08078v1.pdf filename=2402.08078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>By formally defining the training processes of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> which usually encompasses pre-training, <b>supervised</b> <b>fine-tuning,</b> and <b>reinforcement</b> <b>learning</b> with human feedback, within a single and unified machine learning paradigm, we can glean pivotal insights for advancing <b>LLM</b> technologies. This position paper delineates the parallels between the training methods of <b>LLMs</b> and the strategies employed for the development of agents in two-player games, as studied in game theory, <b>reinforcement</b> <b>learning,</b> and multi-agent systems. We propose a re-conceptualization of <b>LLM</b> learning processes in terms of agent learning in language-based games. This framework unveils innovative perspectives on the successes and challenges in <b>LLM</b> development, offering a fresh understanding of addressing alignment issues among other strategic considerations. Furthermore, our two-player game approach sheds light on novel data preparation and machine learning techniques for training <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1639--78252-aya-model-an-instruction-finetuned-open-access-multilingual-language-model-ahmet-üstün-et-al-2024>(16/39 | 78/252) Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model (Ahmet Üstün et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D&rsquo;souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, Sara Hooker. (2024)<br><strong>Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model</strong><br><button class=copy-to-clipboard title="Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, Pruning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07827v1.pdf filename=2402.07827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent breakthroughs in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages &ndash; including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal <b>finetuning</b> mixture composition, data <b>pruning,</b> as well as the toxicity, bias, and safety of our models. We open-source our instruction datasets and our model at <a href=https://hf.co/CohereForAI/aya-101>https://hf.co/CohereForAI/aya-101</a></p></p class="citation"></blockquote><h3 id=1739--79252-teller-a-trustworthy-framework-for-explainable-generalizable-and-controllable-fake-news-detection-hui-liu-et-al-2024>(17/39 | 79/252) TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection (Hui Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hui Liu, Wenya Wang, Haoru Li, Haoliang Li. (2024)<br><strong>TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection</strong><br><button class=copy-to-clipboard title="TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fake News Detection, Reasoning, Fake News Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07776v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07776v1.pdf filename=2402.07776v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of <b>fake</b> <b>news</b> <b>has</b> emerged as a severe societal problem, raising significant interest from industry and academia. While existing deep-learning based methods have made progress in detecting <b>fake</b> <b>news</b> <b>accurately,</b> their reliability may be compromised caused by the non-transparent <b>reasoning</b> processes, poor generalization abilities and inherent risks of integration with <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> To address this challenge, we propose {\methodname}, a novel framework for trustworthy <b>fake</b> <b>news</b> <b>detection</b> that prioritizes explainability, generalizability and controllability of models. This is achieved via a dual-system framework that integrates cognition and decision systems, adhering to the principles above. The cognition system harnesses human expertise to generate logical predicates, which guide <b>LLMs</b> in generating human-readable logic atoms. Meanwhile, the decision system deduces generalizable logic rules to aggregate these atoms, enabling the identification of the truthfulness of the input news across diverse domains and enhancing transparency in the decision-making process. Finally, we present comprehensive evaluation results on four datasets, demonstrating the feasibility and trustworthiness of our proposed framework. Our implementation is available at \url{https://github.com/less-and-less-bugs/Trust_TELLER}.</p></p class="citation"></blockquote><h3 id=1839--80252-detecting-the-clinical-features-of-difficult-to-treat-depression-using-synthetic-data-from-large-language-models-isabelle-lorge-et-al-2024>(18/39 | 80/252) Detecting the Clinical Features of Difficult-to-Treat Depression using Synthetic Data from Large Language Models (Isabelle Lorge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Isabelle Lorge, Dan W. Joyce, Niall Taylor, Alejo Nevado-Holgado, Andrea Cipriani, Andrey Kormilitzin. (2024)<br><strong>Detecting the Clinical Features of Difficult-to-Treat Depression using Synthetic Data from Large Language Models</strong><br><button class=copy-to-clipboard title="Detecting the Clinical Features of Difficult-to-Treat Depression using Synthetic Data from Large Language Models" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: BERT, GPT-3, GPT-3.5, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07645v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07645v1.pdf filename=2402.07645v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Difficult-to-treat depression (DTD) has been proposed as a broader and more clinically comprehensive perspective on a person&rsquo;s depressive disorder where despite treatment, they continue to experience significant burden. We sought to develop a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)-based</b> tool capable of interrogating routinely-collected, narrative (free-text) electronic health record (EHR) data to locate published prognostic factors that capture the clinical syndrome of DTD. In this work, we use <b>LLM-generated</b> synthetic data <b>(GPT3.5)</b> and a Non-Maximum Suppression (NMS) algorithm to train a <b>BERT-based</b> span extraction model. The resulting model is then able to extract and label spans related to a variety of relevant positive and negative factors in real clinical data (i.e. spans of text that increase or decrease the likelihood of a patient matching the DTD syndrome). We show it is possible to obtain good overall performance (0.70 F1 across polarity) on real clinical data on a set of as many as 20 different factors, and high performance (0.85 F1 with 0.95 precision) on a subset of important DTD factors such as history of abuse, family history of affective disorder, illness severity and suicidality by training the model exclusively on synthetic data. Our results show promise for future healthcare applications especially in applications where traditionally, highly confidential medical data and human-expert annotation would normally be required.</p></p class="citation"></blockquote><h3 id=1939--81252-step-on-feet-tuning-scaling-self-alignment-of-llms-via-bootstrapping-haoyu-wang-et-al-2024>(19/39 | 81/252) Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping (Haoyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Wang, Guozheng Ma, Ziqiao Meng, Zeyu Qin, Li Shen, Zhong Zhang, Bingzhe Wu, Liu Liu, Yatao Bian, Tingyang Xu, Xueqian Wang, Peilin Zhao. (2024)<br><strong>Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping</strong><br><button class=copy-to-clipboard title="Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Few-shot, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07610v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07610v1.pdf filename=2402.07610v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability. However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models. This gives rise to a key query: What if we do multi-time bootstrapping self-alignment? Does this strategy enhance model performance or lead to rapid degradation? In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on <b>large</b> <b>language</b> <b>models.</b> Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from <b>in-context</b> <b>learning.</b> To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model. Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model&rsquo;s continuously enhanced <b>few-shot</b> ability to boost zero or one-shot performance. Based on easy-to-hard training recipe, we propose SOFT+ which further boost self-alignment&rsquo;s performance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across various classification and generation tasks, highlighting the potential of bootstrapping self-alignment on continually enhancing model alignment performance.</p></p class="citation"></blockquote><h3 id=2039--82252-mafia-multi-adapter-fused-inclusive-language-models-prachi-jain-et-al-2024>(20/39 | 82/252) MAFIA: Multi-Adapter Fused Inclusive LanguAge Models (Prachi Jain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prachi Jain, Ashutosh Sathe, Varun Gumma, Kabir Ahuja, Sunayana Sitaram. (2024)<br><strong>MAFIA: Multi-Adapter Fused Inclusive LanguAge Models</strong><br><button class=copy-to-clipboard title="MAFIA: Multi-Adapter Fused Inclusive LanguAge Models" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 50<br>Keywords: Counter-factual, Data Augmentation, Fine-tuning, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07519v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07519v1.pdf filename=2402.07519v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pretrained</b> <b>Language</b> <b>Models</b> <b>(PLMs)</b> are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion. Moreover, the methods typically involve <b>finetuning</b> the full model to maintain the performance on the downstream task. In this work, we aim to modularly debias a <b>pretrained</b> <b>language</b> <b>model</b> across multiple dimensions. Previous works extensively explored debiasing <b>PLMs</b> using limited US-centric <b>counterfactual</b> <b>data</b> <b>augmentation</b> (CDA). We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way. We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously. An extensive evaluation on multiple tasks and languages demonstrates the efficacy of our approach.</p></p class="citation"></blockquote><h3 id=2139--83252-retrieval-augmented-thought-process-as-sequential-decision-making-thomas-pouplin-et-al-2024>(21/39 | 83/252) Retrieval-Augmented Thought Process as Sequential Decision Making (Thomas Pouplin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Pouplin, Hao Sun, Samuel Holt, Mihaela van der Schaar. (2024)<br><strong>Retrieval-Augmented Thought Process as Sequential Decision Making</strong><br><button class=copy-to-clipboard title="Retrieval-Augmented Thought Process as Sequential Decision Making" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: H-3-3; I-2-6; I-2-7; I-2-8, cs-AI, cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Question Answering, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07812v1.pdf filename=2402.07812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated their strong ability to assist people and show &ldquo;sparks of intelligence&rdquo;. However, several open challenges hinder their wider application: such as concerns over privacy, tendencies to produce hallucinations, and difficulties in handling long contexts. In this work, we address those challenges by introducing the Retrieval-Augmented Thought Process (RATP). Given access to external knowledge, RATP formulates the thought generation of <b>LLMs</b> as a multiple-step decision process. To optimize such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a Q-value estimator that permits cost-efficient inference. In addressing the task of <b>question-answering</b> <b>with</b> private data, where ethical and security concerns limit <b>LLM</b> training methods, RATP achieves a 50% improvement over existing <b>in-context</b> retrieval-augmented language models.</p></p class="citation"></blockquote><h3 id=2239--84252-unsupervised-sign-language-translation-and-generation-zhengsheng-guo-et-al-2024>(22/39 | 84/252) Unsupervised Sign Language Translation and Generation (Zhengsheng Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengsheng Guo, Zhiwei He, Wenxiang Jiao, Xing Wang, Rui Wang, Kehai Chen, Zhaopeng Tu, Yong Xu, Min Zhang. (2024)<br><strong>Unsupervised Sign Language Translation and Generation</strong><br><button class=copy-to-clipboard title="Unsupervised Sign Language Translation and Generation" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Supervised Learning, Unsupervised Learning, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07726v1.pdf filename=2402.07726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by the success of <b>unsupervised</b> <b>neural</b> <b>machine</b> <b>translation</b> (UNMT), we introduce an <b>unsupervised</b> sign language translation and generation network (USLNet), which learns from abundant single-modality (text and video) data without parallel sign language data. USLNet comprises two main components: single-modality reconstruction modules (text and video) that rebuild the input from its noisy version in the same modality and cross-modality back-translation modules (text-video-text and video-text-video) that reconstruct the input from its noisy version in the different modality using back-translation procedure.Unlike the single-modality back-translation procedure in text-based UNMT, USLNet faces the cross-modality discrepancy in feature representation, in which the length and the feature dimension mismatch between text and video sequences. We propose a sliding window method to address the issues of aligning variable-length text with video sequences. To our knowledge, USLNet is the first <b>unsupervised</b> sign language translation and generation model capable of generating both natural language text and sign language video in a unified manner. Experimental results on the BBC-Oxford Sign Language dataset (BOBSL) and Open-Domain American Sign Language dataset (OpenASL) reveal that USLNet achieves competitive results compared to <b>supervised</b> baseline models, indicating its effectiveness in sign language translation and generation.</p></p class="citation"></blockquote><h3 id=2339--85252-topic-modeling-as-multi-objective-contrastive-optimization-thong-nguyen-et-al-2024>(23/39 | 85/252) Topic Modeling as Multi-Objective Contrastive Optimization (Thong Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thong Nguyen, Xiaobao Wu, Xinshuai Dong, Cong-Duy T Nguyen, See-Kiong Ng, Anh Tuan Luu. (2024)<br><strong>Topic Modeling as Multi-Objective Contrastive Optimization</strong><br><button class=copy-to-clipboard title="Topic Modeling as Multi-Objective Contrastive Optimization" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Contrastive Learning, Mutual Information, Topic Model, Topic Modeling<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07577v1.pdf filename=2402.07577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent representation learning approaches enhance neural <b>topic</b> <b>models</b> by optimizing the weighted linear combination of the evidence lower bound (ELBO) of the log-likelihood and the <b>contrastive</b> <b>learning</b> objective that contrasts pairs of input documents. However, document-level <b>contrastive</b> <b>learning</b> might capture low-level <b>mutual</b> <b>information,</b> such as word ratio, which disturbs <b>topic</b> <b>modeling.</b> Moreover, there is a potential conflict between the ELBO loss that memorizes input details for better reconstruction quality, and the <b>contrastive</b> <b>loss</b> which attempts to learn <b>topic</b> <b>representations</b> that generalize among input documents. To address these issues, we first introduce a novel <b>contrastive</b> <b>learning</b> method oriented towards sets of <b>topic</b> <b>vectors</b> to capture useful semantics that are shared among a set of input documents. Secondly, we explicitly cast <b>contrastive</b> <b>topic</b> <b>modeling</b> as a gradient-based multi-objective optimization problem, with the goal of achieving a Pareto stationary solution that balances the trade-off between the ELBO and the <b>contrastive</b> <b>objective.</b> Extensive experiments demonstrate that our framework consistently produces higher-performing neural <b>topic</b> <b>models</b> in terms of <b>topic</b> <b>coherence,</b> <b>topic</b> <b>diversity,</b> and downstream performance.</p></p class="citation"></blockquote><h3 id=2439--86252-quality-does-matter-a-detailed-look-at-the-quality-and-utility-of-web-mined-parallel-corpora-surangika-ranathunga-et-al-2024>(24/39 | 86/252) Quality Does Matter: A Detailed Look at the Quality and Utility of Web-Mined Parallel Corpora (Surangika Ranathunga et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Surangika Ranathunga, Nisansa de Silva, Menan Velayuthan, Aloka Fernando, Charitha Rathnayake. (2024)<br><strong>Quality Does Matter: A Detailed Look at the Quality and Utility of Web-Mined Parallel Corpora</strong><br><button class=copy-to-clipboard title="Quality Does Matter: A Detailed Look at the Quality and Utility of Web-Mined Parallel Corpora" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Low-Resource, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07446v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07446v2.pdf filename=2402.07446v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We conducted a detailed analysis on the quality of web-mined corpora for two <b>low-resource</b> languages (making three language pairs, English-Sinhala, English-Tamil and Sinhala-Tamil). We ranked each corpus according to a similarity measure and carried out an intrinsic and extrinsic evaluation on different portions of this ranked corpus. We show that there are significant quality differences between different portions of web-mined corpora and that the quality varies across languages and datasets. We also show that, for some web-mined datasets, <b>Neural</b> <b>Machine</b> <b>Translation</b> <b>(NMT)</b> models trained with their highest-ranked 25k portion can be on par with human-curated datasets.</p></p class="citation"></blockquote><h3 id=2539--87252-refined-direct-preference-optimization-with-synthetic-data-for-behavioral-alignment-of-llms-víctor-gallego-2024>(25/39 | 87/252) Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs (Víctor Gallego, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Víctor Gallego. (2024)<br><strong>Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs</strong><br><button class=copy-to-clipboard title="Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08005v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08005v1.pdf filename=2402.08005v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce \emph{refined Direct Preference Optimization} (rDPO), a method for improving the behavioral alignment of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> without the need for human-annotated data. The method involves creating synthetic data using self-critique <b>prompting</b> by a teacher <b>LLM</b> and then utilising a generalized DPO loss function to distil to a student <b>LLM.</b> The loss function incorporates an additional external reward model to improve the quality of synthetic data, making rDPO robust to potential noise in the synthetic dataset. rDPO is shown to be effective in a diverse set of behavioural alignment tasks, such as improved safety, robustness against role-playing, and reduced sycophancy. Code to be released at <a href=https://github.com/vicgalle/refined-dpo>https://github.com/vicgalle/refined-dpo</a>.</p></p class="citation"></blockquote><h3 id=2639--88252-show-me-how-its-done-the-role-of-explanations-in-fine-tuning-language-models-mohamad-ballout-et-al-2024>(26/39 | 88/252) Show Me How It&rsquo;s Done: The Role of Explanations in Fine-Tuning Language Models (Mohamad Ballout et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamad Ballout, Ulf Krumnack, Gunther Heidemann, Kai-Uwe Kuehnberger. (2024)<br><strong>Show Me How It&rsquo;s Done: The Role of Explanations in Fine-Tuning Language Models</strong><br><button class=copy-to-clipboard title="Show Me How It's Done: The Role of Explanations in Fine-Tuning Language Models" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07543v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07543v1.pdf filename=2402.07543v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our research demonstrates the significant benefits of using <b>fine-tuning</b> with explanations to enhance the performance of language models. Unlike <b>prompting,</b> which maintains the model&rsquo;s parameters, <b>fine-tuning</b> allows the model to learn and update its parameters during a training phase. In this study, we applied <b>fine-tuning</b> to various sized language models using data that contained explanations of the output rather than merely presenting the answers. We found that even smaller language models with as few as 60 million parameters benefited substantially from this approach. Interestingly, our results indicated that the detailed explanations were more beneficial to smaller models than larger ones, with the latter gaining nearly the same advantage from any form of explanation, irrespective of its length. Additionally, we demonstrate that the inclusion of explanations enables the models to solve tasks that they were not able to solve without explanations. Lastly, we argue that despite the challenging nature of adding explanations, samples that contain explanations not only reduce the volume of data required for training but also promote a more effective generalization by the model. In essence, our findings suggest that <b>fine-tuning</b> with explanations significantly bolsters the performance of <b>large</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=2739--89252-the-balancing-act-unmasking-and-alleviating-asr-biases-in-portuguese-ajinkya-kulkarni-et-al-2024>(27/39 | 89/252) The Balancing Act: Unmasking and Alleviating ASR Biases in Portuguese (Ajinkya Kulkarni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ajinkya Kulkarni, Anna Tokareva, Rameez Qureshi, Miguel Couceiro. (2024)<br><strong>The Balancing Act: Unmasking and Alleviating ASR Biases in Portuguese</strong><br><button class=copy-to-clipboard title="The Balancing Act: Unmasking and Alleviating ASR Biases in Portuguese" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs.CL<br>Keyword Score: 30<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07513v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07513v1.pdf filename=2402.07513v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of spoken language understanding, systems like Whisper and Multilingual Massive <b>Speech</b> <b>(MMS)</b> have shown state-of-the-art performances. This study is dedicated to a comprehensive exploration of the Whisper and MMS systems, with a focus on assessing biases in <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> inherent to casual conversation <b>speech</b> <b>specific</b> to the Portuguese language. Our investigation encompasses various categories, including gender, age, skin tone color, and geo-location. Alongside traditional <b>ASR</b> evaluation metrics such as Word Error Rate (WER), we have incorporated p-value statistical significance for gender bias analysis. Furthermore, we extensively examine the impact of data distribution and empirically show that oversampling techniques alleviate such stereotypical biases. This research represents a pioneering effort in quantifying biases in the Portuguese language context through the application of MMS and Whisper, contributing to a better understanding of <b>ASR</b> systems&rsquo; performance in multilingual settings.</p></p class="citation"></blockquote><h3 id=2839--90252-asking-multimodal-clarifying-questions-in-mixed-initiative-conversational-search-yifei-yuan-et-al-2024>(28/39 | 90/252) Asking Multimodal Clarifying Questions in Mixed-Initiative Conversational Search (Yifei Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Yuan, Clemencia Siro, Mohammad Aliannejadi, Maarten de Rijke, Wai Lam. (2024)<br><strong>Asking Multimodal Clarifying Questions in Mixed-Initiative Conversational Search</strong><br><button class=copy-to-clipboard title="Asking Multimodal Clarifying Questions in Mixed-Initiative Conversational Search" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 26<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07742v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07742v1.pdf filename=2402.07742v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In mixed-initiative conversational search systems, clarifying questions are used to help users who struggle to express their intentions in a single query. These questions aim to uncover user&rsquo;s information needs and resolve query ambiguities. We hypothesize that in scenarios where <b>multimodal</b> information is pertinent, the clarification process can be improved by using non-textual information. Therefore, we propose to add images to clarifying questions and formulate the novel task of asking <b>multimodal</b> clarifying questions in open-domain, mixed-initiative conversational search systems. To facilitate research into this task, we collect a dataset named Melon that contains over 4k <b>multimodal</b> clarifying questions, enriched with over 14k images. We also propose a <b>multimodal</b> query clarification model named Marto and adopt a <b>prompt-based,</b> generative <b>fine-tuning</b> strategy to perform the training of different stages with different <b>prompts.</b> Several analyses are conducted to understand the importance of <b>multimodal</b> contents during the query clarification phase. Experimental results indicate that the addition of images leads to significant improvements of up to 90% in retrieval performance when selecting the relevant images. Extensive analyses are also performed to show the superiority of Marto compared with discriminative baselines in terms of effectiveness and efficiency.</p></p class="citation"></blockquote><h3 id=2939--91252-do-membership-inference-attacks-work-on-large-language-models-michael-duan-et-al-2024>(29/39 | 91/252) Do Membership Inference Attacks Work on Large Language Models? (Michael Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, Hannaneh Hajishirzi. (2024)<br><strong>Do Membership Inference Attacks Work on Large Language Models?</strong><br><button class=copy-to-clipboard title="Do Membership Inference Attacks Work on Large Language Models?" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07841v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07841v1.pdf filename=2402.07841v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model&rsquo;s training data. Despite extensive research on traditional machine learning models, there has been limited work studying MIA on the pre-training data of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> We perform a <b>large-scale</b> <b>evaluation</b> <b>of</b> MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M to 12B parameters. We find that MIAs barely outperform random guessing for most settings across varying <b>LLM</b> sizes and domains. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a <b>large</b> <b>dataset</b> <b>and</b> few training iterations, and (2) an inherently fuzzy boundary between members and non-members. We identify specific settings where <b>LLMs</b> have been shown to be vulnerable to membership inference and show that the apparent success in such settings can be attributed to a distribution shift, such as when members and non-members are drawn from the seemingly identical domain but with different temporal ranges. We release our code and data as a unified <b>benchmark</b> package that includes all existing MIAs, supporting future work.</p></p class="citation"></blockquote><h3 id=3039--92252-auxiliary-tasks-to-boost-biaffine-semantic-dependency-parsing-marie-candito-2024>(30/39 | 92/252) Auxiliary Tasks to Boost Biaffine Semantic Dependency Parsing (Marie Candito, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marie Candito. (2024)<br><strong>Auxiliary Tasks to Boost Biaffine Semantic Dependency Parsing</strong><br><button class=copy-to-clipboard title="Auxiliary Tasks to Boost Biaffine Semantic Dependency Parsing" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Graph, Transformer, Dependency Parsing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07682v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07682v1.pdf filename=2402.07682v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The biaffine parser of Dozat and Manning (2017) was successfully extended to semantic <b>dependency</b> <b>parsing</b> (SDP) (Dozat and Manning, 2018). Its performance on <b>graphs</b> is surprisingly high given that, without the constraint of producing a tree, all arcs for a given sentence are predicted independently from each other (modulo a shared representation of tokens). To circumvent such an independence of decision, while retaining the O(n^2) complexity and highly parallelizable architecture, we propose to use simple auxiliary tasks that introduce some form of interdependence between arcs. Experiments on the three English acyclic datasets of SemEval 2015 task 18 (Oepen et al., 2015), and on French deep syntactic cyclic <b>graphs</b> (Ribeyre et al., 2014) show modest but systematic performance gains on a near state-of-the-art baseline using <b>transformer-based</b> contextualized representations. This provides a simple and robust method to boost SDP performance.</p></p class="citation"></blockquote><h3 id=3139--93252-multi-intent-attribute-aware-text-matching-in-searching-mingzhe-li-et-al-2024>(31/39 | 93/252) Multi-Intent Attribute-Aware Text Matching in Searching (Mingzhe Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingzhe Li, Xiuying Chen, Jing Xiang, Qishen Zhang, Changsheng Ma, Chenchen Dai, Jinxiong Chang, Zhongyi Liu, Guannan Zhang. (2024)<br><strong>Multi-Intent Attribute-Aware Text Matching in Searching</strong><br><button class=copy-to-clipboard title="Multi-Intent Attribute-Aware Text Matching in Searching" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07788v1.pdf filename=2402.07788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text matching systems have become a fundamental service in most searching platforms. For instance, they are responsible for matching user queries to relevant candidate items, or rewriting the user-input query to a pre-selected high-performing one for a better search experience. In practice, both the queries and items often contain multiple attributes, such as the category of the item and the location mentioned in the query, which represent condensed key information that is helpful for matching. However, most of the existing works downplay the effectiveness of attributes by integrating them into text representations as supplementary information. Hence, in this work, we focus on exploring the relationship between the attributes from two sides. Since attributes from two ends are often not aligned in terms of number and type, we propose to exploit the benefit of attributes by multiple-intent modeling. The intents extracted from attributes <b>summarize</b> the diverse needs of queries and provide rich content of items, which are more refined and abstract, and can be aligned for paired inputs. Concretely, we propose a multi-intent attribute-aware matching model (MIM), which consists of three main components: attribute-aware encoder, multi-intent modeling, and intent-aware matching. In the attribute-aware encoder, the text and attributes are weighted and processed through a scaled attention mechanism with regard to the attributes&rsquo; importance. Afterward, the multi-intent modeling extracts intents from two ends and aligns them. Herein, we come up with a distribution loss to ensure the learned intents are diverse but concentrated, and a kullback-leibler divergence loss that aligns the learned intents. Finally, in the intent-aware matching, the intents are evaluated by a <b>self-supervised</b> masking task, and then incorporated to output the final matching result.</p></p class="citation"></blockquote><h3 id=3239--94252-text-detoxification-as-style-transfer-in-english-and-hindi-sourabrata-mukherjee-et-al-2024>(32/39 | 94/252) Text Detoxification as Style Transfer in English and Hindi (Sourabrata Mukherjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sourabrata Mukherjee, Akanksha Bansal, Atul Kr. Ojha, John P. McCrae, Ondřej Dušek. (2024)<br><strong>Text Detoxification as Style Transfer in English and Hindi</strong><br><button class=copy-to-clipboard title="Text Detoxification as Style Transfer in English and Hindi" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Knowledge Transfer, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07767v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07767v1.pdf filename=2402.07767v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper focuses on text detoxification, i.e., automatically converting toxic text into non-toxic text. This task contributes to safer and more respectful online communication and can be considered a Text <b>Style</b> <b>Transfer</b> (TST) task, where the text <b>style</b> <b>changes</b> while its content is preserved. We present three approaches: <b>knowledge</b> <b>transfer</b> from a similar task, multi-task learning approach, combining sequence-to-sequence modeling with various toxicity classification tasks, and, delete and reconstruct approach. To support our research, we utilize a dataset provided by Dementieva et al.(2021), which contains multiple versions of detoxified texts corresponding to toxic texts. In our experiments, we selected the best variants through expert human annotators, creating a dataset where each toxic sentence is paired with a single, appropriate detoxified version. Additionally, we introduced a small Hindi parallel dataset, aligning with a part of the English dataset, suitable for evaluation purposes. Our results demonstrate that our approach effectively balances text detoxication while preserving the actual content and maintaining fluency.</p></p class="citation"></blockquote><h3 id=3339--95252-intrinsic-task-based-evaluation-for-referring-expression-generation-guanyi-chen-et-al-2024>(33/39 | 95/252) Intrinsic Task-based Evaluation for Referring Expression Generation (Guanyi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guanyi Chen, Fahime Same, Kees van Deemter. (2024)<br><strong>Intrinsic Task-based Evaluation for Referring Expression Generation</strong><br><button class=copy-to-clipboard title="Intrinsic Task-based Evaluation for Referring Expression Generation" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Language Generation, Natural Language Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07432v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07432v1.pdf filename=2402.07432v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, a human evaluation study of Referring Expression Generation (REG) models had an unexpected conclusion: on \textsc{webnlg}, Referring Expressions (REs) generated by the state-of-the-art neural models were not only indistinguishable from the REs in \textsc{webnlg} but also from the REs generated by a simple rule-based system. Here, we argue that this limitation could stem from the use of a purely ratings-based human evaluation (which is a common practice in <b>Natural</b> <b>Language</b> <b>Generation).</b> To investigate these issues, we propose an intrinsic task-based evaluation for REG models, in which, in addition to rating the quality of REs, participants were asked to accomplish two meta-level tasks. One of these tasks concerns the referential success of each RE; the other task asks participants to suggest a better alternative for each RE. The outcomes suggest that, in comparison to previous evaluations, the new evaluation protocol assesses the performance of each REG model more comprehensively and makes the participants&rsquo; ratings more reliable and discriminable.</p></p class="citation"></blockquote><h3 id=3439--96252-salad-smart-ai-language-assistant-daily-ragib-amin-nihal-et-al-2024>(34/39 | 96/252) SALAD: Smart AI Language Assistant Daily (Ragib Amin Nihal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ragib Amin Nihal, Tran Dong Huu Quoc, Lin Zirui, Xu Yimimg, Liu Haoran, An Zhaoyi, Kyou Ma. (2024)<br><strong>SALAD: Smart AI Language Assistant Daily</strong><br><button class=copy-to-clipboard title="SALAD: Smart AI Language Assistant Daily" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 20<br>Keywords: Automatic Speech Recognition, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07431v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07431v2.pdf filename=2402.07431v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>SALAD is an AI-driven language-learning application designed to help foreigners learn Japanese. It offers translations in Kanji-Kana-Romaji, <b>speech</b> <b>recognition,</b> translated audio, vocabulary tracking, grammar explanations, and songs generated from newly learned words. The app targets beginners and intermediate learners, aiming to make language acquisition more accessible and enjoyable. SALAD uses daily translations to enhance fluency and comfort in communication with native speakers. The primary objectives include effective Japanese language learning, user engagement, and progress tracking. A survey by us found that 39% of foreigners in Japan face discomfort in conversations with Japanese speakers. Over 60% of foreigners expressed confidence in SALAD&rsquo;s ability to enhance their Japanese language skills. The app uses <b>large</b> <b>language</b> <b>models,</b> <b>speech</b> <b>recognition,</b> and diffusion models to bridge the language gap and foster a more inclusive community in Japan.</p></p class="citation"></blockquote><h3 id=3539--97252-label-efficient-model-selection-for-text-generation-shir-ashury-tahan-et-al-2024>(35/39 | 97/252) Label-Efficient Model Selection for Text Generation (Shir Ashury-Tahan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shir Ashury-Tahan, Benjamin Sznajder, Leshem Choshen, Liat Ein-Dor, Eyal Shnarch, Ariel Gera. (2024)<br><strong>Label-Efficient Model Selection for Text Generation</strong><br><button class=copy-to-clipboard title="Label-Efficient Model Selection for Text Generation" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 13<br>Keywords: Clustering, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07891v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07891v1.pdf filename=2402.07891v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model selection for a given target task can be costly, as it may entail extensive annotation of the quality of outputs of different models. We introduce DiffUse, an efficient method to make an informed decision between candidate <b>text</b> <b>generation</b> models. DiffUse reduces the required amount of preference annotations, thus saving valuable time and resources in performing evaluation. DiffUse intelligently selects instances by <b>clustering</b> embeddings that represent the semantic differences between model outputs. Thus, it is able to identify a subset of examples that are more informative for preference decisions. Our method is model-agnostic, and can be applied to any <b>text</b> <b>generation</b> model. Moreover, we propose a practical iterative approach for dynamically determining how many instances to annotate. In a series of experiments over hundreds of model pairs, we demonstrate that DiffUse can dramatically reduce the required number of annotations &ndash; by up to 75% &ndash; while maintaining high evaluation reliability.</p></p class="citation"></blockquote><h3 id=3639--98252-a-systematic-investigation-of-learnability-from-single-child-linguistic-input-yulu-qin-et-al-2024>(36/39 | 98/252) A systematic investigation of learnability from single child linguistic input (Yulu Qin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yulu Qin, Wentao Wang, Brenden M. Lake. (2024)<br><strong>A systematic investigation of learnability from single child linguistic input</strong><br><button class=copy-to-clipboard title="A systematic investigation of learnability from single child linguistic input" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07899v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07899v1.pdf filename=2402.07899v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability. However, a significant gap exists between the training data for these models and the linguistic input a child receives. LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our research focuses on training LMs on subsets of a single child&rsquo;s linguistic input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered <b>LSTMs</b> and simpler neural networks trained from just one single-child dataset. Here, to examine the robustness of learnability from single-child input, we systematically train six different model architectures on five datasets (3 single-child and 2 baselines). We find that the models trained on single-child datasets showed consistent results that matched with previous work, underscoring the robustness of forming meaningful syntactic and semantic representations from a subset of a child&rsquo;s linguistic input.</p></p class="citation"></blockquote><h3 id=3739--99252-diffusion-of-thoughts-chain-of-thought-reasoning-in-diffusion-language-models-jiacheng-ye-et-al-2024>(37/39 | 99/252) Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models (Jiacheng Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Zhenguo Li, Wei Bi, Lingpeng Kong. (2024)<br><strong>Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models</strong><br><button class=copy-to-clipboard title="Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07754v1.pdf filename=2402.07754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the <b>reasoning</b> ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing <b>reasoning</b> steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and <b>reasoning</b> performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing <b>reasoning-enhancing</b> techniques like self-consistency decoding. Our findings contribute to the understanding and development of <b>reasoning</b> capabilities in diffusion language models.</p></p class="citation"></blockquote><h3 id=3839--100252-orderbkd-textual-backdoor-attack-through-repositioning-irina-alekseevskaia-et-al-2024>(38/39 | 100/252) OrderBkd: Textual backdoor attack through repositioning (Irina Alekseevskaia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Irina Alekseevskaia, Konstantin Arkhipenko. (2024)<br><strong>OrderBkd: Textual backdoor attack through repositioning</strong><br><button class=copy-to-clipboard title="OrderBkd: Textual backdoor attack through repositioning" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07689v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07689v1.pdf filename=2402.07689v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The use of third-party datasets and pre-trained machine learning models poses a threat to NLP systems due to possibility of hidden backdoor attacks. Existing attacks involve poisoning the data samples such as insertion of tokens or sentence paraphrasing, which either alter the semantics of the original texts or can be detected. Our main difference from the previous work is that we use the reposition of a two words in a sentence as a trigger. By designing and applying specific part-of-speech (POS) based rules for selecting these tokens, we maintain high attack success rate on SST-2 and AG classification datasets while outperforming existing attacks in terms of <b>perplexity</b> and semantic similarity to the clean samples. In addition, we show the robustness of our attack to the ONION defense method. All the code and data for the paper can be obtained at <a href=https://github.com/alekseevskaia/OrderBkd>https://github.com/alekseevskaia/OrderBkd</a>.</p></p class="citation"></blockquote><h3 id=3939--101252-araspider-democratizing-arabic-to-sql-ahmed-heakl-et-al-2024>(39/39 | 101/252) AraSpider: Democratizing Arabic-to-SQL (Ahmed Heakl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmed Heakl, Youssef Mohamed, Ahmed B. Zaky. (2024)<br><strong>AraSpider: Democratizing Arabic-to-SQL</strong><br><button class=copy-to-clipboard title="AraSpider: Democratizing Arabic-to-SQL" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-DB, cs-IR, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07448v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07448v1.pdf filename=2402.07448v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents AraSpider, the first Arabic version of the Spider dataset, aimed at improving natural language processing (NLP) in the Arabic-speaking community. Four multilingual translation models were tested for their effectiveness in translating English to Arabic. Additionally, two models were assessed for their ability to generate SQL queries from Arabic text. The results showed that using back translation significantly improved the performance of both <b>ChatGPT</b> 3.5 and SQLCoder models, which are considered top performers on the Spider dataset. Notably, <b>ChatGPT</b> 3.5 demonstrated high-quality translation, while SQLCoder excelled in text-to-SQL tasks. The study underscores the importance of incorporating contextual schema and employing back translation strategies to enhance model performance in Arabic NLP tasks. Moreover, the provision of detailed methodologies for reproducibility and translation of the dataset into other languages highlights the research&rsquo;s commitment to promoting transparency and collaborative knowledge sharing in the field. Overall, these contributions advance NLP research, empower Arabic-speaking researchers, and enrich the global discourse on language comprehension and database interrogation.</p></p class="citation"></blockquote><h2 id=csro-8>cs.RO (8)</h2><h3 id=18--102252-pivot-iterative-visual-prompting-elicits-actionable-knowledge-for-vlms-soroush-nasiriany-et-al-2024>(1/8 | 102/252) PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs (Soroush Nasiriany et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, Quan Vuong, Tingnan Zhang, Tsang-Wei Edward Lee, Kuang-Huei Lee, Peng Xu, Sean Kirmani, Yuke Zhu, Andy Zeng, Karol Hausman, Nicolas Heess, Chelsea Finn, Sergey Levine, Brian Ichter. (2024)<br><strong>PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs</strong><br><button class=copy-to-clipboard title="PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CL, cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 100<br>Keywords: Fine-tuning, Simulation, Simulator, Zero-shot, Instruction Following, Question Answering, Reasoning, Visual Question Answering, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07872v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07872v1.pdf filename=2402.07872v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical <b>reasoning</b> to <b>visual</b> <b>understanding.</b> <b>This</b> opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without <b>fine-tuning</b> on task-specific data? In this paper, we propose a novel <b>visual</b> <b>prompting</b> <b>approach</b> for VLMs that we call <b>Prompting</b> with Iterative <b>Visual</b> <b>Optimization</b> <b>(PIVOT),</b> which casts tasks as iterative <b>visual</b> <b>question</b> <b>answering.</b> In each iteration, the image is annotated with a <b>visual</b> <b>representation</b> <b>of</b> proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, <b>instruction</b> <b>following</b> in <b>simulation,</b> and additional spatial inference tasks such as localization. We find, perhaps surprisingly, that our approach enables <b>zero-shot</b> control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial <b>reasoning</b> domains. Website: pivot-prompt.github.io and HuggingFace: <a href=https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo>https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo</a>.</p></p class="citation"></blockquote><h3 id=28--103252-customizable-perturbation-synthesis-for-robust-slam-benchmarking-xiaohao-xu-et-al-2024>(2/8 | 103/252) Customizable Perturbation Synthesis for Robust SLAM Benchmarking (Xiaohao Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaohao Xu, Tianyi Zhang, Sibo Wang, Xiang Li, Yongqi Chen, Ye Li, Bhiksha Raj, Matthew Johnson-Roberson, Xiaonan Huang. (2024)<br><strong>Customizable Perturbation Synthesis for Robust SLAM Benchmarking</strong><br><button class=copy-to-clipboard title="Customizable Perturbation Synthesis for Robust SLAM Benchmarking" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-MM, cs-RO, cs.RO<br>Keyword Score: 29<br>Keywords: Benchmarking, Benchmarking, Multi-modal, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08125v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08125v1.pdf filename=2402.08125v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robustness is a crucial factor for the successful deployment of robots in unstructured environments, particularly in the domain of Simultaneous Localization and Mapping (SLAM). <b>Simulation-based</b> <b>benchmarks</b> have emerged as a highly scalable approach for robustness evaluation compared to real-world data collection. However, crafting a challenging and controllable noisy world with diverse perturbations remains relatively under-explored. To this end, we propose a novel, customizable pipeline for noisy data synthesis, aimed at assessing the resilience of <b>multi-modal</b> SLAM models against various perturbations. This pipeline incorporates customizable hardware setups, software components, and perturbed environments. In particular, we introduce comprehensive perturbation taxonomy along with a perturbation composition toolbox, allowing the transformation of clean <b>simulations</b> into challenging noisy environments. Utilizing the pipeline, we instantiate the Robust-SLAM <b>benchmark,</b> which includes diverse perturbation types, to evaluate the risk tolerance of existing advanced <b>multi-modal</b> SLAM models. Our extensive analysis uncovers the susceptibilities of existing SLAM models to real-world disturbance, despite their demonstrated accuracy in standard <b>benchmarks.</b> Our perturbation synthesis toolbox, SLAM robustness evaluation pipeline, and Robust-SLAM <b>benchmark</b> will be made publicly available at <a href=https://github.com/Xiaohao-Xu/SLAM-under-Perturbation/>https://github.com/Xiaohao-Xu/SLAM-under-Perturbation/</a>.</p></p class="citation"></blockquote><h3 id=38--104252-deformnet-latent-space-modeling-and-dynamics-prediction-for-deformable-object-manipulation-chenchang-li-et-al-2024>(3/8 | 104/252) DeformNet: Latent Space Modeling and Dynamics Prediction for Deformable Object Manipulation (Chenchang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenchang Li, Zihao Ai, Tong Wu, Xiaosa Li, Wenbo Ding, Huazhe Xu. (2024)<br><strong>DeformNet: Latent Space Modeling and Dynamics Prediction for Deformable Object Manipulation</strong><br><button class=copy-to-clipboard title="DeformNet: Latent Space Modeling and Dynamics Prediction for Deformable Object Manipulation" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07648v1.pdf filename=2402.07648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Manipulating deformable objects is a ubiquitous task in household environments, demanding adequate representation and accurate dynamics prediction due to the objects&rsquo; infinite degrees of freedom. This work proposes DeformNet, which utilizes latent space modeling with a learned 3D representation model to tackle these challenges effectively. The proposed representation model combines a PointNet encoder and a conditional neural radiance field (NeRF), facilitating a thorough acquisition of object deformations and variations in lighting conditions. To model the complex dynamics, we employ a recurrent state-space model (RSSM) that accurately predicts the transformation of the latent representation over time. Extensive <b>simulation</b> experiments with diverse objectives demonstrate the generalization capabilities of DeformNet for various deformable object manipulation tasks, even in the presence of previously unseen goals. Finally, we deploy DeformNet on an actual UR5 robotic arm to demonstrate its capability in real-world scenarios.</p></p class="citation"></blockquote><h3 id=48--105252-dart-a-compact-platform-for-autonomous-driving-research-lorenzo-lyons-et-al-2024>(4/8 | 105/252) DART: A Compact Platform For Autonomous Driving Research (Lorenzo Lyons et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lorenzo Lyons, Thijs Niesten, Laura Ferranti. (2024)<br><strong>DART: A Compact Platform For Autonomous Driving Research</strong><br><button class=copy-to-clipboard title="DART: A Compact Platform For Autonomous Driving Research" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07602v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07602v1.pdf filename=2402.07602v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents the design of a research platform for autonomous driving applications, the Delft&rsquo;s Autonomous-driving Robotic Testbed (DART). Our goal was to design a small-scale car-like robot equipped with all the hardware needed for on-board navigation and control while keeping it cost-effective and easy to replicate. To develop DART, we built on an existing off-the-shelf model and augmented its sensor suite to improve its capabilities for control and motion planning tasks. We detail the hardware setup and the system identification challenges to derive the vehicle&rsquo;s models. Furthermore, we present some use cases where we used DART to test different motion planning applications to show the versatility of the platform. Finally, we provide a git repository with all the details to replicate DART, complete with a <b>simulation</b> environment and the data used for system identification.</p></p class="citation"></blockquote><h3 id=58--106252-digital-twins-below-the-surface-enhancing-underwater-teleoperation-favour-o-adetunji-et-al-2024>(5/8 | 106/252) Digital Twins Below the Surface: Enhancing Underwater Teleoperation (Favour O. Adetunji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Favour O. Adetunji, Niamh Ellis, Maria Koskinopoulou, Ignacio Carlucho, Yvan R. Petillot. (2024)<br><strong>Digital Twins Below the Surface: Enhancing Underwater Teleoperation</strong><br><button class=copy-to-clipboard title="Digital Twins Below the Surface: Enhancing Underwater Teleoperation" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07556v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07556v1.pdf filename=2402.07556v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Subsea exploration, inspection, and intervention operations heavily rely on remotely operated vehicles (ROVs). However, the inherent complexity of the underwater environment presents significant challenges to the operators of these vehicles. This paper delves into the challenges associated with navigation and maneuvering tasks in the teleoperation of ROVs, such as reduced situational awareness and heightened teleoperator workload. To address these challenges, we introduce an underwater Digital Twin (DT) system designed to enhance underwater teleoperation, enable autonomous navigation, support system monitoring, and facilitate system testing through <b>simulation.</b> Our approach involves a dynamic representation of the underwater robot and its environment using desktop virtual reality, as well as the integration of mapping, localization, path planning and <b>simulation</b> capabilities within the DT system. Our research demonstrates the system&rsquo;s adaptability, versatility and feasibility, highlighting significant challenges and, in turn, improving the teleoperators&rsquo; situational awareness and reducing their workload.</p></p class="citation"></blockquote><h3 id=68--107252-extending-3d-body-pose-estimation-for-robotic-assistive-therapies-of-autistic-children-laura-santos-et-al-2024>(6/8 | 107/252) Extending 3D body pose estimation for robotic-assistive therapies of autistic children (Laura Santos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Laura Santos, Bernardo Carvalho, Catarina Barata, José Santos-Victor. (2024)<br><strong>Extending 3D body pose estimation for robotic-assistive therapies of autistic children</strong><br><button class=copy-to-clipboard title="Extending 3D body pose estimation for robotic-assistive therapies of autistic children" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-HC, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08006v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08006v1.pdf filename=2402.08006v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic-assistive therapy has demonstrated very encouraging results for children with Autism. Accurate estimation of the child&rsquo;s pose is essential both for human-robot interaction and for therapy assessment purposes. Non-intrusive methods are the sole viable option since these children are sensitive to touch. While depth cameras have been used extensively, existing methods face two major limitations: (i) they are usually trained with adult-only data and do not correctly estimate a child&rsquo;s pose, and (ii) they fail in scenarios with a high number of occlusions. Therefore, our goal was to develop a 3D pose estimator for children, by adapting an existing state-of-the-art 3D body modelling method and incorporating a linear regression model to <b>fine-tune</b> one of its inputs, thereby correcting the pose of children&rsquo;s 3D meshes. In controlled settings, our method has an error below $0.3m$, which is considered acceptable for this kind of application and lower than current state-of-the-art methods. In real-world settings, the proposed model performs similarly to a Kinect depth camera and manages to successfully estimate the 3D body poses in a much higher number of frames.</p></p class="citation"></blockquote><h3 id=78--108252-evaluation-of-a-smart-mobile-robotic-system-for-industrial-plant-inspection-and-supervision-georg-k-j-fischer-et-al-2024>(7/8 | 108/252) Evaluation of a Smart Mobile Robotic System for Industrial Plant Inspection and Supervision (Georg K. J. Fischer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Georg K. J. Fischer, Max Bergau, D. Adriana Gómez-Rosal, Andreas Wachaja, Johannes Gräter, Matthias Odenweller, Uwe Piechottka, Fabian Hoeflinger, Nikhil Gosala, Niklas Wetzel, Daniel Büscher, Abhinav Valada, Wolfram Burgard. (2024)<br><strong>Evaluation of a Smart Mobile Robotic System for Industrial Plant Inspection and Supervision</strong><br><button class=copy-to-clipboard title="Evaluation of a Smart Mobile Robotic System for Industrial Plant Inspection and Supervision" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07691v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07691v1.pdf filename=2402.07691v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated and autonomous industrial inspection is a longstanding research field, driven by the necessity to enhance safety and efficiency within industrial settings. In addressing this need, we introduce an autonomously navigating robotic system designed for comprehensive plant inspection. This innovative system comprises a robotic platform equipped with a diverse array of sensors integrated to facilitate the detection of various process and infrastructure parameters. These sensors encompass optical (LiDAR, Stereo, UV/IR/RGB cameras), olfactory (electronic nose), and acoustic (microphone array) capabilities, enabling the identification of factors such as methane leaks, flow rates, and infrastructural anomalies. The proposed system underwent individual evaluation at a wastewater treatment site within a chemical plant, providing a practical and challenging environment for testing. The evaluation process encompassed key aspects such as <b>object</b> <b>detection,</b> 3D localization, and path planning. Furthermore, specific evaluations were conducted for optical methane leak detection and localization, as well as acoustic assessments focusing on pump equipment and gas leak localization.</p></p class="citation"></blockquote><h3 id=88--109252-uav-assisted-visual-slam-generating-reconstructed-3d-scene-graphs-in-gps-denied-environments-ahmed-radwan-et-al-2024>(8/8 | 109/252) UAV-assisted Visual SLAM Generating Reconstructed 3D Scene Graphs in GPS-denied Environments (Ahmed Radwan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmed Radwan, Ali Tourani, Hriday Bavle, Holger Voos, Jose Luis Sanchez-Lopez. (2024)<br><strong>UAV-assisted Visual SLAM Generating Reconstructed 3D Scene Graphs in GPS-denied Environments</strong><br><button class=copy-to-clipboard title="UAV-assisted Visual SLAM Generating Reconstructed 3D Scene Graphs in GPS-denied Environments" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: I-4-9; I-2-9; I-2-10, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07537v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07537v1.pdf filename=2402.07537v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aerial robots play a vital role in various applications where the situational awareness of the robots concerning the environment is a fundamental demand. As one such use case, drones in GPS-denied environments require equipping with different sensors (e.g., vision sensors) that provide reliable sensing results while performing pose estimation and localization. In this paper, reconstructing the maps of indoor environments alongside generating 3D scene <b>graphs</b> for a high-level representation using a camera mounted on a drone is targeted. Accordingly, an aerial robot equipped with a companion computer and an RGB-D camera was built and employed to be appropriately integrated with a Visual Simultaneous Localization and Mapping (VSLAM) framework proposed by the authors. To enhance the situational awareness of the robot while reconstructing maps, various structural elements, including doors and walls, were labeled with printed fiducial markers, and a dictionary of the topological relations among them was fed to the system. The VSLAM system detects markers and reconstructs the map of the indoor areas enriched with higher-level semantic entities, including corridors and rooms. Another achievement is generating multi-layered vision-based situational <b>graphs</b> containing enhanced hierarchical representations of the indoor environment. In this regard, integrating VSLAM into the employed drone is the primary target of this paper to provide an end-to-end robot application for GPS-denied environments. To show the practicality of the system, various real-world condition experiments have been conducted in indoor scenarios with dissimilar structural layouts. Evaluations show the proposed drone application can perform adequately w.r.t. the ground-truth data and its baseline.</p></p class="citation"></blockquote><h2 id=csai-22>cs.AI (22)</h2><h3 id=122--110252-t-rag-lessons-from-the-llm-trenches-masoomali-fatehkia-et-al-2024>(1/22 | 110/252) T-RAG: Lessons from the LLM Trenches (Masoomali Fatehkia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masoomali Fatehkia, Ji Kim Lucas, Sanjay Chawla. (2024)<br><strong>T-RAG: Lessons from the LLM Trenches</strong><br><button class=copy-to-clipboard title="T-RAG: Lessons from the LLM Trenches" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 80<br>Keywords: Fine-tuning, Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07483v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07483v1.pdf filename=2402.07483v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is <b>question</b> <b>answering</b> over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> has emerged as the most prominent framework for building <b>LLM-based</b> applications. While building a <b>RAG</b> is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an <b>LLM</b> application for <b>question</b> <b>answering</b> over private organizational documents. Our application combines the use of <b>RAG</b> with a <b>finetuned</b> open-source <b>LLM.</b> Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure to represent entity hierarchies within the organization. This is used to generate a textual description to augment the context when responding to user queries pertaining to entities within the organization&rsquo;s hierarchy. Our evaluations show that this combination performs better than a simple <b>RAG</b> or <b>finetuning</b> implementation. Finally, we share some lessons learned based on our experiences building an <b>LLM</b> application for real-world use.</p></p class="citation"></blockquote><h3 id=222--111252-on-the-self-verification-limitations-of-large-language-models-on-reasoning-and-planning-tasks-kaya-stechly-et-al-2024>(2/22 | 111/252) On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks (Kaya Stechly et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati. (2024)<br><strong>On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks</strong><br><button class=copy-to-clipboard title="On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 63<br>Keywords: Graph, GPT, GPT-4, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08115v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08115v1.pdf filename=2402.08115v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There has been considerable divergence of opinion on the <b>reasoning</b> abilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> While the initial optimism that <b>reasoning</b> might emerge automatically with scale has been tempered thanks to a slew of counterexamples&ndash;ranging from multiplication to simple planning&ndash;there persists a wide spread belief that <b>LLMs</b> can self-critique and improve their own solutions in an iterative fashion. This belief seemingly rests on the assumption that verification of correctness should be easier than generation&ndash;a rather classical argument from computational complexity&ndash;which should be irrelevant to <b>LLMs</b> to the extent that what they are doing is approximate retrieval. In this paper, we set out to systematically investigate the effectiveness of iterative <b>prompting</b> in the context of <b>reasoning</b> and planning. We present a principled empirical study of the performance of <b>GPT-4</b> in three domains: Game of 24, <b>Graph</b> Coloring, and STRIPS planning. We experiment both with the model critiquing its own answers and with an external correct reasoner verifying proposed solutions. In each case, we analyze whether the content of criticisms actually affects bottom line performance, and whether we can ablate elements of the augmented system without losing performance. We observe significant performance collapse with self-critique, significant performance gains with sound external verification, but that the content of critique doesn&rsquo;t matter to the performance of the system. In fact, merely re-prompting with a sound verifier maintains most of the benefits of more involved setups.</p></p class="citation"></blockquote><h3 id=322--112252-cybermetric-a-benchmark-dataset-for-evaluating-large-language-models-knowledge-in-cybersecurity-norbert-tihanyi-et-al-2024>(3/22 | 112/252) CyberMetric: A Benchmark Dataset for Evaluating Large Language Models Knowledge in Cybersecurity (Norbert Tihanyi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Norbert Tihanyi, Mohamed Amine Ferrag, Ridhi Jain, Merouane Debbah. (2024)<br><strong>CyberMetric: A Benchmark Dataset for Evaluating Large Language Models Knowledge in Cybersecurity</strong><br><button class=copy-to-clipboard title="CyberMetric: A Benchmark Dataset for Evaluating Large Language Models Knowledge in Cybersecurity" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CR, cs.AI<br>Keyword Score: 63<br>Keywords: Benchmarking, GPT, GPT-3, GPT-3.5, falcon, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07688v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07688v1.pdf filename=2402.07688v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> excel across various domains, from computer vision to medical diagnostics. However, understanding the diverse landscape of cybersecurity, encompassing cryptography, reverse engineering, and managerial facets like risk assessment, presents a challenge, even for human experts. In this paper, we introduce CyberMetric, a <b>benchmark</b> dataset comprising 10,000 questions sourced from standards, certifications, research papers, books, and other publications in the cybersecurity domain. The questions are created through a collaborative process, i.e., merging expert knowledge with <b>LLMs,</b> including <b>GPT-3.5</b> and <b>Falcon-180B.</b> Human experts spent over 200 hours verifying their accuracy and relevance. Beyond assessing <b>LLMs&rsquo;</b> knowledge, the dataset&rsquo;s main goal is to facilitate a fair comparison between humans and different <b>LLMs</b> in cybersecurity. To achieve this, we carefully selected 80 questions covering a wide range of topics within cybersecurity and involved 30 participants of diverse expertise levels, facilitating a comprehensive comparison between human and machine intelligence in this area. The findings revealed that <b>LLMs</b> outperformed humans in almost every aspect of cybersecurity.</p></p class="citation"></blockquote><h3 id=422--113252-semtra-a-semantic-skill-translator-for-cross-domain-zero-shot-policy-adaptation-sangwoo-shin-et-al-2024>(4/22 | 113/252) SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy Adaptation (Sangwoo Shin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sangwoo Shin, Minjong Yoo, Jeongwoo Lee, Honguk Woo. (2024)<br><strong>SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy Adaptation</strong><br><button class=copy-to-clipboard title="SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy Adaptation" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 63<br>Keywords: Contrastive Learning, Multi-modal, Zero-shot, Reasoning, Pre-trained Language Model, Prompt, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07418v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07418v1.pdf filename=2402.07418v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work explores the <b>zero-shot</b> <b>adaptation</b> capability of semantic skills, semantically interpretable experts&rsquo; behavior patterns, in cross-domain settings, where a user input in interleaved <b>multi-modal</b> snippets can <b>prompt</b> a new long-horizon task for different domains. In these cross-domain settings, we present a semantic skill translator framework SemTra which utilizes a set of <b>multi-modal</b> models to extract skills from the snippets, and leverages the <b>reasoning</b> capabilities of a <b>pretrained</b> <b>language</b> <b>model</b> to adapt these extracted skills to the target domain. The framework employs a two-level hierarchy for adaptation: task adaptation and skill adaptation. During task adaptation, seq-to-seq translation by the language model transforms the extracted skills into a semantic skill sequence, which is tailored to fit the cross-domain contexts. Skill adaptation focuses on optimizing each semantic skill for the target domain context, through parametric instantiations that are facilitated by language <b>prompting</b> and <b>contrastive</b> <b>learning-based</b> context inferences. This hierarchical adaptation empowers the framework to not only infer a complex task specification in one-shot from the interleaved <b>multi-modal</b> snippets, but also adapt it to new domains with <b>zero-shot</b> <b>learning</b> abilities. We evaluate our framework with Meta-World, Franka Kitchen, RLBench, and CARLA environments. The results clarify the framework&rsquo;s superiority in performing long-horizon tasks and adapting to different domains, showing its broad applicability in practical use cases, such as cognitive robots interpreting abstract instructions and autonomous vehicles operating under varied configurations.</p></p class="citation"></blockquote><h3 id=522--114252-breakgpt-a-large-language-model-with-multi-stage-structure-for-financial-breakout-detection-kang-zhang-et-al-2024>(5/22 | 114/252) BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection (Kang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kang Zhang, Osamu Yoshie, Weiran Huang. (2024)<br><strong>BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection</strong><br><button class=copy-to-clipboard title="BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 50<br>Keywords: ChatGPT, GPT, GPT-3, GPT-3.5, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07536v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07536v1.pdf filename=2402.07536v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trading range breakout (TRB) is a key method in the technical analysis of financial trading, widely employed by traders in financial markets such as stocks, futures, and foreign exchange. However, distinguishing between true and false breakout and providing the correct rationale cause significant challenges to investors. Recently, <b>large</b> <b>language</b> <b>models</b> have achieved success in various downstream applications, but their effectiveness in the domain of financial breakout detection has been subpar. The reason is that the unique data and specific knowledge are required in breakout detection. To address these issues, we introduce BreakGPT, the first <b>large</b> <b>language</b> <b>model</b> for financial breakout detection. Furthermore, we have developed a novel framework for <b>large</b> <b>language</b> <b>models,</b> namely multi-stage structure, effectively reducing mistakes in downstream applications. Experimental results indicate that compared to <b>GPT-3.5,</b> BreakGPT improves the accuracy of answers and rational by 44%, with the multi-stage structure contributing 17.6% to the improvement. Additionally, it outperforms <b>ChatGPT-4</b> by 42.07%. Our Code is publicly available: <a href=https://github.com/Neviim96/BreakGPT>https://github.com/Neviim96/BreakGPT</a></p></p class="citation"></blockquote><h3 id=622--115252-secret-collusion-among-generative-ai-agents-sumeet-ramesh-motwani-et-al-2024>(6/22 | 115/252) Secret Collusion Among Generative AI Agents (Sumeet Ramesh Motwani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sumeet Ramesh Motwani, Mikhail Baranchuk, Martin Strohmeier, Vijay Bolina, Philip H. S. Torr, Lewis Hammond, Christian Schroeder de Witt. (2024)<br><strong>Secret Collusion Among Generative AI Agents</strong><br><button class=copy-to-clipboard title="Secret Collusion Among Generative AI Agents" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CR, cs.AI<br>Keyword Score: 50<br>Keywords: Generative AI, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07510v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07510v1.pdf filename=2402.07510v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent capability increases in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> open up applications in which teams of communicating <b>generative</b> <b>AI</b> agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of <b>generative</b> <b>AI</b> agents by drawing on relevant concepts from both the AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary <b>LLMs.</b> While the steganographic capabilities of current models remain limited, <b>GPT-4</b> displays a capability jump suggesting the need for continuous monitoring of steganographic frontier model capabilities. We conclude by laying out a comprehensive research program to mitigate future risks of collusion between <b>generative</b> <b>AI</b> models.</p></p class="citation"></blockquote><h3 id=722--116252-enhancing-multi-criteria-decision-analysis-with-ai-integrating-analytic-hierarchy-process-and-gpt-4-for-automated-decision-support-igor-svoboda-et-al-2024>(7/22 | 116/252) Enhancing Multi-Criteria Decision Analysis with AI: Integrating Analytic Hierarchy Process and GPT-4 for Automated Decision Support (Igor Svoboda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Igor Svoboda, Dmytro Lande. (2024)<br><strong>Enhancing Multi-Criteria Decision Analysis with AI: Integrating Analytic Hierarchy Process and GPT-4 for Automated Decision Support</strong><br><button class=copy-to-clipboard title="Enhancing Multi-Criteria Decision Analysis with AI: Integrating Analytic Hierarchy Process and GPT-4 for Automated Decision Support" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-1; I-2-8; H-1-1, cs-AI, cs-CR, cs-MA, cs.AI<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07404v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07404v1.pdf filename=2402.07404v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our study presents a new framework that incorporates the Analytic Hierarchy Process (AHP) and Generative Pre-trained <b>Transformer</b> 4 <b>(GPT-4)</b> <b>large</b> <b>language</b> <b>model</b> <b>(LLM),</b> bringing novel approaches to cybersecurity Multiple-criteria Decision Making (MCDA). By utilizing the capabilities of <b>GPT-4</b> autonomous agents as virtual experts, we automate the decision-making process, enhancing both efficiency and reliability. This new approach focuses on leveraging <b>LLMs</b> for sophisticated decision analysis, highlighting the synergy between traditional decision-making models and cutting-edge AI technologies. Our innovative methodology demonstrates significant advancements in using AI-driven agents for complex decision-making scenarios, highlighting the importance of AI in strategic cybersecurity applications. The findings reveal the transformative potential of combining AHP and <b>LLMs,</b> establishing a new paradigm for intelligent decision support systems in cybersecurity and beyond.</p></p class="citation"></blockquote><h3 id=822--117252-extensible-multi-granularity-fusion-network-for-aspect-based-sentiment-analysis-xiaowei-zhao-et-al-2024>(8/22 | 117/252) Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis (Xiaowei Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaowei Zhao, Yong Zhou, Xiujuan Xu, Yu Liu. (2024)<br><strong>Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis</strong><br><button class=copy-to-clipboard title="Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 48<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Knowledge Graph, Aspect-based Sentiment Analysis, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07787v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07787v2.pdf filename=2402.07787v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Aspect-based</b> <b>Sentiment</b> <b>Analysis</b> (ABSA) evaluates <b>sentiment</b> <b>expressions</b> within a text to comprehend <b>sentiment</b> <b>information.</b> Previous studies integrated external <b>knowledge,</b> <b>such</b> as <b>knowledge</b> <b>graphs,</b> <b>to</b> <b>enhance</b> the semantic features in ABSA models. Recent research has examined the use of <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> on dependency and constituent trees for syntactic analysis. With the ongoing development of ABSA, more innovative linguistic and structural features are being incorporated (e.g. latent <b>graph),</b> <b>but</b> <b>this</b> also introduces complexity and confusion. As of now, a scalable framework for integrating diverse linguistic and structural features into ABSA does not exist. This paper presents the Extensible Multi-Granularity Fusion (EMGF) network, which integrates information from dependency and constituent syntactic, attention semantic , and external <b>knowledge</b> <b>graphs.</b> <b>EMGF,</b> <b>equipped</b> with multi-anchor triplet learning and orthogonal projection, efficiently harnesses the combined potential of each granularity feature and their synergistic interactions, resulting in a cumulative effect without additional computational expenses. Experimental findings on SemEval 2014 and Twitter datasets confirm EMGF&rsquo;s superiority over existing ABSA methods.</p></p class="citation"></blockquote><h3 id=922--118252-vislinginstruct-elevating-zero-shot-learning-in-multi-modal-language-models-with-autonomous-instruction-optimization-dongsheng-zhu-et-al-2024>(9/22 | 118/252) VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization (Dongsheng Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongsheng Zhu, Xunzhu Tang, Weidong Han, Jinghui Lu, Yukun Zhao, Guoliang Xing, Junfeng Wang, Dawei Yin. (2024)<br><strong>VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization</strong><br><button class=copy-to-clipboard title="VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 43<br>Keywords: Multi-modal, Zero-shot, In-context Learning, In-context Learning, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07398v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07398v1.pdf filename=2402.07398v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents VisLingInstruct, a novel approach to advancing <b>Multi-Modal</b> Language Models (MMLMs) in <b>zero-shot</b> <b>learning.</b> Current MMLMs show impressive <b>zero-shot</b> <b>abilities</b> in <b>multi-modal</b> tasks, but their performance depends heavily on the quality of instructions. VisLingInstruct tackles this by autonomously evaluating and optimizing instructional texts through <b>In-Context</b> <b>Learning,</b> improving the synergy between visual perception and linguistic expression in MMLMs. Alongside this instructional advancement, we have also optimized the visual feature extraction modules in MMLMs, further augmenting their responsiveness to textual cues. Our comprehensive experiments on MMLMs, based on FlanT5 and Vicuna, show that VisLingInstruct significantly improves <b>zero-shot</b> <b>performance</b> in visual <b>multi-modal</b> tasks. Notably, it achieves a 13.1% and 9% increase in accuracy over the prior state-of-the-art on the TextVQA and HatefulMemes datasets.</p></p class="citation"></blockquote><h3 id=1022--119252-towards-unified-alignment-between-agents-humans-and-environment-zonghan-yang-et-al-2024>(10/22 | 119/252) Towards Unified Alignment Between Agents, Humans, and Environment (Zonghan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zonghan Yang, An Liu, Zijun Liu, Kaiming Liu, Fangzhou Xiong, Yile Wang, Zeyuan Yang, Qingyuan Hu, Xinrui Chen, Zhenhe Zhang, Fuwen Luo, Zhicheng Guo, Peng Li, Yang Liu. (2024)<br><strong>Towards Unified Alignment Between Agents, Humans, and Environment</strong><br><button class=copy-to-clipboard title="Towards Unified Alignment Between Agents, Humans, and Environment" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 33<br>Keywords: Benchmarking, Foundation Model, Rerank, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07744v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07744v2.pdf filename=2402.07744v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid progress of <b>foundation</b> <b>models</b> has led to the prosperity of autonomous agents, which leverage the universal capabilities of <b>foundation</b> <b>models</b> to conduct <b>reasoning,</b> decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of $\mathbf{U}$nified $\mathbf{A}$lignment for $\mathbf{A}$gents ($\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of $\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent <b>benchmarks</b> and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized <b>reranking</b> for complex environmental dynamics, and runtime cost statistics to reflect self-constraints. We then follow the principles of $\mathbf{UA}^2$ to propose an initial design of our agent, and <b>benchmark</b> its performance with several candidate baselines in the retrofitted WebShop. The extensive experimental results further prove the importance of the principles of $\mathbf{UA}^2$. Our research sheds light on the next steps of autonomous agent research with improved general problem-solving abilities.</p></p class="citation"></blockquote><h3 id=1122--120252-out-of-distribution-detection-and-data-drift-monitoring-using-statistical-process-control-ghada-zamzmi-et-al-2024>(11/22 | 120/252) Out-of-Distribution Detection and Data Drift Monitoring using Statistical Process Control (Ghada Zamzmi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ghada Zamzmi, Kesavan Venkatesh, Brandon Nelson, Smriti Prathapan, Paul H. Yi, Berkman Sahiner, Jana G. Delfino. (2024)<br><strong>Out-of-Distribution Detection and Data Drift Monitoring using Statistical Process Control</strong><br><button class=copy-to-clipboard title="Out-of-Distribution Detection and Data Drift Monitoring using Statistical Process Control" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI, eess-IV<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08088v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08088v1.pdf filename=2402.08088v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background: Machine learning (ML) methods often fail with data that deviates from their training distribution. This is a significant concern for ML-enabled devices in clinical settings, where data drift may cause unexpected performance that jeopardizes patient safety. Method: We propose a ML-enabled Statistical Process Control (SPC) framework for <b>out-of-distribution</b> (OOD) detection and drift monitoring. SPC is advantageous as it visually and statistically highlights deviations from the expected distribution. To demonstrate the utility of the proposed framework for monitoring data drift in radiological images, we investigated different design choices, including methods for extracting feature representations, drift quantification, and SPC parameter selection. Results: We demonstrate the effectiveness of our framework for two tasks: 1) differentiating axial vs. non-axial computed tomography (CT) images and 2) separating chest x-ray (CXR) from other modalities. For both tasks, we achieved high accuracy in detecting OOD inputs, with 0.913 in CT and 0.995 in CXR, and sensitivity of 0.980 in CT and 0.984 in CXR. Our framework was also adept at monitoring data streams and identifying the time a drift occurred. In a <b>simulation</b> with 100 daily CXR cases, we detected a drift in OOD input percentage from 0-1% to 3-5% within two days, maintaining a low false-positive rate. Through additional experimental results, we demonstrate the framework&rsquo;s data-agnostic nature and independence from the underlying model&rsquo;s structure. Conclusion: We propose a framework for OOD detection and drift monitoring that is agnostic to data, modality, and model. The framework is customizable and can be adapted for specific applications.</p></p class="citation"></blockquote><h3 id=1222--121252-beyond-llms-advancing-the-landscape-of-complex-reasoning-jennifer-chu-carroll-et-al-2024>(12/22 | 121/252) Beyond LLMs: Advancing the Landscape of Complex Reasoning (Jennifer Chu-Carroll et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jennifer Chu-Carroll, Andrew Beck, Greg Burnham, David OS Melville, David Nachman, A. Erdem Özcan, David Ferrucci. (2024)<br><strong>Beyond LLMs: Advancing the Landscape of Complex Reasoning</strong><br><button class=copy-to-clipboard title="Beyond LLMs: Advancing the Landscape of Complex Reasoning" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08064v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08064v1.pdf filename=2402.08064v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since the advent of <b>Large</b> <b>Language</b> <b>Models</b> a few years ago, they have often been considered the de facto solution for many AI problems. However, in addition to the many deficiencies of <b>LLMs</b> that prevent them from broad industry adoption, such as reliability, cost, and speed, there is a whole class of common real world problems that <b>Large</b> <b>Language</b> <b>Models</b> perform poorly on, namely, constraint satisfaction and optimization problems. These problems are ubiquitous and current solutions are highly specialized and expensive to implement. At Elemental Cognition, we developed our EC AI platform which takes a neuro-symbolic approach to solving constraint satisfaction and optimization problems. The platform employs, at its core, a precise and high performance logical <b>reasoning</b> engine, and leverages <b>LLMs</b> for knowledge acquisition and user interaction. This platform supports developers in specifying application logic in natural and concise language while generating application user interfaces to interact with users effectively. We evaluated <b>LLMs</b> against systems built on the EC AI platform in three domains and found the EC AI systems to significantly outperform <b>LLMs</b> on constructing valid and optimal solutions, on validating proposed solutions, and on repairing invalid solutions.</p></p class="citation"></blockquote><h3 id=1322--122252-maidcrl-semi-centralized-multi-agent-influence-dense-cnn-reinforcement-learning-ayesha-siddika-nipu-et-al-2024>(13/22 | 122/252) MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning (Ayesha Siddika Nipu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayesha Siddika Nipu, Siming Liu, Anthony Harris. (2024)<br><strong>MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning</strong><br><button class=copy-to-clipboard title="MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07890v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07890v1.pdf filename=2402.07890v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributed decision-making in multi-agent systems presents difficult challenges for interactive behavior learning in both cooperative and competitive systems. To mitigate this complexity, MAIDRL presents a semi-centralized Dense <b>Reinforcement</b> <b>Learning</b> algorithm enhanced by agent influence maps (AIMs), for learning effective multi-agent control on StarCraft Multi-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN <b>Reinforcement</b> <b>Learning,</b> MAIDCRL, by incorporating <b>convolutional</b> layers into the deep model architecture, and evaluate the performance on both homogeneous and heterogeneous scenarios. The results show that the <b>CNN-enabled</b> MAIDCRL significantly improved the learning performance and achieved a faster learning rate compared to the existing MAIDRL, especially on more complicated heterogeneous SMAC scenarios. We further investigate the stability and robustness of our model. The statistics reflect that our model not only achieves higher winning rate in all the given scenarios but also boosts the agent&rsquo;s learning process in fine-grained decision-making.</p></p class="citation"></blockquote><h3 id=1422--123252-food-recommendation-as-language-processing-f-rlp-a-personalized-and-contextual-paradigm-ali-rostami-et-al-2024>(14/22 | 123/252) Food Recommendation as Language Processing (F-RLP): A Personalized and Contextual Paradigm (Ali Rostami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Rostami, Ramesh Jain, Amir M. Rahmani. (2024)<br><strong>Food Recommendation as Language Processing (F-RLP): A Personalized and Contextual Paradigm</strong><br><button class=copy-to-clipboard title="Food Recommendation as Language Processing (F-RLP): A Personalized and Contextual Paradigm" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Recommendation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07477v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07477v2.pdf filename=2402.07477v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>State-of-the-art rule-based and classification-based food <b>recommendation</b> systems face significant challenges in becoming practical and useful. This difficulty arises primarily because most machine learning models struggle with problems characterized by an almost infinite number of classes and a limited number of samples within an unbalanced dataset. Conversely, the emergence of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> as <b>recommendation</b> engines offers a promising avenue. However, a general-purpose <b>Recommendation</b> as Language Processing (RLP) approach lacks the critical components necessary for effective food <b>recommendations.</b> To address this gap, we introduce Food <b>Recommendation</b> as Language Processing (F-RLP), a novel framework that offers a food-specific, tailored infrastructure. F-RLP leverages the capabilities of <b>LLMs</b> to maximize their potential, thereby paving the way for more accurate, personalized food <b>recommendations.</b></p></p class="citation"></blockquote><h3 id=1522--124252-game-agent-driven-by-free-form-text-command-using-llm-based-code-generation-and-behavior-branch-ray-ito-et-al-2024>(15/22 | 124/252) Game Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and Behavior Branch (Ray Ito et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ray Ito, Junichiro Takahashi. (2024)<br><strong>Game Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and Behavior Branch</strong><br><button class=copy-to-clipboard title="Game Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and Behavior Branch" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07442v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07442v1.pdf filename=2402.07442v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several attempts have been made to implement text command control for game agents. However, current technologies are limited to processing predefined format commands. This paper proposes a pioneering text command control system for a game agent that can understand natural language commands expressed in free-form. The proposed system uses a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> for <b>code</b> <b>generation</b> to interpret and transform natural language commands into behavior branch, a proposed knowledge expression based on behavior trees, which facilitates execution by the game agent. This study conducted empirical validation within a game environment that simulates a Pok'emon game and involved multiple participants. The results confirmed the system&rsquo;s ability to understand and carry out natural language commands, representing a noteworthy in the realm of real-time language interactive game agents. Notice for the use of this material. The copyright of this material is retained by the Japanese Society for Artificial Intelligence (JSAI). This material is published here with the agreement of JSAI. Please be complied with Copyright Law of Japan if any users wish to reproduce, make derivative work, distribute or make available to the public any part or whole thereof. All Rights Reserved, Copyright (C) The Japanese Society for Artificial Intelligence.</p></p class="citation"></blockquote><h3 id=1622--125252-os-copilot-towards-generalist-computer-agents-with-self-improvement-zhiyong-wu-et-al-2024>(16/22 | 125/252) OS-Copilot: Towards Generalist Computer Agents with Self-Improvement (Zhiyong Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, Lingpeng Kong. (2024)<br><strong>OS-Copilot: Towards Generalist Computer Agents with Self-Improvement</strong><br><button class=copy-to-clipboard title="OS-Copilot: Towards Generalist Computer Agents with Self-Improvement" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07456v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07456v2.pdf filename=2402.07456v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous interaction with the computer has been a longstanding challenge with great potential, and the recent proliferation of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has markedly accelerated progress in building digital agents. However, most of these agents are designed to interact with a narrow domain, such as a specific software or website. This narrow focus constrains their applicability for general computer tasks. To this end, we introduce OS-Copilot, a framework to build generalist agents capable of interfacing with comprehensive elements in an operating system (OS), including the web, code terminals, files, multimedia, and various third-party applications. We use OS-Copilot to create FRIDAY, a self-improving embodied agent for automating general computer tasks. On GAIA, a general AI assistants <b>benchmark,</b> FRIDAY outperforms previous methods by 35%, showcasing strong generalization to unseen applications via accumulated skills from previous tasks. We also present numerical and quantitative evidence that FRIDAY learns to control and self-improve on Excel and Powerpoint with minimal supervision. Our OS-Copilot framework and empirical findings provide infrastructure and insights for future research toward more capable and general-purpose computer agents.</p></p class="citation"></blockquote><h3 id=1722--126252-recursive-joint-simulation-in-games-vojtech-kovarik-et-al-2024>(17/22 | 126/252) Recursive Joint Simulation in Games (Vojtech Kovarik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vojtech Kovarik, Caspar Oesterheld, Vincent Conitzer. (2024)<br><strong>Recursive Joint Simulation in Games</strong><br><button class=copy-to-clipboard title="Recursive Joint Simulation in Games" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-GT, cs.AI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08128v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08128v1.pdf filename=2402.08128v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Game-theoretic dynamics between AI agents could differ from traditional human-human interactions in various ways. One such difference is that it may be possible to accurately simulate an AI agent, for example because its source code is known. Our aim is to explore ways of leveraging this possibility to achieve more cooperative outcomes in strategic settings. In this paper, we study an interaction between AI agents where the agents run a recursive joint <b>simulation.</b> That is, the agents first jointly observe a <b>simulation</b> of the situation they face. This <b>simulation</b> in turn recursively includes additional <b>simulations</b> (with a small chance of failure, to avoid infinite recursion), and the results of all these nested <b>simulations</b> are observed before an action is chosen. We show that the resulting interaction is strategically equivalent to an infinitely repeated version of the original game, allowing a direct transfer of existing results such as the various folk theorems.</p></p class="citation"></blockquote><h3 id=1822--127252-wildfiregpt-tailored-large-language-model-for-wildfire-analysis-yangxinyu-xie-et-al-2024>(18/22 | 127/252) WildfireGPT: Tailored Large Language Model for Wildfire Analysis (Yangxinyu Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yangxinyu Xie, Tanwi Mallick, Joshua David Bergerson, John K. Hutchison, Duane R. Verner, Jordan Branham, M. Ross Alexander, Robert B. Ross, Yan Feng, Leslie-Anne Levy, Weijie Su. (2024)<br><strong>WildfireGPT: Tailored Large Language Model for Wildfire Analysis</strong><br><button class=copy-to-clipboard title="WildfireGPT: Tailored Large Language Model for Wildfire Analysis" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07877v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07877v1.pdf filename=2402.07877v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent advancement of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> represents a transformational capability at the frontier of artificial intelligence (AI) and machine learning (ML). However, <b>LLMs</b> are generalized models, trained on extensive text corpus, and often struggle to provide context-specific information, particularly in areas requiring specialized knowledge such as wildfire details within the broader context of climate change. For decision-makers and policymakers focused on wildfire resilience and adaptation, it is crucial to obtain responses that are not only precise but also domain-specific, rather than generic. To that end, we developed WildfireGPT, a prototype <b>LLM</b> agent designed to transform user queries into actionable insights on wildfire risks. We enrich WildfireGPT by providing additional context such as climate projections and scientific literature to ensure its information is current, relevant, and scientifically accurate. This enables WildfireGPT to be an effective tool for delivering detailed, user-specific insights on wildfire risks to support a diverse set of end users, including researchers, engineers, urban planners, emergency managers, and infrastructure operators.</p></p class="citation"></blockquote><h3 id=1922--128252-generalising-planning-environment-redesign-alberto-pozanco-et-al-2024>(19/22 | 128/252) Generalising Planning Environment Redesign (Alberto Pozanco et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alberto Pozanco, Ramon Fraga Pereira, Daniel Borrajo. (2024)<br><strong>Generalising Planning Environment Redesign</strong><br><button class=copy-to-clipboard title="Generalising Planning Environment Redesign" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 13<br>Keywords: Benchmarking, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07799v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07799v2.pdf filename=2402.07799v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Environment Design, one interested party seeks to affect another agent&rsquo;s decisions by applying changes to the environment. Most research on planning environment (re)design assumes the interested party&rsquo;s objective is to facilitate the recognition of goals and plans, and search over the space of environment modifications to find the minimal set of changes that simplify those tasks and optimise a particular metric. This search space is usually intractable, so existing approaches devise metric-dependent <b>pruning</b> techniques for performing search more efficiently. This results in approaches that are not able to generalise across different objectives and/or metrics. In this paper, we argue that the interested party could have objectives and metrics that are not necessarily related to recognising agents&rsquo; goals or plans. Thus, to generalise the task of Planning Environment Redesign, we develop a general environment redesign approach that is metric-agnostic and leverages recent research on top-quality planning to efficiently redesign planning environments according to any interested party&rsquo;s objective and metric. Experiments over a set of environment redesign <b>benchmarks</b> show that our general approach outperforms existing approaches when using well-known metrics, such as facilitating the recognition of goals, as well as its effectiveness when solving environment redesign tasks that optimise a novel set of different metrics.</p></p class="citation"></blockquote><h3 id=2022--129252-end-to-end-learning-for-fair-multiobjective-optimization-under-uncertainty-my-h-dinh-et-al-2024>(20/22 | 129/252) End-to-End Learning for Fair Multiobjective Optimization Under Uncertainty (My H Dinh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>My H Dinh, James Kotary, Ferdinando Fioretto. (2024)<br><strong>End-to-End Learning for Fair Multiobjective Optimization Under Uncertainty</strong><br><button class=copy-to-clipboard title="End-to-End Learning for Fair Multiobjective Optimization Under Uncertainty" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07772v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07772v1.pdf filename=2402.07772v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many decision processes in artificial intelligence and operations research are modeled by parametric optimization problems whose defining parameters are unknown and must be inferred from observable data. The Predict-Then-Optimize (PtO) paradigm in machine learning aims to maximize downstream decision quality by training the parametric inference model end-to-end with the subsequent constrained optimization. This requires backpropagation through the optimization problem using approximation techniques specific to the problem&rsquo;s form, especially for nondifferentiable linear and mixed-integer programs. This paper extends the PtO methodology to optimization problems with nondifferentiable Ordered Weighted Averaging (OWA) objectives, known for their ability to ensure properties of <b>fairness</b> and robustness in decision models. Through a collection of training techniques and proposed application settings, it shows how optimization of OWA functions can be effectively integrated with parametric prediction for fair and robust optimization under uncertainty.</p></p class="citation"></blockquote><h3 id=2122--130252-news-recommendation-with-attention-mechanism-tianrui-liu-et-al-2024>(21/22 | 130/252) News Recommendation with Attention Mechanism (Tianrui Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianrui Liu, Changxin Xu, Yuxin Qiao, Chufeng Jiang, Weisheng Chen. (2024)<br><strong>News Recommendation with Attention Mechanism</strong><br><button class=copy-to-clipboard title="News Recommendation with Attention Mechanism" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07422v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07422v1.pdf filename=2402.07422v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the area of news <b>recommendation,</b> a key component of online information sharing. Initially, we provide a clear introduction to news <b>recommendation,</b> defining the core problem and summarizing current methods and notable recent algorithms. We then present our work on implementing the NRAM (News <b>Recommendation</b> with Attention Mechanism), an attention-based approach for news <b>recommendation,</b> and assess its effectiveness. Our evaluation shows that NRAM has the potential to significantly improve how news content is personalized for users on digital news platforms.</p></p class="citation"></blockquote><h3 id=2222--131252-clustering-dynamics-for-improved-speed-prediction-deriving-from-topographical-gps-registrations-sarah-almeida-carneiro-et-al-2024>(22/22 | 131/252) Clustering Dynamics for Improved Speed Prediction Deriving from Topographical GPS Registrations (Sarah Almeida Carneiro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sarah Almeida Carneiro, Giovanni Chierchia, Aurelie Pirayre, Laurent Najman. (2024)<br><strong>Clustering Dynamics for Improved Speed Prediction Deriving from Topographical GPS Registrations</strong><br><button class=copy-to-clipboard title="Clustering Dynamics for Improved Speed Prediction Deriving from Topographical GPS Registrations" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07507v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07507v1.pdf filename=2402.07507v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A persistent challenge in the field of Intelligent Transportation Systems is to extract accurate traffic insights from geographic regions with scarce or no data coverage. To this end, we propose solutions for speed prediction using sparse GPS data points and their associated topographical and road design features. Our goal is to investigate whether we can use similarities in the terrain and infrastructure to train a machine learning model that can predict speed in regions where we lack transportation data. For this we create a Temporally Orientated Speed Dictionary Centered on Topographically Clustered Roads, which helps us to provide speed correlations to selected feature configurations. Our results show qualitative and quantitative improvement over new and standard regression methods. The presented framework provides a fresh perspective on devising strategies for missing data traffic analysis.</p></p class="citation"></blockquote><h2 id=csar-3>cs.AR (3)</h2><h3 id=13--132252-ir-aware-eco-timing-optimization-using-reinforcement-learning-vidya-a-chhabria-et-al-2024>(1/3 | 132/252) IR-Aware ECO Timing Optimization Using Reinforcement Learning (Vidya A. Chhabria et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vidya A. Chhabria, Wenjing Jiang, Sachin S. Sapatnekar. (2024)<br><strong>IR-Aware ECO Timing Optimization Using Reinforcement Learning</strong><br><button class=copy-to-clipboard title="IR-Aware ECO Timing Optimization Using Reinforcement Learning" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-LG, cs.AR<br>Keyword Score: 73<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Fine-tuning, Reinforcement Learning, Zero-shot, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07781v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07781v1.pdf filename=2402.07781v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Engineering change orders (ECOs) in late stages make minimal design fixes to recover from timing shifts due to excessive IR drops. This paper integrates IR-drop-aware timing analysis and ECO timing optimization using <b>reinforcement</b> <b>learning</b> (RL). The method operates after physical design and power grid synthesis, and rectifies IR-drop-induced timing degradation through gate sizing. It incorporates the Lagrangian relaxation (LR) technique into a novel RL framework, which trains a relational <b>graph</b> <b>convolutional</b> <b>network</b> (R-GCN) agent to sequentially size gates to fix timing violations. The R-GCN agent outperforms a classical LR-only algorithm: in an open 45nm technology, it (a) moves the Pareto front of the delay-area tradeoff curve to the left and (b) saves runtime over the classical method by running fast inference using trained models at iso-quality. The RL model is transferable across timing specifications, and transferable to unseen designs with <b>zero-shot</b> <b>learning</b> or fine tuning.</p></p class="citation"></blockquote><h3 id=23--133252-lfoc-a-fair-os-level-cache-clustering-policy-for-commodity-multicore-systems-juan-carlos-saez-et-al-2024>(2/3 | 133/252) LFOC+: A Fair OS-level Cache-Clustering Policy for Commodity Multicore Systems (Juan Carlos Saez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juan Carlos Saez, Fernando Castro, Graziano Fanizzi, Manuel Prieto-Matias. (2024)<br><strong>LFOC+: A Fair OS-level Cache-Clustering Policy for Commodity Multicore Systems</strong><br><button class=copy-to-clipboard title="LFOC+: A Fair OS-level Cache-Clustering Policy for Commodity Multicore Systems" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 30<br>Keywords: Fairness, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07693v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07693v1.pdf filename=2402.07693v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Commodity multicore systems are increasingly adopting hardware support that enables the system software to partition the last-level cache (LLC). This support makes it possible for the operating system (OS) or the Virtual Machine Monitor (VMM) to mitigate shared-resource contention effects on multicores by assigning different co-running applications to various cache partitions. Recently cache-clustering (or partition-sharing) strategies have emerged as a way to improve system throughput and <b>fairness</b> on new platforms with cache-partitioning support. As opposed to strict cache-partitioning, which allocates separate cache partitions to each application, cache-clustering allows partitions to be shared by a group of applications. In this article we propose LFOC+, a <b>fairness-aware</b> OS-level cache-clustering policy for commodity multicore systems. LFOC+ tries to mimic the behavior of the optimal cache-clustering solution for <b>fairness,</b> which we could obtain for different workload scenarios by using a <b>simulation</b> tool. Our dynamic cache-clustering strategy continuously gathers data from performance monitoring counters to classify applications at runtime based on the degree of cache sensitivity and contentiousness, and effectively separates cache-sensitive applications from aggressor programs to improve <b>fairness,</b> while providing acceptable system throughput. We implemented LFOC+ in the Linux kernel and evaluated it on a real system featuring an Intel Skylake processor, where we compare its effectiveness to that of four previously proposed cache-clustering policies. Our experimental analysis reveals that LFOC+ constitutes a lightweight OS-level policy and improves <b>fairness</b> relative to two other state-of-the-art <b>fairness-aware</b> strategies &ndash;Dunn and LFOC&ndash;, by up to 22% and up to 20.6%, respectively, and by 9% and 4.9% on average.</p></p class="citation"></blockquote><h3 id=33--134252-a-precision-optimized-fixed-point-near-memory-digital-processing-unit-for-analog-in-memory-computing-elena-ferro-et-al-2024>(3/3 | 134/252) A Precision-Optimized Fixed-Point Near-Memory Digital Processing Unit for Analog In-Memory Computing (Elena Ferro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elena Ferro, Athanasios Vasilopoulos, Corey Lammie, Manuel Le Gallo, Luca Benini, Irem Boybat, Abu Sebastian. (2024)<br><strong>A Precision-Optimized Fixed-Point Near-Memory Digital Processing Unit for Analog In-Memory Computing</strong><br><button class=copy-to-clipboard title="A Precision-Optimized Fixed-Point Near-Memory Digital Processing Unit for Analog In-Memory Computing" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-ET, cs-LG, cs.AR<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07549v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07549v1.pdf filename=2402.07549v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Analog In-Memory Computing (AIMC) is an emerging technology for fast and energy-efficient Deep Learning (DL) inference. However, a certain amount of digital post-processing is required to deal with circuit mismatches and non-idealities associated with the memory devices. Efficient near-memory digital logic is critical to retain the high area/energy efficiency and low latency of AIMC. Existing systems adopt Floating Point 16 (FP16) arithmetic with limited parallelization capability and high latency. To overcome these limitations, we propose a Near-Memory digital Processing Unit (NMPU) based on fixed-point arithmetic. It achieves competitive accuracy and higher computing throughput than previous approaches while minimizing the area overhead. Moreover, the NMPU supports standard DL activation steps, such as ReLU and Batch Normalization. We perform a physical implementation of the NMPU design in a 14 nm CMOS technology and provide detailed performance, power, and area assessments. We validate the efficacy of the NMPU by using data from an AIMC chip and demonstrate that a simulated AIMC system with the proposed NMPU outperforms existing FP16-based implementations, providing 139$\times$ speed-up, 7.8$\times$ smaller area, and a competitive power consumption. Additionally, our approach achieves an inference accuracy of 86.65 %/65.06 %, with an accuracy drop of just 0.12 %/0.4 % compared to the FP16 baseline when <b>benchmarked</b> with ResNet9/ResNet32 networks trained on the CIFAR10/CIFAR100 datasets, respectively.</p></p class="citation"></blockquote><h2 id=cscr-9>cs.CR (9)</h2><h3 id=19--135252-large-language-models-are-few-shot-generators-proposing-hybrid-prompt-algorithm-to-generate-webshell-escape-samples-mingrui-ma-et-al-2024>(1/9 | 135/252) Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples (Mingrui Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingrui Ma, Lansheng Han, Chunjie Zhou. (2024)<br><strong>Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples</strong><br><button class=copy-to-clipboard title="Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 73<br>Keywords: Benchmarking, Few-shot, GPT, GPT-4, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07408v1.pdf filename=2402.07408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The frequent occurrence of cyber-attacks has made webshell attacks and defense gradually become a research hotspot in the field of network security. However, the lack of publicly available <b>benchmark</b> datasets and the over-reliance on manually defined rules for webshell escape sample generation have slowed down the progress of research related to webshell escape sample generation strategies and artificial intelligence-based webshell detection algorithms. To address the drawbacks of weak webshell sample escape capabilities, the lack of webshell datasets with complex malicious features, and to promote the development of webshell detection technology, we propose the Hybrid <b>Prompt</b> algorithm for webshell escape sample generation with the help of <b>large</b> <b>language</b> <b>models.</b> As a <b>prompt</b> algorithm specifically developed for webshell sample generation, the Hybrid <b>Prompt</b> algorithm not only combines various <b>prompt</b> ideas including Chain of Thought, Tree of Thought, but also incorporates various components such as webshell hierarchical module and <b>few-shot</b> example to facilitate the <b>LLM</b> in learning and <b>reasoning</b> webshell escape strategies. Experimental results show that the Hybrid <b>Prompt</b> algorithm can work with multiple <b>LLMs</b> with excellent code <b>reasoning</b> ability to generate high-quality webshell samples with high Escape Rate (88.61% with <b>GPT-4</b> model on VIRUSTOTAL detection engine) and Survival Rate (54.98% with <b>GPT-4</b> model).</p></p class="citation"></blockquote><h3 id=29--136252-poisonedrag-knowledge-poisoning-attacks-to-retrieval-augmented-generation-of-large-language-models-wei-zou-et-al-2024>(2/9 | 136/252) PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models (Wei Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Zou, Runpeng Geng, Binghui Wang, Jinyuan Jia. (2024)<br><strong>PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models</strong><br><button class=copy-to-clipboard title="PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 53<br>Keywords: Benchmarking, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07867v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07867v1.pdf filename=2402.07867v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> is a state-of-the-art technique to mitigate those limitations. In particular, given a question, <b>RAG</b> retrieves relevant knowledge from a knowledge database to augment the input of the <b>LLM.</b> For instance, the retrieved knowledge could be a set of top-k texts that are most semantically similar to the given question when the knowledge database contains millions of texts collected from Wikipedia. As a result, the <b>LLM</b> could utilize the retrieved knowledge as the context to generate an answer for the given question. Existing studies mainly focus on improving the accuracy or efficiency of <b>RAG,</b> leaving its security largely unexplored. We aim to bridge the gap in this work. Particularly, we propose PoisonedRAG , a set of knowledge poisoning attacks to <b>RAG,</b> where an attacker could inject a few poisoned texts into the knowledge database such that the <b>LLM</b> generates an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge poisoning attacks as an optimization problem, whose solution is a set of poisoned texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on the <b>RAG,</b> we propose two solutions to solve the optimization problem, respectively. Our results on multiple <b>benchmark</b> datasets and <b>LLMs</b> show our attacks could achieve 90% attack success rates when injecting 5 poisoned texts for each target question into a database with millions of texts. We also evaluate recent defenses and our results show they are insufficient to defend against our attacks, highlighting the need for new defenses.</p></p class="citation"></blockquote><h3 id=39--137252-resilient-watermarking-for-llm-generated-codes-boquan-li-et-al-2024>(3/9 | 137/252) Resilient Watermarking for LLM-Generated Codes (Boquan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boquan Li, Mengdi Zhang, Peixin Zhang, Jun Sun, Xingmei Wang. (2024)<br><strong>Resilient Watermarking for LLM-Generated Codes</strong><br><button class=copy-to-clipboard title="Resilient Watermarking for LLM-Generated Codes" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 50<br>Keywords: Fine-tuning, ChatGPT, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07518v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07518v1.pdf filename=2402.07518v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the development of <b>large</b> <b>language</b> <b>models,</b> multiple AIs are now made available for <b>code</b> <b>generation</b> (such as <b>ChatGPT</b> and StarCoder) and are adopted widely. It is often desirable to know whether a piece of <b>code</b> <b>is</b> generated by AI, and furthermore, which AI is the author. For instance, if a certain version of AI is known to generate vulnerable <b>code,</b> <b>it</b> is particularly important to know the creator. Existing approaches are not satisfactory as watermarking <b>codes</b> <b>are</b> challenging compared with watermarking text data, as <b>codes</b> <b>can</b> be altered with relative ease via widely-used <b>code</b> <b>refactoring</b> methods. In this work, we propose ACW (AI <b>Code</b> <b>Watermarking),</b> a novel method for watermarking AI-generated <b>codes.</b> <b>ACW</b> is efficient as it requires no training or <b>fine-tuning</b> and works in a black-box manner. It is resilient as the watermark cannot be easily removed or tampered through common <b>code</b> <b>refactoring</b> methods. The key idea of ACW is to selectively apply a set of carefully-designed semantic-preserving, idempotent <b>code</b> <b>transformations,</b> whose presence (or absence) allows us to determine the existence of the watermark. Our experimental results show that ACW is effective (i.e., achieving high accuracy, true positive rates and false positive rates), resilient and efficient, significantly outperforming existing approaches.</p></p class="citation"></blockquote><h3 id=49--138252-adaptive-artificial-immune-networks-for-mitigating-dos-flooding-attacks-jorge-maestre-vidal-et-al-2024>(4/9 | 138/252) Adaptive Artificial Immune Networks for Mitigating DoS flooding Attacks (Jorge Maestre Vidal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jorge Maestre Vidal, Ana Lucila Sandoval Orozco, Luis Javier García Villalba. (2024)<br><strong>Adaptive Artificial Immune Networks for Mitigating DoS flooding Attacks</strong><br><button class=copy-to-clipboard title="Adaptive Artificial Immune Networks for Mitigating DoS flooding Attacks" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07714v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07714v1.pdf filename=2402.07714v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Denial of service attacks pose a threat in constant growth. This is mainly due to their tendency to gain in sophistication, ease of implementation, obfuscation and the recent improvements in occultation of fingerprints. On the other hand, progress towards self-organizing networks, and the different techniques involved in their development, such as software-defined networking, network-function virtualization, artificial intelligence or cloud computing, facilitates the design of new defensive strategies, more complete, consistent and able to adapt the defensive deployment to the current status of the network. In order to contribute to their development, in this paper, the use of artificial immune systems to mitigate denial of service attacks is proposed. The approach is based on building networks of distributed sensors suited to the requirements of the monitored environment. These components are capable of identifying threats and reacting according to the behavior of the biological defense mechanisms in human beings. It is accomplished by emulating the different immune reactions, the establishment of quarantine areas and the construction of immune memory. For their assessment, experiments with public domain datasets <b>(KDD'99,</b> CAIDA'07 and CAIDA'08) and <b>simulations</b> on various network configurations based on traffic samples gathered by the University Complutense of Madrid and flooding attacks generated by the tool DDoSIM were performed.</p></p class="citation"></blockquote><h3 id=59--139252-utilizing-large-languagemodels-to-detect-privacy-leaks-in-mini-app-code-liming-jiang-2024>(5/9 | 139/252) Utilizing Large LanguageModels to Detect Privacy Leaks in Mini-App Code (Liming Jiang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liming Jiang. (2024)<br><strong>Utilizing Large LanguageModels to Detect Privacy Leaks in Mini-App Code</strong><br><button class=copy-to-clipboard title="Utilizing Large LanguageModels to Detect Privacy Leaks in Mini-App Code" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07367v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07367v1.pdf filename=2402.07367v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mini-applications, commonly referred to as mini-apps, are compact software programs embedded within larger applications or platforms, offering targeted functionality without the need for separate installations. Typically web-based or cloud-hosted, these mini-apps streamline user experiences by providing focused services accessible through web browsers or mobile apps. Their simplicity, speed, and integration capabilities make them valuable additions to messaging platforms, social media networks, e-commerce sites, and various digital environments. WeChat Mini Programs, a prominent feature of China&rsquo;s leading messaging app, exemplify this trend, offering users a seamless array of services without additional downloads. Leveraging WeChat&rsquo;s extensive user base and payment infrastructure, Mini Programs facilitate efficient transactions and bridge online and offline experiences, shaping China&rsquo;s digital landscape significantly. This paper investigates the potential of employing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to detect privacy breaches within WeChat Mini Programs. Given the widespread use of Mini Programs and growing concerns about data privacy, this research seeks to determine if <b>LLMs</b> can effectively identify instances of privacy leakage within this ecosystem. Through meticulous analysis and experimentation, we aim to highlight the efficacy of <b>LLMs</b> in safeguarding user privacy and security within the WeChat Mini Program environment, thereby contributing to a more secure digital landscape.</p></p class="citation"></blockquote><h3 id=69--140252-game-of-trojans-adaptive-adversaries-against-output-based-trojaned-model-detectors-dinuka-sahabandu-et-al-2024>(6/9 | 140/252) Game of Trojans: Adaptive Adversaries Against Output-based Trojaned-Model Detectors (Dinuka Sahabandu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dinuka Sahabandu, Xiaojun Xu, Arezoo Rajabi, Luyao Niu, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran. (2024)<br><strong>Game of Trojans: Adaptive Adversaries Against Output-based Trojaned-Model Detectors</strong><br><button class=copy-to-clipboard title="Game of Trojans: Adaptive Adversaries Against Output-based Trojaned-Model Detectors" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: MNIST<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08695v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08695v1.pdf filename=2402.08695v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose and analyze an adaptive adversary that can retrain a Trojaned DNN and is also aware of SOTA output-based Trojaned model detectors. We show that such an adversary can ensure (1) high accuracy on both trigger-embedded and clean samples and (2) bypass detection. Our approach is based on an observation that the high dimensionality of the DNN parameters provides sufficient degrees of freedom to simultaneously achieve these objectives. We also enable SOTA detectors to be adaptive by allowing retraining to recalibrate their parameters, thus modeling a co-evolution of parameters of a Trojaned model and detectors. We then show that this co-evolution can be modeled as an iterative game, and prove that the resulting (optimal) solution of this interactive game leads to the adversary successfully achieving the above objectives. In addition, we provide a greedy algorithm for the adversary to select a minimum number of input samples for embedding triggers. We show that for cross-entropy or log-likelihood loss functions used by the DNNs, the greedy algorithm provides provable guarantees on the needed number of trigger-embedded input samples. Extensive experiments on four diverse datasets &ndash; <b>MNIST,</b> CIFAR-10, CIFAR-100, and SpeechCommand &ndash; reveal that the adversary effectively evades four SOTA output-based Trojaned model detectors: MNTD, NeuralCleanse, STRIP, and TABOR.</p></p class="citation"></blockquote><h3 id=79--141252-discovering-universal-semantic-triggers-for-text-to-image-synthesis-shengfang-zhai-et-al-2024>(7/9 | 141/252) Discovering Universal Semantic Triggers for Text-to-Image Synthesis (Shengfang Zhai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengfang Zhai, Weilong Wang, Jiajun Li, Yinpeng Dong, Hang Su, Qingni Shen. (2024)<br><strong>Discovering Universal Semantic Triggers for Text-to-Image Synthesis</strong><br><button class=copy-to-clipboard title="Discovering Universal Semantic Triggers for Text-to-Image Synthesis" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07562v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07562v1.pdf filename=2402.07562v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently <b>text-to-image</b> models have gained widespread attention in the community due to their controllable and high-quality generation ability. However, the robustness of such models and their potential ethical issues have not been fully explored. In this paper, we introduce Universal Semantic Trigger, a meaningless token sequence that can be added at any location within the input text yet can induce generated images towards a preset semantic target.To thoroughly investigate it, we propose Semantic Gradient-based Search (SGS) framework. SGS automatically discovers the potential universal semantic triggers based on the given semantic targets. Furthermore, we design evaluation metrics to comprehensively evaluate semantic shift of images caused by these triggers. And our empirical analyses reveal that the mainstream open-source <b>text-to-image</b> models are vulnerable to our triggers, which could pose significant ethical threats. Our work contributes to a further understanding of <b>text-to-image</b> synthesis and helps users to automatically auditing their models before deployment.</p></p class="citation"></blockquote><h3 id=89--142252-malicious-package-detection-using-metadata-information-s-halder-et-al-2024>(8/9 | 142/252) Malicious Package Detection using Metadata Information (S. Halder et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>S. Halder, M. Bewong, A. Mahboubi, Y. Jiang, R. Islam, Z. Islam, R. Ip, E. Ahmed, G. Ramachandran, A. Babar. (2024)<br><strong>Malicious Package Detection using Metadata Information</strong><br><button class=copy-to-clipboard title="Malicious Package Detection using Metadata Information" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07444v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07444v1.pdf filename=2402.07444v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Protecting software supply chains from malicious packages is paramount in the evolving landscape of software development. Attacks on the software supply chain involve attackers injecting harmful software into commonly used packages or libraries in a software repository. For instance, JavaScript uses Node Package Manager (NPM), and Python uses Python Package Index (PyPi) as their respective package repositories. In the past, NPM has had vulnerabilities such as the event-stream incident, where a malicious package was introduced into a popular NPM package, potentially impacting a wide range of projects. As the integration of third-party packages becomes increasingly ubiquitous in modern software development, accelerating the creation and deployment of applications, the need for a robust detection mechanism has become critical. On the other hand, due to the sheer volume of new packages being released daily, the task of identifying malicious packages presents a significant challenge. To address this issue, in this paper, we introduce a metadata-based malicious package detection model, MeMPtec. This model extracts a set of features from package metadata information. These extracted features are classified as either easy-to-manipulate (ETM) or difficult-to-manipulate (DTM) features based on monotonicity and restricted control properties. By utilising these metadata features, not only do we improve the effectiveness of detecting malicious packages, but also we demonstrate its resistance to <b>adversarial</b> <b>attacks</b> in comparison with existing state-of-the-art. Our experiments indicate a significant reduction in both false positives (up to 97.56%) and false negatives (up to 91.86%).</p></p class="citation"></blockquote><h3 id=99--143252-using-graph-theory-for-improving-machine-learning-based-detection-of-cyber-attacks-giacomo-zonneveld-et-al-2024>(9/9 | 143/252) Using Graph Theory for Improving Machine Learning-based Detection of Cyber Attacks (Giacomo Zonneveld et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giacomo Zonneveld, Lorenzo Principi, Marco Baldi. (2024)<br><strong>Using Graph Theory for Improving Machine Learning-based Detection of Cyber Attacks</strong><br><button class=copy-to-clipboard title="Using Graph Theory for Improving Machine Learning-based Detection of Cyber Attacks" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07878v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07878v1.pdf filename=2402.07878v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Early detection of network intrusions and cyber threats is one of the main pillars of cybersecurity. One of the most effective approaches for this purpose is to analyze network traffic with the help of artificial intelligence algorithms, with the aim of detecting the possible presence of an attacker by distinguishing it from a legitimate user. This is commonly done by collecting the traffic exchanged between terminals in a network and analyzing it on a per-packet or per-connection basis. In this paper, we propose instead to perform pre-processing of network traffic under analysis with the aim of extracting some new metrics on which we can perform more efficient detection and overcome some limitations of classical approaches. These new metrics are based on <b>graph</b> theory, and consider the network as a whole, rather than focusing on individual packets or connections. Our approach is validated through experiments performed on publicly available data sets, from which it results that it can not only overcome some of the limitations of classical approaches, but also achieve a better detection capability of cyber threats.</p></p class="citation"></blockquote><h2 id=cscv-27>cs.CV (27)</h2><h3 id=127--144252-multi-attribute-vision-transformers-are-efficient-and-robust-learners-hanan-gani-et-al-2024>(1/27 | 144/252) Multi-Attribute Vision Transformers are Efficient and Robust Learners (Hanan Gani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanan Gani, Nada Saadi, Noor Hussein, Karthik Nandakumar. (2024)<br><strong>Multi-Attribute Vision Transformers are Efficient and Robust Learners</strong><br><button class=copy-to-clipboard title="Multi-Attribute Vision Transformers are Efficient and Robust Learners" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Vision Transformer, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08070v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08070v1.pdf filename=2402.08070v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since their inception, <b>Vision</b> <b>Transformers</b> (ViTs) have emerged as a compelling alternative to <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> across a wide spectrum of tasks. ViTs exhibit notable characteristics, including global attention, resilience against occlusions, and adaptability to distribution shifts. One underexplored aspect of ViTs is their potential for multi-attribute learning, referring to their ability to simultaneously grasp multiple attribute-related tasks. In this paper, we delve into the multi-attribute learning capability of ViTs, presenting a straightforward yet effective strategy for training various attributes through a single ViT network as distinct tasks. We assess the resilience of multi-attribute ViTs against <b>adversarial</b> <b>attacks</b> and compare their performance against ViTs designed for single attributes. Moreover, we further evaluate the robustness of multi-attribute ViTs against a recent <b>transformer</b> based attack called Patch-Fool. Our empirical findings on the CelebA dataset provide validation for our assertion.</p></p class="citation"></blockquote><h3 id=227--145252-aydiv-adaptable-yielding-3d-object-detection-via-integrated-contextual-vision-transformer-tanmoy-dam-et-al-2024>(2/27 | 145/252) AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual Vision Transformer (Tanmoy Dam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanmoy Dam, Sanjay Bhargav Dharavath, Sameer Alam, Nimrod Lilith, Supriyo Chakraborty, Mir Feroskhan. (2024)<br><strong>AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual Vision Transformer</strong><br><button class=copy-to-clipboard title="AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual Vision Transformer" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Object Detection, Fine-tuning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07680v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07680v1.pdf filename=2402.07680v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Combining LiDAR and camera data has shown potential in enhancing short-distance <b>object</b> <b>detection</b> in autonomous driving systems. Yet, the fusion encounters difficulties with extended distance detection due to the contrast between LiDAR&rsquo;s sparse data and the dense resolution of cameras. Besides, discrepancies in the two data representations further complicate fusion methods. We introduce AYDIV, a novel framework integrating a tri-phase alignment process specifically designed to enhance long-distance detection even amidst data discrepancies. AYDIV consists of the Global Contextual Fusion Alignment <b>Transformer</b> (GCFAT), which improves the extraction of camera features and provides a deeper understanding of large-scale patterns; the Sparse Fused Feature Attention (SFFA), which <b>fine-tunes</b> the fusion of LiDAR and camera details; and the Volumetric Grid Attention (VGA) for a comprehensive spatial data fusion. AYDIV&rsquo;s performance on the Waymo Open Dataset (WOD) with an improvement of 1.24% in mAPH value(L2 difficulty) and the Argoverse2 Dataset with a performance improvement of 7.40% in AP value demonstrates its efficacy in comparison to other existing fusion-based methods. Our code is publicly available at <a href=https://github.com/sanjay-810/AYDIV2>https://github.com/sanjay-810/AYDIV2</a></p></p class="citation"></blockquote><h3 id=327--146252-modiphy-multimodal-obscured-detection-for-iot-using-phantom-convolution-enabled-faster-yolo-shubhabrata-mukherjee-et-al-2024>(3/27 | 146/252) MODIPHY: Multimodal Obscured Detection for IoT using PHantom Convolution-Enabled Faster YOLO (Shubhabrata Mukherjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shubhabrata Mukherjee, Cory Beard, Zhu Li. (2024)<br><strong>MODIPHY: Multimodal Obscured Detection for IoT using PHantom Convolution-Enabled Faster YOLO</strong><br><button class=copy-to-clipboard title="MODIPHY: Multimodal Obscured Detection for IoT using PHantom Convolution-Enabled Faster YOLO" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 49<br>Keywords: Yolo, Object Detection, Benchmarking, Convolution, Multi-modal, Multi-modal, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07894v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07894v1.pdf filename=2402.07894v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low-light conditions and occluded scenarios impede <b>object</b> <b>detection</b> in real-world Internet of Things (IoT) applications like autonomous vehicles and security systems. While advanced machine learning models strive for accuracy, their computational demands clash with the limitations of resource-constrained devices, hampering real-time performance. In our current research, we tackle this challenge, by introducing <b>&ldquo;YOLO</b> Phantom&rdquo;, one of the smallest <b>YOLO</b> models ever conceived. <b>YOLO</b> Phantom utilizes the novel Phantom <b>Convolution</b> block, achieving comparable accuracy to the latest YOLOv8n model while simultaneously reducing both parameters and model size by 43%, resulting in a significant 19% reduction in Giga Floating Point Operations (GFLOPs). <b>YOLO</b> Phantom leverages <b>transfer</b> <b>learning</b> on our <b>multimodal</b> RGB-infrared dataset to address low-light and occlusion issues, equipping it with robust vision under adverse conditions. Its real-world efficacy is demonstrated on an IoT platform with advanced low-light and RGB cameras, seamlessly connecting to an AWS-based notification endpoint for efficient real-time <b>object</b> <b>detection.</b> <b>Benchmarks</b> reveal a substantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB detection, respectively, compared to the baseline YOLOv8n model. For community contribution, both the code and the <b>multimodal</b> dataset are available on GitHub.</p></p class="citation"></blockquote><h3 id=427--147252-lumos--empowering-multimodal-llms-with-scene-text-recognition-ashish-shenoy-et-al-2024>(4/27 | 147/252) Lumos : Empowering Multimodal LLMs with Scene Text Recognition (Ashish Shenoy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashish Shenoy, Yichao Lu, Srihari Jayakumar, Debojeet Chatterjee, Mohsen Moslehpour, Pierce Chuang, Abhay Harpale, Vikas Bhardwaj, Di Xu, Shicong Zhao, Longfang Zhao, Ankit Ramchandani, Xin Luna Dong, Anuj Kumar. (2024)<br><strong>Lumos : Empowering Multimodal LLMs with Scene Text Recognition</strong><br><button class=copy-to-clipboard title="Lumos : Empowering Multimodal LLMs with Scene Text Recognition" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, Question Answering, Large Language Model, Large Language Model, Text Understanding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08017v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08017v1.pdf filename=2402.08017v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Lumos, the first end-to-end <b>multimodal</b> <b>question-answering</b> <b>system</b> with <b>text</b> <b>understanding</b> capabilities. At the core of Lumos is a Scene <b>Text</b> <b>Recognition</b> (STR) component that extracts <b>text</b> <b>from</b> first person point-of-view images, the output of which is used to augment input to a <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Model</b> (MM-LLM). While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference. In this paper, we delve into those challenges, and discuss the system architecture, design choices, and modeling techniques employed to overcome these obstacles. We also provide a comprehensive evaluation for each component, showcasing high quality and efficiency.</p></p class="citation"></blockquote><h3 id=527--148252-beyond-the-mud-datasets-and-benchmarks-for-computer-vision-in-off-road-racing-jacob-tyo-et-al-2024>(5/27 | 148/252) Beyond the Mud: Datasets and Benchmarks for Computer Vision in Off-Road Racing (Jacob Tyo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacob Tyo, Motolani Olarinre, Youngseog Chung, Zachary C. Lipton. (2024)<br><strong>Beyond the Mud: Datasets and Benchmarks for Computer Vision in Off-Road Racing</strong><br><button class=copy-to-clipboard title="Beyond the Mud: Datasets and Benchmarks for Computer Vision in Off-Road Racing" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Optical Character Recognition, Optical Character Recognition, Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08025v1.pdf filename=2402.08025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite significant progress in <b>optical</b> <b>character</b> <b>recognition</b> <b>(OCR)</b> and computer vision systems, robustly recognizing text and identifying people in images taken in unconstrained \emph{in-the-wild} environments remain an ongoing challenge. However, such obstacles must be overcome in practical applications of vision systems, such as identifying racers in photos taken during off-road racing events. To this end, we introduce two new challenging real-world datasets - the off-road motorcycle Racer Number Dataset (RND) and the Muddy Racer re-iDentification Dataset (MUDD) - to highlight the shortcomings of current methods and drive advances in <b>OCR</b> and person re-identification (ReID) under extreme conditions. These two datasets feature over 6,300 images taken during off-road competitions which exhibit a variety of factors that undermine even modern vision systems, namely mud, complex poses, and motion blur. We establish <b>benchmark</b> performance on both datasets using state-of-the-art models. Off-the-shelf models transfer poorly, reaching only 15% end-to-end (E2E) F1 score on text spotting, and 33% rank-1 accuracy on ReID. <b>Fine-tuning</b> yields major improvements, bringing model performance to 53% F1 score for E2E text spotting and 79% rank-1 accuracy on ReID, but still falls short of good performance. Our analysis exposes open problems in real-world <b>OCR</b> and ReID that necessitate domain-targeted techniques. With these datasets and analysis of model limitations, we aim to foster innovations in handling real-world conditions like mud and complex poses to drive progress in robust computer vision. All data was sourced from PerformancePhoto.co, a website used by professional motorsports photographers, racers, and fans. The top-performing text spotting and ReID models are deployed on this platform to power real-time race photo search.</p></p class="citation"></blockquote><h3 id=627--149252-unmasking-honey-adulteration--a-breakthrough-in-quality-assurance-through-cutting-edge-convolutional-neural-network-analysis-of-thermal-images-ilias-boulbarj-et-al-2024>(6/27 | 149/252) Unmasking honey adulteration : a breakthrough in quality assurance through cutting-edge convolutional neural network analysis of thermal images (Ilias Boulbarj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ilias Boulbarj, Bouklouze Abdelaziz, Yousra El Alami, Douzi Samira, Douzi Hassan. (2024)<br><strong>Unmasking honey adulteration : a breakthrough in quality assurance through cutting-edge convolutional neural network analysis of thermal images</strong><br><button class=copy-to-clipboard title="Unmasking honey adulteration : a breakthrough in quality assurance through cutting-edge convolutional neural network analysis of thermal images" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08122v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08122v1.pdf filename=2402.08122v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Honey, a natural product generated from organic sources, is widely recognized for its revered reputation. Nevertheless, honey is susceptible to adulteration, a situation that has substantial consequences for both the well-being of the general population and the financial well-being of a country. Conventional approaches for detecting honey adulteration are often associated with extensive time requirements and restricted sensitivity. This paper presents a novel approach to address the aforementioned issue by employing <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> for the classification of honey samples based on thermal images. The use of thermal imaging technique offers a significant advantage in detecting adulterants, as it can reveal differences in temperature in honey samples caused by variations in sugar composition, moisture levels, and other substances used for adulteration. To establish a meticulous approach to categorizing honey, a thorough dataset comprising thermal images of authentic and tainted honey samples was collected. Several state-of-the-art <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> models were trained and optimized using the dataset that was gathered. Within this set of models, there exist pre-trained models such as InceptionV3, Xception, VGG19, and ResNet that have exhibited exceptional performance, achieving classification accuracies ranging from 88% to 98%. Furthermore, we have implemented a more streamlined and less complex <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> model, outperforming comparable models with an outstanding accuracy rate of 99%. This simplification offers not only the sole advantage of the model, but it also concurrently offers a more efficient solution in terms of resources and time. This approach offers a viable way to implement quality control measures in the honey business, so guaranteeing the genuineness and safety of this valuable organic commodity.</p></p class="citation"></blockquote><h3 id=727--150252-contrastive-multiple-instance-learning-for-weakly-supervised-person-reid-jacob-tyo-et-al-2024>(7/27 | 150/252) Contrastive Multiple Instance Learning for Weakly Supervised Person ReID (Jacob Tyo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacob Tyo, Zachary C. Lipton. (2024)<br><strong>Contrastive Multiple Instance Learning for Weakly Supervised Person ReID</strong><br><button class=copy-to-clipboard title="Contrastive Multiple Instance Learning for Weakly Supervised Person ReID" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Multiple Instance Learning, Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07685v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07685v1.pdf filename=2402.07685v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The acquisition of large-scale, precisely labeled datasets for person re-identification (ReID) poses a significant challenge. Weakly <b>supervised</b> ReID has begun to address this issue, although its performance lags behind fully <b>supervised</b> methods. In response, we introduce Contrastive <b>Multiple</b> <b>Instance</b> <b>Learning</b> (CMIL), a novel framework tailored for more effective weakly <b>supervised</b> ReID. CMIL distinguishes itself by requiring only a single model and no pseudo labels while leveraging contrastive losses &ndash; a technique that has significantly enhanced traditional ReID performance yet is absent in all prior MIL-based approaches. Through extensive experiments and analysis across three datasets, CMIL not only matches state-of-the-art performance on the large-scale SYSU-30k dataset with fewer assumptions but also consistently outperforms all baselines on the WL-market1501 and Weakly Labeled MUddy racer re-iDentification dataset (WL-MUDD) datasets. We introduce and release the WL-MUDD dataset, an extension of the MUDD dataset featuring naturally occurring weak labels from the real-world application at PerformancePhoto.co. All our code and data are accessible at <a href="https://drive.google.com/file/d/1rjMbWB6m-apHF3Wg_cfqc8QqKgQ21AsT/view?usp=drive_link">https://drive.google.com/file/d/1rjMbWB6m-apHF3Wg_cfqc8QqKgQ21AsT/view?usp=drive_link</a>.</p></p class="citation"></blockquote><h3 id=827--151252-unsupervised-discovery-of-object-centric-neural-fields-rundong-luo-et-al-2024>(8/27 | 151/252) Unsupervised Discovery of Object-Centric Neural Fields (Rundong Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rundong Luo, Hong-Xing Yu, Jiajun Wu. (2024)<br><strong>Unsupervised Discovery of Object-Centric Neural Fields</strong><br><button class=copy-to-clipboard title="Unsupervised Discovery of Object-Centric Neural Fields" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Unsupervised Learning, Unsupervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07376v1.pdf filename=2402.07376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study inferring 3D object-centric scene representations from a single image. While recent methods have shown potential in <b>unsupervised</b> <b>3D</b> object discovery from simple synthetic images, they fail to generalize to real-world scenes with visually rich and diverse objects. This limitation stems from their object representations, which entangle objects&rsquo; intrinsic attributes like shape and appearance with extrinsic, viewer-centric properties such as their 3D location. To address this bottleneck, we propose <b>Unsupervised</b> <b>discovery</b> of Object-Centric neural Fields (uOCF). uOCF focuses on learning the intrinsics of objects and models the extrinsics separately. Our approach significantly improves systematic generalization, thus enabling <b>unsupervised</b> <b>learning</b> of high-fidelity object-centric scene representations from sparse real-world images. To evaluate our approach, we collect three new datasets, including two real kitchen environments. Extensive experiments show that uOCF enables <b>unsupervised</b> <b>discovery</b> of visually rich objects from a single real image, allowing applications such as 3D object segmentation and scene manipulation. Notably, uOCF demonstrates <b>zero-shot</b> generalization to unseen objects from a single real image. Project page: <a href=https://red-fairy.github.io/uOCF/>https://red-fairy.github.io/uOCF/</a></p></p class="citation"></blockquote><h3 id=927--152252-real-world-atmospheric-turbulence-correction-via-domain-adaptation-xijun-wang-et-al-2024>(9/27 | 152/252) Real-World Atmospheric Turbulence Correction via Domain Adaptation (Xijun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xijun Wang, Santiago López-Tapia, Aggelos K. Katsaggelos. (2024)<br><strong>Real-World Atmospheric Turbulence Correction via Domain Adaptation</strong><br><button class=copy-to-clipboard title="Real-World Atmospheric Turbulence Correction via Domain Adaptation" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 30<br>Keywords: Supervised Learning, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07371v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07371v1.pdf filename=2402.07371v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Atmospheric turbulence, a common phenomenon in daily life, is primarily caused by the uneven heating of the Earth&rsquo;s surface. This phenomenon results in distorted and blurred acquired images or videos and can significantly impact downstream vision tasks, particularly those that rely on capturing clear, stable images or videos from outdoor environments, such as accurately detecting or recognizing objects. Therefore, people have proposed ways to simulate atmospheric turbulence and designed effective deep learning-based methods to remove the atmospheric turbulence effect. However, these synthesized turbulent images can not cover all the range of real-world turbulence effects. Though the models have achieved great performance for synthetic scenarios, there always exists a performance drop when applied to real-world cases. Moreover, reducing real-world turbulence is a more challenging task as there are no clean ground truth counterparts provided to the models during training. In this paper, we propose a real-world atmospheric turbulence mitigation model under a <b>domain</b> <b>adaptation</b> framework, which links the <b>supervised</b> simulated atmospheric turbulence correction with the <b>unsupervised</b> real-world atmospheric turbulence correction. We will show our proposed method enhances performance in real-world atmospheric turbulence scenarios, improving both image quality and downstream vision tasks.</p></p class="citation"></blockquote><h3 id=1027--153252-multiple-random-masking-autoencoder-ensembles-for-robust-multimodal-semi-supervised-learning-alexandru-raul-todoran-et-al-2024>(10/27 | 153/252) Multiple Random Masking Autoencoder Ensembles for Robust Multimodal Semi-supervised Learning (Alexandru-Raul Todoran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandru-Raul Todoran, Marius Leordeanu. (2024)<br><strong>Multiple Random Masking Autoencoder Ensembles for Robust Multimodal Semi-supervised Learning</strong><br><button class=copy-to-clipboard title="Multiple Random Masking Autoencoder Ensembles for Robust Multimodal Semi-supervised Learning" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Autoencoder, Multi-modal, Multi-modal, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08035v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08035v1.pdf filename=2402.08035v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is an increasing number of real-world problems in computer vision and machine learning requiring to take into consideration multiple interpretation layers (modalities or views) of the world and learn how they relate to each other. For example, in the case of Earth Observations from satellite data, it is important to be able to predict one observation layer (e.g. vegetation index) from other layers (e.g. water vapor, snow cover, temperature etc), in order to best understand how the Earth System functions and also be able to reliably predict information for one layer when the data is missing (e.g. due to measurement failure or error).</p></p class="citation"></blockquote><h3 id=1127--154252-exploring-perceptual-limitation-of-multimodal-large-language-models-jiarui-zhang-et-al-2024>(11/27 | 154/252) Exploring Perceptual Limitation of Multimodal Large Language Models (Jiarui Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiarui Zhang, Jinyi Hu, Mahyar Khayatkhoei, Filip Ilievski, Maosong Sun. (2024)<br><strong>Exploring Perceptual Limitation of Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="Exploring Perceptual Limitation of Multimodal Large Language Models" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07384v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07384v1.pdf filename=2402.07384v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) have recently shown remarkable perceptual capability in answering visual <b>questions,</b> <b>however,</b> little is known about the limits of their perception. In particular, while prior works have provided anecdotal evidence of MLLMs&rsquo; sensitivity to object size, this phenomenon and its underlying causes have not been explored comprehensively. In this work, we quantitatively study the perception of small visual objects in several state-of-the-art MLLMs and reveal a pervasive limitation in answering <b>questions</b> <b>about</b> small objects in images. Next, we identify four independent factors that can contribute to this limitation &ndash; object quality, size, distractors, and location &ndash; and conduct controlled intervention studies to measure the effect of each factor on MLLMs&rsquo; perception. In particular, we find that lower object quality and smaller object size can both independently reduce MLLMs&rsquo; ability to answer visual <b>questions.</b> <b>More</b> surprisingly, we find that the location of the object in the image and the presence of visual distractors can also significantly reduce MLLMs&rsquo; <b>question</b> <b>answering</b> accuracy. Our study provides a better understanding of the perceptual limitation of MLLMs and contributes new evaluation protocols for analyzing the perception of future MLLMs. To facilitate further investigations, we release our code and data.</p></p class="citation"></blockquote><h3 id=1227--155252-towards-meta-pruning-via-optimal-transport-alexander-theus-et-al-2024>(12/27 | 155/252) Towards Meta-Pruning via Optimal Transport (Alexander Theus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Theus, Olin Geimer, Friedrich Wicke, Thomas Hofmann, Sotiris Anagnostidis, Sidak Pal Singh. (2024)<br><strong>Towards Meta-Pruning via Optimal Transport</strong><br><button class=copy-to-clipboard title="Towards Meta-Pruning via Optimal Transport" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Fine-tuning, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07839v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07839v2.pdf filename=2402.07839v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Structural <b>pruning</b> of neural networks conventionally relies on identifying and discarding less important neurons, a practice often resulting in significant accuracy loss that necessitates subsequent <b>fine-tuning</b> efforts. This paper introduces a novel approach named Intra-Fusion, challenging this prevailing <b>pruning</b> paradigm. Unlike existing methods that focus on designing meaningful neuron importance metrics, Intra-Fusion redefines the overlying <b>pruning</b> procedure. Through utilizing the concepts of model fusion and Optimal Transport, we leverage an agnostically given importance metric to arrive at a more effective sparse model representation. Notably, our approach achieves substantial accuracy recovery without the need for resource-intensive <b>fine-tuning,</b> making it an efficient and promising tool for neural network compression. Additionally, we explore how fusion can be added to the <b>pruning</b> process to significantly decrease the training time while maintaining competitive performance. We <b>benchmark</b> our results for various networks on commonly used datasets such as CIFAR-10, CIFAR-100, and ImageNet. More broadly, we hope that the proposed Intra-Fusion approach invigorates exploration into a fresh alternative to the predominant compression approaches. Our code is available here: <a href=https://github.com/alexandertheus/Intra-Fusion>https://github.com/alexandertheus/Intra-Fusion</a>.</p></p class="citation"></blockquote><h3 id=1327--156252-a-benchmark-grocery-dataset-of-realworld-point-clouds-from-single-view-shivanand-venkanna-sheshappanavar-et-al-2024>(13/27 | 156/252) A Benchmark Grocery Dataset of Realworld Point Clouds From Single View (Shivanand Venkanna Sheshappanavar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shivanand Venkanna Sheshappanavar, Tejas Anvekar, Shivanand Kundargi, Yufan Wang, Chandra Kambhamettu. (2024)<br><strong>A Benchmark Grocery Dataset of Realworld Point Clouds From Single View</strong><br><button class=copy-to-clipboard title="A Benchmark Grocery Dataset of Realworld Point Clouds From Single View" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Continual Learning, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07819v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07819v1.pdf filename=2402.07819v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fine-grained grocery object recognition is an important computer vision problem with broad applications in automatic checkout, in-store robotic navigation, and assistive technologies for the visually impaired. Existing datasets on groceries are mainly 2D images. Models trained on these datasets are limited to learning features from the regular 2D grids. While portable 3D sensors such as Kinect were commonly available for mobile phones, sensors such as LiDAR and TrueDepth, have recently been integrated into mobile phones. Despite the availability of mobile 3D sensors, there are currently no dedicated real-world large-scale <b>benchmark</b> 3D datasets for grocery. In addition, existing 3D datasets lack fine-grained grocery categories and have limited training samples. Furthermore, collecting data by going around the object versus the traditional photo capture makes data collection cumbersome. Thus, we introduce a large-scale grocery dataset called 3DGrocery100. It constitutes 100 classes, with a total of 87,898 3D point clouds created from 10,755 RGB-D single-view images. We <b>benchmark</b> our dataset on six recent state-of-the-art 3D point cloud classification models. Additionally, we also <b>benchmark</b> the dataset on <b>few-shot</b> and <b>continual</b> <b>learning</b> point cloud classification tasks. Project Page: <a href=https://bigdatavision.org/3DGrocery100/>https://bigdatavision.org/3DGrocery100/</a>.</p></p class="citation"></blockquote><h3 id=1427--157252-wavefront-randomization-improves-deconvolution-amit-kohli-et-al-2024>(14/27 | 157/252) Wavefront Randomization Improves Deconvolution (Amit Kohli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amit Kohli, Anastasios N. Angelopoulos, Laura Waller. (2024)<br><strong>Wavefront Randomization Improves Deconvolution</strong><br><button class=copy-to-clipboard title="Wavefront Randomization Improves Deconvolution" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-4-4, cs-CV, cs.CV, eess-IV, physics-optics<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07900v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07900v2.pdf filename=2402.07900v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The performance of an imaging system is limited by optical aberrations, which cause blurriness in the resulting image. Digital correction techniques, such as deconvolution, have limited ability to correct the blur, since some spatial frequencies in the scene are not measured adequately (i.e., &lsquo;zeros&rsquo; of the system transfer function). We prove that the addition of a random mask to an imaging system removes its dependence on aberrations, reducing the likelihood of zeros in the transfer function and consequently decreasing the sensitivity to noise during deconvolution. In <b>simulation,</b> we show that this strategy improves image quality over a range of aberration types, aberration strengths, and signal-to-noise ratios.</p></p class="citation"></blockquote><h3 id=1527--158252-prismatic-vlms-investigating-the-design-space-of-visually-conditioned-language-models-siddharth-karamcheti-et-al-2024>(15/27 | 158/252) Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models (Siddharth Karamcheti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, Dorsa Sadigh. (2024)<br><strong>Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models</strong><br><button class=copy-to-clipboard title="Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Question Answering, Visual Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07865v1.pdf filename=2402.07865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visually-conditioned language models (VLMs) have seen growing adoption in applications such as <b>visual</b> <b>dialogue,</b> <b>scene</b> understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning <b>visual</b> <b>question</b> <b>answering,</b> object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM&rsquo;s capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained <b>visual</b> <b>representations</b> <b>and</b> quantifying the tradeoffs of using base vs. instruct-tuned language models, amongst others. We couple our analysis with three resource contributions: (1) a unified framework for evaluating VLMs, (2) optimized, flexible code for VLM training, and (3) checkpoints for all models, including a family of VLMs at the 7-13B scale that strictly outperform InstructBLIP and LLaVa v1.5, the state-of-the-art in open-source VLMs.</p></p class="citation"></blockquote><h3 id=1627--159252-complete-instances-mining-for-weakly-supervised-instance-segmentation-zecheng-li-et-al-2024>(16/27 | 159/252) Complete Instances Mining for Weakly Supervised Instance Segmentation (Zecheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zecheng Li, Zening Zeng, Yuqi Liang, Jin-Gang Yu. (2024)<br><strong>Complete Instances Mining for Weakly Supervised Instance Segmentation</strong><br><button class=copy-to-clipboard title="Complete Instances Mining for Weakly Supervised Instance Segmentation" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07633v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07633v1.pdf filename=2402.07633v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Weakly <b>supervised</b> instance segmentation (WSIS) using only image-level labels is a challenging task due to the difficulty of aligning coarse annotations with the finer task. However, with the advancement of deep neural networks (DNNs), WSIS has garnered significant attention. Following a proposal-based paradigm, we encounter a redundant segmentation problem resulting from a single instance being represented by multiple proposals. For example, we feed a picture of a dog and proposals into the network and expect to output only one proposal containing a dog, but the network outputs multiple proposals. To address this problem, we propose a novel approach for WSIS that focuses on the online refinement of complete instances through the use of MaskIoU heads to predict the integrity scores of proposals and a Complete Instances Mining (CIM) strategy to explicitly model the redundant segmentation problem and generate refined pseudo labels. Our approach allows the network to become aware of multiple instances and complete instances, and we further improve its robustness through the incorporation of an Anti-noise strategy. Empirical evaluations on the PASCAL VOC 2012 and MS COCO datasets demonstrate that our method achieves state-of-the-art performance with a notable margin. Our implementation will be made available at <a href=https://github.com/ZechengLi19/CIM>https://github.com/ZechengLi19/CIM</a>.</p></p class="citation"></blockquote><h3 id=1727--160252-an-empirical-study-into-what-matters-for-calibrating-vision-language-models-weijie-tu-et-al-2024>(17/27 | 160/252) An Empirical Study Into What Matters for Calibrating Vision-Language Models (Weijie Tu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weijie Tu, Weijian Deng, Dylan Campbell, Stephen Gould, Tom Gedeon. (2024)<br><strong>An Empirical Study Into What Matters for Calibrating Vision-Language Models</strong><br><button class=copy-to-clipboard title="An Empirical Study Into What Matters for Calibrating Vision-Language Models" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07417v1.pdf filename=2402.07417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision&ndash;Language</b> Models (VLMs) have emerged as the dominant approach for <b>zero-shot</b> recognition, adept at handling diverse scenarios and significant distribution changes. However, their deployment in risk-sensitive areas requires a deeper understanding of their uncertainty estimation capabilities, a relatively uncharted area. In this study, we explore the calibration properties of VLMs across different architectures, datasets, and training strategies. In particular, we analyze the uncertainty estimation performance of VLMs when calibrated in one domain, label set or hierarchy level, and tested in a different one. Our findings reveal that while VLMs are not inherently calibrated for uncertainty, temperature scaling significantly and consistently improves calibration, even across shifts in distribution and changes in label set. Moreover, VLMs can be calibrated with a very small set of examples. Through detailed experimentation, we highlight the potential applications and importance of our insights, aiming for more reliable and effective use of VLMs in critical, real-world scenarios.</p></p class="citation"></blockquote><h3 id=1827--161252-selfswapper-self-supervised-face-swapping-via-shape-agnostic-masked-autoencoder-jaeseong-lee-et-al-2024>(18/27 | 161/252) SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder (Jaeseong Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaeseong Lee, Junha Hyung, Sohyun Jeong, Jaegul Choo. (2024)<br><strong>SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder</strong><br><button class=copy-to-clipboard title="SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Autoencoder, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07370v1.pdf filename=2402.07370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Face swapping has gained significant attention for its varied applications. The majority of previous face swapping approaches have relied on the seesaw game training scheme, which often leads to the instability of the model training and results in undesired samples with blended identities due to the target identity leakage problem. This paper introduces the Shape Agnostic Masked <b>AutoEncoder</b> (SAMAE) training scheme, a novel <b>self-supervised</b> approach designed to enhance face swapping model training. Our training scheme addresses the limitations of traditional training methods by circumventing the conventional seesaw game and introducing clear ground truth through its self-reconstruction training regime. It effectively mitigates identity leakage by masking facial regions of the input images and utilizing learned disentangled identity and non-identity features. Additionally, we tackle the shape misalignment problem with new techniques including perforation confusion and random mesh scaling, and establishes a new state-of-the-art, surpassing other baseline methods, preserving both identity and non-identity attributes, without sacrificing on either aspect.</p></p class="citation"></blockquote><h3 id=1927--162252-task-conditioned-adaptation-of-visual-features-in-multi-task-policy-learning-pierre-marza-et-al-2024>(19/27 | 162/252) Task-conditioned adaptation of visual features in multi-task policy learning (Pierre Marza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pierre Marza, Laetitia Matignon, Olivier Simonin, Christian Wolf. (2024)<br><strong>Task-conditioned adaptation of visual features in multi-task policy learning</strong><br><button class=copy-to-clipboard title="Task-conditioned adaptation of visual features in multi-task policy learning" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-RO, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07739v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07739v1.pdf filename=2402.07739v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules. An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task. Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning. We introduce task-conditioned adapters that do not require <b>finetuning</b> any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks. We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations. To this end, we propose a new optimization-based estimator. We evaluate the method on a wide variety of tasks of the CortexBench <b>benchmark</b> and show that, compared to existing work, it can be addressed with a single policy. In particular, we demonstrate that adapting visual features is a key design choice and that the method generalizes to unseen tasks given visual demonstrations.</p></p class="citation"></blockquote><h3 id=2027--163252-detection-of-spider-mites-on-labrador-beans-through-machine-learning-approaches-using-custom-datasets-violet-liu-et-al-2024>(20/27 | 163/252) Detection of Spider Mites on Labrador Beans through Machine Learning Approaches Using Custom Datasets (Violet Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Violet Liu, Jason Chen, Ans Qureshi, Mahla Nejati. (2024)<br><strong>Detection of Spider Mites on Labrador Beans through Machine Learning Approaches Using Custom Datasets</strong><br><button class=copy-to-clipboard title="Detection of Spider Mites on Labrador Beans through Machine Learning Approaches Using Custom Datasets" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07895v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07895v1.pdf filename=2402.07895v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Amidst growing food production demands, early plant disease detection is essential to safeguard crops; this study proposes a visual machine learning approach for plant disease detection, harnessing RGB and NIR data collected in real-world conditions through a JAI FS-1600D-10GE camera to build an RGBN dataset. A two-stage early plant disease detection model with YOLOv8 and a sequential <b>CNN</b> was used to train on a dataset with partial labels, which showed a 3.6% increase in mAP compared to a single-stage end-to-end segmentation model. The sequential <b>CNN</b> model achieved 90.62% validation accuracy utilising RGBN data. An average of 6.25% validation accuracy increase is found using RGBN in classification compared to RGB using ResNet15 and the sequential <b>CNN</b> models. Further research and dataset improvements are needed to meet food production demands.</p></p class="citation"></blockquote><h3 id=2127--164252-a-flow-based-credibility-metric-for-safety-critical-pedestrian-detection-maria-lyssenko-et-al-2024>(21/27 | 164/252) A Flow-based Credibility Metric for Safety-critical Pedestrian Detection (Maria Lyssenko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maria Lyssenko, Christoph Gladisch, Christian Heinzemann, Matthias Woehrle, Rudolph Triebel. (2024)<br><strong>A Flow-based Credibility Metric for Safety-critical Pedestrian Detection</strong><br><button class=copy-to-clipboard title="A Flow-based Credibility Metric for Safety-critical Pedestrian Detection" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07642v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07642v1.pdf filename=2402.07642v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Safety is of utmost importance for perception in automated driving (AD). However, a prime safety concern in state-of-the art <b>object</b> <b>detection</b> is that standard evaluation schemes utilize safety-agnostic metrics to argue sufficient detection performance. Hence, it is imperative to leverage supplementary domain knowledge to accentuate safety-critical misdetections during evaluation tasks. To tackle the underspecification, this paper introduces a novel credibility metric, called c-flow, for pedestrian bounding boxes. To this end, c-flow relies on a complementary optical flow signal from image sequences and enhances the analyses of safety-critical misdetections without requiring additional labels. We implement and evaluate c-flow with a state-of-the-art pedestrian detector on a large AD dataset. Our analysis demonstrates that c-flow allows developers to identify safety-critical misdetections.</p></p class="citation"></blockquote><h3 id=2227--165252-sheet-music-transformer-end-to-end-optical-music-recognition-beyond-monophonic-transcription-antonio-ríos-vila-et-al-2024>(22/27 | 165/252) Sheet Music Transformer: End-To-End Optical Music Recognition Beyond Monophonic Transcription (Antonio Ríos-Vila et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonio Ríos-Vila, Jorge Calvo-Zaragoza, Thierry Paquet. (2024)<br><strong>Sheet Music Transformer: End-To-End Optical Music Recognition Beyond Monophonic Transcription</strong><br><button class=copy-to-clipboard title="Sheet Music Transformer: End-To-End Optical Music Recognition Beyond Monophonic Transcription" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-SD, cs.CV, eess-AS<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07596v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07596v1.pdf filename=2402.07596v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>State-of-the-art end-to-end Optical Music Recognition (OMR) has, to date, primarily been carried out using monophonic transcription techniques to handle complex score layouts, such as polyphony, often by resorting to simplifications or specific adaptations. Despite their efficacy, these approaches imply challenges related to scalability and limitations. This paper presents the Sheet Music <b>Transformer,</b> the first end-to-end OMR model designed to transcribe complex musical scores without relying solely on monophonic strategies. Our model employs a <b>Transformer-based</b> image-to-sequence framework that predicts score transcriptions in a standard digital music encoding format from input images. Our model has been tested on two polyphonic music datasets and has proven capable of handling these intricate music structures effectively. The experimental outcomes not only indicate the competence of the model, but also show that it is better than the state-of-the-art methods, thus contributing to advancements in end-to-end OMR transcription.</p></p class="citation"></blockquote><h3 id=2327--166252-triaug-out-of-distribution-detection-for-robust-classification-of-imbalanced-breast-lesion-in-ultrasound-yinyu-ye-et-al-2024>(23/27 | 166/252) TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound (Yinyu Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinyu Ye, Shijing Chen, Dong Ni, Ruobing Huang. (2024)<br><strong>TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound</strong><br><button class=copy-to-clipboard title="TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07452v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07452v1.pdf filename=2402.07452v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Different diseases, such as histological subtypes of breast lesions, have severely varying incidence rates. Even trained with substantial amount of in-distribution (ID) data, models often encounter <b>out-of-distribution</b> (OOD) samples belonging to unseen classes in clinical reality. To address this, we propose a novel framework built upon a long-tailed OOD detection task for breast ultrasound images. It is equipped with a triplet state augmentation (TriAug) which improves ID classification accuracy while maintaining a promising OOD detection performance. Meanwhile, we designed a balanced sphere loss to handle the class imbalanced problem.</p></p class="citation"></blockquote><h3 id=2427--167252-a-closer-look-at-the-robustness-of-contrastive-language-image-pre-training-clip-weijie-tu-et-al-2024>(24/27 | 167/252) A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP) (Weijie Tu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weijie Tu, Weijian Deng, Tom Gedeon. (2024)<br><strong>A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)</strong><br><button class=copy-to-clipboard title="A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07410v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07410v1.pdf filename=2402.07410v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable generalization capabilities across multiple challenging distribution shifts. However, there is still much to be explored in terms of their robustness to the variations of specific visual factors. In real-world applications, reliable and safe systems must consider other safety objectives beyond classification accuracy, such as predictive uncertainty. Yet, the effectiveness of CLIP models on such safety-related features is less-explored. Driven by the above, this work comprehensively investigates the safety objectives of CLIP models, specifically focusing on three key properties: resilience to visual factor variations, calibrated uncertainty estimations, and the ability to detect anomalous inputs. To this end, we study 83 CLIP models and 127 ImageNet classifiers. They are diverse in architecture, (pre)training distribution and training strategies. We consider 10 visual factors (e.g., shape and pattern), 5 types of <b>out-of-distribution</b> data, and 8 natural and challenging test conditions with different shift types, such as texture, style, and perturbation shifts. Our study has unveiled several previously unknown insights into CLIP models. For instance, they are not consistently more calibrated than other ImageNet models, which contradicts existing findings. Additionally, our analysis underscores the significance of training source design by showcasing its profound influence on the three safety-related properties. We believe our comprehensive study can shed light on and help guide the development of more robust and reliable CLIP models.</p></p class="citation"></blockquote><h3 id=2527--168252-make-it-more-specific-a-novel-uncertainty-based-airway-segmentation-application-on-3d-u-net-and-its-variants-shiyi-wang-et-al-2024>(25/27 | 168/252) Make it more specific: A novel uncertainty based airway segmentation application on 3D U-Net and its variants (Shiyi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiyi Wang, Yang Nan, Felder Federico N, Sheng Zhang, Walsh Simon L F, Guang Yang. (2024)<br><strong>Make it more specific: A novel uncertainty based airway segmentation application on 3D U-Net and its variants</strong><br><button class=copy-to-clipboard title="Make it more specific: A novel uncertainty based airway segmentation application on 3D U-Net and its variants" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07403v1.pdf filename=2402.07403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Each medical segmentation task should be considered with a specific AI algorithm based on its scenario so that the most accurate prediction model can be obtained. The most popular algorithms in medical segmentation, 3D U-Net and its variants, can directly implement the task of lung trachea segmentation, but its failure to consider the special tree-like structure of the trachea suggests that there is much room for improvement in its segmentation accuracy. Therefore, a research gap exists because a great amount of state-of-the-art DL algorithms are vanilla 3D U-Net structures, which do not introduce the various performance-enhancing modules that come with special natural image modality in lung airway segmentation. In this paper, we proposed two different network structures Branch-Level U-Net (B-UNet) and Branch-Level CE-UNet (B-CE-UNet) which are based on U-Net structure and compared the prediction results with the same dataset. Specially, both of the two networks add branch loss and central line loss to learn the feature of fine branch endings of the airways. Uncertainty estimation algorithms are also included to attain confident predictions and thereby, increase the overall trustworthiness of our whole model. In addition, predictions of the lung trachea based on the maximum connectivity rate were calculated and extracted during post-processing for segmentation refinement and <b>pruning.</b></p></p class="citation"></blockquote><h3 id=2627--169252-exploring-saliency-bias-in-manipulation-detection-joshua-krinsky-et-al-2024>(26/27 | 169/252) Exploring Saliency Bias in Manipulation Detection (Joshua Krinsky et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joshua Krinsky, Alan Bettis, Qiuyu Tang, Daniel Moreira, Aparna Bharati. (2024)<br><strong>Exploring Saliency Bias in Manipulation Detection</strong><br><button class=copy-to-clipboard title="Exploring Saliency Bias in Manipulation Detection" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07338v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07338v2.pdf filename=2402.07338v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The social media-fuelled explosion of <b>fake</b> <b>news</b> and misinformation supported by tampered images has led to growth in the development of models and datasets for image manipulation detection. However, existing detection methods mostly treat media objects in isolation, without considering the impact of specific manipulations on viewer perception. Forensic datasets are usually analyzed based on the manipulation operations and corresponding pixel-based masks, but not on the semantics of the manipulation, i.e., type of scene, objects, and viewers&rsquo; attention to scene content. The semantics of the manipulation play an important role in spreading misinformation through manipulated images. In an attempt to encourage further development of semantic-aware forensic approaches to understand visual misinformation, we propose a framework to analyze the trends of visual and semantic saliency in popular image manipulation datasets and their impact on detection.</p></p class="citation"></blockquote><h3 id=2727--170252-gbot-graph-based-3d-object-tracking-for-augmented-reality-assisted-assembly-guidance-shiyu-li-et-al-2024>(27/27 | 170/252) GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted Assembly Guidance (Shiyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiyu Li, Hannah Schieber, Niklas Corell, Bernhard Egger, Julian Kreimeier, Daniel Roth. (2024)<br><strong>GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted Assembly Guidance</strong><br><button class=copy-to-clipboard title="GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted Assembly Guidance" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07677v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07677v1.pdf filename=2402.07677v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Guidance for assemblable parts is a promising field for augmented reality. Augmented reality assembly guidance requires 6D object poses of target objects in real time. Especially in time-critical medical or industrial settings, continuous and markerless tracking of individual parts is essential to visualize instructions superimposed on or next to the target object parts. In this regard, occlusions by the user&rsquo;s hand or other objects and the complexity of different assembly states complicate robust and real-time markerless multi-object tracking. To address this problem, we present <b>Graph-based</b> Object Tracking (GBOT), a novel <b>graph-based</b> single-view RGB-D tracking approach. The real-time markerless multi-object tracking is initialized via 6D pose estimation and updates the <b>graph-based</b> assembly poses. The tracking through various assembly states is achieved by our novel multi-state assembly <b>graph.</b> We update the multi-state assembly <b>graph</b> by utilizing the relative poses of the individual assembly parts. Linking the individual objects in this <b>graph</b> enables more robust object tracking during the assembly process. For evaluation, we introduce a synthetic dataset of publicly available and 3D printable assembly assets as a <b>benchmark</b> for future work. Quantitative experiments in synthetic data and further qualitative study in real test data show that GBOT can outperform existing work towards enabling context-aware augmented reality assembly guidance. Dataset and code will be made publically available.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--171252-developing-a-multi-variate-prediction-model-for-covid-19-from-crowd-sourced-respiratory-voice-data-yuyang-yan-et-al-2024>(1/2 | 171/252) Developing a Multi-variate Prediction Model For COVID-19 From Crowd-sourced Respiratory Voice Data (Yuyang Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuyang Yan, Wafaa Aljbawi, Sami O. Simons, Visara Urovi. (2024)<br><strong>Developing a Multi-variate Prediction Model For COVID-19 From Crowd-sourced Respiratory Voice Data</strong><br><button class=copy-to-clipboard title="Developing a Multi-variate Prediction Model For COVID-19 From Crowd-sourced Respiratory Voice Data" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 70<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, BERT, LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07619v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07619v1.pdf filename=2402.07619v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>COVID-19 has affected more than 223 countries worldwide and in the Post-COVID Era, there is a pressing need for non-invasive, low-cost, and highly scalable solutions to detect COVID-19. We develop a deep learning model to identify COVID-19 from voice recording data. The novelty of this work is in the development of deep learning models for COVID-19 identification from only voice recordings. We use the Cambridge COVID-19 Sound database which contains 893 speech samples, crowd-sourced from 4352 participants via a COVID-19 Sounds app. Voice features including Mel-spectrograms and Mel-frequency cepstral coefficients (MFCC) and <b>CNN</b> Encoder features are extracted. Based on the voice data, we develop deep learning classification models to detect COVID-19 cases. These models include <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> and <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> and Hidden-Unit <b>BERT</b> (HuBERT). We compare their predictive power to baseline machine learning models. HuBERT achieves the highest accuracy of 86% and the highest AUC of 0.93. The results achieved with the proposed models suggest promising results in COVID-19 diagnosis from voice recordings when compared to the results obtained from the state-of-the-art.</p></p class="citation"></blockquote><h3 id=22--172252-slit-boosting-audio-text-pre-training-via-multi-stage-learning-and-instruction-tuning-hang-zhao-et-al-2024>(2/2 | 172/252) SLIT: Boosting Audio-Text Pre-Training via Multi-Stage Learning and Instruction Tuning (Hang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hang Zhao, Yifei Xin, Zhesong Yu, Bilei Zhu, Lu Lu, Zejun Ma. (2024)<br><strong>SLIT: Boosting Audio-Text Pre-Training via Multi-Stage Learning and Instruction Tuning</strong><br><button class=copy-to-clipboard title="SLIT: Boosting Audio-Text Pre-Training via Multi-Stage Learning and Instruction Tuning" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 30<br>Keywords: Zero-shot, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07485v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07485v1.pdf filename=2402.07485v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Audio-text pre-training (ATP) has witnessed remarkable strides across a variety of downstream tasks. Yet, most existing pretrained audio models only specialize in either discriminative tasks or generative tasks. In this study, we develop SLIT, a novel ATP framework which transfers flexibly to both audio-text understanding and generation tasks, bootstrapping audio-text pre-training from frozen pretrained audio encoders and <b>large</b> <b>language</b> <b>models.</b> To bridge the modality gap during pre-training, we leverage Q-Former, which undergoes a multi-stage pre-training process. The first stage enhances audio-text representation learning from a frozen audio encoder, while the second stage boosts audio-to-text generative learning with a frozen language model. Furthermore, we introduce an ATP <b>instruction</b> <b>tuning</b> strategy, which enables flexible and informative feature extraction tailered to the given <b>instructions</b> <b>for</b> different tasks. Experiments show that SLIT achieves superior performances on a variety of audio-text understanding and generation tasks, and even demonstrates strong generalization capabilities when directly applied to <b>zero-shot</b> scenarios.</p></p class="citation"></blockquote><h2 id=eessas-3>eess.AS (3)</h2><h3 id=13--173252-air-bench-benchmarking-large-audio-language-models-via-generative-comprehension-qian-yang-et-al-2024>(1/3 | 173/252) AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension (Qian Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, Jingren Zhou. (2024)<br><strong>AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension</strong><br><button class=copy-to-clipboard title="AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-CL, cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 66<br>Keywords: Benchmarking, Benchmarking, GPT, GPT-4, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Instruction Following<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07729v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07729v1.pdf filename=2402.07729v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>instruction-following</b> <b>audio-language</b> models have received broad attention for human-audio interaction. However, the absence of <b>benchmarks</b> capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as <b>Automatic</b> <b>Speech</b> <b>Recognition</b> <b>(ASR),</b> and lack an assessment of the open-ended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement. In this paper, we introduce AIR-Bench (\textbf{A}udio \textbf{I}nst\textbf{R}uction \textbf{Bench}mark), the first <b>benchmark</b> designed to evaluate the ability of LALMs to understand various types of audio signals (including human <b>speech,</b> <b>natural</b> sounds, and music), and furthermore, to interact with humans in the textual format. AIR-Bench encompasses two dimensions: \textit{foundation} and \textit{chat} <b>benchmarks.</b> The former consists of 19 tasks with approximately 19k single-choice questions, intending to inspect the basic single-task ability of LALMs. The latter one contains 2k instances of open-ended question-and-answer data, directly assessing the comprehension of the model on complex audio and its capacity to follow <b>instructions.</b> <b>Both</b> <b>benchmarks</b> require the model to generate hypotheses directly. We design a unified framework that leverages advanced language models, such as <b>GPT-4,</b> to evaluate the scores of generated hypotheses given the meta-information of the audio. Experimental results demonstrate a high level of consistency between <b>GPT-4-based</b> evaluation and human evaluation. By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research.</p></p class="citation"></blockquote><h3 id=23--174252-making-flow-matching-based-zero-shot-text-to-speech-laugh-as-you-like-naoyuki-kanda-et-al-2024>(2/3 | 174/252) Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like (Naoyuki Kanda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naoyuki Kanda, Xiaofei Wang, Sefik Emre Eskimez, Manthan Thakker, Hemin Yang, Zirun Zhu, Min Tang, Canrun Li, Steven Tsai, Zhen Xiao, Yufei Xia, Jinzhu Li, Yanqing Liu, Sheng Zhao, Michael Zeng. (2024)<br><strong>Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like</strong><br><button class=copy-to-clipboard title="Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-CL, cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 50<br>Keywords: Fine-tuning, Zero-shot, Text-to-speech, Text-to-speech, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07383v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07383v1.pdf filename=2402.07383v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Laughter is one of the most expressive and natural aspects of human speech, conveying emotions, social cues, and humor. However, most <b>text-to-speech</b> <b>(TTS)</b> systems lack the ability to produce realistic and appropriate laughter sounds, limiting their applications and user experience. While there have been prior works to generate natural laughter, they fell short in terms of controlling the timing and variety of the laughter to be generated. In this work, we propose ELaTE, a <b>zero-shot</b> <b>TTS</b> that can generate natural laughing speech of any speaker based on a short audio <b>prompt</b> with precise control of laughter timing and expression. Specifically, ELaTE works on the audio <b>prompt</b> to mimic the voice characteristic, the text <b>prompt</b> to indicate the contents of the generated speech, and the input to control the laughter expression, which can be either the start and end times of laughter, or the additional audio <b>prompt</b> that contains laughter to be mimicked. We develop our model based on the foundation of conditional flow-matching-based <b>zero-shot</b> <b>TTS,</b> and <b>fine-tune</b> it with frame-level representation from a laughter detector as additional conditioning. With a simple scheme to mix small-scale laughter-conditioned data with large-scale pre-training data, we demonstrate that a pre-trained <b>zero-shot</b> <b>TTS</b> model can be readily <b>fine-tuned</b> to generate natural laughter with precise controllability, without losing any quality of the pre-trained <b>zero-shot</b> <b>TTS</b> model. Through the evaluations, we show that ELaTE can generate laughing speech with significantly higher quality and controllability compared to conventional models. See <a href=https://aka.ms/elate/>https://aka.ms/elate/</a> for demo samples.</p></p class="citation"></blockquote><h3 id=33--175252-interactive-singing-melody-extraction-based-on-active-adaptation-kavya-ranjan-saxena-et-al-2024>(3/3 | 175/252) Interactive singing melody extraction based on active adaptation (Kavya Ranjan Saxena et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kavya Ranjan Saxena, Vipul Arora. (2024)<br><strong>Interactive singing melody extraction based on active adaptation</strong><br><button class=copy-to-clipboard title="Interactive singing melody extraction based on active adaptation" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 20<br>Keywords: Meta Learning, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07599v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07599v1.pdf filename=2402.07599v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Extraction of predominant pitch from polyphonic audio is one of the fundamental tasks in the field of music <b>information</b> <b>retrieval</b> and computational musicology. To accomplish this task using machine learning, a large amount of labeled audio data is required to train the model. However, a classical model pre-trained on data from one domain (source), e.g., songs of a particular singer or genre, may not perform comparatively well in extracting melody from other domains (target). The performance of such models can be boosted by adapting the model using very little annotated data from the target domain. In this work, we propose an efficient interactive melody adaptation method. Our method selects the regions in the target audio that require human annotation using a confidence criterion based on normalized true class probability. The annotations are used by the model to adapt itself to the target domain using <b>meta-learning.</b> <b>Our</b> method also provides a novel <b>meta-learning</b> <b>approach</b> that handles class imbalance, i.e., a few representative samples from a few classes are available for adaptation in the target domain. Experimental results show that the proposed method outperforms other adaptive melody extraction baselines. The proposed method is model-agnostic and hence can be applied to other non-adaptive melody extraction models to boost their performance. Also, we released a Hindustani Alankaar and Raga (HAR) dataset containing 523 audio files of about 6.86 hours of duration intended for singing melody extraction tasks.</p></p class="citation"></blockquote><h2 id=csmm-2>cs.MM (2)</h2><h3 id=12--176252-bdiqa-a-new-dataset-for-video-question-answering-to-explore-cognitive-reasoning-through-theory-of-mind-yuanyuan-mao-et-al-2024>(1/2 | 176/252) BDIQA: A New Dataset for Video Question Answering to Explore Cognitive Reasoning through Theory of Mind (Yuanyuan Mao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanyuan Mao, Xin Lin, Qin Ni, Liang He. (2024)<br><strong>BDIQA: A New Dataset for Video Question Answering to Explore Cognitive Reasoning through Theory of Mind</strong><br><button class=copy-to-clipboard title="BDIQA: A New Dataset for Video Question Answering to Explore Cognitive Reasoning through Theory of Mind" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-AI, cs-MM, cs.MM<br>Keyword Score: 63<br>Keywords: Benchmarking, Few-shot, Supervised Learning, Supervised Learning, Zero-shot, Question Answering, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07402v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07402v1.pdf filename=2402.07402v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a foundational component of cognitive intelligence, theory of mind (ToM) can make AI more closely resemble human thought processes, thereby enhancing their interaction and collaboration with human. In particular, it can significantly improve a model&rsquo;s comprehension of videos in complex scenes. However, current video <b>question</b> <b>answer</b> (VideoQA) datasets focus on studying causal <b>reasoning</b> within events few of them genuinely incorporating human ToM. Consequently, there is a lack of development in ToM <b>reasoning</b> tasks within the area of VideoQA. This paper presents BDIQA, the first <b>benchmark</b> to explore the cognitive <b>reasoning</b> capabilities of VideoQA models in the context of ToM. BDIQA is inspired by the cognitive development of children&rsquo;s ToM and addresses the current deficiencies in machine ToM within datasets and tasks. Specifically, it offers tasks at two difficulty levels, assessing Belief, Desire and Intention (BDI) <b>reasoning</b> in both simple and complex scenarios. We conduct evaluations on several mainstream methods of VideoQA and diagnose their capabilities with zero shot, few shot and <b>supervised</b> <b>learning.</b> We find that the performance of pre-trained models on cognitive <b>reasoning</b> tasks remains unsatisfactory. To counter this challenge, we undertake thorough analysis and experimentation, ultimately presenting two guidelines to enhance cognitive <b>reasoning</b> derived from ablation analysis.</p></p class="citation"></blockquote><h3 id=22--177252-synthesizing-sentiment-controlled-feedback-for-multimodal-text-and-image-data-puneet-kumar-et-al-2024>(2/2 | 177/252) Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data (Puneet Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Puneet Kumar, Sarthak Malik, Balasubramanian Raman, Xiaobai Li. (2024)<br><strong>Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data</strong><br><button class=copy-to-clipboard title="Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-AI, cs-MM, cs.MM<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07640v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07640v1.pdf filename=2402.07640v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability to generate sentiment-controlled feedback in response to <b>multimodal</b> inputs, comprising both text and images, addresses a critical gap in human-computer interaction by enabling systems to provide empathetic, accurate, and engaging responses. This capability has profound applications in healthcare, marketing, and education. To this end, we construct a large-scale Controllable <b>Multimodal</b> Feedback Synthesis (CMFeed) dataset and propose a controllable feedback synthesis system. The proposed system includes an encoder, decoder, and controllability block for textual and visual inputs. It extracts textual and visual features using a <b>transformer</b> and Faster R-CNN networks and combines them to generate feedback. The CMFeed dataset encompasses images, text, reactions to the post, human comments with relevance scores, and reactions to the comments. The reactions to the post and comments are utilized to train the proposed model to produce feedback with a particular (positive or negative) sentiment. A sentiment classification accuracy of 77.23% has been achieved, 18.82% higher than the accuracy without using the controllability. Moreover, the system incorporates a similarity module for assessing feedback relevance through rank-based metrics. It implements an interpretability technique to analyze the contribution of textual and visual features during the generation of uncontrolled and controlled feedback.</p></p class="citation"></blockquote><h2 id=csir-8>cs.IR (8)</h2><h3 id=18--178252-multi-behavior-collaborative-filtering-with-partial-order-graph-convolutional-networks-yijie-zhang-et-al-2024>(1/8 | 178/252) Multi-Behavior Collaborative Filtering with Partial Order Graph Convolutional Networks (Yijie Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijie Zhang, Yuanchen Bei, Hao Chen, Qijie Shen, Zheng Yuan, Huan Gong, Senzhang Wang, Feiran Huang, Xiao Huang. (2024)<br><strong>Multi-Behavior Collaborative Filtering with Partial Order Graph Convolutional Networks</strong><br><button class=copy-to-clipboard title="Multi-Behavior Collaborative Filtering with Partial Order Graph Convolutional Networks" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 56<br>Keywords: Graph Convolutional Network, Graph, Benchmarking, Convolution, Convolutional Neural Network, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07659v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07659v1.pdf filename=2402.07659v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Representing the information of multiple behaviors in the single <b>graph</b> <b>collaborative</b> <b>filtering</b> (CF) vector has been a long-standing challenge. This is because different behaviors naturally form separate behavior <b>graphs</b> <b>and</b> <b>learn</b> separate CF embeddings. Existing models merge the separate embeddings by appointing the CF embeddings for some behaviors as the primary embedding and utilizing other auxiliaries to enhance the primary embedding. However, this approach often results in the joint embedding performing well on the main tasks but poorly on the auxiliary ones. To address the problem arising from the separate behavior <b>graphs,</b> <b>we</b> <b>propose</b> the concept of Partial Order <b>Graphs</b> <b>(POG).</b> <b>POG</b> defines the partial order relation of multiple behaviors and models behavior combinations as weighted edges to merge separate behavior <b>graphs</b> <b>into</b> <b>a</b> joint POG. Theoretical proof verifies that POG can be generalized to any given set of multiple behaviors. Based on POG, we propose the tailored Partial Order <b>Graph</b> <b>Convolutional</b> <b>Networks</b> (POGCN) that convolute neighbors&rsquo; information while considering the behavior relations between users and items. POGCN also introduces a partial-order BPR sampling strategy for efficient and effective multiple-behavior CF training. POGCN has been successfully deployed on the homepage of Alibaba for two months, providing <b>recommendation</b> services for over one billion users. Extensive offline experiments conducted on three public <b>benchmark</b> datasets demonstrate that POGCN outperforms state-of-the-art multi-behavior baselines across all types of behaviors. Furthermore, online A/B tests confirm the superiority of POGCN in billion-scale <b>recommender</b> <b>systems.</b></p></p class="citation"></blockquote><h3 id=28--179252-grillbot-in-practice-lessons-and-tradeoffs-deploying-large-language-models-for-adaptable-conversational-task-assistants-sophie-fischer-et-al-2024>(2/8 | 179/252) GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants (Sophie Fischer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sophie Fischer, Carlos Gemmell, Niklas Tecklenburg, Iain Mackie, Federico Rossetto, Jeffrey Dalton. (2024)<br><strong>GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants</strong><br><button class=copy-to-clipboard title="GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 56<br>Keywords: Multi-modal, Multi-modal, Code Generation, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07647v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07647v1.pdf filename=2402.07647v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We tackle the challenge of building real-world <b>multimodal</b> assistants for complex real-world tasks. We describe the practicalities and challenges of developing and deploying GRILLBot, a leading (first and second prize winning in 2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture that leverages <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and specialised models tuned for specific subtasks requiring very low latency. OAT allows us to define when, how and which <b>LLMs</b> should be used in a structured and deployable manner. For knowledge-grounded <b>question</b> <b>answering</b> and live task adaptations, we show that <b>LLM</b> <b>reasoning</b> abilities over task context and world knowledge outweigh latency concerns. For dialogue state management, we implement a <b>code</b> <b>generation</b> approach and show that specialised smaller models have 84% effectiveness with 100x lower latency. Overall, we provide insights and discuss tradeoffs for deploying both traditional models and <b>LLMs</b> to users in complex real-world <b>multimodal</b> environments in the Alexa TaskBot challenge. These experiences will continue to evolve as <b>LLMs</b> become more capable and efficient &ndash; fundamentally reshaping OAT and future assistant architectures.</p></p class="citation"></blockquote><h3 id=38--180252-quantitative-knowledge-retrieval-from-large-language-models-david-selby-et-al-2024>(3/8 | 180/252) Quantitative knowledge retrieval from large language models (David Selby et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Selby, Kai Spriestersbach, Yuichiro Iwashita, Dennis Bappert, Archana Warrier, Sumantrak Mukherjee, Muhammad Nabeel Asim, Koichi Kise, Sebastian Vollmer. (2024)<br><strong>Quantitative knowledge retrieval from large language models</strong><br><button class=copy-to-clipboard title="Quantitative knowledge retrieval from large language models" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR, stat-AP<br>Keyword Score: 40<br>Keywords: Information Retrieval, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07770v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07770v1.pdf filename=2402.07770v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been extensively studied for their abilities to generate convincing natural language sequences, however their utility for quantitative <b>information</b> <b>retrieval</b> is less well understood. In this paper we explore the feasibility of <b>LLMs</b> as a mechanism for quantitative knowledge retrieval to aid data analysis tasks such as elicitation of prior distributions for Bayesian models and imputation of missing data. We present a <b>prompt</b> engineering framework, treating an <b>LLM</b> as an interface to a latent space of scientific literature, comparing responses in different contexts and domains against more established approaches. Implications and challenges of using <b>LLMs</b> as &rsquo;experts&rsquo; are discussed.</p></p class="citation"></blockquote><h3 id=48--181252-benchmarking-and-building-long-context-retrieval-models-with-loco-and-m2-bert-jon-saad-falcon-et-al-2024>(4/8 | 181/252) Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT (Jon Saad-Falcon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jon Saad-Falcon, Daniel Y. Fu, Simran Arora, Neel Guha, Christopher Ré. (2024)<br><strong>Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT</strong><br><button class=copy-to-clipboard title="Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 36<br>Keywords: Benchmarking, Benchmarking, Fine-tuning, Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07440v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07440v2.pdf filename=2402.07440v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Retrieval pipelines-an integral component of many machine learning systems-perform poorly in domains where documents are long (e.g., 10K tokens or more) and where identifying the relevant document requires synthesizing information across the entire text. Developing long-context retrieval encoders suitable for these domains raises three challenges: (1) how to evaluate long-context retrieval performance, (2) how to pretrain a base language model to represent both short contexts (corresponding to queries) and long contexts (corresponding to documents), and (3) how to <b>fine-tune</b> this model for retrieval under the batch size limitations imposed by GPU memory constraints. To address these challenges, we first introduce LoCoV1, a novel 12 task <b>benchmark</b> constructed to measure long-context retrieval where chunking is not possible or not effective. We next present the M2-BERT retrieval encoder, an 80M parameter state-space encoder model built from the Monarch Mixer architecture, capable of scaling to documents up to 32K tokens long. We describe a pretraining data mixture which allows this encoder to process both short and long context sequences, and a <b>finetuning</b> approach that adapts this base model to retrieval with only single-sample batches. Finally, we validate the M2-BERT retrieval encoder on LoCoV1, finding that it outperforms competitive <b>Transformer-based</b> models by at least 23.3 points, despite containing upwards of 90x fewer parameters.</p></p class="citation"></blockquote><h3 id=58--182252-vcr-video-representation-for-contextual-retrieval-oron-nir-et-al-2024>(5/8 | 182/252) VCR: Video representation for Contextual Retrieval (Oron Nir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oron Nir, Idan Vidra, Avi Neeman, Barak Kinarti, Ariel Shamir. (2024)<br><strong>VCR: Video representation for Contextual Retrieval</strong><br><button class=copy-to-clipboard title="VCR: Video representation for Contextual Retrieval" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-MM, cs.IR<br>Keyword Score: 30<br>Keywords: Recommendation, GPT, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07466v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07466v1.pdf filename=2402.07466v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Streamlining content discovery within media archives requires integrating advanced data representations and effective visualization techniques for clear communication of video topics to users. The proposed system addresses the challenge of efficiently navigating large video collections by exploiting a fusion of visual, audio, and textual features to accurately index and categorize video content through a text-based method. Additionally, semantic embeddings are employed to provide contextually relevant information and <b>recommendations</b> to users, resulting in an intuitive and engaging exploratory experience over our topics ontology map using OpenAI <b>GPT-4.</b></p></p class="citation"></blockquote><h3 id=68--183252-debiasing-recommendation-with-personal-popularity-wentao-ning-et-al-2024>(6/8 | 183/252) Debiasing Recommendation with Personal Popularity (Wentao Ning et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wentao Ning, Reynold Cheng, Xiao Yan, Ben Kao, Nan Huo, Nur AI Hasan Haldar, Bo Tang. (2024)<br><strong>Debiasing Recommendation with Personal Popularity</strong><br><button class=copy-to-clipboard title="Debiasing Recommendation with Personal Popularity" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Counter-factual, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07425v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07425v1.pdf filename=2402.07425v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Global popularity (GP) bias is the phenomenon that popular items are recommended much more frequently than they should be, which goes against the goal of providing personalized <b>recommendations</b> and harms user experience and <b>recommendation</b> accuracy. Many methods have been proposed to reduce GP bias but they fail to notice the fundamental problem of GP, i.e., it considers popularity from a \textit{global} perspective of \textit{all users} and uses a single set of popular items, and thus cannot capture the interests of individual users. As such, we propose a user-aware version of item popularity named \textit{personal popularity} (PP), which identifies different popular items for each user by considering the users that share similar interests. As PP models the preferences of individual users, it naturally helps to produce personalized <b>recommendations</b> and mitigate GP bias. To integrate PP into <b>recommendation,</b> we design a general \textit{personal popularity aware counterfactual} (PPAC) framework, which adapts easily to existing <b>recommendation</b> models. In particular, PPAC recognizes that PP and GP have both direct and indirect effects on <b>recommendations</b> and controls direct effects with <b>counterfactual</b> inference techniques for unbiased <b>recommendations.</b> All codes and datasets are available at \url{https://github.com/Stevenn9981/PPAC}.</p></p class="citation"></blockquote><h3 id=78--184252-multimodal-learned-sparse-retrieval-for-image-suggestion-thong-nguyen-et-al-2024>(7/8 | 184/252) Multimodal Learned Sparse Retrieval for Image Suggestion (Thong Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thong Nguyen, Mariya Hendriksen, Andrew Yates. (2024)<br><strong>Multimodal Learned Sparse Retrieval for Image Suggestion</strong><br><button class=copy-to-clipboard title="Multimodal Learned Sparse Retrieval for Image Suggestion" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-MM, cs.IR<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07736v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07736v1.pdf filename=2402.07736v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learned Sparse Retrieval (LSR) is a group of neural methods designed to encode queries and documents into sparse lexical vectors. These vectors can be efficiently indexed and retrieved using an inverted index. While LSR has shown promise in text retrieval, its potential in <b>multi-modal</b> retrieval remains largely unexplored. Motivated by this, in this work, we explore the application of LSR in the <b>multi-modal</b> domain, i.e., we focus on <b>Multi-Modal</b> Learned Sparse Retrieval (MLSR). We conduct experiments using several MLSR model configurations and evaluate the performance on the image suggestion task. We find that solving the task solely based on the image content is challenging. Enriching the image content with its caption improves the model performance significantly, implying the importance of image captions to provide fine-grained concepts and context information of images. Our approach presents a practical and effective solution for training LSR retrieval models in <b>multi-modal</b> settings.</p></p class="citation"></blockquote><h3 id=88--185252-utilizing-low-dimensional-molecular-embeddings-for-rapid-chemical-similarity-search-kathryn-e-kirchoff-et-al-2024>(8/8 | 185/252) Utilizing Low-Dimensional Molecular Embeddings for Rapid Chemical Similarity Search (Kathryn E. Kirchoff et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kathryn E. Kirchoff, James Wellnitz, Joshua E. Hochuli, Travis Maxfield, Konstantin I. Popov, Shawn Gomez, Alexander Tropsha. (2024)<br><strong>Utilizing Low-Dimensional Molecular Embeddings for Rapid Chemical Similarity Search</strong><br><button class=copy-to-clipboard title="Utilizing Low-Dimensional Molecular Embeddings for Rapid Chemical Similarity Search" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07970v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07970v1.pdf filename=2402.07970v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nearest neighbor-based similarity searching is a common task in chemistry, with notable use cases in drug discovery. Yet, some of the most commonly used approaches for this task still leverage a brute-force approach. In practice this can be computationally costly and overly time-consuming, due in part to the sheer size of modern chemical databases. Previous computational advancements for this task have generally relied on improvements to hardware or dataset-specific tricks that lack generalizability. Approaches that leverage lower-complexity searching algorithms remain relatively underexplored. However, many of these algorithms are approximate solutions and/or struggle with typical high-dimensional chemical embeddings. Here we evaluate whether a combination of low-dimensional chemical embeddings and a k-d tree data structure can achieve fast nearest neighbor queries while maintaining performance on standard chemical similarity search <b>benchmarks.</b> We examine different dimensionality reductions of standard chemical embeddings as well as a learned, structurally-aware embedding &ndash; SmallSA &ndash; for this task. With this framework, searches on over one billion chemicals execute in less than a second on a single CPU core, five orders of magnitude faster than the brute-force approach. We also demonstrate that SmallSA achieves competitive performance on chemical similarity <b>benchmarks.</b></p></p class="citation"></blockquote><h2 id=cscy-4>cs.CY (4)</h2><h3 id=14--186252-ai-augmented-predictions-llm-assistants-improve-human-forecasting-accuracy-philipp-schoenegger-et-al-2024>(1/4 | 186/252) AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy (Philipp Schoenegger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Schoenegger, Peter S. Park, Ezra Karger, Philip E. Tetlock. (2024)<br><strong>AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy</strong><br><button class=copy-to-clipboard title="AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CL, cs-CY, cs-LG, cs.CY<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, GPT-4 turbo, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07862v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07862v1.pdf filename=2402.07862v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> show impressive capabilities, matching and sometimes exceeding human performance in many domains. This study explores the potential of <b>LLMs</b> to augment judgement in forecasting tasks. We evaluated the impact on forecasting accuracy of two <b>GPT-4-Turbo</b> <b>assistants:</b> one designed to provide high-quality advice (&lsquo;superforecasting&rsquo;), and the other designed to be overconfident and base-rate-neglecting. Participants (N = 991) had the option to consult their assigned <b>LLM</b> assistant throughout the study, in contrast to a control group that used a less advanced model (DaVinci-003) without direct forecasting support. Our preregistered analyses reveal that <b>LLM</b> augmentation significantly enhances forecasting accuracy by 23% across both types of assistants, compared to the control group. This improvement occurs despite the superforecasting assistant&rsquo;s higher accuracy in predictions, indicating the augmentation&rsquo;s benefit is not solely due to model prediction accuracy. Exploratory analyses showed a pronounced effect in one forecasting item, without which we find that the superforecasting assistant increased accuracy by 43%, compared with 28% for the biased assistant. We further examine whether <b>LLM</b> augmentation disproportionately benefits less skilled forecasters, degrades the wisdom-of-the-crowd by reducing prediction diversity, or varies in effectiveness with question difficulty. Our findings do not consistently support these hypotheses. Our results suggest that access to an <b>LLM</b> assistant, even a biased one, can be a helpful decision aid in cognitively demanding tasks where the answer is not known at the time of interaction.</p></p class="citation"></blockquote><h3 id=24--187252-leveraging-ai-to-advance-science-and-computing-education-across-africa-progress-challenges-and-opportunities-george-boateng-2024>(2/4 | 187/252) Leveraging AI to Advance Science and Computing Education across Africa: Progress, Challenges, and Opportunities (George Boateng, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>George Boateng. (2024)<br><strong>Leveraging AI to Advance Science and Computing Education across Africa: Progress, Challenges, and Opportunities</strong><br><button class=copy-to-clipboard title="Leveraging AI to Advance Science and Computing Education across Africa: Progress, Challenges, and Opportunities" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CL, cs-CY, cs-HC, cs.CY<br>Keyword Score: 30<br>Keywords: BERT, GPT, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07397v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07397v1.pdf filename=2402.07397v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Across the African continent, students grapple with various educational challenges, including limited access to essential resources such as computers, internet connectivity, reliable electricity, and a shortage of qualified teachers. Despite these challenges, recent advances in AI such as <b>BERT,</b> and <b>GPT-4</b> have demonstrated their potential for advancing education. Yet, these AI tools tend to be deployed and evaluated predominantly within the context of Western educational settings, with limited attention directed towards the unique needs and challenges faced by students in Africa. In this book chapter, we describe our works developing and deploying AI in Education tools in Africa: (1) SuaCode, an AI-powered app that enables Africans to learn to code using their smartphones, (2) AutoGrad, an automated grading, and feedback tool for graphical and interactive coding assignments, (3) a tool for code plagiarism detection that shows visual evidence of plagiarism, (4) Kwame, a bilingual AI teaching assistant for coding courses, (5) Kwame for Science, a web-based AI teaching assistant that provides instant answers to students&rsquo; science questions and (6) Brilla AI, an AI contestant for the National Science and Maths Quiz competition. We discuss challenges and potential opportunities to use AI to advance science and computing education across Africa.</p></p class="citation"></blockquote><h3 id=34--188252-auditing-work-exploring-the-new-york-city-algorithmic-bias-audit-regime-lara-groves-et-al-2024>(3/4 | 188/252) Auditing Work: Exploring the New York City algorithmic bias audit regime (Lara Groves et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lara Groves, Jacob Metcalf, Alayna Kennedy, Briana Vecchione, Andrew Strait. (2024)<br><strong>Auditing Work: Exploring the New York City algorithmic bias audit regime</strong><br><button class=copy-to-clipboard title="Auditing Work: Exploring the New York City algorithmic bias audit regime" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08101v1.pdf filename=2402.08101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In July 2023, New York City (NYC) initiated the first algorithm auditing system for commercial machine-learning systems. Local Law 144 (LL 144) mandates NYC-based employers using automated employment decision-making tools (AEDTs) in hiring to undergo annual bias audits conducted by an independent auditor. This paper examines lessons from LL 144 for other national algorithm auditing attempts. Through qualitative interviews with 16 experts and practitioners within the regime, we find that LL 144 has not effectively established an auditing regime. The law fails to clearly define key aspects, such as AEDTs and independent auditors, leading auditors, AEDT vendors, and companies using AEDTs to define the law&rsquo;s practical implementation in ways that failed to protect job applicants. Contributing factors include the law&rsquo;s flawed transparency-driven theory of change, industry lobbying narrowing the definition of AEDTs, practical and cultural challenges faced by auditors in accessing data, and wide disagreement over what constitutes a legitimate auditor, resulting in four distinct &lsquo;auditor roles.&rsquo; We conclude with four <b>recommendations</b> for policymakers seeking to create similar bias auditing regimes, emphasizing clearer definitions, metrics, and increased accountability. By exploring LL 144 through the lens of auditors, our paper advances the evidence base around audit as an accountability mechanism, providing guidance for policymakers seeking to create similar regimes.</p></p class="citation"></blockquote><h3 id=44--189252-algorithmic-fairness-and-color-blind-racism-navigating-the-intersection-jamelle-watson-daniels-2024>(4/4 | 189/252) Algorithmic Fairness and Color-blind Racism: Navigating the Intersection (Jamelle Watson-Daniels, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jamelle Watson-Daniels. (2024)<br><strong>Algorithmic Fairness and Color-blind Racism: Navigating the Intersection</strong><br><button class=copy-to-clipboard title="Algorithmic Fairness and Color-blind Racism: Navigating the Intersection" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07778v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07778v1.pdf filename=2402.07778v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our focus lies at the intersection between two broader research perspectives: (1) the scientific study of algorithms and (2) the scholarship on race and racism. Many streams of research related to algorithmic <b>fairness</b> have been born out of interest at this intersection. We think about this intersection as the product of work derived from both sides. From (1) algorithms to (2) racism, the starting place might be an algorithmic question or method connected to a conceptualization of racism. On the other hand, from (2) racism to (1) algorithms, the starting place could be recognizing a setting where a legacy of racism is known to persist and drawing connections between that legacy and the introduction of algorithms into this setting. In either direction, meaningful disconnection can occur when conducting research at the intersection of racism and algorithms. The present paper urges collective reflection on research directions at this intersection. Despite being primarily motivated by instances of racial bias, research in algorithmic <b>fairness</b> remains mostly disconnected from scholarship on racism. In particular, there has not been an examination connecting algorithmic <b>fairness</b> discussions directly to the ideology of color-blind racism; we aim to fill this gap. We begin with a review of an essential account of color-blind racism then we review racial discourse within algorithmic <b>fairness</b> research and underline significant patterns, shifts and disconnects. Ultimately, we argue that researchers can improve the navigation of the landscape at the intersection by recognizing ideological shifts as such and iteratively re-orienting towards maintaining meaningful connections across interdisciplinary lines.</p></p class="citation"></blockquote><h2 id=cshc-6>cs.HC (6)</h2><h3 id=16--190252-enhancing-programming-error-messages-in-real-time-with-generative-ai-bailey-kimmel-et-al-2024>(1/6 | 190/252) Enhancing Programming Error Messages in Real Time with Generative AI (Bailey Kimmel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bailey Kimmel, Austin Geisert, Lily Yaro, Brendan Gipson, Taylor Hotchkiss, Sidney Osae-Asante, Hunter Vaught, Grant Wininger, Chase Yamaguchi. (2024)<br><strong>Enhancing Programming Error Messages in Real Time with Generative AI</strong><br><button class=copy-to-clipboard title="Enhancing Programming Error Messages in Real Time with Generative AI" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: Generative AI, ChatGPT, GPT, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08072v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08072v1.pdf filename=2402.08072v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI</b> is changing the way that many disciplines are taught, including computer science. Researchers have shown that <b>generative</b> <b>AI</b> tools are capable of solving programming problems, writing extensive blocks of code, and explaining complex code in simple terms. Particular promise has been shown in using <b>generative</b> <b>AI</b> to enhance programming error messages. Both students and instructors have complained for decades that these messages are often cryptic and difficult to understand. Yet recent work has shown that students make fewer repeated errors when enhanced via <b>GPT-4.</b> We extend this work by implementing feedback from <b>ChatGPT</b> for all programs submitted to our automated assessment tool, Athene, providing help for compiler, run-time, and logic errors. Our results indicate that adding <b>generative</b> <b>AI</b> to an automated assessment tool does not necessarily make it better and that design of the interface matters greatly to the usability of the feedback that <b>GPT-4</b> provided.</p></p class="citation"></blockquote><h3 id=26--191252-why-and-when-llm-based-assistants-can-go-wrong-investigating-the-effectiveness-of-prompt-based-interactions-for-software-help-seeking-anjali-khurana-et-al-2024>(2/6 | 191/252) Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking (Anjali Khurana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anjali Khurana, Hari Subramonyam, Parmit K Chilana. (2024)<br><strong>Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking</strong><br><button class=copy-to-clipboard title="Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs-LG, cs.HC<br>Keyword Score: 40<br>Keywords: ChatGPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08030v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08030v1.pdf filename=2402.08030v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> assistants, such as <b>ChatGPT,</b> have emerged as potential alternatives to search methods for helping users navigate complex, feature-rich software. <b>LLMs</b> use vast training data from domain-specific texts, software manuals, and code repositories to mimic human-like interactions, offering tailored assistance, including step-by-step instructions. In this work, we investigated <b>LLM-generated</b> software guidance through a within-subject experiment with 16 participants and follow-up interviews. We compared a baseline <b>LLM</b> assistant with an <b>LLM</b> optimized for particular software contexts, SoftAIBot, which also offered guidelines for constructing appropriate <b>prompts.</b> We assessed task completion, perceived accuracy, relevance, and trust. Surprisingly, although SoftAIBot outperformed the baseline <b>LLM,</b> our results revealed no significant difference in <b>LLM</b> usage and user perceptions with or without <b>prompt</b> guidelines and the integration of domain context. Most users struggled to understand how the <b>prompt&rsquo;s</b> text related to the <b>LLM&rsquo;s</b> responses and often followed the <b>LLM&rsquo;s</b> suggestions verbatim, even if they were incorrect. This resulted in difficulties when using the <b>LLM&rsquo;s</b> advice for software tasks, leading to low task completion rates. Our detailed analysis also revealed that users remained unaware of inaccuracies in the <b>LLM&rsquo;s</b> responses, indicating a gap between their lack of software expertise and their ability to evaluate the <b>LLM&rsquo;s</b> assistance. With the growing push for designing domain-specific <b>LLM</b> assistants, we emphasize the importance of incorporating explainable, context-aware cues into <b>LLMs</b> to help users understand <b>prompt-based</b> interactions, identify biases, and maximize the utility of <b>LLM</b> assistants.</p></p class="citation"></blockquote><h3 id=36--192252-imagining-a-future-of-designing-with-ai-dynamic-grounding-constructive-negotiation-and-sustainable-motivation-priyan-vaithilingam-et-al-2024>(3/6 | 192/252) Imagining a Future of Designing with AI: Dynamic Grounding, Constructive Negotiation, and Sustainable Motivation (Priyan Vaithilingam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Priyan Vaithilingam, Ian Arawjo, Elena L. Glassman. (2024)<br><strong>Imagining a Future of Designing with AI: Dynamic Grounding, Constructive Negotiation, and Sustainable Motivation</strong><br><button class=copy-to-clipboard title="Imagining a Future of Designing with AI: Dynamic Grounding, Constructive Negotiation, and Sustainable Motivation" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: J-6; I-2-0; H-5-2, cs-AI, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Foundation Model, Grounding, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07342v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07342v1.pdf filename=2402.07342v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We ideate a future design workflow that involves AI technology. Drawing from activity and communication theory, we attempt to isolate the new value large AI models can provide design compared to past technologies. We arrive at three affordances &ndash; dynamic <b>grounding,</b> constructive negotiation, and sustainable motivation &ndash; that <b>summarize</b> latent qualities of natural language-enabled <b>foundation</b> <b>models</b> that, if explicitly designed for, can support the process of design. Through design fiction, we then imagine a future interface as a diegetic prototype, the story of Squirrel Game, that demonstrates each of our three affordances in a realistic usage scenario. Our design process, terminology, and diagrams aim to contribute to future discussions about the relative affordances of AI technology with regard to collaborating with human designers.</p></p class="citation"></blockquote><h3 id=46--193252-portobello-extending-driving-simulation-from-the-lab-to-the-road-fanjun-bu-et-al-2024>(4/6 | 193/252) Portobello: Extending Driving Simulation from the Lab to the Road (Fanjun Bu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fanjun Bu, Stacey Li, David Goedicke, Mark Colley, Gyanendra Sharma, Hiroshi Yasuda, Wendy Ju. (2024)<br><strong>Portobello: Extending Driving Simulation from the Lab to the Road</strong><br><button class=copy-to-clipboard title="Portobello: Extending Driving Simulation from the Lab to the Road" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08061v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08061v1.pdf filename=2402.08061v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In automotive user interface design, testing often starts with lab-based driving simulators and migrates toward on-road studies to mitigate risks. Mixed reality (XR) helps translate virtual study designs to the real road to increase ecological validity. However, researchers rarely run the same study in both in-lab and on-road simulators due to the challenges of replicating studies in both physical and virtual worlds. To provide a common infrastructure to port in-lab study designs on-road, we built a platform-portable infrastructure, Portobello, to enable us to run twinned physical-virtual studies. As a proof-of-concept, we extended the on-road simulator XR-OOM with Portobello. We ran a within-subjects, autonomous-vehicle crosswalk cooperation study (N=32) both in-lab and on-road to investigate study design portability and platform-driven influences on study outcomes. To our knowledge, this is the first system that enables the twinning of studies originally designed for in-lab simulators to be carried out in an on-road platform.</p></p class="citation"></blockquote><h3 id=56--194252-pkg-api-a-tool-for-personal-knowledge-graph-management-nolwenn-bernard-et-al-2024>(5/6 | 194/252) PKG API: A Tool for Personal Knowledge Graph Management (Nolwenn Bernard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nolwenn Bernard, Ivica Kostric, Weronika Łajewska, Krisztian Balog, Petra Galuščáková, Vinay Setty, Martin G. Skjæveland. (2024)<br><strong>PKG API: A Tool for Personal Knowledge Graph Management</strong><br><button class=copy-to-clipboard title="PKG API: A Tool for Personal Knowledge Graph Management" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CL, cs-HC, cs.HC<br>Keyword Score: 8<br>Keywords: Graph, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07540v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07540v1.pdf filename=2402.07540v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Personal <b>knowledge</b> <b>graphs</b> (PKGs) offer individuals a way to store and consolidate their fragmented personal data in a central place, improving service personalization while maintaining full user control. Despite their potential, practical PKG implementations with user-friendly interfaces remain scarce. This work addresses this gap by proposing a complete solution to represent, manage, and interface with PKGs. Our approach includes (1) a user-facing PKG Client, enabling end-users to administer their personal data easily via natural language statements, and (2) a service-oriented PKG API. To tackle the complexity of representing these statements within a PKG, we present an RDF-based PKG vocabulary that supports this, along with properties for access rights and provenance.</p></p class="citation"></blockquote><h3 id=66--195252-renelib-real-time-neural-listening-behavior-generation-for-socially-interactive-agents-daksitha-withanage-don-et-al-2024>(6/6 | 195/252) ReNeLiB: Real-time Neural Listening Behavior Generation for Socially Interactive Agents (Daksitha Withanage Don et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daksitha Withanage Don, Philipp Müller, Fabrizio Nunnari, Elisabeth André, Patrick Gebhard. (2024)<br><strong>ReNeLiB: Real-time Neural Listening Behavior Generation for Socially Interactive Agents</strong><br><button class=copy-to-clipboard title="ReNeLiB: Real-time Neural Listening Behavior Generation for Socially Interactive Agents" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08079v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08079v1.pdf filename=2402.08079v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Flexible and natural nonverbal reactions to human behavior remain a challenge for socially interactive agents (SIAs) that are predominantly animated using hand-crafted rules. While recently proposed machine learning based approaches to conversational behavior generation are a promising way to address this challenge, they have not yet been employed in SIAs. The primary reason for this is the lack of a software toolkit integrating such approaches with SIA frameworks that conforms to the challenging real-time requirements of human-agent interaction scenarios. In our work, we for the first time present such a toolkit consisting of three main components: (1) real-time feature extraction capturing <b>multi-modal</b> social cues from the user; (2) behavior generation based on a recent state-of-the-art neural network approach; (3) visualization of the generated behavior supporting both FLAME-based and Apple ARKit-based interactive agents. We comprehensively evaluate the real-time performance of the whole framework and its components. In addition, we introduce pre-trained behavioral generation models derived from psychotherapy sessions for domain-specific listening behaviors. Our software toolkit, pivotal for deploying and assessing SIAs&rsquo; listening behavior in real-time, is publicly available. Resources, including code, behavioural <b>multi-modal</b> features extracted from therapeutic interactions, are hosted at <a href=https://daksitha.github.io/ReNeLib>https://daksitha.github.io/ReNeLib</a></p></p class="citation"></blockquote><h2 id=eessiv-6>eess.IV (6)</h2><h3 id=16--196252-comparative-analysis-of-imagenet-pre-trained-deep-learning-models-and-dinov2-in-medical-imaging-classification-yuning-huang-et-al-2024>(1/6 | 196/252) Comparative Analysis of ImageNet Pre-Trained Deep Learning Models and DINOv2 in Medical Imaging Classification (Yuning Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuning Huang, Jingchen Zou, Lanxi Meng, Xin Yue, Qing Zhao, Jianqiang Li, Changwei Song, Gabriel Jimenez, Shaowu Li, Guanghui Fu. (2024)<br><strong>Comparative Analysis of ImageNet Pre-Trained Deep Learning Models and DINOv2 in Medical Imaging Classification</strong><br><button class=copy-to-clipboard title="Comparative Analysis of ImageNet Pre-Trained Deep Learning Models and DINOv2 in Medical Imaging Classification" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Vision Transformer, Transfer Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07595v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07595v2.pdf filename=2402.07595v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical image analysis frequently encounters data scarcity challenges. <b>Transfer</b> <b>learning</b> has been effective in addressing this issue while conserving computational resources. The recent advent of foundational models like the DINOv2, which uses the <b>vision</b> <b>transformer</b> architecture, has opened new opportunities in the field and gathered significant interest. However, DINOv2&rsquo;s performance on clinical data still needs to be verified. In this paper, we performed a glioma grading task using three clinical modalities of brain MRI data. We compared the performance of various pre-trained deep learning models, including those based on ImageNet and DINOv2, in a <b>transfer</b> <b>learning</b> context. Our focus was on understanding the impact of the freezing mechanism on performance. We also validated our findings on three other types of public datasets: chest radiography, fundus radiography, and dermoscopy. Our findings indicate that in our clinical dataset, DINOv2&rsquo;s performance was not as strong as ImageNet-based pre-trained models, whereas in public datasets, DINOv2 generally outperformed other models, especially when using the frozen mechanism. Similar performance was observed with various sizes of DINOv2 models across different tasks. In summary, DINOv2 is viable for medical image classification tasks, particularly with data resembling natural images. However, its effectiveness may vary with data that significantly differs from natural images such as MRI. In addition, employing smaller versions of the model can be adequate for medical task, offering resource-saving benefits. Our codes are available at <a href=https://github.com/GuanghuiFU/medical_DINOv2_eval>https://github.com/GuanghuiFU/medical_DINOv2_eval</a>.</p></p class="citation"></blockquote><h3 id=26--197252-weakly-supervised-detection-of-pheochromocytomas-and-paragangliomas-in-ct-david-c-oluigboa-et-al-2024>(2/6 | 197/252) Weakly Supervised Detection of Pheochromocytomas and Paragangliomas in CT (David C. Oluigboa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David C. Oluigboa, Bikash Santra, Tejas Sudharshan Mathai, Pritam Mukherjee, Jianfei Liu, Abhishek Jha, Mayank Patel, Karel Pacak, Ronald M. Summers. (2024)<br><strong>Weakly Supervised Detection of Pheochromocytomas and Paragangliomas in CT</strong><br><button class=copy-to-clipboard title="Weakly Supervised Detection of Pheochromocytomas and Paragangliomas in CT" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08697v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08697v1.pdf filename=2402.08697v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pheochromocytomas and Paragangliomas (PPGLs) are rare adrenal and extra-adrenal tumors which have the potential to metastasize. For the management of patients with PPGLs, CT is the preferred modality of choice for precise localization and estimation of their progression. However, due to the myriad variations in size, morphology, and appearance of the tumors in different anatomical regions, radiologists are posed with the challenge of accurate detection of PPGLs. Since clinicians also need to routinely measure their size and track their changes over time across patient visits, manual demarcation of PPGLs is quite a time-consuming and cumbersome process. To ameliorate the manual effort spent for this task, we propose an automated method to detect PPGLs in CT studies via a proxy segmentation task. As only weak annotations for PPGLs in the form of prospectively marked 2D bounding boxes on an axial slice were available, we extended these 2D boxes into weak 3D annotations and trained a 3D full-resolution nnUNet model to directly segment PPGLs. We evaluated our approach on a dataset consisting of chest-abdomen-pelvis CTs of 255 patients with confirmed PPGLs. We obtained a precision of 70% and sensitivity of 64.1% with our proposed approach when tested on 53 CT studies. Our findings highlight the promising nature of detecting PPGLs via segmentation, and furthers the state-of-the-art in this exciting yet challenging area of rare cancer management.</p></p class="citation"></blockquote><h3 id=36--198252-automated-classification-of-body-mri-sequence-type-using-convolutional-neural-networks-kimberly-helm-et-al-2024>(3/6 | 198/252) Automated Classification of Body MRI Sequence Type Using Convolutional Neural Networks (Kimberly Helm et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kimberly Helm, Tejas Sudharshan Mathai, Boah Kim, Pritam Mukherjee, Jianfei Liu, Ronald M. Summers. (2024)<br><strong>Automated Classification of Body MRI Sequence Type Using Convolutional Neural Networks</strong><br><button class=copy-to-clipboard title="Automated Classification of Body MRI Sequence Type Using Convolutional Neural Networks" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08098v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08098v1.pdf filename=2402.08098v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-parametric MRI of the body is routinely acquired for the identification of abnormalities and diagnosis of diseases. However, a standard naming convention for the MRI protocols and associated sequences does not exist due to wide variations in imaging practice at institutions and myriad MRI scanners from various manufacturers being used for imaging. The intensity distributions of MRI sequences differ widely as a result, and there also exists information conflicts related to the sequence type in the DICOM headers. At present, clinician oversight is necessary to ensure that the correct sequence is being read and used for diagnosis. This poses a challenge when specific series need to be considered for building a cohort for a large clinical study or for developing AI algorithms. In order to reduce clinician oversight and ensure the validity of the DICOM headers, we propose an automated method to classify the 3D MRI sequence acquired at the levels of the chest, abdomen, and pelvis. In our pilot work, our 3D DenseNet-121 model achieved an F1 score of 99.5% at differentiating 5 common MRI sequences obtained by three Siemens scanners (Aera, Verio, Biograph mMR). To the best of our knowledge, we are the first to develop an automated method for the 3D classification of MRI sequences in the chest, abdomen, and pelvis, and our work has outperformed the previous state-of-the-art MRI series classifiers.</p></p class="citation"></blockquote><h3 id=46--199252-minimally-interactive-segmentation-of-soft-tissue-tumors-on-ct-and-mri-using-deep-learning-douwe-j-spaanderman-et-al-2024>(4/6 | 199/252) Minimally Interactive Segmentation of Soft-Tissue Tumors on CT and MRI using Deep Learning (Douwe J. Spaanderman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Douwe J. Spaanderman, Martijn P. A. Starmans, Gonnie C. M. van Erp, David F. Hanff, Judith H. Sluijter, Anne-Rose W. Schut, Geert J. L. H. van Leenders, Cornelis Verhoef, Dirk J. Grunhagen, Wiro J. Niessen, Jacob J. Visser, Stefan Klein. (2024)<br><strong>Minimally Interactive Segmentation of Soft-Tissue Tumors on CT and MRI using Deep Learning</strong><br><button class=copy-to-clipboard title="Minimally Interactive Segmentation of Soft-Tissue Tumors on CT and MRI using Deep Learning" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07746v1.pdf filename=2402.07746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Segmentations are crucial in medical imaging to obtain morphological, volumetric, and radiomics biomarkers. Manual segmentation is accurate but not feasible in the radiologist&rsquo;s clinical workflow, while automatic segmentation generally obtains sub-par performance. We therefore developed a minimally interactive deep learning-based segmentation method for soft-tissue tumors (STTs) on CT and MRI. The method requires the user to click six points near the tumor&rsquo;s extreme boundaries. These six points are transformed into a distance map and serve, with the image, as input for a <b>Convolutional</b> <b>Neural</b> <b>Network.</b> For training and validation, a multicenter dataset containing 514 patients and nine STT types in seven anatomical locations was used, resulting in a Dice Similarity Coefficient (DSC) of 0.85$\pm$0.11 (mean $\pm$ standard deviation (SD)) for CT and 0.84$\pm$0.12 for T1-weighted MRI, when compared to manual segmentations made by expert radiologists. Next, the method was externally validated on a dataset including five unseen STT phenotypes in extremities, achieving 0.81$\pm$0.08 for CT, 0.84$\pm$0.09 for T1-weighted MRI, and 0.88\pm0.08 for previously unseen T2-weighted fat-saturated (FS) MRI. In conclusion, our minimally interactive segmentation method effectively segments different types of STTs on CT and MRI, with robust generalization to previously unseen phenotypes and imaging modalities.</p></p class="citation"></blockquote><h3 id=56--200252-re-diffinet-modeling-discrepancies-loss-in-tumor-segmentation-using-diffusion-models-tianyi-ren-et-al-2024>(5/6 | 200/252) Re-DiffiNet: Modeling discrepancies loss in tumor segmentation using diffusion models (Tianyi Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyi Ren, Abhishek Sharma, Juampablo Heras Rivera, Harshitha Rebala, Ethan Honey, Agamdeep Chopra, Jacob Ruzevick, Mehmet Kurt. (2024)<br><strong>Re-DiffiNet: Modeling discrepancies loss in tumor segmentation using diffusion models</strong><br><button class=copy-to-clipboard title="Re-DiffiNet: Modeling discrepancies loss in tumor segmentation using diffusion models" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07354v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07354v3.pdf filename=2402.07354v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identification of tumor margins is essential for surgical decision-making for glioblastoma patients and provides reliable assistance for neurosurgeons. Despite improvements in deep learning architectures for tumor segmentation over the years, creating a fully autonomous system suitable for clinical floors remains a formidable challenge because the model predictions have not yet reached the desired level of accuracy and generalizability for clinical applications. <b>Generative</b> <b>modeling</b> <b>techniques</b> have seen significant improvements in recent times. Specifically, <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> and Denoising-diffusion-based models (DDPMs) have been used to generate higher-quality images with fewer artifacts and finer attributes. In this work, we introduce a framework called Re-Diffinet for modeling the discrepancy between the outputs of a segmentation model like U-Net and the ground truth, using DDPMs. By explicitly modeling the discrepancy, the results show an average improvement of 0.55% in the Dice score and 16.28% in HD95 from cross-validation over 5-folds, compared to the state-of-the-art U-Net segmentation model.</p></p class="citation"></blockquote><h3 id=66--201252-inference-stage-denoising-for-undersampled-mri-reconstruction-yuyang-xue-et-al-2024>(6/6 | 201/252) Inference Stage Denoising for Undersampled MRI Reconstruction (Yuyang Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuyang Xue, Chen Qin, Sotirios A. Tsaftaris. (2024)<br><strong>Inference Stage Denoising for Undersampled MRI Reconstruction</strong><br><button class=copy-to-clipboard title="Inference Stage Denoising for Undersampled MRI Reconstruction" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08692v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08692v1.pdf filename=2402.08692v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconstruction of magnetic resonance imaging (MRI) <b>data</b> <b>has</b> been positively affected by deep learning. A key challenge remains: to improve generalisation to distribution shifts between the training and testing <b>data.</b> <b>Most</b> approaches aim to address this via inductive design or <b>data</b> <b>augmentation.</b> However, they can be affected by misleading <b>data,</b> <b>e.g.</b> random noise, and cases where the inference stage <b>data</b> <b>do</b> not match assumptions in the modelled shifts. In this work, by employing a conditional hyperparameter network, we eliminate the need of augmentation, yet maintain robust performance under various levels of Gaussian noise. We demonstrate that our model withstands various input noise levels while producing high-definition reconstructions during the test stage. Moreover, we present a hyperparameter sampling strategy that accelerates the convergence of training. Our proposed method achieves the highest accuracy and image quality in all settings compared to baseline methods.</p></p class="citation"></blockquote><h2 id=q-biogn-1>q-bio.GN (1)</h2><h3 id=11--202252-efficient-and-scalable-fine-tune-of-language-models-for-genome-understanding-huixin-zhan-et-al-2024>(1/1 | 202/252) Efficient and Scalable Fine-Tune of Language Models for Genome Understanding (Huixin Zhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huixin Zhan, Ying Nian Wu, Zijun Zhang. (2024)<br><strong>Efficient and Scalable Fine-Tune of Language Models for Genome Understanding</strong><br><button class=copy-to-clipboard title="Efficient and Scalable Fine-Tune of Language Models for Genome Understanding" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.GN<br>Categories: cs-AI, cs-LG, q-bio-GN, q-bio.GN<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08075v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08075v1.pdf filename=2402.08075v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although DNA <b>foundation</b> <b>models</b> have advanced the understanding of genomes, they still face significant challenges in the limited scale and diversity of genomic data. This limitation starkly contrasts with the success of natural language <b>foundation</b> <b>models,</b> which thrive on substantially larger scales. Furthermore, genome understanding involves numerous downstream genome annotation tasks with inherent data heterogeneity, thereby necessitating more efficient and robust <b>fine-tuning</b> methods tailored for genomics. Here, we present \textsc{Lingo}: \textsc{L}anguage prefix f\textsc{In}e-tuning for \textsc{G}en\textsc{O}mes. Unlike DNA <b>foundation</b> <b>models,</b> \textsc{Lingo} strategically leverages natural language <b>foundation</b> <b>models&rsquo;</b> contextual cues, recalibrating their linguistic knowledge to genomic sequences. \textsc{Lingo} further accommodates numerous, heterogeneous downstream <b>fine-tune</b> tasks by an adaptive rank sampling method that prunes and stochastically reintroduces pruned singular vectors within small computational budgets. Adaptive rank sampling outperformed existing <b>fine-tuning</b> methods on all <b>benchmarked</b> 14 genome understanding tasks, while requiring fewer than 2% of trainable parameters as genomic-specific adapters. Impressively, applying these adapters on natural language <b>foundation</b> <b>models</b> matched or even exceeded the performance of DNA <b>foundation</b> <b>models.</b> \textsc{Lingo} presents a new paradigm of efficient and scalable genome understanding via genomic-specific adapters on language models.</p></p class="citation"></blockquote><h2 id=statml-7>stat.ML (7)</h2><h3 id=17--203252-graph-structure-inference-with-bam-introducing-the-bilinear-attention-mechanism-philipp-froehlich-et-al-2024>(1/7 | 203/252) Graph Structure Inference with BAM: Introducing the Bilinear Attention Mechanism (Philipp Froehlich et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Froehlich, Heinz Koeppl. (2024)<br><strong>Graph Structure Inference with BAM: Introducing the Bilinear Attention Mechanism</strong><br><button class=copy-to-clipboard title="Graph Structure Inference with BAM: Introducing the Bilinear Attention Mechanism" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 33<br>Keywords: Graph, Simulation, Simulator, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07735v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07735v2.pdf filename=2402.07735v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In statistics and machine learning, detecting dependencies in datasets is a central challenge. We propose a novel neural network model for <b>supervised</b> <b>graph</b> structure learning, i.e., the process of learning a mapping between observational data and their underlying dependence structure. The model is trained with variably shaped and coupled simulated input data and requires only a single forward pass through the trained network for inference. By leveraging structural equation models and employing randomly generated multivariate Chebyshev polynomials for the <b>simulation</b> of training data, our method demonstrates robust generalizability across both linear and various types of non-linear dependencies. We introduce a novel bilinear attention mechanism (BAM) for explicit processing of dependency information, which operates on the level of covariance matrices of transformed data and respects the geometry of the manifold of symmetric positive definite matrices. Empirical evaluation demonstrates the robustness of our method in detecting a wide range of dependencies, excelling in undirected <b>graph</b> estimation and proving competitive in completed partially directed acyclic <b>graph</b> estimation through a novel two-step approach.</p></p class="citation"></blockquote><h3 id=27--204252-stochastic-gradient-flow-dynamics-of-test-risk-and-its-exact-solution-for-weak-features-rodrigo-veiga-et-al-2024>(2/7 | 204/252) Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution for Weak Features (Rodrigo Veiga et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rodrigo Veiga, Anastasia Remizova, Nicolas Macris. (2024)<br><strong>Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution for Weak Features</strong><br><button class=copy-to-clipboard title="Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution for Weak Features" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cond-mat-dis-nn, cs-LG, stat-ML, stat.ML<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07626v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07626v1.pdf filename=2402.07626v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the test risk of continuous-time <b>stochastic</b> <b>gradient</b> <b>flow</b> dynamics in learning theory. Using a path integral formulation we provide, in the regime of a small learning rate, a general formula for computing the difference between test risk curves of pure gradient and <b>stochastic</b> <b>gradient</b> <b>flows.</b> We apply the general theory to a simple model of weak features, which displays the double descent phenomenon, and explicitly compute the corrections brought about by the added <b>stochastic</b> <b>term</b> <b>in</b> the dynamics, as a function of time and model parameters. The analytical results are compared to <b>simulations</b> of discrete-time <b>stochastic</b> <b>gradient</b> <b>descent</b> and show good agreement.</p></p class="citation"></blockquote><h3 id=37--205252-diffeomorphic-measure-matching-with-kernels-for-generative-modeling-biraj-pandey-et-al-2024>(3/7 | 205/252) Diffeomorphic Measure Matching with Kernels for Generative Modeling (Biraj Pandey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Biraj Pandey, Bamdad Hosseini, Pau Batlle, Houman Owhadi. (2024)<br><strong>Diffeomorphic Measure Matching with Kernels for Generative Modeling</strong><br><button class=copy-to-clipboard title="Diffeomorphic Measure Matching with Kernels for Generative Modeling" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: 35Q68 49Q22 62F15 68T07 62R07, cs-LG, math-DS, stat-CO, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08077v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08077v1.pdf filename=2402.08077v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article presents a general framework for the transport of probability measures towards minimum divergence generative modeling and sampling using ordinary differential equations (ODEs) and Reproducing Kernel Hilbert Spaces (RKHSs), inspired by ideas from diffeomorphic matching and image registration. A theoretical analysis of the proposed method is presented, giving a priori error bounds in terms of the complexity of the model, the number of samples in the training set, and model misspecification. An extensive suite of numerical experiments further highlights the properties, strengths, and weaknesses of the method and extends its applicability to other tasks, such as conditional <b>simulation</b> and inference.</p></p class="citation"></blockquote><h3 id=47--206252-noise-adaptive-confidence-sets-for-linear-bandits-and-application-to-bayesian-optimization-kwang-sung-jun-et-al-2024>(4/7 | 206/252) Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization (Kwang-Sung Jun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kwang-Sung Jun, Jungtaek Kim. (2024)<br><strong>Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization</strong><br><button class=copy-to-clipboard title="Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07341v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07341v1.pdf filename=2402.07341v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adapting to a priori unknown noise level is a very important but challenging problem in sequential decision-making as efficient exploration typically requires knowledge of the noise level, which is often loosely specified. We report significant progress in addressing this issue in linear <b>bandits</b> <b>in</b> two respects. First, we propose a novel confidence set that is <code>semi-adaptive' to the unknown sub-Gaussian parameter $\sigma_*^2$ in the sense that the (normalized) confidence width scales with $\sqrt{d\sigma_*^2 + \sigma_0^2}$ where $d$ is the dimension and $\sigma_0^2$ is the specified sub-Gaussian parameter (known) that can be much larger than $\sigma_*^2$. This is a significant improvement over $\sqrt{d\sigma_0^2}$ of the standard confidence set of Abbasi-Yadkori et al. (2011), especially when $d$ is large. We show that this leads to an improved regret bound in linear &lt;b>bandits.&lt;/b> &lt;b>Second,&lt;/b> for bounded rewards, we propose a novel variance-adaptive confidence set that has a much improved numerical performance upon prior art. We then apply this confidence set to develop, as we claim, the first practical variance-adaptive linear &lt;b>bandit&lt;/b> &lt;b>algorithm&lt;/b> via an optimistic approach, which is enabled by our novel regret analysis technique. Both of our confidence sets rely critically on </code>regret equality&rsquo; from online learning. Our empirical evaluation in Bayesian optimization tasks shows that our algorithms demonstrate better or comparable performance compared to existing methods.</p></p class="citation"></blockquote><h3 id=57--207252-replicability-is-asymptotically-free-in-multi-armed-bandits-junpei-komiyama-et-al-2024>(5/7 | 207/252) Replicability is Asymptotically Free in Multi-armed Bandits (Junpei Komiyama et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junpei Komiyama, Shinji Ito, Yuichi Yoshida, Souta Koshino. (2024)<br><strong>Replicability is Asymptotically Free in Multi-armed Bandits</strong><br><button class=copy-to-clipboard title="Replicability is Asymptotically Free in Multi-armed Bandits" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07391v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07391v1.pdf filename=2402.07391v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work is motivated by the growing demand for reproducible machine learning. We study the stochastic multi-armed <b>bandit</b> problem. In particular, we consider a replicable algorithm that ensures, with high probability, that the algorithm&rsquo;s sequence of actions is not affected by the randomness inherent in the dataset. We observe that existing algorithms require $O(1/\rho^2)$ times more regret than nonreplicable algorithms, where $\rho$ is the level of nonreplication. However, we demonstrate that this additional cost is unnecessary when the time horizon $T$ is sufficiently large for a given $\rho$, provided that the magnitude of the confidence bounds is chosen carefully. We introduce an explore-then-commit algorithm that draws arms uniformly before committing to a single arm. Additionally, we examine a successive elimination algorithm that eliminates suboptimal arms at the end of each phase. To ensure the replicability of these algorithms, we incorporate randomness into their decision-making processes. We extend the use of successive elimination to the linear <b>bandit</b> problem as well. For the analysis of these algorithms, we propose a principled approach to limiting the probability of nonreplication. This approach elucidates the steps that existing research has implicitly followed. Furthermore, we derive the first lower bound for the two-armed replicable <b>bandit</b> problem, which implies the optimality of the proposed algorithms up to a $\log\log T$ factor for the two-armed case.</p></p class="citation"></blockquote><h3 id=67--208252-convergence-analysis-of-discrete-diffusion-model-exact-implementation-through-uniformization-hongrui-chen-et-al-2024>(6/7 | 208/252) Convergence Analysis of Discrete Diffusion Model: Exact Implementation through Uniformization (Hongrui Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongrui Chen, Lexing Ying. (2024)<br><strong>Convergence Analysis of Discrete Diffusion Model: Exact Implementation through Uniformization</strong><br><button class=copy-to-clipboard title="Convergence Analysis of Discrete Diffusion Model: Exact Implementation through Uniformization" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08095v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08095v2.pdf filename=2402.08095v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffusion models have achieved huge empirical success in data generation tasks. Recently, some efforts have been made to adapt the framework of diffusion models to discrete state space, providing a more natural approach for modeling intrinsically discrete data, such as language and <b>graphs.</b> This is achieved by formulating both the forward noising process and the corresponding reversed process as Continuous Time Markov Chains (CTMCs). In this paper, we investigate the theoretical properties of the discrete diffusion model. Specifically, we introduce an algorithm leveraging the uniformization of continuous Markov chains, implementing transitions on random time points. Under reasonable assumptions on the learning of the discrete score function, we derive Total Variation distance and KL divergence guarantees for sampling from any distribution on a hypercube. Our results align with state-of-the-art achievements for diffusion models in $\mathbb{R}^d$ and further underscore the advantages of discrete diffusion models in comparison to the $\mathbb{R}^d$ setting.</p></p class="citation"></blockquote><h3 id=77--209252-top-k-ranking-with-a-monotone-adversary-yuepeng-yang-et-al-2024>(7/7 | 209/252) Top-$K$ ranking with a monotone adversary (Yuepeng Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuepeng Yang, Antares Chen, Lorenzo Orecchia, Cong Ma. (2024)<br><strong>Top-$K$ ranking with a monotone adversary</strong><br><button class=copy-to-clipboard title="Top-$K$ ranking with a monotone adversary" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-IT, cs-LG, math-IT, math-ST, stat-ML, stat-TH, stat.ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07445v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07445v1.pdf filename=2402.07445v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address the top-$K$ ranking problem with a monotone adversary. We consider the scenario where a comparison <b>graph</b> is randomly generated and the adversary is allowed to add arbitrary edges. The statistician&rsquo;s goal is then to accurately identify the top-$K$ preferred items based on pairwise comparisons derived from this semi-random comparison <b>graph.</b> The main contribution of this paper is to develop a weighted maximum likelihood estimator (MLE) that achieves near-optimal sample complexity, up to a $\log^2(n)$ factor, where n denotes the number of items under comparison. This is made possible through a combination of analytical and algorithmic innovations. On the analytical front, we provide a refined $\ell_\infty$ error analysis of the weighted MLE that is more explicit and tighter than existing analyses. It relates the $\ell_\infty$ error with the spectral properties of the weighted comparison <b>graph.</b> Motivated by this, our algorithmic innovation involves the development of an SDP-based approach to reweight the semi-random <b>graph</b> and meet specified spectral properties. Additionally, we propose a first-order method based on the Matrix Multiplicative Weight Update (MMWU) framework. This method efficiently solves the resulting SDP in nearly-linear time relative to the size of the semi-random comparison <b>graph.</b></p></p class="citation"></blockquote><h2 id=eesssy-5>eess.SY (5)</h2><h3 id=15--210252-distributed-anomaly-detection-in-modern-power-systems-a-penalty-based-mitigation-approach-erfan-mehdipour-abadi-et-al-2024>(1/5 | 210/252) Distributed Anomaly Detection in Modern Power Systems: A Penalty-based Mitigation Approach (Erfan Mehdipour Abadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erfan Mehdipour Abadi, Masoud H. Nazari. (2024)<br><strong>Distributed Anomaly Detection in Modern Power Systems: A Penalty-based Mitigation Approach</strong><br><button class=copy-to-clipboard title="Distributed Anomaly Detection in Modern Power Systems: A Penalty-based Mitigation Approach" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Anomaly Detection, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07884v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07884v1.pdf filename=2402.07884v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evolving landscape of electric power networks, influenced by the integration of distributed energy resources require the development of novel power system monitoring and control architectures. This paper develops algorithm to monitor and detect anomalies of different parts of a power system that cannot be measured directly, by applying neighboring measurements and a dynamic probing technique in a distributed fashion. Additionally, the proposed method accurately assesses the severity of the <b>anomaly.</b> <b>A</b> decision-making algorithm is introduced to effectively penalize anomalous agents, ensuring vigilant oversight of the entire power system&rsquo;s functioning. <b>Simulation</b> results show the efficacy of algorithms in distributed <b>anomaly</b> <b>detection</b> and mitigation.</p></p class="citation"></blockquote><h3 id=25--211252-on-the-stability-of-undesirable-equilibria-in-the-quadratic-program-framework-for-safety-critical-control-matheus-f-reis-et-al-2024>(2/5 | 211/252) On the Stability of Undesirable Equilibria in the Quadratic Program Framework for Safety-Critical Control (Matheus F. Reis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matheus F. Reis, A. Pedro Aguiar. (2024)<br><strong>On the Stability of Undesirable Equilibria in the Quadratic Program Framework for Safety-Critical Control</strong><br><button class=copy-to-clipboard title="On the Stability of Undesirable Equilibria in the Quadratic Program Framework for Safety-Critical Control" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08027v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08027v1.pdf filename=2402.08027v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Control Lyapunov functions (CLFs) and Control Barrier Functions (CBFs) have been used to develop provably safe controllers by means of quadratic programs (QPs). This framework guarantees safety in the form of trajectory invariance with respect to a given set, but it can introduce undesirable equilibrium points to the closed loop system, which can be asymptotically stable. In this work, we present a detailed study of the formation and stability of equilibrium points with the QP framework for a class of nonlinear systems. We introduce the useful concept of compatibility between a CLF and a family of CBFs, regarding the number of stable equilibrium points other than the CLF minimum. Using this concept, we derive a set of compatibility conditions on the parameters of a quadratic CLF and a family of quadratic CBFs that guarantee that all undesirable equilibrium points are not attractive. Furthermore, we propose an extension to the QP-based controller that dynamically modifies the CLF geometry in order to satisfy the compatibility conditions, guaranteeing safety and quasi-global convergence of the system state to the CLF minimum. Numeric <b>simulations</b> illustrate the applicability of the proposed method for safety-critical, deadlock-free robotic navigation tasks.</p></p class="citation"></blockquote><h3 id=35--212252-correctness-verification-of-neural-networks-approximating-differential-equations-petros-ellinas-et-al-2024>(3/5 | 212/252) Correctness Verification of Neural Networks Approximating Differential Equations (Petros Ellinas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Petros Ellinas, Rahul Nellikath, Ignasi Ventura, Jochen Stiasny, Spyros Chatzivasileiadis. (2024)<br><strong>Correctness Verification of Neural Networks Approximating Differential Equations</strong><br><button class=copy-to-clipboard title="Correctness Verification of Neural Networks Approximating Differential Equations" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07621v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07621v1.pdf filename=2402.07621v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Verification of Neural Networks (NNs) that approximate the solution of Partial Differential Equations (PDEs) is a major milestone towards enhancing their trustworthiness and accelerating their deployment, especially for safety-critical systems. If successful, such NNs can become integral parts of <b>simulation</b> software tools which can accelerate the <b>simulation</b> of complex dynamic systems more than 100 times. However, the verification of these functions poses major challenges; it is not straightforward how to efficiently bound them or how to represent the derivative of the NN. This work addresses both these problems. First, we define the NN derivative as a finite difference approximation. Then, we formulate the PDE residual bounding problem alongside the Initial Value Problem&rsquo;s error propagation. Finally, for the first time, we tackle the problem of bounding an NN function without a priori knowledge of the output domain. For this, we build a parallel branching algorithm that combines the incomplete CROWN solver and Gradient Attack for termination and domain rejection conditions. We demonstrate the strengths and weaknesses of the proposed framework, and we suggest further work to enhance its efficiency.</p></p class="citation"></blockquote><h3 id=45--213252-joint-user-and-beam-selection-in-millimeter-wave-networks-santosh-kumar-singh-et-al-2024>(4/5 | 213/252) Joint User and Beam Selection in Millimeter Wave Networks (Santosh Kumar Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Santosh Kumar Singh, Satyabrata Sahu, Ayushi Thawait, Prasanna Chaporkar, Gaurav S. Kasbekar. (2024)<br><strong>Joint User and Beam Selection in Millimeter Wave Networks</strong><br><button class=copy-to-clipboard title="Joint User and Beam Selection in Millimeter Wave Networks" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SP, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07563v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07563v1.pdf filename=2402.07563v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of selecting a user equipment (UE) and a beam for each access point (AP) for concurrent transmissions in a millimeter wave (mmWave) network, such that the sum of weighted rates of UEs is maximized. We prove that this problem is NP-complete. We propose two algorithms &ndash; Markov Chain Monte Carlo (MCMC) based and local interaction game (LIG) based UE and beam selection &ndash; and prove that both of them asymptotically achieve the optimal solution. Also, we propose two fast greedy algorithms &ndash; NGUB1 and NGUB2 &ndash; for UE and beam selection. Through extensive <b>simulations,</b> we show that our proposed greedy algorithms outperform the most relevant algorithms proposed in prior work and perform close to the asymptotically optimal algorithms.</p></p class="citation"></blockquote><h3 id=55--214252-conformal-predictive-programming-for-chance-constrained-optimization-yiqi-zhao-et-al-2024>(5/5 | 214/252) Conformal Predictive Programming for Chance Constrained Optimization (Yiqi Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiqi Zhao, Xinyi Yu, Jyotirmoy V. Deshmukh, Lars Lindemann. (2024)<br><strong>Conformal Predictive Programming for Chance Constrained Optimization</strong><br><button class=copy-to-clipboard title="Conformal Predictive Programming for Chance Constrained Optimization" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY, math-OC, stat-ML<br>Keyword Score: 10<br>Keywords: Karush-Kuhn-Tucker<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07407v1.pdf filename=2402.07407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by the advances in conformal prediction (CP), we propose conformal predictive programming (CPP), an approach to solve chance constrained optimization (CCO) problems, i.e., optimization problems with nonlinear constraint functions affected by arbitrary random parameters. CPP utilizes samples from these random parameters along with the quantile lemma &ndash; which is central to CP &ndash; to transform the CCO problem into a deterministic optimization problem. We then present two tractable reformulations of CPP by: (1) writing the quantile as a linear program along with its <b>KKT</b> conditions (CPP-KKT), and (2) using mixed integer programming (CPP-MIP). CPP comes with marginal probabilistic feasibility guarantees for the CCO problem that are conceptually different from existing approaches, e.g., the sample approximation and the scenario approach. While we explore algorithmic similarities with the sample approximation approach, we emphasize that the strength of CPP is that it can easily be extended to incorporate different variants of CP. To illustrate this, we present robust conformal predictive programming to deal with distribution shifts in the uncertain parameters of the CCO problem.</p></p class="citation"></blockquote><h2 id=eesssp-2>eess.SP (2)</h2><h3 id=12--215252-deciphering-heartbeat-signatures-a-vision-transformer-approach-to-explainable-atrial-fibrillation-detection-from-ecg-signals-aruna-mohan-et-al-2024>(1/2 | 215/252) Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals (Aruna Mohan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aruna Mohan, Danne Elbers, Or Zilbershot, Fatemeh Afghah, David Vorchheimer. (2024)<br><strong>Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals</strong><br><button class=copy-to-clipboard title="Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-AI, cs-CV, cs-LG, eess-SP, eess.SP<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09474v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09474v1.pdf filename=2402.09474v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remote patient monitoring based on wearable single-lead electrocardiogram (ECG) devices has significant potential for enabling the early detection of heart disease, especially in combination with artificial intelligence (AI) approaches for automated heart disease detection. There have been prior studies applying AI approaches based on deep learning for heart disease detection. However, these models are yet to be widely accepted as a reliable aid for clinical diagnostics, in part due to the current black-box perception surrounding many AI algorithms. In particular, there is a need to identify the key features of the ECG signal that contribute toward making an accurate diagnosis, thereby enhancing the interpretability of the model. In the present study, we develop a <b>vision</b> <b>transformer</b> approach to identify atrial fibrillation based on single-lead ECG data. A residual network (ResNet) approach is also developed for comparison with the <b>vision</b> <b>transformer</b> approach. These models are applied to the Chapman-Shaoxing dataset to classify atrial fibrillation, as well as another common arrhythmia, sinus bradycardia, and normal sinus rhythm heartbeats. The models enable the identification of the key regions of the heartbeat that determine the resulting classification, and highlight the importance of P-waves and T-waves, as well as heartbeat duration and signal amplitude, in distinguishing normal sinus rhythm from atrial fibrillation and sinus bradycardia.</p></p class="citation"></blockquote><h3 id=22--216252-compressive-recovery-of-signals-defined-on-perturbed-graphs-sabyasachi-ghosh-et-al-2024>(2/2 | 216/252) Compressive Recovery of Signals Defined on Perturbed Graphs (Sabyasachi Ghosh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sabyasachi Ghosh, Ajit Rajwade. (2024)<br><strong>Compressive Recovery of Signals Defined on Perturbed Graphs</strong><br><button class=copy-to-clipboard title="Compressive Recovery of Signals Defined on Perturbed Graphs" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-CV, eess-IV, eess-SP, eess.SP<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07637v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07637v2.pdf filename=2402.07637v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recovery of signals with elements defined on the nodes of a <b>graph,</b> from compressive measurements is an important problem, which can arise in various domains such as sensor networks, image reconstruction and group testing. In some scenarios, the <b>graph</b> may not be accurately known, and there may exist a few edge additions or deletions relative to a ground truth <b>graph.</b> Such perturbations, even if small in number, significantly affect the <b>Graph</b> Fourier Transform (GFT). This impedes recovery of signals which may have sparse representations in the GFT bases of the ground truth <b>graph.</b> We present an algorithm which simultaneously recovers the signal from the compressive measurements and also corrects the <b>graph</b> perturbations. We analyze some important theoretical properties of the algorithm. Our approach to correction for <b>graph</b> perturbations is based on model selection techniques such as cross-validation in compressed sensing. We validate our algorithm on signals which have a sparse representation in the GFT bases of many commonly used <b>graphs</b> in the network science literature. An application to compressive image reconstruction is also presented, where <b>graph</b> perturbations are modeled as undesirable <b>graph</b> edges linking pixels with significant intensity difference. In all experiments, our algorithm clearly outperforms baseline techniques which either ignore the perturbations or use first order approximations to the perturbations in the GFT bases.</p></p class="citation"></blockquote><h2 id=cssi-4>cs.SI (4)</h2><h3 id=14--217252-comparing-the-willingness-to-share-for-human-generated-vs-ai-generated-fake-news-amirsiavosh-bashardoust-et-al-2024>(1/4 | 217/252) Comparing the willingness to share for human-generated vs. AI-generated fake news (Amirsiavosh Bashardoust et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amirsiavosh Bashardoust, Stefan Feuerriegel, Yash Raj Shrestha. (2024)<br><strong>Comparing the willingness to share for human-generated vs. AI-generated fake news</strong><br><button class=copy-to-clipboard title="Comparing the willingness to share for human-generated vs. AI-generated fake news" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI, physics-soc-ph<br>Keyword Score: 30<br>Keywords: GPT, GPT-4, Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07395v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07395v1.pdf filename=2402.07395v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative artificial intelligence (AI) presents large risks for society when it is used to create <b>fake</b> <b>news.</b> A crucial factor for <b>fake</b> <b>news</b> to go viral on social media is that users share such content. Here, we aim to shed light on the sharing behavior of users across human-generated vs. AI-generated <b>fake</b> <b>news.</b> Specifically, we study: (1) What is the perceived veracity of human-generated <b>fake</b> <b>news</b> vs. AI-generated <b>fake</b> <b>news?</b> (2) What is the user&rsquo;s willingness to share human-generated <b>fake</b> <b>news</b> vs. AI-generated <b>fake</b> <b>news</b> on social media? (3) What socio-economic characteristics let users fall for AI-generated <b>fake</b> <b>news?</b> To this end, we conducted a pre-registered, online experiment with $N=$ 988 subjects and 20 <b>fake</b> <b>news</b> from the COVID-19 pandemic generated by <b>GPT-4</b> vs. humans. Our findings show that AI-generated <b>fake</b> <b>news</b> is perceived as less accurate than human-generated <b>fake</b> <b>news,</b> but both tend to be shared equally. Further, several socio-economic factors explain who falls for AI-generated <b>fake</b> <b>news.</b></p></p class="citation"></blockquote><h3 id=24--218252-local-centrality-minimization-with-quality-guarantees-atsushi-miyauchi-et-al-2024>(2/4 | 218/252) Local Centrality Minimization with Quality Guarantees (Atsushi Miyauchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atsushi Miyauchi, Lorenzo Severini, Francesco Bonchi. (2024)<br><strong>Local Centrality Minimization with Quality Guarantees</strong><br><button class=copy-to-clipboard title="Local Centrality Minimization with Quality Guarantees" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-DS, cs-SI, cs.SI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07718v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07718v1.pdf filename=2402.07718v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Centrality measures, quantifying the importance of vertices or edges, play a fundamental role in network analysis. To date, triggered by some positive approximability results, a large body of work has been devoted to studying centrality maximization, where the goal is to maximize the centrality score of a target vertex by manipulating the structure of a given network. On the other hand, due to the lack of such results, only very little attention has been paid to centrality minimization, despite its practical usefulness. In this study, we introduce a novel optimization model for local centrality minimization, where the manipulation is allowed only around the target vertex. We prove the NP-hardness of our model and that the most intuitive greedy algorithm has a quite limited performance in terms of approximation ratio. Then we design two effective approximation algorithms: The first algorithm is a highly-scalable algorithm that has an approximation ratio unachievable by the greedy algorithm, while the second algorithm is a bicriteria approximation algorithm that solves a continuous relaxation based on the Lov'asz extension, using a projected subgradient method. To the best of our knowledge, ours are the first polynomial-time algorithms with provable approximation guarantees for centrality minimization. Experiments using a variety of real-world networks demonstrate the effectiveness of our proposed algorithms: Our first algorithm is applicable to million-scale <b>graphs</b> and obtains much better solutions than those of scalable baselines, while our second algorithm is rather strong against adversarial instances.</p></p class="citation"></blockquote><h3 id=34--219252-higher-order-connection-laplacians-for-directed-simplicial-complexes-xue-gong-et-al-2024>(3/4 | 219/252) Higher-order Connection Laplacians for Directed Simplicial Complexes (Xue Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xue Gong, Desmond J. Higham, Konstantinos Zygalakis, Ginestra Bianconi. (2024)<br><strong>Higher-order Connection Laplacians for Directed Simplicial Complexes</strong><br><button class=copy-to-clipboard title="Higher-order Connection Laplacians for Directed Simplicial Complexes" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cond-mat-dis-nn, cond-mat-stat-mech, cs-SI, cs.SI, nlin-AO, physics-data-an<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07631v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07631v1.pdf filename=2402.07631v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Higher-order networks encode the many-body interactions existing in complex systems, such as the brain, protein complexes, and social interactions. Simplicial complexes are higher-order networks that allow a comprehensive investigation of the interplay between topology and dynamics. However, simplicial complexes have the limitation that they only capture undirected higher-order interactions while in real-world scenarios, often there is a need to introduce the direction of simplices, extending the popular notion of direction of edges. On <b>graphs</b> and networks the Magnetic Laplacian, a special case of Connection Laplacian, is becoming a popular operator to treat edge directionality. Here we tackle the challenge of treating directional simplicial complexes by formulating Higher-order Connection Laplacians taking into account the configurations induced by the simplices&rsquo; directions. Specifically, we define all the Connection Laplacians of directed simplicial complexes of dimension two and we discuss the induced higher-order diffusion dynamics by considering instructive synthetic examples of simplicial complexes. The proposed higher-order diffusion processes can be adopted in real scenarios when we want to consider higher-order diffusion displaying non-trivial frustration effects due to conflicting directionalities of the incident simplices.</p></p class="citation"></blockquote><h3 id=44--220252-topic-aware-most-influential-community-search-in-social-networks-long-teng-et-al-2024>(4/4 | 220/252) Topic-aware Most Influential Community Search in Social Networks (Long Teng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Long Teng, Yanhao Wang. (2024)<br><strong>Topic-aware Most Influential Community Search in Social Networks</strong><br><button class=copy-to-clipboard title="Topic-aware Most Influential Community Search in Social Networks" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07601v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07601v1.pdf filename=2402.07601v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Community search is a problem aimed at searching for densely connected subgraphs within a network based on query conditions, which has recently attracted significant attention. However, most previous community search studies have overlooked the coexistence relationship among attributes. They typically assign a single attribute to each node or edge (e.g.,only considering influence scores or keywords), which is difficult for users to obtain a comprehensive and beneficial information. Additionally, most of them also ignored the uncertainty in the attribute <b>graph.</b> Therefore, in this paper, we introduce two novel community models, namely topic-based interaction <b>graph</b> and $(k,l,\eta)$-influential community. The former is a directed ucertain <b>graph</b> generated by the query topic distribution provided by users, while the latter is used for solving the topic-aware most influential community search problem in social networks. Furthermore, we propose an online search algorithm which computes the influence value of each vertex by considering the topic-aware information diffusion process on interaction <b>graphs.</b> And then, we use a peeling-pruning strategy to iteratively find the topic-aware most $(k,l,\eta)$-influential community. To further speed up the search performance, we devise two lightweight index structures which efficiently support the search for the topic-aware most influential community within an optimal time. We also propose three optimization methods to improve the space and time costs of the index-based approach.</p></p class="citation"></blockquote><h2 id=csse-5>cs.SE (5)</h2><h3 id=15--221252-mercury-an-efficiency-benchmark-for-llm-code-synthesis-mingzhe-du-et-al-2024>(1/5 | 221/252) Mercury: An Efficiency Benchmark for LLM Code Synthesis (Mingzhe Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingzhe Du, Anh Tuan Luu, Bin Ji, See-Kiong Ng. (2024)<br><strong>Mercury: An Efficiency Benchmark for LLM Code Synthesis</strong><br><button class=copy-to-clipboard title="Mercury: An Efficiency Benchmark for LLM Code Synthesis" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-SE, cs.SE<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07844v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07844v1.pdf filename=2402.07844v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite advancements in evaluating <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for code synthesis, <b>benchmarks</b> have predominantly focused on functional correctness, overlooking the importance of code efficiency. We present Mercury, the first <b>benchmark</b> designated for assessing the code efficiency of <b>LLM</b> code synthesis tasks. Mercury consists of 1,889 programming tasks covering diverse difficulty levels alongside test case generators generating unlimited cases for comprehensive evaluation. Unlike existing <b>benchmarks,</b> Mercury integrates a novel metric Beyond@K to measure normalized code efficiency based on historical submissions, leading to a new evaluation indicator for code synthesis, which encourages generating functionally correct and computationally efficient code, mirroring the real-world software development standard. Our findings reveal that while <b>LLMs</b> demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, underscoring a new frontier for <b>LLM</b> research and development.</p></p class="citation"></blockquote><h3 id=25--222252-continuous-assurance-of-autonomous-vehicle-behavior-through-machine-learned-correctness-properties-matthew-litton-et-al-2024>(2/5 | 222/252) Continuous Assurance of Autonomous Vehicle Behavior Through Machine Learned Correctness Properties (Matthew Litton et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthew Litton, Doron Drusinsky, James Bret Michael. (2024)<br><strong>Continuous Assurance of Autonomous Vehicle Behavior Through Machine Learned Correctness Properties</strong><br><button class=copy-to-clipboard title="Continuous Assurance of Autonomous Vehicle Behavior Through Machine Learned Correctness Properties" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07791v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07791v1.pdf filename=2402.07791v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Correctness properties are critical to conducting verification and validation on software systems, especially those cyberphysical systems whose functionality changes frequently due to software updates, changes in the operating environment, or newly learned behaviors. We detail a novel method to automatically construct expressive, executable correctness properties in the form of machine-learned correctness properties which can be used to ensure that a system&rsquo;s behavior is correct with respect to its design and operating requirements. We propose a method to bootstrap the creation of these correctness properties using a novel <b>simulation-based</b> generation of training and testing data using multiple extensions to the Cross Entropy algorithm for search-based optimization. Then, we apply this method to a software-in-the-loop evaluation of an autonomous vehicle to demonstrate that such models can assert about important properties of multi-agent cyberphysical systems. We demonstrate that this process brings the task of developing robust correctness properties from the realm of formal methods experts into the domain of system developers and engineers, and that machine-learned correctness properties are expressive enough to capture the correct behavior of cyberphysical systems in their complex environments. This advancement can provide evidence of dependability to system designers and users, enhancing trust in the deployment of autonomous vehicles and other intelligent transportation systems.</p></p class="citation"></blockquote><h3 id=35--223252-interaction-based-driving-scenario-classification-and-labeling-cheng-chang-et-al-2024>(3/5 | 223/252) Interaction-Based Driving Scenario Classification and Labeling (Cheng Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Chang, Jiawei Zhang, Jingwei Ge, Zuo Zhang, Junqing Wei, Li Li. (2024)<br><strong>Interaction-Based Driving Scenario Classification and Labeling</strong><br><button class=copy-to-clipboard title="Interaction-Based Driving Scenario Classification and Labeling" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 13<br>Keywords: Graph, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07720v1.pdf filename=2402.07720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scenario data play a vital role in autonomous driving related researches, and it is essential to obtain refined descriptions and labels to extract and index scenarios with different types of interactions. However, existing methods cannot cope well with the problem of scenario classification and comparison with vehicle interactions as the core. In this paper, we propose a framework for interaction-based refined scenario classification and labeling. Based on the <b>summarized</b> basic types of vehicle interactions, we slice scenario data stream into a series of scenario segments via spatiotemporal scenario evolution tree. The scenario segment statistics of many published scenario datasets are further analyzed. We also propose the scenario metric <b>Graph-DTW</b> based on <b>Graph</b> Computation Tree and Dynamic Time Warping to conduct refined scenario comparison and labeling. The extreme interactive scenarios and corner cases can be efficiently filtered and extracted. Moreover, testing examples on trajectory prediction model demonstrate the effectiveness and advantages of scenario labeling and the proposed metric. The overall framework can provide solid support for the usage and indexing of scenario data.</p></p class="citation"></blockquote><h3 id=45--224252-using-ensemble-inference-to-improve-recall-of-clone-detection-gul-aftab-ahmed-et-al-2024>(4/5 | 224/252) Using Ensemble Inference to Improve Recall of Clone Detection (Gul Aftab Ahmed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gul Aftab Ahmed, James Vincent Patten, Yuanhua Han, Guoxian Lu, David Gregg, Jim Buckley, Muslim Chochlov. (2024)<br><strong>Using Ensemble Inference to Improve Recall of Clone Detection</strong><br><button class=copy-to-clipboard title="Using Ensemble Inference to Improve Recall of Clone Detection" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07523v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07523v1.pdf filename=2402.07523v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale source-code clone detection is a challenging task. In our previous work, we proposed an approach (SSCD) that leverages artificial neural networks and approximates nearest neighbour search to effectively and efficiently locate clones in large-scale bodies of code, in a time-efficient manner. However, our literature review suggests that the relative efficacy of differing neural network models has not been assessed in the context of large-scale clone detection approaches. In this work, we aim to assess several such models individually, in terms of their potential to maximize recall, while preserving a high level of precision during clone detection. We investigate if ensemble inference (in this case, using the results of more than one of these neural network models in combination) can further assist in this task. To assess this, we employed four state-of-the-art neural network models and evaluated them individually/in combination. The results, on an illustrative dataset of approximately 500K lines of C/C++ code, suggest that ensemble inference outperforms individual models in all trialled cases, when recall is concerned. Of individual models, the ADA model (belonging to the <b>ChatGPT</b> family of models) has the best performance. However commercial companies may not be prepared to hand their proprietary source code over to the cloud, as required by that approach. Consequently, they may be more interested in an ensemble-combination of CodeBERT-based and CodeT5 models, resulting in similar (if slightly lesser) recall and precision results.</p></p class="citation"></blockquote><h3 id=55--225252-asap-repair-api-specific-automated-program-repair-based-on-api-usage-graphs-sebastian-nielebock-et-al-2024>(5/5 | 225/252) ASAP-Repair: API-Specific Automated Program Repair Based on API Usage Graphs (Sebastian Nielebock et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sebastian Nielebock, Paul Blockhaus, Jacob Krüger, Frank Ortmeier. (2024)<br><strong>ASAP-Repair: API-Specific Automated Program Repair Based on API Usage Graphs</strong><br><button class=copy-to-clipboard title="ASAP-Repair: API-Specific Automated Program Repair Based on API Usage Graphs" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07542v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07542v1.pdf filename=2402.07542v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern software development relies on the reuse of code via Application Programming Interfaces (APIs). Such reuse relieves developers from learning and developing established algorithms and data structures anew, enabling them to focus on their problem at hand. However, there is also the risk of misusing an API due to a lack of understanding or proper documentation. While many techniques target API misuse detection, only limited efforts have been put into automatically repairing API misuses. In this paper, we present our advances on our technique API-Specific Automated Program Repair (ASAP-Repair). ASAP-Repair is intended to fix API misuses based on API Usage <b>Graphs</b> (AUGs) by leveraging API usage templates of state-of-the-art API misuse detectors. We demonstrate that ASAP-Repair is in principle applicable on an established API misuse dataset. Moreover, we discuss next steps and challenges to evolve ASAP-Repair towards a full-fledged Automatic Program Repair (APR) technique.</p></p class="citation"></blockquote><h2 id=physicsao-ph-1>physics.ao-ph (1)</h2><h3 id=11--226252-robust-and-accurate-simulations-of-flows-over-orography-using-non-conforming-meshes-giuseppe-orlando-et-al-2024>(1/1 | 226/252) Robust and accurate simulations of flows over orography using non-conforming meshes (Giuseppe Orlando et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giuseppe Orlando, Tommaso Benacchio, Luca Bonaventura. (2024)<br><strong>Robust and accurate simulations of flows over orography using non-conforming meshes</strong><br><button class=copy-to-clipboard title="Robust and accurate simulations of flows over orography using non-conforming meshes" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.ao-ph<br>Categories: cs-NA, math-NA, physics-ao-ph, physics-flu-dyn, physics.ao-ph<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07759v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07759v1.pdf filename=2402.07759v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We systematically validate the static local mesh refinement capabilities of a recently proposed IMEX-DG scheme implemented in the framework of the deal.II library. Non-conforming meshes are employed in atmospheric flow <b>simulations</b> to increase the resolution around complex orography. A number of numerical experiments based on classical <b>benchmarks</b> with idealized as well as real orography profiles demonstrate that <b>simulations</b> with the refined mesh are stable for long lead times and no spurious effects arise at the interfaces of mesh regions with different resolutions. Moreover, correct values of the momentum flux are retrieved and the correct large-scale orographic response is established. Hence, large-scale orography-driven flow features can be simulated without loss of accuracy using a much lower total amount of degrees of freedom. In a context of spatial resolutions approaching the hectometric scale in numerical weather prediction models, these results support the use of locally refined, non-conforming meshes as a reliable and effective tool to greatly reduce the dependence of atmospheric models on orographic wave drag parametrizations.</p></p class="citation"></blockquote><h2 id=mathoc-2>math.OC (2)</h2><h3 id=12--227252-tuning-free-stochastic-optimization-ahmed-khaled-et-al-2024>(1/2 | 227/252) Tuning-Free Stochastic Optimization (Ahmed Khaled et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmed Khaled, Chi Jin. (2024)<br><strong>Tuning-Free Stochastic Optimization</strong><br><button class=copy-to-clipboard title="Tuning-Free Stochastic Optimization" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC, stat-ML<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07793v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07793v1.pdf filename=2402.07793v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive. This creates a need for algorithms that can tune themselves on-the-fly. We formalize the notion of &ldquo;tuning-free&rdquo; algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters. We consider in particular algorithms that can match optimally-tuned <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> <b>(SGD).</b> When the domain of optimization is bounded, we show tuning-free matching of <b>SGD</b> is possible and achieved by several existing algorithms. We prove that for the task of minimizing a convex and smooth or Lipschitz function over an unbounded domain, tuning-free optimization is impossible. We discuss conditions under which tuning-free optimization is possible even over unbounded domains. In particular, we show that the recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is sufficiently well-behaved. For the task of finding a stationary point of a smooth and potentially nonconvex function, we give a variant of <b>SGD</b> that matches the best-known high-probability convergence rate for tuned <b>SGD</b> at only an additional polylogarithmic cost. However, we also give an impossibility result that shows no algorithm can hope to match the optimal expected convergence rate for tuned <b>SGD</b> with high probability.</p></p class="citation"></blockquote><h3 id=22--228252-a-deep-learning-method-for-optimal-investment-under-relative-performance-criteria-among-heterogeneous-agents-mathieu-laurière-et-al-2024>(2/2 | 228/252) A Deep Learning Method for Optimal Investment Under Relative Performance Criteria Among Heterogeneous Agents (Mathieu Laurière et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mathieu Laurière, Ludovic Tangpi, Xuchen Zhou. (2024)<br><strong>A Deep Learning Method for Optimal Investment Under Relative Performance Criteria Among Heterogeneous Agents</strong><br><button class=copy-to-clipboard title="A Deep Learning Method for Optimal Investment Under Relative Performance Criteria Among Heterogeneous Agents" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-GT, cs-LG, math-OC, math.OC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07365v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07365v1.pdf filename=2402.07365v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Graphon games have been introduced to study games with many players who interact through a weighted <b>graph</b> of interaction. By passing to the limit, a game with a continuum of players is obtained, in which the interactions are through a graphon. In this paper, we focus on a graphon game for optimal investment under relative performance criteria, and we propose a deep learning method. The method builds upon two key ingredients: first, a characterization of Nash equilibria by forward-backward stochastic differential equations and, second, recent advances of machine learning algorithms for stochastic differential games. We provide numerical experiments on two different financial models. In each model, we compare the effect of several graphons, which correspond to different structures of interactions.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--229252-towards-a-foundation-model-for-brain-age-prediction-using-covariance-neural-networks-saurabh-sihag-et-al-2024>(1/1 | 229/252) Towards a Foundation Model for Brain Age Prediction using coVariance Neural Networks (Saurabh Sihag et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saurabh Sihag, Gonzalo Mateos, Alejandro Ribeiro. (2024)<br><strong>Towards a Foundation Model for Brain Age Prediction using coVariance Neural Networks</strong><br><button class=copy-to-clipboard title="Towards a Foundation Model for Brain Age Prediction using coVariance Neural Networks" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-LG, q-bio-QM, q-bio.QM, stat-AP<br>Keyword Score: 20<br>Keywords: Fine-tuning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07684v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07684v1.pdf filename=2402.07684v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Brain age is the estimate of biological age derived from neuroimaging datasets using machine learning algorithms. Increasing brain age with respect to chronological age can reflect increased vulnerability to neurodegeneration and cognitive decline. In this paper, we study NeuroVNN, based on coVariance neural networks, as a paradigm for <b>foundation</b> <b>model</b> for the brain age prediction application. NeuroVNN is pre-trained as a regression model on healthy population to predict chronological age using cortical thickness features and <b>fine-tuned</b> to estimate brain age in different neurological contexts. Importantly, NeuroVNN adds anatomical interpretability to brain age and has a `scale-free&rsquo; characteristic that allows its transference to datasets curated according to any arbitrary brain atlas. Our results demonstrate that NeuroVNN can extract biologically plausible brain age estimates in different populations, as well as transfer successfully to datasets of dimensionalities distinct from that for the dataset used to train NeuroVNN.</p></p class="citation"></blockquote><h2 id=csgt-2>cs.GT (2)</h2><h3 id=12--230252-rethinking-scaling-laws-for-learning-in-strategic-environments-tinashe-handina-et-al-2024>(1/2 | 230/252) Rethinking Scaling Laws for Learning in Strategic Environments (Tinashe Handina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tinashe Handina, Eric Mazumdar. (2024)<br><strong>Rethinking Scaling Laws for Learning in Strategic Environments</strong><br><button class=copy-to-clipboard title="Rethinking Scaling Laws for Learning in Strategic Environments" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-LG, cs.GT, stat-ML<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07588v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07588v1.pdf filename=2402.07588v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The deployment of ever-larger machine learning models reflects a growing consensus that the more expressive the model$\unicode{x2013}$and the more data one has access to$\unicode{x2013}$the more one can improve performance. As models get deployed in a variety of real world scenarios, they inevitably face strategic environments. In this work, we consider the natural question of how the interplay of models and strategic interactions affects <b>scaling</b> <b>laws.</b> We find that strategic interactions can break the conventional view of <b>scaling</b> <b>laws$\unicode{x2013}$meaning</b> that performance does not necessarily monotonically improve as models get larger and/ or more expressive (even with infinite data). We show the implications of this phenomenon in several contexts including strategic regression, strategic classification, and multi-agent <b>reinforcement</b> <b>learning</b> through examples of strategic environments in which$\unicode{x2013}$by simply restricting the expressivity of one&rsquo;s model or policy class$\unicode{x2013}$one can achieve strictly better equilibrium outcomes. Motivated by these examples, we then propose a new paradigm for model-selection in games wherein an agent seeks to choose amongst different model classes to use as their action set in a game.</p></p class="citation"></blockquote><h3 id=22--231252-automated-design-of-affine-maximizer-mechanisms-in-dynamic-settings-michael-curry-et-al-2024>(2/2 | 231/252) Automated Design of Affine Maximizer Mechanisms in Dynamic Settings (Michael Curry et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Curry, Vinzenz Thoma, Darshan Chakrabarti, Stephen McAleer, Christian Kroer, Tuomas Sandholm, Niao He, Sven Seuken. (2024)<br><strong>Automated Design of Affine Maximizer Mechanisms in Dynamic Settings</strong><br><button class=copy-to-clipboard title="Automated Design of Affine Maximizer Mechanisms in Dynamic Settings" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08129v1.pdf filename=2402.08129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic mechanism design is a challenging extension to ordinary mechanism design in which the mechanism designer must make a sequence of decisions over time in the face of possibly untruthful reports of participating agents. Optimizing dynamic mechanisms for welfare is relatively well understood. However, there has been less work on optimizing for other goals (e.g. revenue), and without restrictive assumptions on valuations, it is remarkably challenging to characterize good mechanisms. Instead, we turn to automated mechanism design to find mechanisms with good performance in specific problem instances. In fact, the situation is similar even in static mechanism design. However, in the static case, optimization/machine learning-based automated mechanism design techniques have been successful in finding high-revenue mechanisms in cases beyond the reach of analytical results. We extend the class of affine maximizer mechanisms to <b>MDPs</b> where agents may untruthfully report their rewards. This extension results in a challenging bilevel optimization problem in which the upper problem involves choosing optimal mechanism parameters, and the lower problem involves solving the resulting MDP. Our approach can find truthful dynamic mechanisms that achieve strong performance on goals other than welfare, and can be applied to essentially any problem setting-without restrictions on valuations-for which RL can learn optimal policies.</p></p class="citation"></blockquote><h2 id=csdc-5>cs.DC (5)</h2><h3 id=15--232252-accelerating-distributed-deep-learning-using-lossless-homomorphic-compression-haoyu-li-et-al-2024>(1/5 | 232/252) Accelerating Distributed Deep Learning using Lossless Homomorphic Compression (Haoyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Li, Yuchen Xu, Jiayi Chen, Rohit Dwivedula, Wenfei Wu, Keqiang He, Aditya Akella, Daehyeok Kim. (2024)<br><strong>Accelerating Distributed Deep Learning using Lossless Homomorphic Compression</strong><br><button class=copy-to-clipboard title="Accelerating Distributed Deep Learning using Lossless Homomorphic Compression" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-DS, cs-LG, cs-NI, cs.DC<br>Keyword Score: 20<br>Keywords: BERT, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07529v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07529v1.pdf filename=2402.07529v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As deep neural networks (DNNs) grow in complexity and size, the resultant increase in communication overhead during distributed training has become a significant bottleneck, challenging the scalability of distributed training systems. Existing solutions, while aiming to mitigate this bottleneck through worker-level compression and in-network aggregation, fall short due to their inability to efficiently reconcile the trade-offs between compression effectiveness and computational overhead, hindering overall performance and scalability. In this paper, we introduce a novel compression algorithm that effectively merges worker-level compression with in-network aggregation. Our solution is both homomorphic, allowing for efficient in-network aggregation without CPU/GPU processing, and lossless, ensuring no compromise on training accuracy. Theoretically optimal in compression and computational efficiency, our approach is empirically validated across diverse DNN models such as NCF, <b>LSTM,</b> VGG19, and <b>BERT-base,</b> showing up to a 6.33$\times$ improvement in aggregation throughput and a 3.74$\times$ increase in per-iteration training speed.</p></p class="citation"></blockquote><h3 id=25--233252-lfoc-a-lightweight-fairness-oriented-cache-clustering-policy-for-commodity-multicores-adrián-garcía-garcía-et-al-2024>(2/5 | 233/252) LFOC: A Lightweight Fairness-Oriented Cache Clustering Policy for Commodity Multicores (Adrián García-García et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adrián García-García, Juan Carlos Sáez, Fernando Castro, Manuel Prieto-Matías. (2024)<br><strong>LFOC: A Lightweight Fairness-Oriented Cache Clustering Policy for Commodity Multicores</strong><br><button class=copy-to-clipboard title="LFOC: A Lightweight Fairness-Oriented Cache Clustering Policy for Commodity Multicores" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-AR, cs-DC, cs.DC<br>Keyword Score: 13<br>Keywords: Clustering, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07578v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07578v1.pdf filename=2402.07578v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multicore processors constitute the main architecture choice for modern computing systems in different market segments. Despite their benefits, the contention that naturally appears when multiple applications compete for the use of shared resources among cores, such as the last-level cache (LLC), may lead to substantial performance degradation. This may have a negative impact on key system aspects such as throughput and <b>fairness.</b> Assigning the various applications in the workload to separate LLC partitions with possibly different sizes, has been proven effective to mitigate shared-resource contention effects. In this article we propose LFOC, a <b>clustering-based</b> cache partitioning scheme that strives to deliver <b>fairness</b> while providing acceptable system throughput. LFOC leverages the Intel Cache Allocation Technology (CAT), which enables the system software to divide the LLC into different partitions. To accomplish its goals, LFOC tries to mimic the behavior of the optimal cache-clustering solution, which we could approximate by means of a simulator in different scenarios. To this end, LFOC effectively identifies streaming aggressor programs and cache sensitive applications, which are then assigned to separate cache partitions. We implemented LFOC in the Linux kernel and evaluated it on a real system featuring an Intel Skylake processor, where we compare its effectiveness to that of two state-of-the-art policies that optimize <b>fairness</b> and throughput, respectively. Our experimental analysis reveals that LFOC is able to bring a higher reduction in unfairness by leveraging a lightweight algorithm suitable for adoption in a real OS.</p></p class="citation"></blockquote><h3 id=35--234252-from-data-to-decisions-the-transformational-power-of-machine-learning-in-business-recommendations-kapilya-gangadharan-et-al-2024>(3/5 | 234/252) From Data to Decisions: The Transformational Power of Machine Learning in Business Recommendations (Kapilya Gangadharan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kapilya Gangadharan, K. Malathi, Anoop Purandaran, Barathi Subramanian, Rathinaraja Jeyaraj. (2024)<br><strong>From Data to Decisions: The Transformational Power of Machine Learning in Business Recommendations</strong><br><button class=copy-to-clipboard title="From Data to Decisions: The Transformational Power of Machine Learning in Business Recommendations" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-IR, cs-LG, cs.DC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08109v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08109v1.pdf filename=2402.08109v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research aims to explore the impact of Machine Learning (ML) on the evolution and efficacy of <b>Recommendation</b> Systems (RS), particularly in the context of their growing significance in commercial business environments. Methodologically, the study delves into the role of ML in crafting and refining these systems, focusing on aspects such as data sourcing, feature engineering, and the importance of evaluation metrics, thereby highlighting the iterative nature of enhancing <b>recommendation</b> algorithms. The deployment of <b>Recommendation</b> Engines (RE), driven by advanced algorithms and data analytics, is explored across various domains, showcasing their significant impact on user experience and decision-making processes. These engines not only streamline information discovery and enhance collaboration but also accelerate knowledge acquisition, proving vital in navigating the digital landscape for businesses. They contribute significantly to sales, revenue, and the competitive edge of enterprises by offering improved <b>recommendations</b> that align with individual customer needs. The research identifies the increasing expectation of users for a seamless, intuitive online experience, where content is personalized and dynamically adapted to changing preferences. Future research directions include exploring advancements in deep learning models, ethical considerations in the deployment of RS, and addressing scalability challenges. This study emphasizes the indispensability of comprehending and leveraging ML in RS for researchers and practitioners, to tap into the full potential of personalized <b>recommendation</b> in commercial business prospects.</p></p class="citation"></blockquote><h3 id=45--235252-enabling-performance-portability-of-data-parallel-openmp-applications-on-asymmetric-multicore-processors-juan-carlos-saez-et-al-2024>(4/5 | 235/252) Enabling performance portability of data-parallel OpenMP applications on asymmetric multicore processors (Juan Carlos Saez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juan Carlos Saez, Fernando Castro, Manuel Prieto-Matias. (2024)<br><strong>Enabling performance portability of data-parallel OpenMP applications on asymmetric multicore processors</strong><br><button class=copy-to-clipboard title="Enabling performance portability of data-parallel OpenMP applications on asymmetric multicore processors" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-OS, cs.DC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07664v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07664v1.pdf filename=2402.07664v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Asymmetric multicore processors (AMPs) couple high-performance big cores and low-power small cores with the same instruction-set architecture but different features, such as clock frequency or microarchitecture. Previous work has shown that asymmetric designs may deliver higher energy efficiency than symmetric multicores for diverse workloads. Despite their benefits, AMPs pose significant challenges to runtime systems of parallel programming models. While previous work has mainly explored how to efficiently execute task-based parallel applications on AMPs, via enhancements in the runtime system, improving the performance of unmodified data-parallel applications on these architectures is still a big challenge. In this work we analyze the particular case of loop-based OpenMP applications, which are widely used today in scientific and engineering domains, and constitute the dominant application type in many parallel <b>benchmark</b> suites used for performance evaluation on multicore systems. We observed that conventional loop-scheduling OpenMP approaches are unable to efficiently cope with the load imbalance that naturally stems from the different performance delivered by big and small cores. To address this shortcoming, we propose \textit{Asymmetric Iteration Distribution} (AID), a set of novel loop-scheduling methods for AMPs that distribute iterations unevenly across worker threads to efficiently deal with performance asymmetry. We implemented AID in \textit{libgomp} &ndash;the GNU OpenMP runtime system&ndash;, and evaluated it on two different asymmetric multicore platforms. Our analysis reveals that the AID methods constitute effective replacements of the \texttt{static} and \texttt{dynamic} methods on AMPs, and are capable of improving performance over these conventional strategies by up to 56% and 16.8%, respectively.</p></p class="citation"></blockquote><h3 id=55--236252-logical-synchrony-networks-a-formal-model-for-deterministic-distribution-logan-kenwright-et-al-2024>(5/5 | 236/252) Logical Synchrony Networks: A formal model for deterministic distribution (Logan Kenwright et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Logan Kenwright, Partha Roop, Nathan Allen, Sanjay Lall, Calin Cascaval, Tammo Spalink, Martin Izzard. (2024)<br><strong>Logical Synchrony Networks: A formal model for deterministic distribution</strong><br><button class=copy-to-clipboard title="Logical Synchrony Networks: A formal model for deterministic distribution" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-FL, cs.DC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07433v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07433v1.pdf filename=2402.07433v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Kahn Process Networks (KPNs) are a deterministic Model of Computation (MoC) for distributed systems. KPNs supports non-blocking writes and blocking reads, with the consequent assumption of unbounded buffers between processes. Variants such as Finite FIFO Platforms (FFP) have been developed, which enforce boundedness. One issue with existing models is that they mix process synchronisation with process execution. In this paper we address how these two facets may be decoupled. This paper explores a recent alternative called bittide, which decouples the execution of a process from the control needed for process synchronisation, and thus preserves determinism and boundedness while ensuring pipelined execution for better throughput. Our intuition is that such an approach could leverage not only determinism and buffer boundedness but may potentially offer better overall throughput. To understand the behavior of these systems we define a formal model &ndash; a deterministic MoC called Logical Synchrony Networks (LSNs). LSNs describes a network of processes modelled as a <b>graph,</b> with edges representing invariant logical delays between a producer process and the corresponding consumer process. We show that this abstraction is satisfied by KPNs. Subsequently, we show that both FFPs and bittide faithfully implement this abstraction. Thus, we show for the first time that FFPs and bittide offer two alternative ways of implementing deterministic distributed systems with the latter being more performant.</p></p class="citation"></blockquote><h2 id=astro-phim-1>astro-ph.IM (1)</h2><h3 id=11--237252-convolutional-neural-networks-for-signal-detection-in-real-ligo-data-ondřej-zelenka-et-al-2024>(1/1 | 237/252) Convolutional Neural Networks for signal detection in real LIGO data (Ondřej Zelenka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ondřej Zelenka, Bernd Brügmann, Frank Ohme. (2024)<br><strong>Convolutional Neural Networks for signal detection in real LIGO data</strong><br><button class=copy-to-clipboard title="Convolutional Neural Networks for signal detection in real LIGO data" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.IM<br>Categories: astro-ph-IM, astro-ph.IM, cs-LG, gr-qc<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07492v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07492v1.pdf filename=2402.07492v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Searching the data of gravitational-wave detectors for signals from compact binary mergers is a computationally demanding task. Recently, machine learning algorithms have been proposed to address current and future challenges. However, the results of these publications often differ greatly due to differing choices in the evaluation procedure. The Machine Learning Gravitational-Wave Search Challenge was organized to resolve these issues and produce a unified framework for machine-learning search evaluation. Six teams submitted contributions, four of which are based on machine learning methods and two are state-of-the-art production analyses. This paper describes the submission from the team TPI FSU Jena and its updated variant. We also apply our algorithm to real O3b data and recover the relevant events of the GWTC-3 catalog.</p></p class="citation"></blockquote><h2 id=hep-ph-1>hep-ph (1)</h2><h3 id=11--238252-improvement-and-generalization-of-abcd-method-with-bayesian-inference-ezequiel-alvarez-et-al-2024>(1/1 | 238/252) Improvement and generalization of ABCD method with Bayesian inference (Ezequiel Alvarez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ezequiel Alvarez, Leandro Da Rold, Manuel Szewc, Alejandro Szynkman, Santiago A. Tanco, Tatiana Tarutina. (2024)<br><strong>Improvement and generalization of ABCD method with Bayesian inference</strong><br><button class=copy-to-clipboard title="Improvement and generalization of ABCD method with Bayesian inference" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: hep-ph<br>Categories: cs-LG, hep-ex, hep-ph, hep-ph<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08001v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08001v1.pdf filename=2402.08001v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To find New Physics or to refine our knowledge of the Standard Model at the LHC is an enterprise that involves many factors. We focus on taking advantage of available information and pour our effort in re-thinking the usual data-driven ABCD method to improve it and to generalize it using Bayesian Machine Learning tools. We propose that a dataset consisting of a signal and many backgrounds is well described through a mixture model. Signal, backgrounds and their relative fractions in the sample can be well extracted by exploiting the prior knowledge and the dependence between the different observables at the event-by-event level with Bayesian tools. We show how, in contrast to the ABCD method, one can take advantage of understanding some properties of the different backgrounds and of having more than two independent observables to measure in each event. In addition, instead of regions defined through hard cuts, the Bayesian framework uses the information of continuous distribution to obtain soft-assignments of the events which are statistically more robust. To compare both methods we use a toy problem inspired by $pp\to hh\to b\bar b b \bar b$, selecting a reduced and simplified number of processes and analysing the flavor of the four jets and the invariant mass of the jet-pairs, modeled with simplified distributions. Taking advantage of all this information, and starting from a combination of biased and agnostic priors, leads us to a very good posterior once we use the Bayesian framework to exploit the data and the <b>mutual</b> <b>information</b> of the observables at the event-by-event level. We show how, in this simplified model, the Bayesian framework outperforms the ABCD method sensitivity in obtaining the signal fraction in scenarios with $1%$ and $0.5%$ true signal fractions in the dataset. We also show that the method is robust against the absence of signal.</p></p class="citation"></blockquote><h2 id=csit-2>cs.IT (2)</h2><h3 id=12--239252-a-lattice-reduction-aided-vector-perturbation-precoder-relying-on-quantum-annealing-samuel-winter-et-al-2024>(1/2 | 239/252) A Lattice-Reduction Aided Vector Perturbation Precoder Relying on Quantum Annealing (Samuel Winter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Winter, Yangyishi Zhang, Gan Zheng, Lajos Hanzo. (2024)<br><strong>A Lattice-Reduction Aided Vector Perturbation Precoder Relying on Quantum Annealing</strong><br><button class=copy-to-clipboard title="A Lattice-Reduction Aided Vector Perturbation Precoder Relying on Quantum Annealing" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07643v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07643v1.pdf filename=2402.07643v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum annealing <b>(QA)</b> is proposed for vector perturbation precoding (VPP) in multiple input multiple output (MIMO) communications systems. The mathematical framework of VPP is presented, outlining the problem formulation and the benefits of lattice reduction algorithms. Lattice reduction aided quantum vector perturbation (LRAQVP) is designed by harnessing physical quantum hardware, and the optimization of hardware parameters is discussed. We observe a 5dB gain over lattice reduction zero forcing precoding (LRZFP), which behaves similarly to a quantum annealing algorithm operating without a lattice reduction stage. The proposed algorithm is also shown to approach the performance of a sphere encoder, which exhibits an exponentially escalating complexity.</p></p class="citation"></blockquote><h3 id=22--240252-semantic-data-for-humanities-and-social-sciences-sdhss-an-ecosystem-of-cidoc-crm-extensions-for-research-data-production-and-reuse-francesco-beretta-2024>(2/2 | 240/252) Semantic Data for Humanities and Social Sciences (SDHSS): an Ecosystem of CIDOC CRM Extensions for Research Data Production and Reuse (Francesco Beretta, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Beretta. (2024)<br><strong>Semantic Data for Humanities and Social Sciences (SDHSS): an Ecosystem of CIDOC CRM Extensions for Research Data Production and Reuse</strong><br><button class=copy-to-clipboard title="Semantic Data for Humanities and Social Sciences (SDHSS): an Ecosystem of CIDOC CRM Extensions for Research Data Production and Reuse" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 8<br>Keywords: Graph, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07531v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07531v1.pdf filename=2402.07531v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the challenge of giant <b>knowledge</b> <b>graphs</b> created by major eco-nomic actors, which could virtually replace research in the Humani-ties and Social Sciences (HSS) in responding to public concerns, thequestion arises of how to increase the value of research data throughtheir publication and networking, applying the FAIR principles. Bothan epistemological and a semantic analysis show that the most rel-evant part of research data is factual information, understood as arepresentation of the objects observed by the scientific disciplines,their properties and their relationships.This rich universe of information will be made understandable andtherefore reusable through the application of foundational ontologiesand a methodology based on the distinction between different levelsof abstraction, allowing the collective development of one or moreshared and reusable domain ontologies. This vision is being carriedout around the CIDOC CRM, as core ontology, and Semantic Datafor Humanities and Social Sciences (SDHSS), as a high-level exten-sion of it, as well as an ecosystem of sub-domain extensions that canbe easily managed through the ontome.net application. This willresult in an interoperability that is semantically richer than the sim-ple alignment of ontologies and less costly in terms of resources, andabove all adapted to the scientific and humanistic project of the HSS.</p></p class="citation"></blockquote><h2 id=physicscomp-ph-1>physics.comp-ph (1)</h2><h3 id=11--241252-cartesian-atomic-cluster-expansion-for-machine-learning-interatomic-potentials-bingqing-cheng-2024>(1/1 | 241/252) Cartesian atomic cluster expansion for machine learning interatomic potentials (Bingqing Cheng, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingqing Cheng. (2024)<br><strong>Cartesian atomic cluster expansion for machine learning interatomic potentials</strong><br><button class=copy-to-clipboard title="Cartesian atomic cluster expansion for machine learning interatomic potentials" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.comp-ph<br>Categories: cs-LG, physics-chem-ph, physics-comp-ph, physics.comp-ph<br>Keyword Score: 10<br>Keywords: Message-Passing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07472v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07472v1.pdf filename=2402.07472v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning interatomic potentials are revolutionizing large-scale, accurate atomistic modelling in material science and chemistry. These potentials often use atomic cluster expansion or equivariant message passing with spherical harmonics as basis functions. However, the dependence on Clebsch-Gordan coefficients for maintaining rotational symmetry leads to computational inefficiencies and redundancies. We propose an alternative: a Cartesian-coordinates-based atomic density expansion. This approach provides a complete description of atomic environments while maintaining interaction body orders. Additionally, we integrate low-dimensional embeddings of various chemical elements and inter-atomic message passing. The resulting potential, named Cartesian Atomic Cluster Expansion (CACE), exhibits good accuracy, stability, and generalizability. We validate its performance in diverse systems, including bulk water, small molecules, and 25-element high-entropy alloys.</p></p class="citation"></blockquote><h2 id=csne-2>cs.NE (2)</h2><h3 id=12--242252-impact-of-spatial-transformations-on-landscape-features-of-cec2022-basic-benchmark-problems-haoran-yin-et-al-2024>(1/2 | 242/252) Impact of spatial transformations on landscape features of CEC2022 basic benchmark problems (Haoran Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoran Yin, Diederick Vermetten, Furong Ye, Thomas H. W. Bäck, Anna V. Kononova. (2024)<br><strong>Impact of spatial transformations on landscape features of CEC2022 basic benchmark problems</strong><br><button class=copy-to-clipboard title="Impact of spatial transformations on landscape features of CEC2022 basic benchmark problems" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07654v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07654v1.pdf filename=2402.07654v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When <b>benchmarking</b> optimization heuristics, we need to take care to avoid an algorithm exploiting biases in the construction of the used problems. One way in which this might be done is by providing different versions of each problem but with transformations applied to ensure the algorithms are equipped with mechanisms for successfully tackling a range of problems. In this paper, we investigate several of these problem transformations and show how they influence the low-level landscape features of a set of 5 problems from the CEC2022 <b>benchmark</b> suite. Our results highlight that even relatively small transformations can significantly alter the measured landscape features. This poses a wider question of what properties we want to preserve when creating problem transformations, and how to fairly measure them.</p></p class="citation"></blockquote><h3 id=22--243252-on-the-nature-of-the-phenotype-in-tree-genetic-programming-wolfgang-banzhaf-et-al-2024>(2/2 | 243/252) On The Nature Of The Phenotype In Tree Genetic Programming (Wolfgang Banzhaf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wolfgang Banzhaf, Illya Bakurov. (2024)<br><strong>On The Nature Of The Phenotype In Tree Genetic Programming</strong><br><button class=copy-to-clipboard title="On The Nature Of The Phenotype In Tree Genetic Programming" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08011v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08011v1.pdf filename=2402.08011v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this contribution, we discuss the basic concepts of genotypes and phenotypes in tree-based GP (TGP), and then analyze their behavior using five <b>benchmark</b> datasets. We show that TGP exhibits the same behavior that we can observe in other GP representations: At the genotypic level trees show frequently unchecked growth with seemingly ineffective code, but on the phenotypic level, much smaller trees can be observed. To generate phenotypes, we provide a unique technique for removing semantically ineffective code from GP trees. The approach extracts considerably simpler phenotypes while not being limited to local operations in the genotype. We generalize this transformation based on a problem-independent parameter that enables a further simplification of the exact phenotype by coarse-graining to produce approximate phenotypes. The concept of these phenotypes (exact and approximate) allows us to clarify what evolved solutions truly predict, making GP models considered at the phenotypic level much better interpretable.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--244252-perfect-stable-regularity-lemma-and-slice-wise-stable-hypergraphs-artem-chernikov-et-al-2024>(1/1 | 244/252) Perfect stable regularity lemma and slice-wise stable hypergraphs (Artem Chernikov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Artem Chernikov, Henry Towsner. (2024)<br><strong>Perfect stable regularity lemma and slice-wise stable hypergraphs</strong><br><button class=copy-to-clipboard title="Perfect stable regularity lemma and slice-wise stable hypergraphs" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 03C45, 05C35, 05C65, 05C75, cs-DM, math-CO, math-LO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07870v1.pdf filename=2402.07870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate various forms of (model-theoretic) stability for hypergraphs and their corresponding strengthenings of the hypergraph regularity lemma with respect to partitions of vertices. On the one hand, we provide a complete classification of the various possibilities in the ternary case. On the other hand, we provide an example of a family of slice-wise stable 3-hypergraphs so that for no partition of the vertices, any triple of parts has density close to 0 or 1. In particular, this addresses some questions and conjectures of Terry and Wolf. We work in the general measure theoretic context of graded probability spaces, so all our results apply both to measures in ultraproducts of finite <b>graphs,</b> leading to the aforementioned combinatorial applications, and to commuting definable Keisler measures, leading to applications in model theory.</p></p class="citation"></blockquote><h2 id=csds-3>cs.DS (3)</h2><h3 id=13--245252-an-approximation-algorithm-for-maximum-dicut-vs-cut-tamio-vesa-nakajima-et-al-2024>(1/3 | 245/252) An approximation algorithm for Maximum DiCut vs. Cut (Tamio-Vesa Nakajima et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tamio-Vesa Nakajima, Stanislav Živný. (2024)<br><strong>An approximation algorithm for Maximum DiCut vs. Cut</strong><br><button class=copy-to-clipboard title="An approximation algorithm for Maximum DiCut vs. Cut" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07863v1.pdf filename=2402.07863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Goemans and Williamson designed a 0.878-approximation algorithm for Max-Cut in undirected <b>graphs</b> [JACM'95]. Khot, Kindler, Mosel, and O&rsquo;Donnel showed that the approximation ratio of the Goemans-Williamson algorithm is optimal assuming Khot&rsquo;s Unique Games Conjecture [SICOMP'07]. In the problem of maximum cuts in directed <b>graphs</b> (Max-DiCut), in which we seek as many edges going from one particular side of the cut to the other, the situation is more complicated but the recent work of Brakensiek, Huang, Potechin, and Zwick showed that their 0.874-approximation algorithm is tight under the Unique Games Conjecture (up to a small delta)[FOCS'23]. We consider a promise version of the problem and design an SDP-based algorithm which, if given a directed <b>graph</b> G that has a directed cut of value rho, finds an undirected cut in G (ignoring edge directions) with value at least \rho.</p></p class="citation"></blockquote><h3 id=23--246252-insights-into-kρ-shortcutting-algorithms-alexander-leonhardt-et-al-2024>(2/3 | 246/252) Insights into $(k,ρ)$-shortcutting algorithms (Alexander Leonhardt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Leonhardt, Ulrich Meyer, Manuel Penschuck. (2024)<br><strong>Insights into $(k,ρ)$-shortcutting algorithms</strong><br><button class=copy-to-clipboard title="Insights into $(k,ρ)$-shortcutting algorithms" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07771v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07771v1.pdf filename=2402.07771v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A <b>graph</b> is called a $(k,\rho)$-graph iff every node can reach $\rho$ of its nearest neighbors in at most k hops. This property proved useful in the analysis and design of parallel shortest-path algorithms. Any <b>graph</b> can be transformed into a $(k,\rho)$-graph by adding shortcuts. Formally, the $(k,\rho)$-Minimum-Shortcut problem asks to find an appropriate shortcut set of minimal cardinality. We show that the $(k,\rho)$-Minimum-Shortcut problem is NP-complete in the practical regime of $k \ge 3$ and $\rho = \Theta(n^\epsilon)$ for $\epsilon > 0$. With a related construction, we bound the approximation factor of known $(k,\rho)$-Minimum-Shortcut problem heuristics from below and propose algorithmic countermeasures improving the approximation quality. Further, we describe an integer linear problem (ILP) solving the $(k,\rho)$-Minimum-Shortcut problem optimally. Finally, we compare the practical performance and quality of all algorithms in an empirical campaign.</p></p class="citation"></blockquote><h3 id=33--247252-engineering-weighted-connectivity-augmentation-algorithms-marcelo-fonseca-faraj-et-al-2024>(3/3 | 247/252) Engineering Weighted Connectivity Augmentation Algorithms (Marcelo Fonseca Faraj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcelo Fonseca Faraj, Ernestine Großmann, Felix Joos, Thomas Möller, Christian Schulz. (2024)<br><strong>Engineering Weighted Connectivity Augmentation Algorithms</strong><br><button class=copy-to-clipboard title="Engineering Weighted Connectivity Augmentation Algorithms" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07753v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07753v1.pdf filename=2402.07753v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Increasing the connectivity of a <b>graph</b> is a pivotal challenge in robust network design. The weighted connectivity augmentation problem is a common version of the problem that takes link costs into consideration. The problem is then to find a minimum cost subset of a given set of weighted links that increases the connectivity of a <b>graph</b> by one when the links are added to the edge set of the input instance. In this work, we give a first implementation of recently discovered better-than-2 approximations. Furthermore, we propose three new heuristic and one exact approach. These include a greedy algorithm considering link costs and the number of unique cuts covered, an approach based on minimum spanning trees and a local search algorithm that may improve a given solution by swapping links of paths. Our exact approach uses an ILP formulation with efficient cut enumeration as well as a fast initialization routine. We then perform an extensive experimental evaluation which shows that our algorithms are faster and yield the best solutions compared to the current state-of-the-art as well as the recently discovered better-than-2 approximation algorithms. Our novel local search algorithm can improve solution quality even further.</p></p class="citation"></blockquote><h2 id=quant-ph-2>quant-ph (2)</h2><h3 id=12--248252-quantum-walks-the-discrete-wave-equation-and-chebyshev-polynomials-simon-apers-et-al-2024>(1/2 | 248/252) Quantum walks, the discrete wave equation and Chebyshev polynomials (Simon Apers et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Apers, Laurent Miclo. (2024)<br><strong>Quantum walks, the discrete wave equation and Chebyshev polynomials</strong><br><button class=copy-to-clipboard title="Quantum walks, the discrete wave equation and Chebyshev polynomials" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-DS, math-PR, quant-ph, quant-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07809v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07809v1.pdf filename=2402.07809v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A quantum walk is the quantum analogue of a random walk. While it is relatively well understood how quantum walks can speed up random walk hitting times, it is a long-standing open question to what extent quantum walks can speed up the spreading or mixing rate of random walks on <b>graphs.</b> In this expository paper, inspired by a blog post by Terence Tao, we describe a particular perspective on this question that derives quantum walks from the discrete wave equation on <b>graphs.</b> This yields a description of the quantum walk dynamics as simply applying a Chebyshev polynomial to the random walk transition matrix. This perspective decouples the problem from its quantum origin, and highlights connections to earlier (non-quantum) work and the use of Chebyshev polynomials in random walk theory as in the Varopoulos-Carne bound. We illustrate the approach by proving a weak limit of the quantum walk dynamics on the lattice. This gives a different proof of the quadratically improved spreading behavior of quantum walks on lattices.</p></p class="citation"></blockquote><h3 id=22--249252-expansion-of-higher-dimensional-cubical-complexes-with-application-to-quantum-locally-testable-codes-irit-dinur-et-al-2024>(2/2 | 249/252) Expansion of higher-dimensional cubical complexes with application to quantum locally testable codes (Irit Dinur et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Irit Dinur, Ting-Chun Lin, Thomas Vidick. (2024)<br><strong>Expansion of higher-dimensional cubical complexes with application to quantum locally testable codes</strong><br><button class=copy-to-clipboard title="Expansion of higher-dimensional cubical complexes with application to quantum locally testable codes" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-CC, cs-IT, math-IT, quant-ph, quant-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07476v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07476v1.pdf filename=2402.07476v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a higher-dimensional &ldquo;cubical&rdquo; chain complex and apply it to the design of quantum locally testable codes. Our cubical chain complex can be constructed for any dimension $t$, and in a precise sense generalizes the Sipser-Spielman construction of expander codes (case $t=1$) and the constructions by Dinur et. al and Panteleev and Kalachev of a square complex (case $t$=2), which have been applied to the design of classical locally testable and quantum low-density parity check codes respectively. For $t=4$ our construction gives a family of quantum locally testable codes conditional on a conjecture about robustness of four-tuples of random linear maps. These codes have linear dimension, inverse poly-logarithmic relative distance and soundness, and polylogarithmic-size parity checks. Our complex can be built in a modular way from two ingredients. Firstly, the geometry (edges, faces, cubes, etc.) is provided by a set $G$ of size $N$, together with pairwise commuting sets of actions $A_1,\ldots,A_t$ on it. Secondly, the chain complex itself is obtained by associating local coefficient spaces based on codes, with each geometric object, and introducing local maps on those coefficient spaces. We bound the cycle and co-cycle expansion of the chain complex. The assumptions we need are two-fold: firstly, each Cayley <b>graph</b> $Cay(G,A_j)$ needs to be a good (spectral) expander, and secondly, the families of codes and their duals both need to satisfy a form of robustness (that generalizes the condition of agreement testability for pairs of codes). While the first assumption is easy to satisfy, it is currently not known if the second can be achieved.</p></p class="citation"></blockquote><h2 id=csdm-1>cs.DM (1)</h2><h3 id=11--250252-growth-rate-of-the-number-of-empty-triangles-in-the-plane-bhaswar-b-bhattacharya-et-al-2024>(1/1 | 250/252) Growth Rate of the Number of Empty Triangles in the Plane (Bhaswar B. Bhattacharya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bhaswar B. Bhattacharya, Sandip Das, Sk Samim Islam, Saumya Sen. (2024)<br><strong>Growth Rate of the Number of Empty Triangles in the Plane</strong><br><button class=copy-to-clipboard title="Growth Rate of the Number of Empty Triangles in the Plane" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs.DM<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07775v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07775v1.pdf filename=2402.07775v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a set $P$ of $n$ points in the plane, in general position, denote by $N_\Delta(P)$ the number of empty triangles with vertices in $P$. In this paper we investigate by how much $N_\Delta(P)$ changes if a point $x$ is removed from $P$. By constructing a <b>graph</b> $G_P(x)$ based on the arrangement of the empty triangles incident on $x$, we transform this geometric problem to the problem of counting triangles in the <b>graph</b> $G_P(x)$. We study properties of the <b>graph</b> $G_P(x)$ and, in particular, show that it is kite-free. This relates the growth rate of the number of empty triangles to the famous Ruzsa-Szemer'edi problem.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--251252-fully-dynamic-geometric-vertex-cover-and-matching-sujoy-bhore-et-al-2024>(1/1 | 251/252) Fully Dynamic Geometric Vertex Cover and Matching (Sujoy Bhore et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sujoy Bhore, Timothy M. Chan. (2024)<br><strong>Fully Dynamic Geometric Vertex Cover and Matching</strong><br><button class=copy-to-clipboard title="Fully Dynamic Geometric Vertex Cover and Matching" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs.CG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07441v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07441v2.pdf filename=2402.07441v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we study two fundamental <b>graph</b> optimization problems, minimum vertex cover (MVC) and maximum-cardinality matching (MCM), for intersection <b>graphs</b> of geometric objects, e.g., disks, rectangles, hypercubes, etc., in $d$-dimensional Euclidean space. We consider the problems in fully dynamic settings, allowing insertions and deletions of objects. We develop a general framework for dynamic MVC in intersection <b>graphs,</b> achieving sublinear amortized update time for most natural families of geometric objects. In particular, we show that - - For a dynamic collection of disks in $\mathbb{R}^2$ or hypercubes in $\mathbb{R}^d$ (for constant $d$), it is possible to maintain a $(1+\varepsilon)$-approximate vertex cover in polylog amortized update time. These results also hold in the bipartite case. - For a dynamic collection of rectangles in $\mathbb{R}^2$, it is possible to maintain a $(\frac{3}{2}+\varepsilon)$-approximate vertex cover in polylog amortized update time. Along the way, we obtain the first near-linear time static algorithms for MVC in the above two cases with the same approximation factors. Next, we turn our attention to the MCM problem. Although our MVC algorithms automatically allow us to approximate the size of the MCM in bipartite geometric intersection <b>graphs,</b> they do not produce a matching. We give another general framework to maintain an approximate maximum matching, and further extend the approach to handle non-bipartite intersection <b>graphs.</b> In particular, we show that - - For a dynamic collection of (bichromatic or monochromatic) disks in $\mathbb{R}^2$ or hypercubes in $\mathbb{R}^d$ (for constant $d$), it is possible to maintain a $(1+\varepsilon)$-approximate matching in polylog amortized update time.</p></p class="citation"></blockquote><h2 id=mathst-1>math.ST (1)</h2><h3 id=11--252252-the-limits-of-assumption-free-tests-for-algorithm-performance-yuetian-luo-et-al-2024>(1/1 | 252/252) The Limits of Assumption-free Tests for Algorithm Performance (Yuetian Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuetian Luo, Rina Foygel Barber. (2024)<br><strong>The Limits of Assumption-free Tests for Algorithm Performance</strong><br><button class=copy-to-clipboard title="The Limits of Assumption-free Tests for Algorithm Performance" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.ST<br>Categories: cs-LG, math-ST, math.ST, stat-ML, stat-TH<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07388v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07388v1.pdf filename=2402.07388v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Algorithm evaluation and comparison are fundamental questions in machine learning and statistics &ndash; how well does an algorithm perform at a given modeling task, and which algorithm performs best? Many methods have been developed to assess algorithm performance, often based around cross-validation type strategies, retraining the algorithm of interest on different subsets of the data and assessing its performance on the held-out data points. Despite the broad use of such procedures, the theoretical properties of these methods are not yet fully understood. In this work, we explore some fundamental limits for answering these questions with limited amounts of data. In particular, we make a distinction between two questions: how good is an algorithm $A$ at the problem of learning from a training set of size $n$, versus, how good is a particular fitted model produced by running $A$ on a particular training data set of size $n$? Our main results prove that, for any test that treats the algorithm $A$ as a ``black box&rsquo;&rsquo; (i.e., we can only study the behavior of $A$ empirically), there is a fundamental limit on our ability to carry out inference on the performance of $A$, unless the number of available data points $N$ is many times larger than the <b>sample</b> <b>size</b> $n$ of interest. (On the other hand, evaluating the performance of a particular fitted model is easy as long as a holdout data set is available &ndash; that is, as long as $N-n$ is not too small.) We also ask whether an assumption of algorithmic stability might be sufficient to circumvent this hardness result. Surprisingly, we find that this is not the case: the same hardness result still holds for the problem of evaluating the performance of $A$, aside from a high-stability regime where fitted models are essentially nonrandom. Finally, we also establish similar hardness results for the problem of comparing multiple algorithms.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.13</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.15</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cslg-62>cs.LG (62)</a><ul><li><a href=#162--1252-g-retriever-retrieval-augmented-generation-for-textual-graph-understanding-and-question-answering-xiaoxin-he-et-al-2024>(1/62 | 1/252) G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering (Xiaoxin He et al., 2024)</a></li><li><a href=#262--2252-transaxx-efficient-transformers-with-approximate-computing-dimitrios-danopoulos-et-al-2024>(2/62 | 2/252) TransAxx: Efficient Transformers with Approximate Computing (Dimitrios Danopoulos et al., 2024)</a></li><li><a href=#362--3252-grounding-data-science-code-generation-with-input-output-specifications-yeming-wen-et-al-2024>(3/62 | 3/252) Grounding Data Science Code Generation with Input-Output Specifications (Yeming Wen et al., 2024)</a></li><li><a href=#462--4252-base-tts-lessons-from-building-a-billion-parameter-text-to-speech-model-on-100k-hours-of-data-mateusz-łajszczak-et-al-2024>(4/62 | 4/252) BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data (Mateusz Łajszczak et al., 2024)</a></li><li><a href=#562--5252-differentially-private-zeroth-order-methods-for-scalable-large-language-model-finetuning-z-liu-et-al-2024>(5/62 | 5/252) Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning (Z Liu et al., 2024)</a></li><li><a href=#662--6252-text-centric-alignment-for-multi-modality-learning-yun-da-tsai-et-al-2024>(6/62 | 6/252) Text-centric Alignment for Multi-Modality Learning (Yun-Da Tsai et al., 2024)</a></li><li><a href=#762--7252-message-detouring-a-simple-yet-effective-cycle-representation-for-expressive-graph-learning-ziquan-wei-et-al-2024>(7/62 | 7/252) Message Detouring: A Simple Yet Effective Cycle Representation for Expressive Graph Learning (Ziquan Wei et al., 2024)</a></li><li><a href=#862--8252-ugmae-a-unified-framework-for-graph-masked-autoencoders-yijun-tian-et-al-2024>(8/62 | 8/252) UGMAE: A Unified Framework for Graph Masked Autoencoders (Yijun Tian et al., 2024)</a></li><li><a href=#962--9252-active-preference-learning-for-large-language-models-william-muldrew-et-al-2024>(9/62 | 9/252) Active Preference Learning for Large Language Models (William Muldrew et al., 2024)</a></li><li><a href=#1062--10252-empowering-federated-learning-for-massive-models-with-nvidia-flare-holger-r-roth-et-al-2024>(10/62 | 10/252) Empowering Federated Learning for Massive Models with NVIDIA FLARE (Holger R. Roth et al., 2024)</a></li><li><a href=#1162--11252-foundational-inference-models-for-dynamical-systems-patrick-seifner-et-al-2024>(11/62 | 11/252) Foundational Inference Models for Dynamical Systems (Patrick Seifner et al., 2024)</a></li><li><a href=#1262--12252-only-the-curve-shape-matters-training-foundation-models-for-zero-shot-multivariate-time-series-forecasting-through-next-curve-shape-prediction-cheng-feng-et-al-2024>(12/62 | 12/252) Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction (Cheng Feng et al., 2024)</a></li><li><a href=#1362--13252-bayesian-federated-learning-via-expectation-maximization-and-turbo-deep-approximate-message-passing-wei-xu-et-al-2024>(13/62 | 13/252) Bayesian Federated Learning Via Expectation Maximization and Turbo Deep Approximate Message Passing (Wei Xu et al., 2024)</a></li><li><a href=#1462--14252-universal-link-predictor-by-in-context-learning-on-graphs-kaiwen-dong-et-al-2024>(14/62 | 14/252) Universal Link Predictor By In-Context Learning on Graphs (Kaiwen Dong et al., 2024)</a></li><li><a href=#1562--15252-assessing-generalization-for-subpopulation-representative-modeling-via-in-context-learning-gabriel-simmons-et-al-2024>(15/62 | 15/252) Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning (Gabriel Simmons et al., 2024)</a></li><li><a href=#1662--16252-on-the-resurgence-of-recurrent-models-for-long-sequences----survey-and-research-opportunities-in-the-transformer-era-matteo-tiezzi-et-al-2024>(16/62 | 16/252) On the Resurgence of Recurrent Models for Long Sequences &ndash; Survey and Research Opportunities in the Transformer Era (Matteo Tiezzi et al., 2024)</a></li><li><a href=#1762--17252-policy-improvement-using-language-feedback-models-victor-zhong-et-al-2024>(17/62 | 17/252) Policy Improvement using Language Feedback Models (Victor Zhong et al., 2024)</a></li><li><a href=#1862--18252-fourier-circuits-in-neural-networks-unlocking-the-potential-of-large-language-models-in-mathematical-reasoning-and-modular-arithmetic-jiuxiang-gu-et-al-2024>(18/62 | 18/252) Fourier Circuits in Neural Networks: Unlocking the Potential of Large Language Models in Mathematical Reasoning and Modular Arithmetic (Jiuxiang Gu et al., 2024)</a></li><li><a href=#1962--19252-accuracy-of-textfooler-black-box-adversarial-attacks-on-01-loss-sign-activation-neural-network-ensemble-yunzhe-xue-et-al-2024>(19/62 | 19/252) Accuracy of TextFooler black box adversarial attacks on 01 loss sign activation neural network ensemble (Yunzhe Xue et al., 2024)</a></li><li><a href=#2062--20252-an-investigation-into-using-unsupervised-metrics-to-optimise-gnns-for-node-clustering-william-leeney-et-al-2024>(20/62 | 20/252) An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering (William Leeney et al., 2024)</a></li><li><a href=#2162--21252-clustertabnet-supervised-clustering-method-for-table-detection-and-table-structure-recognition-marek-polewczyk-et-al-2024>(21/62 | 21/252) ClusterTabNet: Supervised clustering method for table detection and table structure recognition (Marek Polewczyk et al., 2024)</a></li><li><a href=#2262--22252-a-competition-winning-deep-reinforcement-learning-agent-in-microrts-scott-goodfriend-2024>(22/62 | 22/252) A Competition Winning Deep Reinforcement Learning Agent in microRTS (Scott Goodfriend, 2024)</a></li><li><a href=#2362--23252-netinfof-framework-measuring-and-exploiting-network-usable-information-meng-chieh-lee-et-al-2024>(23/62 | 23/252) NetInfoF Framework: Measuring and Exploiting Network Usable Information (Meng-Chieh Lee et al., 2024)</a></li><li><a href=#2462--24252-towards-an-understanding-of-stepwise-inference-in-transformers-a-synthetic-graph-navigation-model-mikail-khona-et-al-2024>(24/62 | 24/252) Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model (Mikail Khona et al., 2024)</a></li><li><a href=#2562--25252-unveiling-group-specific-distributed-concept-drift-a-fairness-imperative-in-federated-learning-teresa-salazar-et-al-2024>(25/62 | 25/252) Unveiling Group-Specific Distributed Concept Drift: A Fairness Imperative in Federated Learning (Teresa Salazar et al., 2024)</a></li><li><a href=#2662--26252-one-train-for-two-tasks-an-encrypted-traffic-classification-framework-using-supervised-contrastive-learning-haozhen-zhang-et-al-2024>(26/62 | 26/252) One Train for Two Tasks: An Encrypted Traffic Classification Framework Using Supervised Contrastive Learning (Haozhen Zhang et al., 2024)</a></li><li><a href=#2762--27252-topological-safeguard-for-evasion-attack-interpreting-the-neural-networks-behavior-xabier-echeberria-barrio-et-al-2024>(27/62 | 27/252) Topological safeguard for evasion attack interpreting the neural networks&rsquo; behavior (Xabier Echeberria-Barrio et al., 2024)</a></li><li><a href=#2862--28252-implicit-bias-of-policy-gradient-in-linear-quadratic-control-extrapolation-to-unseen-initial-states-noam-razin-et-al-2024>(28/62 | 28/252) Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States (Noam Razin et al., 2024)</a></li><li><a href=#2962--29252-scaling-laws-for-fine-grained-mixture-of-experts-jakub-krajewski-et-al-2024>(29/62 | 29/252) Scaling Laws for Fine-Grained Mixture of Experts (Jakub Krajewski et al., 2024)</a></li><li><a href=#3062--30252-lora-drop-efficient-lora-parameter-pruning-based-on-output-evaluation-hongyun-zhou-et-al-2024>(30/62 | 30/252) LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation (Hongyun Zhou et al., 2024)</a></li><li><a href=#3162--31252-model-collapse-demystified-the-case-of-regression-elvis-dohmatob-et-al-2024>(31/62 | 31/252) Model Collapse Demystified: The Case of Regression (Elvis Dohmatob et al., 2024)</a></li><li><a href=#3262--32252-optimization-of-sparse-convolution-for-3d-point-cloud-on-gpus-with-cuda-chester-luo-et-al-2024>(32/62 | 32/252) Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA (Chester Luo et al., 2024)</a></li><li><a href=#3362--33252-sourcerer-sample-based-maximum-entropy-source-distribution-estimation-julius-vetter-et-al-2024>(33/62 | 33/252) Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation (Julius Vetter et al., 2024)</a></li><li><a href=#3462--34252-weisfeiler-leman-at-the-margin-when-more-expressivity-matters-billy-j-franks-et-al-2024>(34/62 | 34/252) Weisfeiler-Leman at the margin: When more expressivity matters (Billy J. Franks et al., 2024)</a></li><li><a href=#3562--35252-understanding-deep-learning-defenses-against-adversarial-examples-through-visualizations-for-dynamic-risk-assessment-xabier-echeberria-barrio-et-al-2024>(35/62 | 35/252) Understanding Deep Learning defenses Against Adversarial Examples Through Visualizations for Dynamic Risk Assessment (Xabier Echeberria-Barrio et al., 2024)</a></li><li><a href=#3662--36252-contextual-multinomial-logit-bandits-with-general-value-functions-mengxiao-zhang-et-al-2024>(36/62 | 36/252) Contextual Multinomial Logit Bandits with General Value Functions (Mengxiao Zhang et al., 2024)</a></li><li><a href=#3762--37252-which-pretrain-samples-to-rehearse-when-finetuning-pretrained-models-andrew-bai-et-al-2024>(37/62 | 37/252) Which Pretrain Samples to Rehearse when Finetuning Pretrained Models? (Andrew Bai et al., 2024)</a></li><li><a href=#3862--38252-avoiding-catastrophe-in-continuous-spaces-by-asking-for-help-benjamin-plaut-et-al-2024>(38/62 | 38/252) Avoiding Catastrophe in Continuous Spaces by Asking for Help (Benjamin Plaut et al., 2024)</a></li><li><a href=#3962--39252-which-frequencies-do-cnns-need-emergent-bottleneck-structure-in-feature-learning-yuxiao-wen-et-al-2024>(39/62 | 39/252) Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning (Yuxiao Wen et al., 2024)</a></li><li><a href=#4062--40252-fast-factorizable-attention-for-speeding-up-transformers-armin-gerami-et-al-2024>(40/62 | 40/252) FAST: Factorizable Attention for Speeding up Transformers (Armin Gerami et al., 2024)</a></li><li><a href=#4162--41252-comparing-skill-of-historical-rainfall-data-based-monsoon-rainfall-prediction-in-india-with-ncep-nwp-forecasts-apoorva-narula-et-al-2024>(41/62 | 41/252) Comparing skill of historical rainfall data based monsoon rainfall prediction in India with NCEP-NWP forecasts (Apoorva Narula et al., 2024)</a></li><li><a href=#4262--42252-tighter-bounds-on-the-information-bottleneck-with-application-to-deep-learning-nir-weingarten-et-al-2024>(42/62 | 42/252) Tighter Bounds on the Information Bottleneck with Application to Deep Learning (Nir Weingarten et al., 2024)</a></li><li><a href=#4362--43252-near-minimax-optimal-distributional-reinforcement-learning-with-a-generative-model-mark-rowland-et-al-2024>(43/62 | 43/252) Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model (Mark Rowland et al., 2024)</a></li><li><a href=#4462--44252-rolling-diffusion-models-david-ruhe-et-al-2024>(44/62 | 44/252) Rolling Diffusion Models (David Ruhe et al., 2024)</a></li><li><a href=#4562--45252-score-based-physics-informed-neural-networks-for-high-dimensional-fokker-planck-equations-zheyuan-hu-et-al-2024>(45/62 | 45/252) Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck Equations (Zheyuan Hu et al., 2024)</a></li><li><a href=#4662--46252-the-io-complexity-of-attention-or-how-optimal-is-flash-attention-barna-saha-et-al-2024>(46/62 | 46/252) The I/O Complexity of Attention, or How Optimal is Flash Attention? (Barna Saha et al., 2024)</a></li><li><a href=#4762--47252-measurement-scheduling-for-icu-patients-with-offline-reinforcement-learning-zongliang-ji-et-al-2024>(47/62 | 47/252) Measurement Scheduling for ICU Patients with Offline Reinforcement Learning (Zongliang Ji et al., 2024)</a></li><li><a href=#4862--48252-efficient-contextual-bandits-with-uninformed-feedback-graphs-mengxiao-zhang-et-al-2024>(48/62 | 48/252) Efficient Contextual Bandits with Uninformed Feedback Graphs (Mengxiao Zhang et al., 2024)</a></li><li><a href=#4962--49252-hypo-hyperspherical-out-of-distribution-generalization-haoyue-bai-et-al-2024>(49/62 | 49/252) HYPO: Hyperspherical Out-of-Distribution Generalization (Haoyue Bai et al., 2024)</a></li><li><a href=#5062--50252-differentially-private-decentralized-learning-with-random-walks-edwige-cyffers-et-al-2024>(50/62 | 50/252) Differentially Private Decentralized Learning with Random Walks (Edwige Cyffers et al., 2024)</a></li><li><a href=#5162--51252-conditional-generative-models-are-sufficient-to-sample-from-any-causal-effect-estimand-md-musfiqur-rahman-et-al-2024>(51/62 | 51/252) Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand (Md Musfiqur Rahman et al., 2024)</a></li><li><a href=#5262--52252-random-geometric-graph-alignment-with-graph-neural-networks-suqi-liu-et-al-2024>(52/62 | 52/252) Random Geometric Graph Alignment with Graph Neural Networks (Suqi Liu et al., 2024)</a></li><li><a href=#5362--53252-leveraging-digital-cousins-for-ensemble-q-learning-in-large-scale-wireless-networks-talha-bozkus-et-al-2024>(53/62 | 53/252) Leveraging Digital Cousins for Ensemble Q-Learning in Large-Scale Wireless Networks (Talha Bozkus et al., 2024)</a></li><li><a href=#5462--54252-one-for-many-counterfactual-explanations-by-column-generation-andrea-lodi-et-al-2024>(54/62 | 54/252) One-for-many Counterfactual Explanations by Column Generation (Andrea Lodi et al., 2024)</a></li><li><a href=#5562--55252-accelerated-smoothing-a-scalable-approach-to-randomized-smoothing-devansh-bhardwaj-et-al-2024>(55/62 | 55/252) Accelerated Smoothing: A Scalable Approach to Randomized Smoothing (Devansh Bhardwaj et al., 2024)</a></li><li><a href=#5662--56252-score-based-diffusion-models-via-stochastic-differential-equations----a-technical-tutorial-wenpin-tang-et-al-2024>(56/62 | 56/252) Score-based Diffusion Models via Stochastic Differential Equations &ndash; a Technical Tutorial (Wenpin Tang et al., 2024)</a></li><li><a href=#5762--57252-bandit-feedback-online-multiclass-classification-variants-and-tradeoffs-yuval-filmus-et-al-2024>(57/62 | 57/252) Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs (Yuval Filmus et al., 2024)</a></li><li><a href=#5862--58252-context-aware-multi-model-object-detection-for-diversely-heterogeneous-compute-systems-justin-davis-et-al-2024>(58/62 | 58/252) Context-aware Multi-Model Object Detection for Diversely Heterogeneous Compute Systems (Justin Davis et al., 2024)</a></li><li><a href=#5962--59252-auxiliary-reward-generation-with-transition-distance-representation-learning-siyuan-li-et-al-2024>(59/62 | 59/252) Auxiliary Reward Generation with Transition Distance Representation Learning (Siyuan Li et al., 2024)</a></li><li><a href=#6062--60252-data-distribution-based-curriculum-learning-shonal-chaudhry-et-al-2024>(60/62 | 60/252) Data Distribution-based Curriculum Learning (Shonal Chaudhry et al., 2024)</a></li><li><a href=#6162--61252-learning-cartesian-product-graphs-with-laplacian-constraints-changhao-shi-et-al-2024>(61/62 | 61/252) Learning Cartesian Product Graphs with Laplacian Constraints (Changhao Shi et al., 2024)</a></li><li><a href=#6262--62252-boundary-exploration-for-bayesian-optimization-with-unknown-physical-constraints-yunsheng-tian-et-al-2024>(62/62 | 62/252) Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints (Yunsheng Tian et al., 2024)</a></li></ul></li><li><a href=#cscl-39>cs.CL (39)</a><ul><li><a href=#139--63252-addressing-cognitive-bias-in-medical-language-models-samuel-schmidgall-et-al-2024>(1/39 | 63/252) Addressing cognitive bias in medical language models (Samuel Schmidgall et al., 2024)</a></li><li><a href=#239--64252-large-language-models-ad-referendum-how-good-are-they-at-machine-translation-in-the-legal-domain-vicent-briva-iglesias-et-al-2024>(2/39 | 64/252) Large Language Models &lsquo;Ad Referendum&rsquo;: How Good Are They at Machine Translation in the Legal Domain? (Vicent Briva-Iglesias et al., 2024)</a></li><li><a href=#339--65252-suppressing-pink-elephants-with-direct-principle-feedback-louis-castricato-et-al-2024>(3/39 | 65/252) Suppressing Pink Elephants with Direct Principle Feedback (Louis Castricato et al., 2024)</a></li><li><a href=#439--66252-lissard-long-and-simple-sequential-reasoning-datasets-mirelle-bueno-et-al-2024>(4/39 | 66/252) Lissard: Long and Simple Sequential Reasoning Datasets (Mirelle Bueno et al., 2024)</a></li><li><a href=#539--67252-investigating-the-impact-of-data-contamination-of-large-language-models-in-text-to-sql-translation-federico-ranaldi-et-al-2024>(5/39 | 67/252) Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation (Federico Ranaldi et al., 2024)</a></li><li><a href=#639--68252-the-sound-of-healthcare-improving-medical-transcription-asr-accuracy-with-large-language-models-ayo-adedeji-et-al-2024>(6/39 | 68/252) The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models (Ayo Adedeji et al., 2024)</a></li><li><a href=#739--69252-can-llms-produce-faithful-explanations-for-fact-checking-towards-faithful-explainable-fact-checking-via-multi-agent-debate-kyungha-kim-et-al-2024>(7/39 | 69/252) Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate (Kyungha Kim et al., 2024)</a></li><li><a href=#839--70252-dólares-or-dollars-unraveling-the-bilingual-prowess-of-financial-llms-between-spanish-and-english-xiao-zhang-et-al-2024>(8/39 | 70/252) Dólares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English (Xiao Zhang et al., 2024)</a></li><li><a href=#939--71252-chain-of-layer-iteratively-prompting-large-language-models-for-taxonomy-induction-from-limited-examples-qingkai-zeng-et-al-2024>(9/39 | 71/252) Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy Induction from Limited Examples (Qingkai Zeng et al., 2024)</a></li><li><a href=#1039--72252-enhancing-amharic-llama-integrating-task-specific-and-generative-datasets-israel-abebe-azime-et-al-2024>(10/39 | 72/252) Enhancing Amharic-LLaMA: Integrating Task Specific and Generative Datasets (Israel Abebe Azime et al., 2024)</a></li><li><a href=#1139--73252-injecting-wiktionary-to-improve-token-level-contextual-representations-using-contrastive-learning-anna-mosolova-et-al-2024>(11/39 | 73/252) Injecting Wiktionary to improve token-level contextual representations using contrastive learning (Anna Mosolova et al., 2024)</a></li><li><a href=#1239--74252-automathtext-autonomous-data-selection-with-language-models-for-mathematical-texts-yifan-zhang-et-al-2024>(12/39 | 74/252) AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts (Yifan Zhang et al., 2024)</a></li><li><a href=#1339--75252-anchor-based-large-language-models-jianhui-pang-et-al-2024>(13/39 | 75/252) Anchor-based Large Language Models (Jianhui Pang et al., 2024)</a></li><li><a href=#1439--76252-pushing-the-limit-of-llm-capacity-for-text-classification-yazhou-zhang-et-al-2024>(14/39 | 76/252) Pushing The Limit of LLM Capacity for Text Classification (Yazhou Zhang et al., 2024)</a></li><li><a href=#1539--77252-large-language-models-as-agents-in-two-player-games-yang-liu-et-al-2024>(15/39 | 77/252) Large Language Models as Agents in Two-Player Games (Yang Liu et al., 2024)</a></li><li><a href=#1639--78252-aya-model-an-instruction-finetuned-open-access-multilingual-language-model-ahmet-üstün-et-al-2024>(16/39 | 78/252) Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model (Ahmet Üstün et al., 2024)</a></li><li><a href=#1739--79252-teller-a-trustworthy-framework-for-explainable-generalizable-and-controllable-fake-news-detection-hui-liu-et-al-2024>(17/39 | 79/252) TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection (Hui Liu et al., 2024)</a></li><li><a href=#1839--80252-detecting-the-clinical-features-of-difficult-to-treat-depression-using-synthetic-data-from-large-language-models-isabelle-lorge-et-al-2024>(18/39 | 80/252) Detecting the Clinical Features of Difficult-to-Treat Depression using Synthetic Data from Large Language Models (Isabelle Lorge et al., 2024)</a></li><li><a href=#1939--81252-step-on-feet-tuning-scaling-self-alignment-of-llms-via-bootstrapping-haoyu-wang-et-al-2024>(19/39 | 81/252) Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping (Haoyu Wang et al., 2024)</a></li><li><a href=#2039--82252-mafia-multi-adapter-fused-inclusive-language-models-prachi-jain-et-al-2024>(20/39 | 82/252) MAFIA: Multi-Adapter Fused Inclusive LanguAge Models (Prachi Jain et al., 2024)</a></li><li><a href=#2139--83252-retrieval-augmented-thought-process-as-sequential-decision-making-thomas-pouplin-et-al-2024>(21/39 | 83/252) Retrieval-Augmented Thought Process as Sequential Decision Making (Thomas Pouplin et al., 2024)</a></li><li><a href=#2239--84252-unsupervised-sign-language-translation-and-generation-zhengsheng-guo-et-al-2024>(22/39 | 84/252) Unsupervised Sign Language Translation and Generation (Zhengsheng Guo et al., 2024)</a></li><li><a href=#2339--85252-topic-modeling-as-multi-objective-contrastive-optimization-thong-nguyen-et-al-2024>(23/39 | 85/252) Topic Modeling as Multi-Objective Contrastive Optimization (Thong Nguyen et al., 2024)</a></li><li><a href=#2439--86252-quality-does-matter-a-detailed-look-at-the-quality-and-utility-of-web-mined-parallel-corpora-surangika-ranathunga-et-al-2024>(24/39 | 86/252) Quality Does Matter: A Detailed Look at the Quality and Utility of Web-Mined Parallel Corpora (Surangika Ranathunga et al., 2024)</a></li><li><a href=#2539--87252-refined-direct-preference-optimization-with-synthetic-data-for-behavioral-alignment-of-llms-víctor-gallego-2024>(25/39 | 87/252) Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs (Víctor Gallego, 2024)</a></li><li><a href=#2639--88252-show-me-how-its-done-the-role-of-explanations-in-fine-tuning-language-models-mohamad-ballout-et-al-2024>(26/39 | 88/252) Show Me How It&rsquo;s Done: The Role of Explanations in Fine-Tuning Language Models (Mohamad Ballout et al., 2024)</a></li><li><a href=#2739--89252-the-balancing-act-unmasking-and-alleviating-asr-biases-in-portuguese-ajinkya-kulkarni-et-al-2024>(27/39 | 89/252) The Balancing Act: Unmasking and Alleviating ASR Biases in Portuguese (Ajinkya Kulkarni et al., 2024)</a></li><li><a href=#2839--90252-asking-multimodal-clarifying-questions-in-mixed-initiative-conversational-search-yifei-yuan-et-al-2024>(28/39 | 90/252) Asking Multimodal Clarifying Questions in Mixed-Initiative Conversational Search (Yifei Yuan et al., 2024)</a></li><li><a href=#2939--91252-do-membership-inference-attacks-work-on-large-language-models-michael-duan-et-al-2024>(29/39 | 91/252) Do Membership Inference Attacks Work on Large Language Models? (Michael Duan et al., 2024)</a></li><li><a href=#3039--92252-auxiliary-tasks-to-boost-biaffine-semantic-dependency-parsing-marie-candito-2024>(30/39 | 92/252) Auxiliary Tasks to Boost Biaffine Semantic Dependency Parsing (Marie Candito, 2024)</a></li><li><a href=#3139--93252-multi-intent-attribute-aware-text-matching-in-searching-mingzhe-li-et-al-2024>(31/39 | 93/252) Multi-Intent Attribute-Aware Text Matching in Searching (Mingzhe Li et al., 2024)</a></li><li><a href=#3239--94252-text-detoxification-as-style-transfer-in-english-and-hindi-sourabrata-mukherjee-et-al-2024>(32/39 | 94/252) Text Detoxification as Style Transfer in English and Hindi (Sourabrata Mukherjee et al., 2024)</a></li><li><a href=#3339--95252-intrinsic-task-based-evaluation-for-referring-expression-generation-guanyi-chen-et-al-2024>(33/39 | 95/252) Intrinsic Task-based Evaluation for Referring Expression Generation (Guanyi Chen et al., 2024)</a></li><li><a href=#3439--96252-salad-smart-ai-language-assistant-daily-ragib-amin-nihal-et-al-2024>(34/39 | 96/252) SALAD: Smart AI Language Assistant Daily (Ragib Amin Nihal et al., 2024)</a></li><li><a href=#3539--97252-label-efficient-model-selection-for-text-generation-shir-ashury-tahan-et-al-2024>(35/39 | 97/252) Label-Efficient Model Selection for Text Generation (Shir Ashury-Tahan et al., 2024)</a></li><li><a href=#3639--98252-a-systematic-investigation-of-learnability-from-single-child-linguistic-input-yulu-qin-et-al-2024>(36/39 | 98/252) A systematic investigation of learnability from single child linguistic input (Yulu Qin et al., 2024)</a></li><li><a href=#3739--99252-diffusion-of-thoughts-chain-of-thought-reasoning-in-diffusion-language-models-jiacheng-ye-et-al-2024>(37/39 | 99/252) Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models (Jiacheng Ye et al., 2024)</a></li><li><a href=#3839--100252-orderbkd-textual-backdoor-attack-through-repositioning-irina-alekseevskaia-et-al-2024>(38/39 | 100/252) OrderBkd: Textual backdoor attack through repositioning (Irina Alekseevskaia et al., 2024)</a></li><li><a href=#3939--101252-araspider-democratizing-arabic-to-sql-ahmed-heakl-et-al-2024>(39/39 | 101/252) AraSpider: Democratizing Arabic-to-SQL (Ahmed Heakl et al., 2024)</a></li></ul></li><li><a href=#csro-8>cs.RO (8)</a><ul><li><a href=#18--102252-pivot-iterative-visual-prompting-elicits-actionable-knowledge-for-vlms-soroush-nasiriany-et-al-2024>(1/8 | 102/252) PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs (Soroush Nasiriany et al., 2024)</a></li><li><a href=#28--103252-customizable-perturbation-synthesis-for-robust-slam-benchmarking-xiaohao-xu-et-al-2024>(2/8 | 103/252) Customizable Perturbation Synthesis for Robust SLAM Benchmarking (Xiaohao Xu et al., 2024)</a></li><li><a href=#38--104252-deformnet-latent-space-modeling-and-dynamics-prediction-for-deformable-object-manipulation-chenchang-li-et-al-2024>(3/8 | 104/252) DeformNet: Latent Space Modeling and Dynamics Prediction for Deformable Object Manipulation (Chenchang Li et al., 2024)</a></li><li><a href=#48--105252-dart-a-compact-platform-for-autonomous-driving-research-lorenzo-lyons-et-al-2024>(4/8 | 105/252) DART: A Compact Platform For Autonomous Driving Research (Lorenzo Lyons et al., 2024)</a></li><li><a href=#58--106252-digital-twins-below-the-surface-enhancing-underwater-teleoperation-favour-o-adetunji-et-al-2024>(5/8 | 106/252) Digital Twins Below the Surface: Enhancing Underwater Teleoperation (Favour O. Adetunji et al., 2024)</a></li><li><a href=#68--107252-extending-3d-body-pose-estimation-for-robotic-assistive-therapies-of-autistic-children-laura-santos-et-al-2024>(6/8 | 107/252) Extending 3D body pose estimation for robotic-assistive therapies of autistic children (Laura Santos et al., 2024)</a></li><li><a href=#78--108252-evaluation-of-a-smart-mobile-robotic-system-for-industrial-plant-inspection-and-supervision-georg-k-j-fischer-et-al-2024>(7/8 | 108/252) Evaluation of a Smart Mobile Robotic System for Industrial Plant Inspection and Supervision (Georg K. J. Fischer et al., 2024)</a></li><li><a href=#88--109252-uav-assisted-visual-slam-generating-reconstructed-3d-scene-graphs-in-gps-denied-environments-ahmed-radwan-et-al-2024>(8/8 | 109/252) UAV-assisted Visual SLAM Generating Reconstructed 3D Scene Graphs in GPS-denied Environments (Ahmed Radwan et al., 2024)</a></li></ul></li><li><a href=#csai-22>cs.AI (22)</a><ul><li><a href=#122--110252-t-rag-lessons-from-the-llm-trenches-masoomali-fatehkia-et-al-2024>(1/22 | 110/252) T-RAG: Lessons from the LLM Trenches (Masoomali Fatehkia et al., 2024)</a></li><li><a href=#222--111252-on-the-self-verification-limitations-of-large-language-models-on-reasoning-and-planning-tasks-kaya-stechly-et-al-2024>(2/22 | 111/252) On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks (Kaya Stechly et al., 2024)</a></li><li><a href=#322--112252-cybermetric-a-benchmark-dataset-for-evaluating-large-language-models-knowledge-in-cybersecurity-norbert-tihanyi-et-al-2024>(3/22 | 112/252) CyberMetric: A Benchmark Dataset for Evaluating Large Language Models Knowledge in Cybersecurity (Norbert Tihanyi et al., 2024)</a></li><li><a href=#422--113252-semtra-a-semantic-skill-translator-for-cross-domain-zero-shot-policy-adaptation-sangwoo-shin-et-al-2024>(4/22 | 113/252) SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy Adaptation (Sangwoo Shin et al., 2024)</a></li><li><a href=#522--114252-breakgpt-a-large-language-model-with-multi-stage-structure-for-financial-breakout-detection-kang-zhang-et-al-2024>(5/22 | 114/252) BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection (Kang Zhang et al., 2024)</a></li><li><a href=#622--115252-secret-collusion-among-generative-ai-agents-sumeet-ramesh-motwani-et-al-2024>(6/22 | 115/252) Secret Collusion Among Generative AI Agents (Sumeet Ramesh Motwani et al., 2024)</a></li><li><a href=#722--116252-enhancing-multi-criteria-decision-analysis-with-ai-integrating-analytic-hierarchy-process-and-gpt-4-for-automated-decision-support-igor-svoboda-et-al-2024>(7/22 | 116/252) Enhancing Multi-Criteria Decision Analysis with AI: Integrating Analytic Hierarchy Process and GPT-4 for Automated Decision Support (Igor Svoboda et al., 2024)</a></li><li><a href=#822--117252-extensible-multi-granularity-fusion-network-for-aspect-based-sentiment-analysis-xiaowei-zhao-et-al-2024>(8/22 | 117/252) Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis (Xiaowei Zhao et al., 2024)</a></li><li><a href=#922--118252-vislinginstruct-elevating-zero-shot-learning-in-multi-modal-language-models-with-autonomous-instruction-optimization-dongsheng-zhu-et-al-2024>(9/22 | 118/252) VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization (Dongsheng Zhu et al., 2024)</a></li><li><a href=#1022--119252-towards-unified-alignment-between-agents-humans-and-environment-zonghan-yang-et-al-2024>(10/22 | 119/252) Towards Unified Alignment Between Agents, Humans, and Environment (Zonghan Yang et al., 2024)</a></li><li><a href=#1122--120252-out-of-distribution-detection-and-data-drift-monitoring-using-statistical-process-control-ghada-zamzmi-et-al-2024>(11/22 | 120/252) Out-of-Distribution Detection and Data Drift Monitoring using Statistical Process Control (Ghada Zamzmi et al., 2024)</a></li><li><a href=#1222--121252-beyond-llms-advancing-the-landscape-of-complex-reasoning-jennifer-chu-carroll-et-al-2024>(12/22 | 121/252) Beyond LLMs: Advancing the Landscape of Complex Reasoning (Jennifer Chu-Carroll et al., 2024)</a></li><li><a href=#1322--122252-maidcrl-semi-centralized-multi-agent-influence-dense-cnn-reinforcement-learning-ayesha-siddika-nipu-et-al-2024>(13/22 | 122/252) MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning (Ayesha Siddika Nipu et al., 2024)</a></li><li><a href=#1422--123252-food-recommendation-as-language-processing-f-rlp-a-personalized-and-contextual-paradigm-ali-rostami-et-al-2024>(14/22 | 123/252) Food Recommendation as Language Processing (F-RLP): A Personalized and Contextual Paradigm (Ali Rostami et al., 2024)</a></li><li><a href=#1522--124252-game-agent-driven-by-free-form-text-command-using-llm-based-code-generation-and-behavior-branch-ray-ito-et-al-2024>(15/22 | 124/252) Game Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and Behavior Branch (Ray Ito et al., 2024)</a></li><li><a href=#1622--125252-os-copilot-towards-generalist-computer-agents-with-self-improvement-zhiyong-wu-et-al-2024>(16/22 | 125/252) OS-Copilot: Towards Generalist Computer Agents with Self-Improvement (Zhiyong Wu et al., 2024)</a></li><li><a href=#1722--126252-recursive-joint-simulation-in-games-vojtech-kovarik-et-al-2024>(17/22 | 126/252) Recursive Joint Simulation in Games (Vojtech Kovarik et al., 2024)</a></li><li><a href=#1822--127252-wildfiregpt-tailored-large-language-model-for-wildfire-analysis-yangxinyu-xie-et-al-2024>(18/22 | 127/252) WildfireGPT: Tailored Large Language Model for Wildfire Analysis (Yangxinyu Xie et al., 2024)</a></li><li><a href=#1922--128252-generalising-planning-environment-redesign-alberto-pozanco-et-al-2024>(19/22 | 128/252) Generalising Planning Environment Redesign (Alberto Pozanco et al., 2024)</a></li><li><a href=#2022--129252-end-to-end-learning-for-fair-multiobjective-optimization-under-uncertainty-my-h-dinh-et-al-2024>(20/22 | 129/252) End-to-End Learning for Fair Multiobjective Optimization Under Uncertainty (My H Dinh et al., 2024)</a></li><li><a href=#2122--130252-news-recommendation-with-attention-mechanism-tianrui-liu-et-al-2024>(21/22 | 130/252) News Recommendation with Attention Mechanism (Tianrui Liu et al., 2024)</a></li><li><a href=#2222--131252-clustering-dynamics-for-improved-speed-prediction-deriving-from-topographical-gps-registrations-sarah-almeida-carneiro-et-al-2024>(22/22 | 131/252) Clustering Dynamics for Improved Speed Prediction Deriving from Topographical GPS Registrations (Sarah Almeida Carneiro et al., 2024)</a></li></ul></li><li><a href=#csar-3>cs.AR (3)</a><ul><li><a href=#13--132252-ir-aware-eco-timing-optimization-using-reinforcement-learning-vidya-a-chhabria-et-al-2024>(1/3 | 132/252) IR-Aware ECO Timing Optimization Using Reinforcement Learning (Vidya A. Chhabria et al., 2024)</a></li><li><a href=#23--133252-lfoc-a-fair-os-level-cache-clustering-policy-for-commodity-multicore-systems-juan-carlos-saez-et-al-2024>(2/3 | 133/252) LFOC+: A Fair OS-level Cache-Clustering Policy for Commodity Multicore Systems (Juan Carlos Saez et al., 2024)</a></li><li><a href=#33--134252-a-precision-optimized-fixed-point-near-memory-digital-processing-unit-for-analog-in-memory-computing-elena-ferro-et-al-2024>(3/3 | 134/252) A Precision-Optimized Fixed-Point Near-Memory Digital Processing Unit for Analog In-Memory Computing (Elena Ferro et al., 2024)</a></li></ul></li><li><a href=#cscr-9>cs.CR (9)</a><ul><li><a href=#19--135252-large-language-models-are-few-shot-generators-proposing-hybrid-prompt-algorithm-to-generate-webshell-escape-samples-mingrui-ma-et-al-2024>(1/9 | 135/252) Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples (Mingrui Ma et al., 2024)</a></li><li><a href=#29--136252-poisonedrag-knowledge-poisoning-attacks-to-retrieval-augmented-generation-of-large-language-models-wei-zou-et-al-2024>(2/9 | 136/252) PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models (Wei Zou et al., 2024)</a></li><li><a href=#39--137252-resilient-watermarking-for-llm-generated-codes-boquan-li-et-al-2024>(3/9 | 137/252) Resilient Watermarking for LLM-Generated Codes (Boquan Li et al., 2024)</a></li><li><a href=#49--138252-adaptive-artificial-immune-networks-for-mitigating-dos-flooding-attacks-jorge-maestre-vidal-et-al-2024>(4/9 | 138/252) Adaptive Artificial Immune Networks for Mitigating DoS flooding Attacks (Jorge Maestre Vidal et al., 2024)</a></li><li><a href=#59--139252-utilizing-large-languagemodels-to-detect-privacy-leaks-in-mini-app-code-liming-jiang-2024>(5/9 | 139/252) Utilizing Large LanguageModels to Detect Privacy Leaks in Mini-App Code (Liming Jiang, 2024)</a></li><li><a href=#69--140252-game-of-trojans-adaptive-adversaries-against-output-based-trojaned-model-detectors-dinuka-sahabandu-et-al-2024>(6/9 | 140/252) Game of Trojans: Adaptive Adversaries Against Output-based Trojaned-Model Detectors (Dinuka Sahabandu et al., 2024)</a></li><li><a href=#79--141252-discovering-universal-semantic-triggers-for-text-to-image-synthesis-shengfang-zhai-et-al-2024>(7/9 | 141/252) Discovering Universal Semantic Triggers for Text-to-Image Synthesis (Shengfang Zhai et al., 2024)</a></li><li><a href=#89--142252-malicious-package-detection-using-metadata-information-s-halder-et-al-2024>(8/9 | 142/252) Malicious Package Detection using Metadata Information (S. Halder et al., 2024)</a></li><li><a href=#99--143252-using-graph-theory-for-improving-machine-learning-based-detection-of-cyber-attacks-giacomo-zonneveld-et-al-2024>(9/9 | 143/252) Using Graph Theory for Improving Machine Learning-based Detection of Cyber Attacks (Giacomo Zonneveld et al., 2024)</a></li></ul></li><li><a href=#cscv-27>cs.CV (27)</a><ul><li><a href=#127--144252-multi-attribute-vision-transformers-are-efficient-and-robust-learners-hanan-gani-et-al-2024>(1/27 | 144/252) Multi-Attribute Vision Transformers are Efficient and Robust Learners (Hanan Gani et al., 2024)</a></li><li><a href=#227--145252-aydiv-adaptable-yielding-3d-object-detection-via-integrated-contextual-vision-transformer-tanmoy-dam-et-al-2024>(2/27 | 145/252) AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual Vision Transformer (Tanmoy Dam et al., 2024)</a></li><li><a href=#327--146252-modiphy-multimodal-obscured-detection-for-iot-using-phantom-convolution-enabled-faster-yolo-shubhabrata-mukherjee-et-al-2024>(3/27 | 146/252) MODIPHY: Multimodal Obscured Detection for IoT using PHantom Convolution-Enabled Faster YOLO (Shubhabrata Mukherjee et al., 2024)</a></li><li><a href=#427--147252-lumos--empowering-multimodal-llms-with-scene-text-recognition-ashish-shenoy-et-al-2024>(4/27 | 147/252) Lumos : Empowering Multimodal LLMs with Scene Text Recognition (Ashish Shenoy et al., 2024)</a></li><li><a href=#527--148252-beyond-the-mud-datasets-and-benchmarks-for-computer-vision-in-off-road-racing-jacob-tyo-et-al-2024>(5/27 | 148/252) Beyond the Mud: Datasets and Benchmarks for Computer Vision in Off-Road Racing (Jacob Tyo et al., 2024)</a></li><li><a href=#627--149252-unmasking-honey-adulteration--a-breakthrough-in-quality-assurance-through-cutting-edge-convolutional-neural-network-analysis-of-thermal-images-ilias-boulbarj-et-al-2024>(6/27 | 149/252) Unmasking honey adulteration : a breakthrough in quality assurance through cutting-edge convolutional neural network analysis of thermal images (Ilias Boulbarj et al., 2024)</a></li><li><a href=#727--150252-contrastive-multiple-instance-learning-for-weakly-supervised-person-reid-jacob-tyo-et-al-2024>(7/27 | 150/252) Contrastive Multiple Instance Learning for Weakly Supervised Person ReID (Jacob Tyo et al., 2024)</a></li><li><a href=#827--151252-unsupervised-discovery-of-object-centric-neural-fields-rundong-luo-et-al-2024>(8/27 | 151/252) Unsupervised Discovery of Object-Centric Neural Fields (Rundong Luo et al., 2024)</a></li><li><a href=#927--152252-real-world-atmospheric-turbulence-correction-via-domain-adaptation-xijun-wang-et-al-2024>(9/27 | 152/252) Real-World Atmospheric Turbulence Correction via Domain Adaptation (Xijun Wang et al., 2024)</a></li><li><a href=#1027--153252-multiple-random-masking-autoencoder-ensembles-for-robust-multimodal-semi-supervised-learning-alexandru-raul-todoran-et-al-2024>(10/27 | 153/252) Multiple Random Masking Autoencoder Ensembles for Robust Multimodal Semi-supervised Learning (Alexandru-Raul Todoran et al., 2024)</a></li><li><a href=#1127--154252-exploring-perceptual-limitation-of-multimodal-large-language-models-jiarui-zhang-et-al-2024>(11/27 | 154/252) Exploring Perceptual Limitation of Multimodal Large Language Models (Jiarui Zhang et al., 2024)</a></li><li><a href=#1227--155252-towards-meta-pruning-via-optimal-transport-alexander-theus-et-al-2024>(12/27 | 155/252) Towards Meta-Pruning via Optimal Transport (Alexander Theus et al., 2024)</a></li><li><a href=#1327--156252-a-benchmark-grocery-dataset-of-realworld-point-clouds-from-single-view-shivanand-venkanna-sheshappanavar-et-al-2024>(13/27 | 156/252) A Benchmark Grocery Dataset of Realworld Point Clouds From Single View (Shivanand Venkanna Sheshappanavar et al., 2024)</a></li><li><a href=#1427--157252-wavefront-randomization-improves-deconvolution-amit-kohli-et-al-2024>(14/27 | 157/252) Wavefront Randomization Improves Deconvolution (Amit Kohli et al., 2024)</a></li><li><a href=#1527--158252-prismatic-vlms-investigating-the-design-space-of-visually-conditioned-language-models-siddharth-karamcheti-et-al-2024>(15/27 | 158/252) Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models (Siddharth Karamcheti et al., 2024)</a></li><li><a href=#1627--159252-complete-instances-mining-for-weakly-supervised-instance-segmentation-zecheng-li-et-al-2024>(16/27 | 159/252) Complete Instances Mining for Weakly Supervised Instance Segmentation (Zecheng Li et al., 2024)</a></li><li><a href=#1727--160252-an-empirical-study-into-what-matters-for-calibrating-vision-language-models-weijie-tu-et-al-2024>(17/27 | 160/252) An Empirical Study Into What Matters for Calibrating Vision-Language Models (Weijie Tu et al., 2024)</a></li><li><a href=#1827--161252-selfswapper-self-supervised-face-swapping-via-shape-agnostic-masked-autoencoder-jaeseong-lee-et-al-2024>(18/27 | 161/252) SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder (Jaeseong Lee et al., 2024)</a></li><li><a href=#1927--162252-task-conditioned-adaptation-of-visual-features-in-multi-task-policy-learning-pierre-marza-et-al-2024>(19/27 | 162/252) Task-conditioned adaptation of visual features in multi-task policy learning (Pierre Marza et al., 2024)</a></li><li><a href=#2027--163252-detection-of-spider-mites-on-labrador-beans-through-machine-learning-approaches-using-custom-datasets-violet-liu-et-al-2024>(20/27 | 163/252) Detection of Spider Mites on Labrador Beans through Machine Learning Approaches Using Custom Datasets (Violet Liu et al., 2024)</a></li><li><a href=#2127--164252-a-flow-based-credibility-metric-for-safety-critical-pedestrian-detection-maria-lyssenko-et-al-2024>(21/27 | 164/252) A Flow-based Credibility Metric for Safety-critical Pedestrian Detection (Maria Lyssenko et al., 2024)</a></li><li><a href=#2227--165252-sheet-music-transformer-end-to-end-optical-music-recognition-beyond-monophonic-transcription-antonio-ríos-vila-et-al-2024>(22/27 | 165/252) Sheet Music Transformer: End-To-End Optical Music Recognition Beyond Monophonic Transcription (Antonio Ríos-Vila et al., 2024)</a></li><li><a href=#2327--166252-triaug-out-of-distribution-detection-for-robust-classification-of-imbalanced-breast-lesion-in-ultrasound-yinyu-ye-et-al-2024>(23/27 | 166/252) TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound (Yinyu Ye et al., 2024)</a></li><li><a href=#2427--167252-a-closer-look-at-the-robustness-of-contrastive-language-image-pre-training-clip-weijie-tu-et-al-2024>(24/27 | 167/252) A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP) (Weijie Tu et al., 2024)</a></li><li><a href=#2527--168252-make-it-more-specific-a-novel-uncertainty-based-airway-segmentation-application-on-3d-u-net-and-its-variants-shiyi-wang-et-al-2024>(25/27 | 168/252) Make it more specific: A novel uncertainty based airway segmentation application on 3D U-Net and its variants (Shiyi Wang et al., 2024)</a></li><li><a href=#2627--169252-exploring-saliency-bias-in-manipulation-detection-joshua-krinsky-et-al-2024>(26/27 | 169/252) Exploring Saliency Bias in Manipulation Detection (Joshua Krinsky et al., 2024)</a></li><li><a href=#2727--170252-gbot-graph-based-3d-object-tracking-for-augmented-reality-assisted-assembly-guidance-shiyu-li-et-al-2024>(27/27 | 170/252) GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted Assembly Guidance (Shiyu Li et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--171252-developing-a-multi-variate-prediction-model-for-covid-19-from-crowd-sourced-respiratory-voice-data-yuyang-yan-et-al-2024>(1/2 | 171/252) Developing a Multi-variate Prediction Model For COVID-19 From Crowd-sourced Respiratory Voice Data (Yuyang Yan et al., 2024)</a></li><li><a href=#22--172252-slit-boosting-audio-text-pre-training-via-multi-stage-learning-and-instruction-tuning-hang-zhao-et-al-2024>(2/2 | 172/252) SLIT: Boosting Audio-Text Pre-Training via Multi-Stage Learning and Instruction Tuning (Hang Zhao et al., 2024)</a></li></ul></li><li><a href=#eessas-3>eess.AS (3)</a><ul><li><a href=#13--173252-air-bench-benchmarking-large-audio-language-models-via-generative-comprehension-qian-yang-et-al-2024>(1/3 | 173/252) AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension (Qian Yang et al., 2024)</a></li><li><a href=#23--174252-making-flow-matching-based-zero-shot-text-to-speech-laugh-as-you-like-naoyuki-kanda-et-al-2024>(2/3 | 174/252) Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like (Naoyuki Kanda et al., 2024)</a></li><li><a href=#33--175252-interactive-singing-melody-extraction-based-on-active-adaptation-kavya-ranjan-saxena-et-al-2024>(3/3 | 175/252) Interactive singing melody extraction based on active adaptation (Kavya Ranjan Saxena et al., 2024)</a></li></ul></li><li><a href=#csmm-2>cs.MM (2)</a><ul><li><a href=#12--176252-bdiqa-a-new-dataset-for-video-question-answering-to-explore-cognitive-reasoning-through-theory-of-mind-yuanyuan-mao-et-al-2024>(1/2 | 176/252) BDIQA: A New Dataset for Video Question Answering to Explore Cognitive Reasoning through Theory of Mind (Yuanyuan Mao et al., 2024)</a></li><li><a href=#22--177252-synthesizing-sentiment-controlled-feedback-for-multimodal-text-and-image-data-puneet-kumar-et-al-2024>(2/2 | 177/252) Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data (Puneet Kumar et al., 2024)</a></li></ul></li><li><a href=#csir-8>cs.IR (8)</a><ul><li><a href=#18--178252-multi-behavior-collaborative-filtering-with-partial-order-graph-convolutional-networks-yijie-zhang-et-al-2024>(1/8 | 178/252) Multi-Behavior Collaborative Filtering with Partial Order Graph Convolutional Networks (Yijie Zhang et al., 2024)</a></li><li><a href=#28--179252-grillbot-in-practice-lessons-and-tradeoffs-deploying-large-language-models-for-adaptable-conversational-task-assistants-sophie-fischer-et-al-2024>(2/8 | 179/252) GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants (Sophie Fischer et al., 2024)</a></li><li><a href=#38--180252-quantitative-knowledge-retrieval-from-large-language-models-david-selby-et-al-2024>(3/8 | 180/252) Quantitative knowledge retrieval from large language models (David Selby et al., 2024)</a></li><li><a href=#48--181252-benchmarking-and-building-long-context-retrieval-models-with-loco-and-m2-bert-jon-saad-falcon-et-al-2024>(4/8 | 181/252) Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT (Jon Saad-Falcon et al., 2024)</a></li><li><a href=#58--182252-vcr-video-representation-for-contextual-retrieval-oron-nir-et-al-2024>(5/8 | 182/252) VCR: Video representation for Contextual Retrieval (Oron Nir et al., 2024)</a></li><li><a href=#68--183252-debiasing-recommendation-with-personal-popularity-wentao-ning-et-al-2024>(6/8 | 183/252) Debiasing Recommendation with Personal Popularity (Wentao Ning et al., 2024)</a></li><li><a href=#78--184252-multimodal-learned-sparse-retrieval-for-image-suggestion-thong-nguyen-et-al-2024>(7/8 | 184/252) Multimodal Learned Sparse Retrieval for Image Suggestion (Thong Nguyen et al., 2024)</a></li><li><a href=#88--185252-utilizing-low-dimensional-molecular-embeddings-for-rapid-chemical-similarity-search-kathryn-e-kirchoff-et-al-2024>(8/8 | 185/252) Utilizing Low-Dimensional Molecular Embeddings for Rapid Chemical Similarity Search (Kathryn E. Kirchoff et al., 2024)</a></li></ul></li><li><a href=#cscy-4>cs.CY (4)</a><ul><li><a href=#14--186252-ai-augmented-predictions-llm-assistants-improve-human-forecasting-accuracy-philipp-schoenegger-et-al-2024>(1/4 | 186/252) AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy (Philipp Schoenegger et al., 2024)</a></li><li><a href=#24--187252-leveraging-ai-to-advance-science-and-computing-education-across-africa-progress-challenges-and-opportunities-george-boateng-2024>(2/4 | 187/252) Leveraging AI to Advance Science and Computing Education across Africa: Progress, Challenges, and Opportunities (George Boateng, 2024)</a></li><li><a href=#34--188252-auditing-work-exploring-the-new-york-city-algorithmic-bias-audit-regime-lara-groves-et-al-2024>(3/4 | 188/252) Auditing Work: Exploring the New York City algorithmic bias audit regime (Lara Groves et al., 2024)</a></li><li><a href=#44--189252-algorithmic-fairness-and-color-blind-racism-navigating-the-intersection-jamelle-watson-daniels-2024>(4/4 | 189/252) Algorithmic Fairness and Color-blind Racism: Navigating the Intersection (Jamelle Watson-Daniels, 2024)</a></li></ul></li><li><a href=#cshc-6>cs.HC (6)</a><ul><li><a href=#16--190252-enhancing-programming-error-messages-in-real-time-with-generative-ai-bailey-kimmel-et-al-2024>(1/6 | 190/252) Enhancing Programming Error Messages in Real Time with Generative AI (Bailey Kimmel et al., 2024)</a></li><li><a href=#26--191252-why-and-when-llm-based-assistants-can-go-wrong-investigating-the-effectiveness-of-prompt-based-interactions-for-software-help-seeking-anjali-khurana-et-al-2024>(2/6 | 191/252) Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking (Anjali Khurana et al., 2024)</a></li><li><a href=#36--192252-imagining-a-future-of-designing-with-ai-dynamic-grounding-constructive-negotiation-and-sustainable-motivation-priyan-vaithilingam-et-al-2024>(3/6 | 192/252) Imagining a Future of Designing with AI: Dynamic Grounding, Constructive Negotiation, and Sustainable Motivation (Priyan Vaithilingam et al., 2024)</a></li><li><a href=#46--193252-portobello-extending-driving-simulation-from-the-lab-to-the-road-fanjun-bu-et-al-2024>(4/6 | 193/252) Portobello: Extending Driving Simulation from the Lab to the Road (Fanjun Bu et al., 2024)</a></li><li><a href=#56--194252-pkg-api-a-tool-for-personal-knowledge-graph-management-nolwenn-bernard-et-al-2024>(5/6 | 194/252) PKG API: A Tool for Personal Knowledge Graph Management (Nolwenn Bernard et al., 2024)</a></li><li><a href=#66--195252-renelib-real-time-neural-listening-behavior-generation-for-socially-interactive-agents-daksitha-withanage-don-et-al-2024>(6/6 | 195/252) ReNeLiB: Real-time Neural Listening Behavior Generation for Socially Interactive Agents (Daksitha Withanage Don et al., 2024)</a></li></ul></li><li><a href=#eessiv-6>eess.IV (6)</a><ul><li><a href=#16--196252-comparative-analysis-of-imagenet-pre-trained-deep-learning-models-and-dinov2-in-medical-imaging-classification-yuning-huang-et-al-2024>(1/6 | 196/252) Comparative Analysis of ImageNet Pre-Trained Deep Learning Models and DINOv2 in Medical Imaging Classification (Yuning Huang et al., 2024)</a></li><li><a href=#26--197252-weakly-supervised-detection-of-pheochromocytomas-and-paragangliomas-in-ct-david-c-oluigboa-et-al-2024>(2/6 | 197/252) Weakly Supervised Detection of Pheochromocytomas and Paragangliomas in CT (David C. Oluigboa et al., 2024)</a></li><li><a href=#36--198252-automated-classification-of-body-mri-sequence-type-using-convolutional-neural-networks-kimberly-helm-et-al-2024>(3/6 | 198/252) Automated Classification of Body MRI Sequence Type Using Convolutional Neural Networks (Kimberly Helm et al., 2024)</a></li><li><a href=#46--199252-minimally-interactive-segmentation-of-soft-tissue-tumors-on-ct-and-mri-using-deep-learning-douwe-j-spaanderman-et-al-2024>(4/6 | 199/252) Minimally Interactive Segmentation of Soft-Tissue Tumors on CT and MRI using Deep Learning (Douwe J. Spaanderman et al., 2024)</a></li><li><a href=#56--200252-re-diffinet-modeling-discrepancies-loss-in-tumor-segmentation-using-diffusion-models-tianyi-ren-et-al-2024>(5/6 | 200/252) Re-DiffiNet: Modeling discrepancies loss in tumor segmentation using diffusion models (Tianyi Ren et al., 2024)</a></li><li><a href=#66--201252-inference-stage-denoising-for-undersampled-mri-reconstruction-yuyang-xue-et-al-2024>(6/6 | 201/252) Inference Stage Denoising for Undersampled MRI Reconstruction (Yuyang Xue et al., 2024)</a></li></ul></li><li><a href=#q-biogn-1>q-bio.GN (1)</a><ul><li><a href=#11--202252-efficient-and-scalable-fine-tune-of-language-models-for-genome-understanding-huixin-zhan-et-al-2024>(1/1 | 202/252) Efficient and Scalable Fine-Tune of Language Models for Genome Understanding (Huixin Zhan et al., 2024)</a></li></ul></li><li><a href=#statml-7>stat.ML (7)</a><ul><li><a href=#17--203252-graph-structure-inference-with-bam-introducing-the-bilinear-attention-mechanism-philipp-froehlich-et-al-2024>(1/7 | 203/252) Graph Structure Inference with BAM: Introducing the Bilinear Attention Mechanism (Philipp Froehlich et al., 2024)</a></li><li><a href=#27--204252-stochastic-gradient-flow-dynamics-of-test-risk-and-its-exact-solution-for-weak-features-rodrigo-veiga-et-al-2024>(2/7 | 204/252) Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution for Weak Features (Rodrigo Veiga et al., 2024)</a></li><li><a href=#37--205252-diffeomorphic-measure-matching-with-kernels-for-generative-modeling-biraj-pandey-et-al-2024>(3/7 | 205/252) Diffeomorphic Measure Matching with Kernels for Generative Modeling (Biraj Pandey et al., 2024)</a></li><li><a href=#47--206252-noise-adaptive-confidence-sets-for-linear-bandits-and-application-to-bayesian-optimization-kwang-sung-jun-et-al-2024>(4/7 | 206/252) Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization (Kwang-Sung Jun et al., 2024)</a></li><li><a href=#57--207252-replicability-is-asymptotically-free-in-multi-armed-bandits-junpei-komiyama-et-al-2024>(5/7 | 207/252) Replicability is Asymptotically Free in Multi-armed Bandits (Junpei Komiyama et al., 2024)</a></li><li><a href=#67--208252-convergence-analysis-of-discrete-diffusion-model-exact-implementation-through-uniformization-hongrui-chen-et-al-2024>(6/7 | 208/252) Convergence Analysis of Discrete Diffusion Model: Exact Implementation through Uniformization (Hongrui Chen et al., 2024)</a></li><li><a href=#77--209252-top-k-ranking-with-a-monotone-adversary-yuepeng-yang-et-al-2024>(7/7 | 209/252) Top-$K$ ranking with a monotone adversary (Yuepeng Yang et al., 2024)</a></li></ul></li><li><a href=#eesssy-5>eess.SY (5)</a><ul><li><a href=#15--210252-distributed-anomaly-detection-in-modern-power-systems-a-penalty-based-mitigation-approach-erfan-mehdipour-abadi-et-al-2024>(1/5 | 210/252) Distributed Anomaly Detection in Modern Power Systems: A Penalty-based Mitigation Approach (Erfan Mehdipour Abadi et al., 2024)</a></li><li><a href=#25--211252-on-the-stability-of-undesirable-equilibria-in-the-quadratic-program-framework-for-safety-critical-control-matheus-f-reis-et-al-2024>(2/5 | 211/252) On the Stability of Undesirable Equilibria in the Quadratic Program Framework for Safety-Critical Control (Matheus F. Reis et al., 2024)</a></li><li><a href=#35--212252-correctness-verification-of-neural-networks-approximating-differential-equations-petros-ellinas-et-al-2024>(3/5 | 212/252) Correctness Verification of Neural Networks Approximating Differential Equations (Petros Ellinas et al., 2024)</a></li><li><a href=#45--213252-joint-user-and-beam-selection-in-millimeter-wave-networks-santosh-kumar-singh-et-al-2024>(4/5 | 213/252) Joint User and Beam Selection in Millimeter Wave Networks (Santosh Kumar Singh et al., 2024)</a></li><li><a href=#55--214252-conformal-predictive-programming-for-chance-constrained-optimization-yiqi-zhao-et-al-2024>(5/5 | 214/252) Conformal Predictive Programming for Chance Constrained Optimization (Yiqi Zhao et al., 2024)</a></li></ul></li><li><a href=#eesssp-2>eess.SP (2)</a><ul><li><a href=#12--215252-deciphering-heartbeat-signatures-a-vision-transformer-approach-to-explainable-atrial-fibrillation-detection-from-ecg-signals-aruna-mohan-et-al-2024>(1/2 | 215/252) Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals (Aruna Mohan et al., 2024)</a></li><li><a href=#22--216252-compressive-recovery-of-signals-defined-on-perturbed-graphs-sabyasachi-ghosh-et-al-2024>(2/2 | 216/252) Compressive Recovery of Signals Defined on Perturbed Graphs (Sabyasachi Ghosh et al., 2024)</a></li></ul></li><li><a href=#cssi-4>cs.SI (4)</a><ul><li><a href=#14--217252-comparing-the-willingness-to-share-for-human-generated-vs-ai-generated-fake-news-amirsiavosh-bashardoust-et-al-2024>(1/4 | 217/252) Comparing the willingness to share for human-generated vs. AI-generated fake news (Amirsiavosh Bashardoust et al., 2024)</a></li><li><a href=#24--218252-local-centrality-minimization-with-quality-guarantees-atsushi-miyauchi-et-al-2024>(2/4 | 218/252) Local Centrality Minimization with Quality Guarantees (Atsushi Miyauchi et al., 2024)</a></li><li><a href=#34--219252-higher-order-connection-laplacians-for-directed-simplicial-complexes-xue-gong-et-al-2024>(3/4 | 219/252) Higher-order Connection Laplacians for Directed Simplicial Complexes (Xue Gong et al., 2024)</a></li><li><a href=#44--220252-topic-aware-most-influential-community-search-in-social-networks-long-teng-et-al-2024>(4/4 | 220/252) Topic-aware Most Influential Community Search in Social Networks (Long Teng et al., 2024)</a></li></ul></li><li><a href=#csse-5>cs.SE (5)</a><ul><li><a href=#15--221252-mercury-an-efficiency-benchmark-for-llm-code-synthesis-mingzhe-du-et-al-2024>(1/5 | 221/252) Mercury: An Efficiency Benchmark for LLM Code Synthesis (Mingzhe Du et al., 2024)</a></li><li><a href=#25--222252-continuous-assurance-of-autonomous-vehicle-behavior-through-machine-learned-correctness-properties-matthew-litton-et-al-2024>(2/5 | 222/252) Continuous Assurance of Autonomous Vehicle Behavior Through Machine Learned Correctness Properties (Matthew Litton et al., 2024)</a></li><li><a href=#35--223252-interaction-based-driving-scenario-classification-and-labeling-cheng-chang-et-al-2024>(3/5 | 223/252) Interaction-Based Driving Scenario Classification and Labeling (Cheng Chang et al., 2024)</a></li><li><a href=#45--224252-using-ensemble-inference-to-improve-recall-of-clone-detection-gul-aftab-ahmed-et-al-2024>(4/5 | 224/252) Using Ensemble Inference to Improve Recall of Clone Detection (Gul Aftab Ahmed et al., 2024)</a></li><li><a href=#55--225252-asap-repair-api-specific-automated-program-repair-based-on-api-usage-graphs-sebastian-nielebock-et-al-2024>(5/5 | 225/252) ASAP-Repair: API-Specific Automated Program Repair Based on API Usage Graphs (Sebastian Nielebock et al., 2024)</a></li></ul></li><li><a href=#physicsao-ph-1>physics.ao-ph (1)</a><ul><li><a href=#11--226252-robust-and-accurate-simulations-of-flows-over-orography-using-non-conforming-meshes-giuseppe-orlando-et-al-2024>(1/1 | 226/252) Robust and accurate simulations of flows over orography using non-conforming meshes (Giuseppe Orlando et al., 2024)</a></li></ul></li><li><a href=#mathoc-2>math.OC (2)</a><ul><li><a href=#12--227252-tuning-free-stochastic-optimization-ahmed-khaled-et-al-2024>(1/2 | 227/252) Tuning-Free Stochastic Optimization (Ahmed Khaled et al., 2024)</a></li><li><a href=#22--228252-a-deep-learning-method-for-optimal-investment-under-relative-performance-criteria-among-heterogeneous-agents-mathieu-laurière-et-al-2024>(2/2 | 228/252) A Deep Learning Method for Optimal Investment Under Relative Performance Criteria Among Heterogeneous Agents (Mathieu Laurière et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--229252-towards-a-foundation-model-for-brain-age-prediction-using-covariance-neural-networks-saurabh-sihag-et-al-2024>(1/1 | 229/252) Towards a Foundation Model for Brain Age Prediction using coVariance Neural Networks (Saurabh Sihag et al., 2024)</a></li></ul></li><li><a href=#csgt-2>cs.GT (2)</a><ul><li><a href=#12--230252-rethinking-scaling-laws-for-learning-in-strategic-environments-tinashe-handina-et-al-2024>(1/2 | 230/252) Rethinking Scaling Laws for Learning in Strategic Environments (Tinashe Handina et al., 2024)</a></li><li><a href=#22--231252-automated-design-of-affine-maximizer-mechanisms-in-dynamic-settings-michael-curry-et-al-2024>(2/2 | 231/252) Automated Design of Affine Maximizer Mechanisms in Dynamic Settings (Michael Curry et al., 2024)</a></li></ul></li><li><a href=#csdc-5>cs.DC (5)</a><ul><li><a href=#15--232252-accelerating-distributed-deep-learning-using-lossless-homomorphic-compression-haoyu-li-et-al-2024>(1/5 | 232/252) Accelerating Distributed Deep Learning using Lossless Homomorphic Compression (Haoyu Li et al., 2024)</a></li><li><a href=#25--233252-lfoc-a-lightweight-fairness-oriented-cache-clustering-policy-for-commodity-multicores-adrián-garcía-garcía-et-al-2024>(2/5 | 233/252) LFOC: A Lightweight Fairness-Oriented Cache Clustering Policy for Commodity Multicores (Adrián García-García et al., 2024)</a></li><li><a href=#35--234252-from-data-to-decisions-the-transformational-power-of-machine-learning-in-business-recommendations-kapilya-gangadharan-et-al-2024>(3/5 | 234/252) From Data to Decisions: The Transformational Power of Machine Learning in Business Recommendations (Kapilya Gangadharan et al., 2024)</a></li><li><a href=#45--235252-enabling-performance-portability-of-data-parallel-openmp-applications-on-asymmetric-multicore-processors-juan-carlos-saez-et-al-2024>(4/5 | 235/252) Enabling performance portability of data-parallel OpenMP applications on asymmetric multicore processors (Juan Carlos Saez et al., 2024)</a></li><li><a href=#55--236252-logical-synchrony-networks-a-formal-model-for-deterministic-distribution-logan-kenwright-et-al-2024>(5/5 | 236/252) Logical Synchrony Networks: A formal model for deterministic distribution (Logan Kenwright et al., 2024)</a></li></ul></li><li><a href=#astro-phim-1>astro-ph.IM (1)</a><ul><li><a href=#11--237252-convolutional-neural-networks-for-signal-detection-in-real-ligo-data-ondřej-zelenka-et-al-2024>(1/1 | 237/252) Convolutional Neural Networks for signal detection in real LIGO data (Ondřej Zelenka et al., 2024)</a></li></ul></li><li><a href=#hep-ph-1>hep-ph (1)</a><ul><li><a href=#11--238252-improvement-and-generalization-of-abcd-method-with-bayesian-inference-ezequiel-alvarez-et-al-2024>(1/1 | 238/252) Improvement and generalization of ABCD method with Bayesian inference (Ezequiel Alvarez et al., 2024)</a></li></ul></li><li><a href=#csit-2>cs.IT (2)</a><ul><li><a href=#12--239252-a-lattice-reduction-aided-vector-perturbation-precoder-relying-on-quantum-annealing-samuel-winter-et-al-2024>(1/2 | 239/252) A Lattice-Reduction Aided Vector Perturbation Precoder Relying on Quantum Annealing (Samuel Winter et al., 2024)</a></li><li><a href=#22--240252-semantic-data-for-humanities-and-social-sciences-sdhss-an-ecosystem-of-cidoc-crm-extensions-for-research-data-production-and-reuse-francesco-beretta-2024>(2/2 | 240/252) Semantic Data for Humanities and Social Sciences (SDHSS): an Ecosystem of CIDOC CRM Extensions for Research Data Production and Reuse (Francesco Beretta, 2024)</a></li></ul></li><li><a href=#physicscomp-ph-1>physics.comp-ph (1)</a><ul><li><a href=#11--241252-cartesian-atomic-cluster-expansion-for-machine-learning-interatomic-potentials-bingqing-cheng-2024>(1/1 | 241/252) Cartesian atomic cluster expansion for machine learning interatomic potentials (Bingqing Cheng, 2024)</a></li></ul></li><li><a href=#csne-2>cs.NE (2)</a><ul><li><a href=#12--242252-impact-of-spatial-transformations-on-landscape-features-of-cec2022-basic-benchmark-problems-haoran-yin-et-al-2024>(1/2 | 242/252) Impact of spatial transformations on landscape features of CEC2022 basic benchmark problems (Haoran Yin et al., 2024)</a></li><li><a href=#22--243252-on-the-nature-of-the-phenotype-in-tree-genetic-programming-wolfgang-banzhaf-et-al-2024>(2/2 | 243/252) On The Nature Of The Phenotype In Tree Genetic Programming (Wolfgang Banzhaf et al., 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--244252-perfect-stable-regularity-lemma-and-slice-wise-stable-hypergraphs-artem-chernikov-et-al-2024>(1/1 | 244/252) Perfect stable regularity lemma and slice-wise stable hypergraphs (Artem Chernikov et al., 2024)</a></li></ul></li><li><a href=#csds-3>cs.DS (3)</a><ul><li><a href=#13--245252-an-approximation-algorithm-for-maximum-dicut-vs-cut-tamio-vesa-nakajima-et-al-2024>(1/3 | 245/252) An approximation algorithm for Maximum DiCut vs. Cut (Tamio-Vesa Nakajima et al., 2024)</a></li><li><a href=#23--246252-insights-into-kρ-shortcutting-algorithms-alexander-leonhardt-et-al-2024>(2/3 | 246/252) Insights into $(k,ρ)$-shortcutting algorithms (Alexander Leonhardt et al., 2024)</a></li><li><a href=#33--247252-engineering-weighted-connectivity-augmentation-algorithms-marcelo-fonseca-faraj-et-al-2024>(3/3 | 247/252) Engineering Weighted Connectivity Augmentation Algorithms (Marcelo Fonseca Faraj et al., 2024)</a></li></ul></li><li><a href=#quant-ph-2>quant-ph (2)</a><ul><li><a href=#12--248252-quantum-walks-the-discrete-wave-equation-and-chebyshev-polynomials-simon-apers-et-al-2024>(1/2 | 248/252) Quantum walks, the discrete wave equation and Chebyshev polynomials (Simon Apers et al., 2024)</a></li><li><a href=#22--249252-expansion-of-higher-dimensional-cubical-complexes-with-application-to-quantum-locally-testable-codes-irit-dinur-et-al-2024>(2/2 | 249/252) Expansion of higher-dimensional cubical complexes with application to quantum locally testable codes (Irit Dinur et al., 2024)</a></li></ul></li><li><a href=#csdm-1>cs.DM (1)</a><ul><li><a href=#11--250252-growth-rate-of-the-number-of-empty-triangles-in-the-plane-bhaswar-b-bhattacharya-et-al-2024>(1/1 | 250/252) Growth Rate of the Number of Empty Triangles in the Plane (Bhaswar B. Bhattacharya et al., 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--251252-fully-dynamic-geometric-vertex-cover-and-matching-sujoy-bhore-et-al-2024>(1/1 | 251/252) Fully Dynamic Geometric Vertex Cover and Matching (Sujoy Bhore et al., 2024)</a></li></ul></li><li><a href=#mathst-1>math.ST (1)</a><ul><li><a href=#11--252252-the-limits-of-assumption-free-tests-for-algorithm-performance-yuetian-luo-et-al-2024>(1/1 | 252/252) The Limits of Assumption-free Tests for Algorithm Performance (Yuetian Luo et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>