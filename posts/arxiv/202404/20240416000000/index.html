<!doctype html><html><head><title>arXiv @ 2024.04.16</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202404/20240416000000/"><meta property="og:site_name" content="Akitenkrad's Blog"><meta property="og:title" content="arXiv @ 2024.04.16"><meta property="og:description" content="Primary Categories astro-ph.EP (1) cs.AI (1) cs.AR (1) cs.CC (1) cs.CL (20) cs.CR (5) cs.CV (25) cs.CY (2) cs.DC (3) cs.HC (3) cs.IT (2) cs.LG (13) cs.MA (1) cs.MM (1) cs.NE (1) cs.NI (2) cs.RO (6) cs.SD (2) cs.SE (4) cs.SI (1) eess.IV (1) eess.SY (3) math.NA (1) physics.ao-ph (1) quant-ph (1) Keywords keyword cs.CL cs.CV cs.LG Adversarial Detection 1 Adversarial Learning 1 Anomaly Detection 1 1 Augmented Reality (AR) 1 BLOOM 1 Benchmarking 7 9 1 Chain-of-thought 1 Clustering 2 1 Continual Learning 1 Contrastive Learning 1 ControlNet 1 Convolution 1 1 Convolutional Neural Network 2 1 Curriculum Learning 1 Data Augmentation 1 1 Deep Neural Network 1 1 Diffusion Model 2 2 Direct Preference Optimization 1 Domain Adaptation 1 Face Recognition 1 Federated Learning 2 2 Few-shot 1 1 Few-shot Learning 1 Fine-tuning 7 3 2 GPT 1 1 GPT-2 1 GPT-4 1 Gemini 1 Generative AI 1 1 Generative Adversarial Network 2 Graph 1 1 3 Graph Attention Networks 1 2 Graph Convolutional Network 1 Graph Embedding 1 Graph Neural Network 4 Grounding 1 High-Resource 1 Image2text 1 In-context Learning 3 Information Compression 1 Instruction Tuning 1 1 Knowledge Distillation 1 3 1 Knowledge Graph 2 1 Knowledge Transfer 1 Large Language Model 25 8 4 Low-Resource 4 Mistral 1 Model Distillation 1 Multi-modal 6 N-gram 1 Named Entity Recognition 3 Natural Language Inference 3 Neural Machine Translation 1 Node Classification 1 Node Embedding 1 Object Detection 4 Offline Reinforcement Learning 1 Optical Character Recognition 1 PaLM 1 Pre-trained Language Model 3 Probabilistic Model 1 Prompt 3 2 Question Answering 4 Reasoning 3 Reinforcement Learning 1 1 Reinforcement Learning from Human Feedback 2 Representation Learning 1 Retrieval-Augmented Generation 3 Scaling Law 1 Security 1 Self-Attention 3 1 Self-supervised Learning 3 2 Semi-Supervised Learning 1 Simulation 2 1 Simulator 2 1 Summarization 2 1 Supervised Learning 1 1 T5 1 Temporal Knowledge Graph 1 Tensor Decomposition 1 Text Generation 1 Text Summarization 1 Text Understanding 1 Text2image 1 Transfer Learning 1 1 Transformer 2 4 Unsupervised Learning 1 1 1 Virtual Reality (VR) 1 Vision Transformer 2 Zero-shot 1 falcon 1 human-in-the-loop 1 cs."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-16T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-16T00:00:00+00:00"><meta property="article:tag" content="ArXiv"><meta property="article:tag" content="Published:2024"><meta name=description content="arXiv @ 2024.04.16"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1,dnt;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE")}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240408000000/ title="arXiv @ 2024.04.08">arXiv @ 2024.04.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/ title="arXiv @ 2024.04.09">arXiv @ 2024.04.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240410000000/ title="arXiv @ 2024.04.10">arXiv @ 2024.04.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/ title="arXiv @ 2024.04.11">arXiv @ 2024.04.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/ title="arXiv @ 2024.04.12">arXiv @ 2024.04.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/ title="arXiv @ 2024.04.13">arXiv @ 2024.04.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240414000000/ title="arXiv @ 2024.04.14">arXiv @ 2024.04.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/ title="arXiv @ 2024.04.15">arXiv @ 2024.04.15</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/ title="arXiv @ 2024.04.16">arXiv @ 2024.04.16</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202404/20240416000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Tuesday, Apr 16, 2024</p></div><div class=title><h1>arXiv @ 2024.04.16</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#astro-phep-1>astro-ph.EP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#csai-1>cs.AI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#cscc-1>cs.CC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#cscl-20>cs.CL (20)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#cscr-5>cs.CR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#cscv-25>cs.CV (25)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#cscy-2>cs.CY (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#csdc-3>cs.DC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#cshc-3>cs.HC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#csit-2>cs.IT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#cslg-13>cs.LG (13)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#csmm-1>cs.MM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#csne-1>cs.NE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#csni-2>cs.NI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#csro-6>cs.RO (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#csse-4>cs.SE (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#cssi-1>cs.SI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#eessiv-1>eess.IV (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#eesssy-3>eess.SY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#mathna-1>math.NA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#physicsao-ph-1>physics.ao-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/#quant-ph-1>quant-ph (1)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Adversarial Detection</td><td></td><td>1</td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>1</td></tr><tr><td>Anomaly Detection</td><td>1</td><td></td><td>1</td></tr><tr><td>Augmented Reality (AR)</td><td></td><td></td><td>1</td></tr><tr><td>BLOOM</td><td>1</td><td></td><td></td></tr><tr><td>Benchmarking</td><td>7</td><td>9</td><td>1</td></tr><tr><td>Chain-of-thought</td><td>1</td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td>2</td><td>1</td></tr><tr><td>Continual Learning</td><td>1</td><td></td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td>1</td><td></td></tr><tr><td>ControlNet</td><td></td><td>1</td><td></td></tr><tr><td>Convolution</td><td></td><td>1</td><td>1</td></tr><tr><td>Convolutional Neural Network</td><td></td><td>2</td><td>1</td></tr><tr><td>Curriculum Learning</td><td>1</td><td></td><td></td></tr><tr><td>Data Augmentation</td><td>1</td><td>1</td><td></td></tr><tr><td>Deep Neural Network</td><td></td><td>1</td><td>1</td></tr><tr><td>Diffusion Model</td><td></td><td>2</td><td>2</td></tr><tr><td>Direct Preference Optimization</td><td>1</td><td></td><td></td></tr><tr><td>Domain Adaptation</td><td>1</td><td></td><td></td></tr><tr><td>Face Recognition</td><td></td><td>1</td><td></td></tr><tr><td>Federated Learning</td><td></td><td>2</td><td>2</td></tr><tr><td>Few-shot</td><td>1</td><td>1</td><td></td></tr><tr><td>Few-shot Learning</td><td>1</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>7</td><td>3</td><td>2</td></tr><tr><td>GPT</td><td>1</td><td></td><td>1</td></tr><tr><td>GPT-2</td><td></td><td></td><td>1</td></tr><tr><td>GPT-4</td><td>1</td><td></td><td></td></tr><tr><td>Gemini</td><td></td><td>1</td><td></td></tr><tr><td>Generative AI</td><td></td><td>1</td><td>1</td></tr><tr><td>Generative Adversarial Network</td><td></td><td>2</td><td></td></tr><tr><td>Graph</td><td>1</td><td>1</td><td>3</td></tr><tr><td>Graph Attention Networks</td><td></td><td>1</td><td>2</td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td>1</td></tr><tr><td>Graph Embedding</td><td></td><td></td><td>1</td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>4</td></tr><tr><td>Grounding</td><td></td><td></td><td>1</td></tr><tr><td>High-Resource</td><td>1</td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td>1</td><td></td></tr><tr><td>In-context Learning</td><td>3</td><td></td><td></td></tr><tr><td>Information Compression</td><td></td><td>1</td><td></td></tr><tr><td>Instruction Tuning</td><td>1</td><td>1</td><td></td></tr><tr><td>Knowledge Distillation</td><td>1</td><td>3</td><td>1</td></tr><tr><td>Knowledge Graph</td><td>2</td><td></td><td>1</td></tr><tr><td>Knowledge Transfer</td><td></td><td></td><td>1</td></tr><tr><td>Large Language Model</td><td>25</td><td>8</td><td>4</td></tr><tr><td>Low-Resource</td><td>4</td><td></td><td></td></tr><tr><td>Mistral</td><td>1</td><td></td><td></td></tr><tr><td>Model Distillation</td><td></td><td></td><td>1</td></tr><tr><td>Multi-modal</td><td></td><td>6</td><td></td></tr><tr><td>N-gram</td><td>1</td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td>3</td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td>3</td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>1</td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td>1</td></tr><tr><td>Node Embedding</td><td></td><td></td><td>1</td></tr><tr><td>Object Detection</td><td></td><td>4</td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td>1</td></tr><tr><td>Optical Character Recognition</td><td></td><td>1</td><td></td></tr><tr><td>PaLM</td><td></td><td>1</td><td></td></tr><tr><td>Pre-trained Language Model</td><td>3</td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td>1</td><td></td></tr><tr><td>Prompt</td><td>3</td><td>2</td><td></td></tr><tr><td>Question Answering</td><td>4</td><td></td><td></td></tr><tr><td>Reasoning</td><td>3</td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td>1</td><td></td><td>1</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>2</td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td>1</td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>3</td><td></td><td></td></tr><tr><td>Scaling Law</td><td></td><td></td><td>1</td></tr><tr><td>Security</td><td></td><td>1</td><td></td></tr><tr><td>Self-Attention</td><td></td><td>3</td><td>1</td></tr><tr><td>Self-supervised Learning</td><td></td><td>3</td><td>2</td></tr><tr><td>Semi-Supervised Learning</td><td>1</td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td>2</td><td>1</td></tr><tr><td>Simulator</td><td></td><td>2</td><td>1</td></tr><tr><td>Summarization</td><td>2</td><td>1</td><td></td></tr><tr><td>Supervised Learning</td><td>1</td><td>1</td><td></td></tr><tr><td>T5</td><td>1</td><td></td><td></td></tr><tr><td>Temporal Knowledge Graph</td><td></td><td></td><td>1</td></tr><tr><td>Tensor Decomposition</td><td></td><td></td><td>1</td></tr><tr><td>Text Generation</td><td>1</td><td></td><td></td></tr><tr><td>Text Summarization</td><td>1</td><td></td><td></td></tr><tr><td>Text Understanding</td><td>1</td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>1</td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td></td><td>1</td></tr><tr><td>Transformer</td><td></td><td>2</td><td>4</td></tr><tr><td>Unsupervised Learning</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Virtual Reality (VR)</td><td></td><td>1</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>2</td><td></td></tr><tr><td>Zero-shot</td><td></td><td>1</td><td></td></tr><tr><td>falcon</td><td>1</td><td></td><td></td></tr><tr><td>human-in-the-loop</td><td>1</td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-20>cs.CL (20)</h2><h3 id=120--1102-gemquad--generating-multilingual-question-answering-datasets-from-large-language-models-using-few-shot-learning-amani-namboori-et-al-2024>(1/20 | 1/102) GeMQuAD : Generating Multilingual Question Answering Datasets from Large Language Models using Few Shot Learning (Amani Namboori et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amani Namboori, Shivam Mangale, Andy Rosenbaum, Saleh Soltan. (2024)<br><strong>GeMQuAD : Generating Multilingual Question Answering Datasets from Large Language Models using Few Shot Learning</strong><br><button class=copy-to-clipboard title="GeMQuAD : Generating Multilingual Question Answering Datasets from Large Language Models using Few Shot Learning" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 120<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, Low-Resource, Semi-Supervised Learning, Neural Machine Translation, Question Answering, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09163v1.pdf filename=2404.09163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emergence of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with capabilities like <b>In-Context</b> <b>Learning</b> <b>(ICL)</b> has ushered in new possibilities for data generation across various domains while minimizing the need for extensive data collection and modeling techniques. Researchers have explored ways to use this generated synthetic data to optimize smaller student models for reduced deployment costs and lower latency in downstream tasks. However, <b>ICL-generated</b> data often suffers from low quality as the task specificity is limited with few examples used in <b>ICL.</b> In this paper, we propose GeMQuAD - a <b>semi-supervised</b> <b>learning</b> approach, extending the WeakDAP framework, applied to a dataset generated through <b>ICL</b> with just one example in the target language using AlexaTM 20B Seq2Seq <b>LLM.</b> Through our approach, we iteratively identify high-quality data to enhance model performance, especially for <b>low-resource</b> multilingual setting in the context of Extractive <b>Question</b> <b>Answering</b> task. Our framework outperforms the <b>machine</b> <b>translation-augmented</b> model by 0.22/1.68 F1/EM (Exact Match) points for Hindi and 0.82/1.37 F1/EM points for Spanish on the MLQA dataset, and it surpasses the performance of model trained on an English-only dataset by 5.05/6.50 F1/EM points for Hindi and 3.81/3.69 points F1/EM for Spanish on the same dataset. Notably, our approach uses a pre-trained <b>LLM</b> for generation with no <b>fine-tuning</b> (FT), utilizing just a single annotated example in <b>ICL</b> to generate data, providing a cost-effective development process.</p></p class="citation"></blockquote><h3 id=220--2102-cross-data-knowledge-graph-construction-for-llm-enabled-educational-question-answering-system-acasestudyathcmut-tuan-bui-et-al-2024>(2/20 | 2/102) Cross-Data Knowledge Graph Construction for LLM-enabled Educational Question-Answering System: A~Case~Study~at~HCMUT (Tuan Bui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tuan Bui, Oanh Tran, Phuong Nguyen, Bao Ho, Long Nguyen, Thang Bui, Tho Quan. (2024)<br><strong>Cross-Data Knowledge Graph Construction for LLM-enabled Educational Question-Answering System: A~Case~Study~at~HCMUT</strong><br><button class=copy-to-clipboard title="Cross-Data Knowledge Graph Construction for LLM-enabled Educational Question-Answering System: A~Case~Study~at~HCMUT" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 93<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Question Answering, Large Language Model, Large Language Model, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09296v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09296v1.pdf filename=2404.09296v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In today&rsquo;s rapidly evolving landscape of Artificial Intelligence, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have emerged as a vibrant research topic. <b>LLMs</b> find applications in various fields and contribute significantly. Despite their powerful language capabilities, similar to <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs),</b> <b>LLMs</b> still face challenges in remembering events, incorporating new information, and addressing domain-specific issues or hallucinations. To overcome these limitations, researchers have proposed <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> techniques, some others have proposed the integration of <b>LLMs</b> with <b>Knowledge</b> <b>Graphs</b> <b>(KGs)</b> to provide factual context, thereby improving performance and delivering more accurate feedback to user queries. Education plays a crucial role in human development and progress. With the technology transformation, traditional education is being replaced by digital or blended education. Therefore, educational data in the digital environment is increasing day by day. Data in higher education institutions are diverse, comprising various sources such as unstructured/structured text, relational databases, web/app-based API access, etc. Constructing a <b>Knowledge</b> <b>Graph</b> from these cross-data sources is not a simple task. This article proposes a method for automatically constructing a <b>Knowledge</b> <b>Graph</b> from multiple data sources and discusses some initial applications (experimental trials) of <b>KG</b> in conjunction with <b>LLMs</b> for <b>question-answering</b> <b>tasks.</b></p></p class="citation"></blockquote><h3 id=320--3102-compass-large-multilingual-language-model-for-south-east-asia-sophia-maria-2024>(3/20 | 3/102) Compass: Large Multilingual Language Model for South-east Asia (Sophia Maria, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sophia Maria. (2024)<br><strong>Compass: Large Multilingual Language Model for South-east Asia</strong><br><button class=copy-to-clipboard title="Compass: Large Multilingual Language Model for South-east Asia" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Curriculum Learning, Direct Preference Optimization, Fine-tuning, Low-Resource, Supervised Learning, falcon, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09220v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09220v1.pdf filename=2404.09220v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> have exhibited significant proficiency in languages endowed with extensive linguistic resources, such as English and Chinese. Nevertheless, their effectiveness notably diminishes when applied to languages characterized by limited linguistic resources, particularly within the Southeast Asian linguistic landscape, such as Indonesian. The scarcity of linguistic resources for these languages presents challenges associated with inadequate training, restricted vocabulary coverage, and challenging evaluation processes. In response to these exigencies, we have introduced CompassLLM, a <b>large</b> <b>multilingual</b> <b>model</b> specifically tailored for Southeast Asian languages, with the primary aim of supporting the developmental requirements of Shopee. Our methodology encompasses several key strategies. To progressively enhance multilingual proficiencies, we implemented a multi-stage pre-training strategy integrated with <b>curriculum</b> <b>learning,</b> gradually intensifying the focus on <b>low-resource</b> languages. Concurrently, to better accommodate <b>low-resource</b> human instructions, we curated and generated a repository of high-quality multilingual human instructions, culminating the CompassLLM-SFT model through <b>supervised</b> instruction <b>fine-tuning.</b> Finally, to reinforce the model&rsquo;s alignment with human preference behaviors, we have embraced the principle of <b>Direct</b> <b>Preference</b> <b>Optimization</b> (DPO) to obtain CompassLLM-DPO model. Preliminary evaluation of the CompassLLM model yields promising results, with our model surpassing <b>benchmark</b> models like Vicuna-7b-v1.5, Sealion, <b>Falcon</b> and SeaLLM, across diverse evaluation tasks, as verified through both automated and human-driven assessments. Notably, our model exhibits its superior performance in South-east Asia languages, such as Indonesian language.</p></p class="citation"></blockquote><h3 id=420--4102-from-bytes-to-borsch-fine-tuning-gemma-and-mistral-for-the-ukrainian-language-representation-artur-kiulian-et-al-2024>(4/20 | 4/102) From Bytes to Borsch: Fine-Tuning Gemma and Mistral for the Ukrainian Language Representation (Artur Kiulian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Artur Kiulian, Anton Polishko, Mykola Khandoga, Oryna Chubych, Jack Connor, Raghav Ravishankar, Adarsh Shirawalmath. (2024)<br><strong>From Bytes to Borsch: Fine-Tuning Gemma and Mistral for the Ukrainian Language Representation</strong><br><button class=copy-to-clipboard title="From Bytes to Borsch: Fine-Tuning Gemma and Mistral for the Ukrainian Language Representation" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 66<br>Keywords: Benchmarking, Benchmarking, Fine-tuning, Low-Resource, Mistral, Large Language Model, Large Language Model, Text Understanding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09138v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09138v1.pdf filename=2404.09138v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly advancing field of AI and NLP, generative <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> stand at the forefront of innovation, showcasing unparalleled abilities in <b>text</b> <b>understanding</b> and generation. However, the limited representation of <b>low-resource</b> languages like Ukrainian poses a notable challenge, restricting the reach and relevance of this technology. Our paper addresses this by <b>fine-tuning</b> the open-source Gemma and <b>Mistral</b> <b>LLMs</b> with Ukrainian datasets, aiming to improve their linguistic proficiency and <b>benchmarking</b> them against other existing models capable of processing Ukrainian language. This endeavor not only aims to mitigate language bias in technology but also promotes inclusivity in the digital realm. Our transparent and reproducible approach encourages further NLP research and development. Additionally, we present the Ukrainian Knowledge and Instruction Dataset (UKID) to aid future efforts in language model <b>fine-tuning.</b> Our research not only advances the field of NLP but also highlights the importance of linguistic diversity in AI, which is crucial for cultural preservation, education, and expanding AI&rsquo;s global utility. Ultimately, we advocate for a future where technology is inclusive, enabling AI to communicate effectively across all languages, especially those currently underrepresented.</p></p class="citation"></blockquote><h3 id=520--5102-post-semantic-thinking-a-robust-strategy-to-distill-reasoning-capacity-from-large-language-models-xiao-chen-et-al-2024>(5/20 | 5/102) Post-Semantic-Thinking: A Robust Strategy to Distill Reasoning Capacity from Large Language Models (Xiao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Chen, Sihang Zhou, Ke Liang, Xinwang Liu. (2024)<br><strong>Post-Semantic-Thinking: A Robust Strategy to Distill Reasoning Capacity from Large Language Models</strong><br><button class=copy-to-clipboard title="Post-Semantic-Thinking: A Robust Strategy to Distill Reasoning Capacity from Large Language Models" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Knowledge Distillation, Reasoning, Chain-of-thought, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09170v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09170v1.pdf filename=2404.09170v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chain of thought <b>finetuning</b> aims to endow small student models with <b>reasoning</b> capacity to improve their performance towards a specific task by allowing them to imitate the <b>reasoning</b> procedure of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> beyond simply predicting the answer to the question. However, the existing methods 1) generate rationale before the answer, making their answer correctness sensitive to the hallucination in the rationale;2) force the student model to repeat the exact <b>LLMs</b> rationale expression word-after-word, which could have the model biased towards learning the expression in rationale but count against the model from understanding the core logic behind it. Therefore, we propose a robust Post-Semantic-Thinking (PST) strategy to generate answers before rationale. Thanks to this answer-first setting, 1) the answering procedure can escape from the adverse effects caused by hallucinations in the rationale; 2) the complex <b>reasoning</b> procedure is tightly bound with the relatively concise answer, making the <b>reasoning</b> for questions easier with the prior information in the answer; 3) the efficiency of the method can also benefit from the setting since users can stop the generation right after answers are outputted when inference is conducted. Furthermore, the PST strategy loose the constraint against the generated rationale to be close to the <b>LLMs</b> gold standard in the hidden semantic space instead of the vocabulary space, thus making the small student model better comprehend the semantic <b>reasoning</b> logic in rationale. Extensive experiments conducted across 12 <b>reasoning</b> tasks demonstrate the effectiveness of PST.</p></p class="citation"></blockquote><h3 id=620--6102-unveiling-llm-evaluation-focused-on-metrics-challenges-and-solutions-taojun-hu-et-al-2024>(6/20 | 6/102) Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions (Taojun Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taojun Hu, Xiao-Hua Zhou. (2024)<br><strong>Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions</strong><br><button class=copy-to-clipboard title="Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Question Answering, Text Generation, Text Summarization, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09135v1.pdf filename=2404.09135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Natural Language Processing (NLP) is witnessing a remarkable breakthrough driven by the success of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> <b>LLMs</b> have gained significant attention across academia and industry for their versatile applications in <b>text</b> <b>generation,</b> <b>question</b> <b>answering,</b> and <b>text</b> <b>summarization.</b> As the landscape of NLP evolves with an increasing number of domain-specific <b>LLMs</b> employing diverse techniques and trained on various corpus, evaluating performance of these models becomes paramount. To quantify the performance, it&rsquo;s crucial to have a comprehensive grasp of existing metrics. Among the evaluation, metrics which quantifying the performance of <b>LLMs</b> play a pivotal role. This paper offers a comprehensive exploration of <b>LLM</b> evaluation from a metrics perspective, providing insights into the selection and interpretation of metrics currently in use. Our main goal is to elucidate their mathematical formulations and statistical interpretations. We shed light on the application of these metrics using recent Biomedical <b>LLMs.</b> Additionally, we offer a succinct comparison of these metrics, aiding researchers in selecting appropriate metrics for diverse tasks. The overarching goal is to furnish researchers with a pragmatic guide for effective <b>LLM</b> evaluation and metric selection, thereby advancing the understanding and application of these <b>large</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=720--7102-confidence-calibration-and-rationalization-for-llms-via-multi-agent-deliberation-ruixin-yang-et-al-2024>(7/20 | 7/102) Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation (Ruixin Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruixin Yang, Dheeraj Rajagopa, Shirley Anugrah Hayati, Bin Hu, Dongyeop Kang. (2024)<br><strong>Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation</strong><br><button class=copy-to-clipboard title="Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09127v1.pdf filename=2404.09127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Uncertainty estimation is a significant issue for current <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> that are generally poorly calibrated and over-confident, especially with <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF).</b> Unlike humans, whose decisions and confidences not only stem from intrinsic beliefs but can also be adjusted through daily observations, existing calibration methods for <b>LLMs</b> focus on estimating or eliciting individual confidence without taking full advantage of the &ldquo;Collective Wisdom&rdquo;: the interaction among multiple <b>LLMs</b> that can collectively improve both accuracy and calibration. In this work, we propose Collaborative Calibration, a post-hoc training-free calibration strategy that leverages the collaborative and expressive capabilities of multiple tool-augmented <b>LLM</b> agents in a simulated group deliberation process. We demonstrate the effectiveness of Collaborative Calibration on generative <b>QA</b> tasks across various domains, showing its potential in harnessing the rationalization of collectively calibrated confidence assessments and improving the reliability of model predictions.</p></p class="citation"></blockquote><h3 id=820--8102-self-selected-attention-span-for-accelerating-large-language-model-inference-tian-jin-et-al-2024>(8/20 | 8/102) Self-Selected Attention Span for Accelerating Large Language Model Inference (Tian Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tian Jin, Wanzin Yazar, Zifei Xu, Sayeh Sharify, Xin Wang. (2024)<br><strong>Self-Selected Attention Span for Accelerating Large Language Model Inference</strong><br><button class=copy-to-clipboard title="Self-Selected Attention Span for Accelerating Large Language Model Inference" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09336v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09336v1.pdf filename=2404.09336v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can solve challenging tasks. However, their inference computation on modern GPUs is highly inefficient due to the increasing number of tokens they must attend to as they generate new ones. To address this inefficiency, we capitalize on <b>LLMs&rsquo;</b> problem-solving capabilities to optimize their own inference-time efficiency. We demonstrate with two specific tasks: (a) evaluating complex arithmetic expressions and (b) summarizing news articles. For both tasks, we create custom datasets to <b>fine-tune</b> an <b>LLM.</b> The goal of <b>fine-tuning</b> is twofold: first, to make the <b>LLM</b> learn to solve the evaluation or <b>summarization</b> task, and second, to train it to identify the minimal attention spans required for each step of the task. As a result, the <b>fine-tuned</b> model is able to convert these self-identified minimal attention spans into sparse attention masks on-the-fly during inference. We develop a custom CUDA kernel to take advantage of the reduced context to attend to. We demonstrate that using this custom CUDA kernel improves the throughput of <b>LLM</b> inference by 28%. Our work presents an end-to-end demonstration showing that training <b>LLMs</b> to self-select their attention spans speeds up autoregressive inference in solving real-world tasks.</p></p class="citation"></blockquote><h3 id=920--9102-jafin-japanese-financial-instruction-dataset-kota-tanabe-et-al-2024>(9/20 | 9/102) JaFIn: Japanese Financial Instruction Dataset (Kota Tanabe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kota Tanabe, Masahiro Suzuki, Hiroki Sakaji, Itsuki Noda. (2024)<br><strong>JaFIn: Japanese Financial Instruction Dataset</strong><br><button class=copy-to-clipboard title="JaFIn: Japanese Financial Instruction Dataset" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CE, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Domain Adaptation, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09260v1.pdf filename=2404.09260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We construct an <b>instruction</b> <b>dataset</b> for the <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> in the Japanese finance <b>domain.</b> <b>Domain</b> <b>adaptation</b> of language models, including <b>LLMs,</b> is receiving more attention as language models become more popular. This study demonstrates the effectiveness of <b>domain</b> <b>adaptation</b> through <b>instruction</b> <b>tuning.</b> To achieve this, we propose an <b>instruction</b> <b>tuning</b> data in Japanese called JaFIn, the Japanese Financial <b>Instruction</b> <b>Dataset.</b> JaFIn is manually constructed based on multiple data sources, including Japanese government websites, which provide extensive financial knowledge. We then utilize JaFIn to apply <b>instruction</b> <b>tuning</b> for several <b>LLMs,</b> demonstrating that our models specialized in finance have better <b>domain</b> <b>adaptability</b> than the original models. The financial-specialized <b>LLMs</b> created were evaluated using a quantitative Japanese financial <b>benchmark</b> and qualitative response comparisons, showing improved performance over the originals.</p></p class="citation"></blockquote><h3 id=1020--10102-dke-research-at-semeval-2024-task-2-incorporating-data-augmentation-with-generative-models-and-biomedical-knowledge-to-enhance-inference-robustness-yuqi-wang-et-al-2024>(10/20 | 10/102) DKE-Research at SemEval-2024 Task 2: Incorporating Data Augmentation with Generative Models and Biomedical Knowledge to Enhance Inference Robustness (Yuqi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqi Wang, Zeqiang Wang, Wei Wang, Qi Chen, Kaizhu Huang, Anh Nguyen, Suparna De. (2024)<br><strong>DKE-Research at SemEval-2024 Task 2: Incorporating Data Augmentation with Generative Models and Biomedical Knowledge to Enhance Inference Robustness</strong><br><button class=copy-to-clipboard title="DKE-Research at SemEval-2024 Task 2: Incorporating Data Augmentation with Generative Models and Biomedical Knowledge to Enhance Inference Robustness" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Data Augmentation, Natural Language Inference, Reasoning, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09206v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09206v1.pdf filename=2404.09206v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Safe and reliable <b>natural</b> <b>language</b> <b>inference</b> is critical for extracting insights from clinical trial reports but poses challenges due to biases in large <b>pre-trained</b> <b>language</b> <b>models.</b> This paper presents a novel <b>data</b> <b>augmentation</b> technique to improve model robustness for biomedical <b>natural</b> <b>language</b> <b>inference</b> in clinical trials. By generating synthetic examples through semantic perturbations and domain-specific vocabulary replacement and adding a new task for numerical and quantitative <b>reasoning,</b> we introduce greater diversity and reduce shortcut learning. Our approach, combined with multi-task learning and the DeBERTa architecture, achieved significant performance gains on the NLI4CT 2024 <b>benchmark</b> compared to the original language models. Ablation studies validate the contribution of each augmentation method in improving robustness. Our best-performing model ranked 12th in terms of faithfulness and 8th in terms of consistency, respectively, out of the 32 participants.</p></p class="citation"></blockquote><h3 id=1120--11102-toner-type-oriented-named-entity-recognition-with-generative-language-model-guochao-jiang-et-al-2024>(11/20 | 11/102) ToNER: Type-oriented Named Entity Recognition with Generative Language Model (Guochao Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guochao Jiang, Ziqin Luo, Yuchen Shi, Dixuan Wang, Jiaqing Liang, Deqing Yang. (2024)<br><strong>ToNER: Type-oriented Named Entity Recognition with Generative Language Model</strong><br><button class=copy-to-clipboard title="ToNER: Type-oriented Named Entity Recognition with Generative Language Model" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Named Entity Recognition, Named Entity Recognition, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09145v1.pdf filename=2404.09145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the <b>fine-tuned</b> generative models have been proven more powerful than the previous tagging-based or span-based models on <b>named</b> <b>entity</b> <b>recognition</b> <b>(NER)</b> task. It has also been found that the information related to entities, such as entity types, can <b>prompt</b> a model to achieve <b>NER</b> better. However, it is not easy to determine the entity types indeed existing in the given sentence in advance, and inputting too many potential entity types would distract the model inevitably. To exploit entity types&rsquo; merit on promoting <b>NER</b> task, in this paper we propose a novel <b>NER</b> framework, namely ToNER based on a generative model. In ToNER, a type matching model is proposed at first to identify the entity types most likely to appear in the sentence. Then, we append a multiple binary classification task to <b>fine-tune</b> the generative model&rsquo;s encoder, so as to generate the refined representation of the input sentence. Moreover, we add an auxiliary task for the model to discover the entity types which further <b>fine-tunes</b> the model to output more accurate results. Our extensive experiments on some <b>NER</b> <b>benchmarks</b> verify the effectiveness of our proposed strategies in ToNER that are oriented towards entity types&rsquo; exploitation.</p></p class="citation"></blockquote><h3 id=1220--12102-low-resource-named-entity-recognition-with-cross-lingual-character-level-neural-conditional-random-fields-ryan-cotterell-et-al-2024>(12/20 | 12/102) Low-Resource Named Entity Recognition with Cross-Lingual, Character-Level Neural Conditional Random Fields (Ryan Cotterell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryan Cotterell, Kevin Duh. (2024)<br><strong>Low-Resource Named Entity Recognition with Cross-Lingual, Character-Level Neural Conditional Random Fields</strong><br><button class=copy-to-clipboard title="Low-Resource Named Entity Recognition with Cross-Lingual, Character-Level Neural Conditional Random Fields" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: High-Resource, Low-Resource, Transfer Learning, Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09383v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09383v1.pdf filename=2404.09383v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Low-resource</b> <b>named</b> <b>entity</b> <b>recognition</b> is still an open problem in NLP. Most state-of-the-art systems require tens of thousands of annotated sentences in order to obtain high performance. However, for most of the world&rsquo;s languages, it is unfeasible to obtain such annotation. In this paper, we present a <b>transfer</b> <b>learning</b> scheme, whereby we train character-level neural CRFs to predict <b>named</b> <b>entities</b> <b>for</b> both <b>high-resource</b> languages and low resource languages jointly. Learning character representations for multiple related languages allows <b>transfer</b> <b>among</b> the languages, improving F1 by up to 9.8 points over a loglinear CRF baseline.</p></p class="citation"></blockquote><h3 id=1320--13102-when-hindsight-is-not-2020-testing-limits-on-reflective-thinking-in-large-language-models-yanhong-li-et-al-2024>(13/20 | 13/102) When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models (Yanhong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanhong Li, Chenghao Yang, Allyson Ettinger. (2024)<br><strong>When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models</strong><br><button class=copy-to-clipboard title="When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09129v1.pdf filename=2404.09129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies suggest that self-reflective <b>prompting</b> can significantly enhance the <b>reasoning</b> capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> However, the use of external feedback as a stop criterion raises doubts about the true extent of <b>LLMs&rsquo;</b> ability to emulate human-like self-reflection. In this paper, we set out to clarify these capabilities under a more stringent evaluation setting in which we disallow any kind of external feedback. Our findings under this setting show a split: while self-reflection enhances performance in TruthfulQA, it adversely affects results in HotpotQA. We conduct follow-up analyses to clarify the contributing factors in these patterns, and find that the influence of self-reflection is impacted both by reliability of accuracy in models&rsquo; initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher. We also find that self-reflection reduces tendency toward majority voting. Based on our findings, we propose guidelines for decisions on when to implement self-reflection. We release the codebase for reproducing our experiments at <a href=https://github.com/yanhong-lbh/LLM-SelfReflection-Eval>https://github.com/yanhong-lbh/LLM-SelfReflection-Eval</a>.</p></p class="citation"></blockquote><h3 id=1420--14102-towards-practical-tool-usage-for-continually-learning-llms-jerry-huang-et-al-2024>(14/20 | 14/102) Towards Practical Tool Usage for Continually Learning LLMs (Jerry Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jerry Huang, Prasanna Parthasarathi, Mehdi Rezagholizadeh, Sarath Chandar. (2024)<br><strong>Towards Practical Tool Usage for Continually Learning LLMs</strong><br><button class=copy-to-clipboard title="Towards Practical Tool Usage for Continually Learning LLMs" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Continual Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09339v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09339v1.pdf filename=2404.09339v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> show an innate skill for solving language based tasks. But insights have suggested an inability to adjust for information or task-solving skills becoming outdated, as their knowledge, stored directly within their parameters, remains static in time. Tool use helps by offloading work to systems that the <b>LLM</b> can access through an interface, but <b>LLMs</b> that use them still must adapt to nonstationary environments for prolonged use, as new tools can emerge and existing tools can change. Nevertheless, tools require less specialized knowledge, therefore we hypothesize they are better suited for <b>continual</b> <b>learning</b> (CL) as they rely less on parametric memory for solving tasks and instead focus on learning when to apply pre-defined tools. To verify this, we develop a synthetic <b>benchmark</b> and follow this by aggregating existing NLP tasks to form a more realistic testing scenario. While we demonstrate scaling model size is not a solution, regardless of tool usage, <b>continual</b> <b>learning</b> techniques can enable tool <b>LLMs</b> to both adapt faster while forgetting less, highlighting their potential as <b>continual</b> <b>learners.</b></p></p class="citation"></blockquote><h3 id=1520--15102-understanding-the-role-of-temperature-in-diverse-question-generation-by-gpt-4-arav-agarwal-et-al-2024>(15/20 | 15/102) Understanding the Role of Temperature in Diverse Question Generation by GPT-4 (Arav Agarwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arav Agarwal, Karthik Mittal, Aidan Doyle, Pragnya Sridhar, Zipiao Wan, Jacob Arthur Doughty, Jaromir Savelka, Majd Sakr. (2024)<br><strong>Understanding the Role of Temperature in Diverse Question Generation by GPT-4</strong><br><button class=copy-to-clipboard title="Understanding the Role of Temperature in Diverse Question Generation by GPT-4" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: BLOOM, GPT, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09366v1.pdf filename=2404.09366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We conduct a preliminary study of the effect of <b>GPT&rsquo;s</b> temperature parameter on the diversity of <b>GPT4-generated</b> questions. We find that using higher temperature values leads to significantly higher diversity, with different temperatures exposing different types of similarity between generated sets of questions. We also demonstrate that diverse question generation is especially difficult for questions targeting lower levels of <b>Bloom&rsquo;s</b> Taxonomy.</p></p class="citation"></blockquote><h3 id=1620--16102-entropy-guided-extrapolative-decoding-to-improve-factuality-in-large-language-models-souvik-das-et-al-2024>(16/20 | 16/102) Entropy Guided Extrapolative Decoding to Improve Factuality in Large Language Models (Souvik Das et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Souvik Das, Lifeng Jin, Linfeng Song, Haitao Mi, Baolin Peng, Dong Yu. (2024)<br><strong>Entropy Guided Extrapolative Decoding to Improve Factuality in Large Language Models</strong><br><button class=copy-to-clipboard title="Entropy Guided Extrapolative Decoding to Improve Factuality in Large Language Models" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09338v1.pdf filename=2404.09338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> exhibit impressive natural language capabilities but suffer from hallucination &ndash; generating content ungrounded in the realities of training data. Recent work has focused on decoding techniques to improve factuality during inference by leveraging <b>LLMs&rsquo;</b> hierarchical representation of factual knowledge, manipulating the predicted distributions at inference time. Current state-of-the-art approaches refine decoding by contrasting early-exit distributions from a lower layer with the final layer to exploit information related to factuality within the model forward procedure. However, such methods often assume the final layer is the most reliable and the lower layer selection process depends on it. In this work, we first propose extrapolation of critical token probabilities beyond the last layer for more accurate contrasting. We additionally employ layer-wise entropy-guided lower layer selection, decoupling the selection process from the final layer. Experiments demonstrate strong performance - surpassing state-of-the-art on multiple different datasets by <b>large</b> <b>margins.</b> <b>Analyses</b> show different kinds of <b>prompts</b> respond to different selection strategies.</p></p class="citation"></blockquote><h3 id=1720--17102-reap-the-wild-wind-detecting-media-storms-in-large-scale-news-corpora-dror-k-markus-et-al-2024>(17/20 | 17/102) Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora (Dror K. Markus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dror K. Markus, Effi Levi, Tamir Sheafer, Shaul R. Shenhav. (2024)<br><strong>Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora</strong><br><button class=copy-to-clipboard title="Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Anomaly Detection, Unsupervised Learning, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09299v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09299v1.pdf filename=2404.09299v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Media Storms, dramatic outbursts of attention to a story, are central components of media dynamics and the attention landscape. Despite their significance, there has been little systematic and empirical research on this concept due to issues of measurement and operationalization. We introduce an iterative <b>human-in-the-loop</b> method to identify media storms in a large-scale corpus of news articles. The text is first transformed into signals of dispersion based on several textual characteristics. In each iteration, we apply <b>unsupervised</b> <b>anomaly</b> <b>detection</b> to these signals; each <b>anomaly</b> <b>is</b> then validated by an expert to confirm the presence of a storm, and those results are then used to tune the <b>anomaly</b> <b>detection</b> in the next iteration. We demonstrate the applicability of this method in two scenarios: first, supplementing an initial list of media storms within a specific time frame; and second, detecting media storms in new time periods. We make available a media storm dataset compiled using both scenarios. Both the method and dataset offer the basis for comprehensive empirical research into the concept of media storms, including characterizing them and predicting their outbursts and durations, in mainstream media or social media platforms.</p></p class="citation"></blockquote><h3 id=1820--18102-tldr-at-semeval-2024-task-2-t5-generated-clinical-language-summaries-for-deberta-report-analysis-spandan-das-et-al-2024>(18/20 | 18/102) TLDR at SemEval-2024 Task 2: T5-generated clinical-Language summaries for DeBERTa Report Analysis (Spandan Das et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Spandan Das, Vinay Samuel, Shahriar Noroozizadeh. (2024)<br><strong>TLDR at SemEval-2024 Task 2: T5-generated clinical-Language summaries for DeBERTa Report Analysis</strong><br><button class=copy-to-clipboard title="TLDR at SemEval-2024 Task 2: T5-generated clinical-Language summaries for DeBERTa Report Analysis" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: T5, Natural Language Inference, Natural Language Inference<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09136v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09136v1.pdf filename=2404.09136v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces novel methodologies for the <b>Natural</b> <b>Language</b> <b>Inference</b> for Clinical Trials (NLI4CT) task. We present TLDR <b>(T5-generated</b> clinical-Language summaries for DeBERTa Report Analysis) which incorporates <b>T5-model</b> generated premise summaries for improved entailment and contradiction analysis in clinical <b>NLI</b> tasks. This approach overcomes the challenges posed by small context windows and lengthy premises, leading to a substantial improvement in Macro F1 scores: a 0.184 increase over truncated premises. Our comprehensive experimental evaluation, including detailed error analysis and ablations, confirms the superiority of TLDR in achieving consistency and faithfulness in predictions against semantically altered inputs.</p></p class="citation"></blockquote><h3 id=1920--19102-large-language-models-are-as-persuasive-as-humans-but-why-about-the-cognitive-effort-and-moral-emotional-language-of-llm-arguments-carlos-carrasco-farre-2024>(19/20 | 19/102) Large Language Models are as persuasive as humans, but why? About the cognitive effort and moral-emotional language of LLM arguments (Carlos Carrasco-Farre, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carlos Carrasco-Farre. (2024)<br><strong>Large Language Models are as persuasive as humans, but why? About the cognitive effort and moral-emotional language of LLM arguments</strong><br><button class=copy-to-clipboard title="Large Language Models are as persuasive as humans, but why? About the cognitive effort and moral-emotional language of LLM arguments" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09329v1.pdf filename=2404.09329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are already as persuasive as humans. However, we know very little about why. This paper investigates the persuasion strategies of <b>LLMs,</b> comparing them with human-generated arguments. Using a dataset of 1,251 participants in an experiment, we analyze the persuaion strategies of <b>LLM-generated</b> and human-generated arguments using measures of cognitive effort (lexical and grammatical complexity) and moral-emotional language (sentiment and moral analysis). The study reveals that <b>LLMs</b> produce arguments that require higher cognitive effort, exhibiting more complex grammatical and lexical structures than human counterparts. Additionally, <b>LLMs</b> demonstrate a significant propensity to engage more deeply with moral language, utilizing both positive and negative moral foundations more frequently than humans. In contrast with previous research, no significant difference was found in the emotional content produced by <b>LLMs</b> and humans. These findings contribute to the discourse on AI and persuasion, highlighting the dual potential of <b>LLMs</b> to both enhance and undermine informational integrity through communication strategies for digital persuasion.</p></p class="citation"></blockquote><h3 id=2020--20102-towards-fast-inference-exploring-and-improving-blockwise-parallel-drafts-taehyeon-kim-et-al-2024>(20/20 | 20/102) Towards Fast Inference: Exploring and Improving Blockwise Parallel Drafts (Taehyeon Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taehyeon Kim, Ananda Theertha Suresh, Kishore Papineni, Michael Riley, Sanjiv Kumar, Adrian Benton. (2024)<br><strong>Towards Fast Inference: Exploring and Improving Blockwise Parallel Drafts</strong><br><button class=copy-to-clipboard title="Towards Fast Inference: Exploring and Improving Blockwise Parallel Drafts" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: N-gram<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09221v1.pdf filename=2404.09221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the remarkable strides made by autoregressive language models, their potential is often hampered by the slow inference speeds inherent in sequential token generation. Blockwise parallel decoding (BPD) was proposed by Stern et al. (2018) as a way to improve inference speed of language models. In this paper, we make two contributions to understanding and improving BPD drafts. We first offer an analysis of the token distributions produced by the BPD prediction heads. Secondly, we use this analysis to inform algorithms to improve BPD inference speed by refining the BPD drafts using small <b>n-gram</b> or neural language models. We empirically show that these refined BPD drafts yield a higher average verified prefix length across tasks.</p></p class="citation"></blockquote><h2 id=cslg-13>cs.LG (13)</h2><h3 id=113--21102-hierarchical-attention-models-for-multi-relational-graphs-roshni-g-iyer-et-al-2024>(1/13 | 21/102) Hierarchical Attention Models for Multi-Relational Graphs (Roshni G. Iyer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roshni G. Iyer, Wei Wang, Yizhou Sun. (2024)<br><strong>Hierarchical Attention Models for Multi-Relational Graphs</strong><br><button class=copy-to-clipboard title="Hierarchical Attention Models for Multi-Relational Graphs" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 113<br>Keywords: Graph Attention Networks, Graph Attention Networks, Graph Convolutional Network, Node Classification, Graph, Graph Neural Network, Graph Neural Network, Node Embedding, Convolution, Convolutional Neural Network, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09365v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09365v1.pdf filename=2404.09365v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Bi-Level Attention-Based Relational <b>Graph</b> <b>Convolutional</b> <b>Networks</b> (BR-GCN), unique neural network architectures that utilize masked <b>self-attentional</b> layers with relational <b>graph</b> <b>convolutions,</b> <b>to</b> effectively operate on highly multi-relational data. BR-GCN models use bi-level attention to learn <b>node</b> <b>embeddings</b> through (1) <b>node-level</b> <b>attention,</b> and (2) relation-level attention. The <b>node-level</b> <b>self-attentional</b> layers use intra-relational <b>graph</b> <b>interactions</b> <b>to</b> learn relation-specific <b>node</b> <b>embeddings</b> using a weighted aggregation of neighborhood features in a sparse subgraph region. The relation-level <b>self-attentional</b> layers use inter-relational <b>graph</b> <b>interactions</b> <b>to</b> learn the final <b>node</b> <b>embeddings</b> using a weighted aggregation of relation-specific <b>node</b> <b>embeddings.</b> The BR-GCN bi-level attention mechanism extends <b>Transformer-based</b> multiplicative attention from the natural language processing (NLP) domain, and <b>Graph</b> <b>Attention</b> <b>Networks</b> <b>(GAT)-based</b> attention, to large-scale heterogeneous <b>graphs</b> <b>(HGs).</b> <b>On</b> <b>node</b> <b>classification,</b> BR-GCN outperforms baselines from 0.29% to 14.95% as a stand-alone model, and on link prediction, BR-GCN outperforms baselines from 0.02% to 7.40% as an auto-encoder model. We also conduct ablation studies to evaluate the quality of BR-GCN&rsquo;s relation-level attention and discuss how its learning of <b>graph</b> <b>structure</b> <b>may</b> be transferred to enrich other <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs).</b> Through various experiments, we show that BR-GCN&rsquo;s attention mechanism is both scalable and more effective in learning compared to state-of-the-art <b>GNNs.</b></p></p class="citation"></blockquote><h3 id=213--22102-foundational-gpt-model-for-meg-richard-csaky-et-al-2024>(2/13 | 22/102) Foundational GPT Model for MEG (Richard Csaky et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Richard Csaky, Mats W. J. van Es, Oiwi Parker Jones, Mark Woolrich. (2024)<br><strong>Foundational GPT Model for MEG</strong><br><button class=copy-to-clipboard title="Foundational GPT Model for MEG" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 80<br>Keywords: Augmented Reality (AR), Fine-tuning, Simulation, Simulator, Unsupervised Learning, GPT, GPT-2, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09256v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09256v1.pdf filename=2404.09256v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning techniques can be used to first training <b>unsupervised</b> models on large amounts of unlabelled data, before <b>fine-tuning</b> the models on specific tasks. This approach has seen massive success for various kinds of data, e.g. images, language, audio, and holds the promise of improving performance in various downstream tasks (e.g. encoding or decoding brain data). However, there has been limited progress taking this approach for modelling brain signals, such as Magneto-/electroencephalography (M/EEG). Here we propose two classes of deep learning foundational models that can be trained using forecasting of unlabelled MEG. First, we consider a modified Wavenet; and second, we consider a modified <b>Transformer-based</b> <b>(GPT2)</b> model. The modified <b>GPT2</b> includes a novel application of tokenisation and embedding methods, allowing a model developed initially for the discrete domain of language to be applied to continuous multichannel time series data. We also extend the forecasting framework to include condition labels as inputs, enabling better modelling (encoding) of task data. We compare the performance of these deep learning models with standard linear autoregressive <b>(AR)</b> modelling on MEG data. This shows that <b>GPT2-based</b> models provide better modelling capabilities than Wavenet and linear <b>AR</b> models, by better reproducing the temporal, spatial and spectral characteristics of real data and evoked activity in task data. We show how the <b>GPT2</b> model scales well to multiple subjects, while adapting its model to each subject through subject embedding. Finally, we show how such a model can be useful in downstream decoding tasks through data <b>simulation.</b> All code is available on GitHub (<a href=https://github.com/ricsinaruto/MEG-transfer-decoding)>https://github.com/ricsinaruto/MEG-transfer-decoding)</a>.</p></p class="citation"></blockquote><h3 id=313--23102-knowledgeable-agents-by-offline-reinforcement-learning-from-large-language-model-rollouts-jing-cheng-pang-et-al-2024>(3/13 | 23/102) Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts (Jing-Cheng Pang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing-Cheng Pang, Si-Hang Yang, Kaiyuan Li, Jiaji Zhang, Xiong-Hui Chen, Nan Tang, Yang Yu. (2024)<br><strong>Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts</strong><br><button class=copy-to-clipboard title="Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Fine-tuning, Offline Reinforcement Learning, Reinforcement Learning, Grounding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09248v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09248v1.pdf filename=2404.09248v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL) trains agents to accomplish complex tasks through environmental interaction data, but its capacity is also limited by the scope of the available data. To obtain a knowledgeable agent, a promising approach is to leverage the knowledge from <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Despite previous studies combining <b>LLMs</b> with RL, seamless integration of the two components remains challenging due to their semantic gap. This paper introduces a novel method, Knowledgeable Agents from Language Model Rollouts (KALM), which extracts knowledge from <b>LLMs</b> in the form of imaginary rollouts that can be easily learned by the agent through <b>offline</b> <b>reinforcement</b> <b>learning</b> methods. The primary challenge of KALM lies in <b>LLM</b> <b>grounding,</b> as <b>LLMs</b> are inherently limited to textual data, whereas environmental data often comprise numerical vectors unseen to <b>LLMs.</b> To address this, KALM <b>fine-tunes</b> the <b>LLM</b> to perform various tasks based on environmental data, including bidirectional translation between natural language descriptions of skills and their corresponding rollout data. This <b>grounding</b> process enhances the <b>LLM&rsquo;s</b> comprehension of environmental dynamics, enabling it to generate diverse and meaningful imaginary rollouts that reflect novel skills. Initial empirical evaluations on the CLEVR-Robot environment demonstrate that KALM enables agents to complete complex rephrasings of task goals and extend their capabilities to novel tasks requiring unprecedented optimal behaviors. KALM achieves a success rate of 46% in executing tasks with unseen goals, substantially surpassing the 26% success rate achieved by baseline methods. Furthermore, KALM effectively enables the <b>LLM</b> to comprehend environmental dynamics, resulting in the generation of meaningful imaginary rollouts that reflect novel skills and demonstrate the seamless integration of <b>large</b> <b>language</b> <b>models</b> and <b>reinforcement</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=413--24102-feddistill-global-model-distillation-for-local-model-de-biasing-in-non-iid-federated-learning-changlin-song-et-al-2024>(4/13 | 24/102) FedDistill: Global Model Distillation for Local Model De-Biasing in Non-IID Federated Learning (Changlin Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changlin Song, Divya Saxena, Jiannong Cao, Yuqing Zhao. (2024)<br><strong>FedDistill: Global Model Distillation for Local Model De-Biasing in Non-IID Federated Learning</strong><br><button class=copy-to-clipboard title="FedDistill: Global Model Distillation for Local Model De-Biasing in Non-IID Federated Learning" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Benchmarking, Federated Learning, Knowledge Distillation, Knowledge Transfer, Model Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09210v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09210v1.pdf filename=2404.09210v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) is a novel approach that allows for collaborative machine learning while preserving data privacy by leveraging <b>models</b> <b>trained</b> on decentralized devices. However, FL faces challenges due to non-uniformly distributed (non-iid) data across clients, which impacts <b>model</b> <b>performance</b> and its generalization capabilities. To tackle the non-iid issue, recent efforts have utilized the global <b>model</b> <b>as</b> a teaching mechanism for local <b>models.</b> <b>However,</b> our pilot study shows that their effectiveness is constrained by imbalanced data distribution, which induces biases in local <b>models</b> <b>and</b> leads to a &rsquo;local forgetting&rsquo; phenomenon, where the ability of <b>models</b> <b>to</b> generalize degrades over time, particularly for underrepresented classes. This paper introduces FedDistill, a framework enhancing the <b>knowledge</b> <b>transfer</b> from the global <b>model</b> <b>to</b> local <b>models,</b> <b>focusing</b> on the issue of imbalanced class distribution. Specifically, FedDistill employs group <b>distillation,</b> segmenting classes based on their frequency in local datasets to facilitate a focused <b>distillation</b> process to classes with fewer samples. Additionally, FedDistill dissects the global <b>model</b> <b>into</b> a feature extractor and a classifier. This separation empowers local <b>models</b> <b>with</b> more generalized data representation capabilities and ensures more accurate classification across all classes. FedDistill mitigates the adverse effects of data imbalance, ensuring that local <b>models</b> <b>do</b> not forget underrepresented classes but instead become more adept at recognizing and classifying them accurately. Our comprehensive experiments demonstrate FedDistill&rsquo;s effectiveness, surpassing existing baselines in accuracy and convergence speed across several <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=513--25102-degnn-dual-experts-graph-neural-network-handling-both-edge-and-node-feature-noise-tai-hasegawa-et-al-2024>(5/13 | 25/102) DEGNN: Dual Experts Graph Neural Network Handling Both Edge and Node Feature Noise (Tai Hasegawa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tai Hasegawa, Sukwon Yun, Xin Liu, Yin Jun Phua, Tsuyoshi Murata. (2024)<br><strong>DEGNN: Dual Experts Graph Neural Network Handling Both Edge and Node Feature Noise</strong><br><button class=copy-to-clipboard title="DEGNN: Dual Experts Graph Neural Network Handling Both Edge and Node Feature Noise" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09207v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09207v1.pdf filename=2404.09207v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have achieved notable success in various applications over <b>graph</b> <b>data.</b> <b>However,</b> recent research has revealed that real-world <b>graphs</b> <b>often</b> <b>contain</b> noise, and <b>GNNs</b> are susceptible to noise in the <b>graph.</b> <b>To</b> <b>address</b> this issue, several <b>Graph</b> <b>Structure</b> <b>Learning</b> (GSL) models have been introduced. While GSL models are tailored to enhance robustness against edge noise through edge reconstruction, a significant limitation surfaces: their high reliance on node features. This inherent dependence amplifies their susceptibility to noise within node features. Recognizing this vulnerability, we present DEGNN, a novel <b>GNN</b> model designed to adeptly mitigate noise in both edges and node features. The core idea of DEGNN is to design two separate experts: an edge expert and a node feature expert. These experts utilize <b>self-supervised</b> <b>learning</b> techniques to produce modified edges and node features. Leveraging these modified representations, DEGNN subsequently addresses downstream tasks, ensuring robustness against noise present in both edges and node features of real-world <b>graphs.</b> <b>Notably,</b> <b>the</b> modification process can be trained end-to-end, empowering DEGNN to adjust dynamically and achieves optimal edge and node representations for specific tasks. Comprehensive experiments demonstrate DEGNN&rsquo;s efficacy in managing noise, both in original real-world <b>graphs</b> <b>and</b> <b>in</b> <b>graphs</b> <b>with</b> <b>synthetic</b> noise.</p></p class="citation"></blockquote><h3 id=613--26102-mitigating-heterogeneity-among-factor-tensors-via-lie-group-manifolds-for-tensor-decomposition-based-temporal-knowledge-graph-embedding-jiang-li-et-al-2024>(6/13 | 26/102) Mitigating Heterogeneity among Factor Tensors via Lie Group Manifolds for Tensor Decomposition Based Temporal Knowledge Graph Embedding (Jiang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiang Li, Xiangdong Su, Yeyun Gong, Guanglai Gao. (2024)<br><strong>Mitigating Heterogeneity among Factor Tensors via Lie Group Manifolds for Tensor Decomposition Based Temporal Knowledge Graph Embedding</strong><br><button class=copy-to-clipboard title="Mitigating Heterogeneity among Factor Tensors via Lie Group Manifolds for Tensor Decomposition Based Temporal Knowledge Graph Embedding" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 38<br>Keywords: Graph, Graph Embedding, Knowledge Graph, Tensor Decomposition, Temporal Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09155v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09155v1.pdf filename=2404.09155v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have highlighted the effectiveness of <b>tensor</b> <b>decomposition</b> methods in the <b>Temporal</b> <b>Knowledge</b> <b>Graphs</b> <b>Embedding</b> (TKGE) task. However, we found that inherent heterogeneity among factor <b>tensors</b> <b>in</b> <b>tensor</b> <b>decomposition</b> significantly hinders the <b>tensor</b> <b>fusion</b> process and further limits the performance of link prediction. To overcome this limitation, we introduce a novel method that maps factor <b>tensors</b> <b>onto</b> a unified smooth Lie group manifold to make the distribution of factor <b>tensors</b> <b>approximating</b> homogeneous in <b>tensor</b> <b>decomposition.</b> We provide the theoretical proof of our motivation that homogeneous <b>tensors</b> <b>are</b> more effective than heterogeneous <b>tensors</b> <b>in</b> <b>tensor</b> <b>fusion</b> and approximating the target for <b>tensor</b> <b>decomposition</b> based TKGE methods. The proposed method can be directly integrated into existing <b>tensor</b> <b>decomposition</b> based TKGE methods without introducing extra parameters. Extensive experiments demonstrate the effectiveness of our method in mitigating the heterogeneity and in enhancing the <b>tensor</b> <b>decomposition</b> based TKGE models.</p></p class="citation"></blockquote><h3 id=713--27102-fault-detection-in-mobile-networks-using-diffusion-models-mohamad-nabeel-et-al-2024>(7/13 | 27/102) Fault Detection in Mobile Networks Using Diffusion Models (Mohamad Nabeel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamad Nabeel, Doumitrou Daniil Nimara, Tahar Zanouda. (2024)<br><strong>Fault Detection in Mobile Networks Using Diffusion Models</strong><br><button class=copy-to-clipboard title="Fault Detection in Mobile Networks Using Diffusion Models" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NI, cs.LG<br>Keyword Score: 30<br>Keywords: Diffusion Model, Anomaly Detection, Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09240v1.pdf filename=2404.09240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In today&rsquo;s hyper-connected world, ensuring the reliability of telecom networks becomes increasingly crucial. Telecom networks encompass numerous underlying and intertwined software and hardware components, each providing different functionalities. To ensure the stability of telecom networks, telecom software, and hardware vendors developed several methods to detect any aberrant behavior in telecom networks and enable instant feedback and alerts. These approaches, although powerful, struggle to generalize due to the unsteady nature of the software-intensive embedded system and the complexity and diversity of multi-standard mobile networks. In this paper, we present a system to detect anomalies in telecom networks using a <b>generative</b> <b>AI</b> model. We evaluate several strategies using <b>diffusion</b> <b>models</b> to train the model for <b>anomaly</b> <b>detection</b> using multivariate time-series data. The contributions of this paper are threefold: (i) A proposal of a framework for utilizing <b>diffusion</b> <b>models</b> for time-series <b>anomaly</b> <b>detection</b> in telecom networks, (ii) A proposal of a particular <b>Diffusion</b> <b>model</b> architecture that outperforms other state-of-the-art techniques, (iii) Experiments on a real-world dataset to demonstrate that our model effectively provides explainable results, exposing some of its limitations and suggesting future research avenues to enhance its capabilities further.</p></p class="citation"></blockquote><h3 id=813--28102-transformerfam-feedback-attention-is-working-memory-dongseong-hwang-et-al-2024>(8/13 | 28/102) TransformerFAM: Feedback attention is working memory (Dongseong Hwang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongseong Hwang, Weiran Wang, Zhuoyuan Huo, Khe Chai Sim, Pedro Moreno Mengibar. (2024)<br><strong>TransformerFAM: Feedback attention is working memory</strong><br><button class=copy-to-clipboard title="TransformerFAM: Feedback attention is working memory" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09173v1.pdf filename=2404.09173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>Transformers</b> have revolutionized deep learning, their quadratic attention complexity hinders their ability to process infinitely long inputs. We propose Feedback Attention Memory (FAM), a novel <b>Transformer</b> architecture that leverages a feedback loop to enable the network to attend to its own latent representations. This design fosters the emergence of working memory within the <b>Transformer,</b> allowing it to process indefinitely long sequences. TransformerFAM requires no additional weights, enabling seamless integration with pre-trained models. Our experiments show that TransformerFAM significantly improves <b>Transformer</b> performance on long-context tasks across various model sizes (1B, 8B, and 24B). These results showcase the potential to empower <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to process sequences of unlimited length.</p></p class="citation"></blockquote><h3 id=913--29102-rf-diffusion-radio-signal-generation-via-time-frequency-diffusion-guoxuan-chi-et-al-2024>(9/13 | 29/102) RF-Diffusion: Radio Signal Generation via Time-Frequency Diffusion (Guoxuan Chi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guoxuan Chi, Zheng Yang, Chenshu Wu, Jingao Xu, Yuchong Gao, Yunhao Liu, Tony Xiao Han. (2024)<br><strong>RF-Diffusion: Radio Signal Generation via Time-Frequency Diffusion</strong><br><button class=copy-to-clipboard title="RF-Diffusion: Radio Signal Generation via Time-Frequency Diffusion" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-0, cs-IT, cs-LG, cs.LG, eess-SP, math-IT<br>Keyword Score: 25<br>Keywords: Diffusion Model, Deep Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09140v1.pdf filename=2404.09140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Along with AIGC shines in CV and NLP, its potential in the wireless domain has also emerged in recent years. Yet, existing RF-oriented generative solutions are ill-suited for generating high-quality, time-series RF data due to limited representation capabilities. In this work, inspired by the stellar achievements of the <b>diffusion</b> <b>model</b> in CV and NLP, we adapt it to the RF domain and propose RF-Diffusion. To accommodate the unique characteristics of RF signals, we first introduce a novel Time-Frequency <b>Diffusion</b> <b>theory</b> to enhance the original <b>diffusion</b> <b>model,</b> enabling it to tap into the information within the time, frequency, and complex-valued domains of RF signals. On this basis, we propose a Hierarchical <b>Diffusion</b> <b>Transformer</b> to translate the theory into a practical generative <b>DNN</b> through elaborated design spanning network architecture, functional block, and complex-valued operator, making RF-Diffusion a versatile solution to generate diverse, high-quality, and time-series RF data. Performance comparison with three prevalent generative models demonstrates the RF-Diffusion&rsquo;s superior performance in synthesizing Wi-Fi and FMCW signals. We also showcase the versatility of RF-Diffusion in boosting Wi-Fi sensing systems and performing channel estimation in 5G networks.</p></p class="citation"></blockquote><h3 id=1013--30102-adversarial-robustness-limits-via-scaling-law-and-human-alignment-studies-brian-r-bartoldson-et-al-2024>(10/13 | 30/102) Adversarial Robustness Limits via Scaling-Law and Human-Alignment Studies (Brian R. Bartoldson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brian R. Bartoldson, James Diffenderfer, Konstantinos Parasyris, Bhavya Kailkhura. (2024)<br><strong>Adversarial Robustness Limits via Scaling-Law and Human-Alignment Studies</strong><br><button class=copy-to-clipboard title="Adversarial Robustness Limits via Scaling-Law and Human-Alignment Studies" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09349v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09349v1.pdf filename=2404.09349v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper revisits the simple, long-studied, yet still unsolved problem of making image classifiers robust to imperceptible perturbations. Taking CIFAR10 as an example, SOTA clean accuracy is about $100$%, but SOTA robustness to $\ell_{\infty}$-norm bounded perturbations barely exceeds $70$%. To understand this gap, we analyze how model size, dataset size, and synthetic data quality affect robustness by developing the first <b>scaling</b> <b>laws</b> for <b>adversarial</b> <b>training.</b> Our <b>scaling</b> <b>laws</b> reveal inefficiencies in prior art and provide actionable feedback to advance the field. For instance, we discovered that SOTA methods diverge notably from compute-optimal setups, using excess compute for their level of robustness. Leveraging a compute-efficient setup, we surpass the prior SOTA with $20$% ($70$%) fewer training (inference) FLOPs. We trained various compute-efficient models, with our best achieving $74$% AutoAttack accuracy ($+3$% gain). However, our <b>scaling</b> <b>laws</b> also predict robustness slowly grows then plateaus at $90$%: dwarfing our new SOTA by <b>scaling</b> <b>is</b> impractical, and perfect robustness is impossible. To better understand this predicted limit, we carry out a small-scale human evaluation on the AutoAttack data that fools our top-performing model. Concerningly, we estimate that human performance also plateaus near $90$%, which we show to be attributable to $\ell_{\infty}$-constrained attacks&rsquo; generation of invalid images not consistent with their original labels. Having characterized limiting roadblocks, we outline promising paths for future research.</p></p class="citation"></blockquote><h3 id=1113--31102-map-model-aggregation-and-personalization-in-federated-learning-with-incomplete-classes-xin-chun-li-et-al-2024>(11/13 | 31/102) MAP: Model Aggregation and Personalization in Federated Learning with Incomplete Classes (Xin-Chun Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin-Chun Li, Shaoming Song, Yinchuan Li, Bingshuai Li, Yunfeng Shao, Yang Yang, De-Chuan Zhan. (2024)<br><strong>MAP: Model Aggregation and Personalization in Federated Learning with Incomplete Classes</strong><br><button class=copy-to-clipboard title="MAP: Model Aggregation and Personalization in Federated Learning with Incomplete Classes" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09232v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09232v1.pdf filename=2404.09232v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In some real-world applications, data samples are usually distributed on local devices, where <b>federated</b> <b>learning</b> (FL) techniques are proposed to coordinate decentralized clients without directly sharing users&rsquo; private data. FL commonly follows the parameter server architecture and contains multiple personalization and aggregation procedures. The natural data heterogeneity across clients, i.e., Non-I.I.D. data, challenges both the aggregation and personalization goals in FL. In this paper, we focus on a special kind of Non-I.I.D. scene where clients own incomplete classes, i.e., each client can only access a partial set of the whole class set. The server aims to aggregate a complete classification model that could generalize to all classes, while the clients are inclined to improve the performance of distinguishing their observed classes. For better model aggregation, we point out that the standard softmax will encounter several problems caused by missing classes and propose &ldquo;restricted softmax&rdquo; as an alternative. For better model personalization, we point out that the hard-won personalized models are not well exploited and propose &ldquo;inherited private model&rdquo; to store the personalization experience. Our proposed algorithm named MAP could simultaneously achieve the aggregation and personalization goals in FL. Abundant experimental studies verify the superiorities of our algorithm.</p></p class="citation"></blockquote><h3 id=1213--32102-intelligent-chemical-purification-technique-based-on-machine-learning-wenchao-wu-et-al-2024>(12/13 | 32/102) Intelligent Chemical Purification Technique Based on Machine Learning (Wenchao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenchao Wu, Hao Xu, Dongxiao Zhang, Fanyang Mo. (2024)<br><strong>Intelligent Chemical Purification Technique Based on Machine Learning</strong><br><button class=copy-to-clipboard title="Intelligent Chemical Purification Technique Based on Machine Learning" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, physics-chem-ph<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09114v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09114v1.pdf filename=2404.09114v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present an innovative of artificial intelligence with column chromatography, aiming to resolve inefficiencies and standardize data collection in chemical separation and purification domain. By developing an automated platform for precise data acquisition and employing advanced machine learning algorithms, we constructed predictive models to forecast key separation parameters, thereby enhancing the efficiency and quality of chromatographic processes. The application of <b>transfer</b> <b>learning</b> allows the model to adapt across various column specifications, broadening its utility. A novel metric, separation probability ($S_p$), quantifies the likelihood of effective compound separation, validated through experimental verification. This study signifies a significant step forward int the application of AI in chemical research, offering a scalable solution to traditional chromatography challenges and providing a foundation for future technological advancements in chemical analysis and purification.</p></p class="citation"></blockquote><h3 id=1313--33102-lsrom-learning-self-refined-organizing-map-for-fast-imbalanced-streaming-data-clustering-yongqi-xu-et-al-2024>(13/13 | 33/102) LSROM: Learning Self-Refined Organizing Map for Fast Imbalanced Streaming Data Clustering (Yongqi Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongqi Xu, Yujian Lee, Rong Zou, Yiqun Zhang, Yiu-Ming Cheung. (2024)<br><strong>LSROM: Learning Self-Refined Organizing Map for Fast Imbalanced Streaming Data Clustering</strong><br><button class=copy-to-clipboard title="LSROM: Learning Self-Refined Organizing Map for Fast Imbalanced Streaming Data Clustering" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09243v1.pdf filename=2404.09243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Streaming data <b>clustering</b> is a popular research topic in the fields of data mining and machine learning. Compared to static data, streaming data, which is usually analyzed in data chunks, is more susceptible to encountering the dynamic cluster imbalanced issue. That is, the imbalanced degree of clusters varies in different streaming data chunks, leading to corruption in either the accuracy or the efficiency of streaming data analysis based on existing <b>clustering</b> methods. Therefore, we propose an efficient approach called Learning Self-Refined Organizing Map (LSROM) to handle the imbalanced streaming data <b>clustering</b> problem, where we propose an advanced SOM for representing the global data distribution. The constructed SOM is first refined for guiding the partition of the dataset to form many micro-clusters to avoid the missing small clusters in imbalanced data. Then an efficient merging of the micro-clusters is conducted through quick retrieval based on the SOM, which can automatically yield a true number of imbalanced clusters. In comparison to existing imbalanced data <b>clustering</b> approaches, LSROM is with a lower time complexity $O(n\log n)$, while achieving very competitive <b>clustering</b> accuracy. Moreover, LSROM is interpretable and insensitive to hyper-parameters. Extensive experiments have verified its efficacy.</p></p class="citation"></blockquote><h2 id=csni-2>cs.NI (2)</h2><h3 id=12--34102-interactive-generative-ai-agents-for-satellite-networks-through-a-mixture-of-experts-transmission-ruichen-zhang-et-al-2024>(1/2 | 34/102) Interactive Generative AI Agents for Satellite Networks through a Mixture of Experts Transmission (Ruichen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruichen Zhang, Hongyang Du, Yinqiu Liu, Dusit Niyato, Jiawen Kang, Zehui Xiong, Abbas Jamalipour, Dong In Kim. (2024)<br><strong>Interactive Generative AI Agents for Satellite Networks through a Mixture of Experts Transmission</strong><br><button class=copy-to-clipboard title="Interactive Generative AI Agents for Satellite Networks through a Mixture of Experts Transmission" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-LG, cs-NI, cs.NI<br>Keyword Score: 93<br>Keywords: Graph Attention Networks, Benchmarking, Generative AI, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09134v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09134v1.pdf filename=2404.09134v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In response to the needs of 6G global communications, satellite communication networks have emerged as a key solution. However, the <b>large-scale</b> <b>development</b> <b>of</b> satellite communication networks is constrained by the complex system models, whose modeling is challenging for massive users. Moreover, transmission interference between satellites and users seriously affects communication performance. To solve these problems, this paper develops <b>generative</b> <b>artificial</b> intelligence (AI) agents for model formulation and then applies a mixture of experts (MoE) approach to design transmission strategies. Specifically, we leverage <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to build an interactive modeling paradigm and utilize <b>retrieval-augmented</b> <b>generation</b> <b>(RAG)</b> to extract satellite expert knowledge that supports mathematical modeling. Afterward, by integrating the expertise of multiple specialized components, we propose an MoE-proximal policy optimization (PPO) approach to solve the formulated problem. Each expert can optimize the optimization variables at which it excels through specialized training through its own network and then aggregates them through the <b>gating</b> network to perform joint optimization. The <b>simulation</b> results validate the accuracy and effectiveness of employing a <b>generative</b> <b>agent</b> for problem formulation. Furthermore, the superiority of the proposed MoE-ppo approach over other <b>benchmarks</b> is confirmed in solving the formulated problem. The adaptability of MoE-PPO to various customized modeling problems has also been demonstrated.</p></p class="citation"></blockquote><h3 id=22--35102-a-paradigm-for-collaborative-pervasive-fog-computing-ecosystems-at-the-network-edge-abderrahmen-mtibaa-2024>(2/2 | 35/102) A Paradigm For Collaborative Pervasive Fog Computing Ecosystems at the Network Edge (Abderrahmen Mtibaa, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abderrahmen Mtibaa. (2024)<br><strong>A Paradigm For Collaborative Pervasive Fog Computing Ecosystems at the Network Edge</strong><br><button class=copy-to-clipboard title="A Paradigm For Collaborative Pervasive Fog Computing Ecosystems at the Network Edge" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09354v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09354v1.pdf filename=2404.09354v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While the success of edge and fog computing increased with the proliferation of the Internet of Things (IoT) solutions, such novel computing paradigm, that moves compute resources closer to the source of data and services, must address many challenges such as reducing communication overhead to/from datacenters, the latency to compute and receive results, as well as energy consumption at the mobile and IoT devices. fog-to-fog (f2f) cooperation has recently been proposed to increase the computation capacity at the network edge through cooperation across multiple stakeholders. In this paper we adopt an analytical approach to studying f2f cooperation paradigm. We highlight the benefits of using such new paradigm in comparison with traditional three-tier fog computing paradigms. We use a <b>Continuous</b> <b>Time</b> Markov Chain (CTMC) model for the N f2f cooperating nodes and cast cooperation as an optimization problem, which we solve using the proposed model.</p></p class="citation"></blockquote><h2 id=cscv-25>cs.CV (25)</h2><h3 id=125--36102-weight-copy-and-low-rank-adaptation-for-few-shot-distillation-of-vision-transformers-diana-nicoleta-grigore-et-al-2024>(1/25 | 36/102) Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision Transformers (Diana-Nicoleta Grigore et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Diana-Nicoleta Grigore, Mariana-Iuliana Georgescu, Jon Alvarez Justo, Tor Johansen, Andreea Iuliana Ionescu, Radu Tudor Ionescu. (2024)<br><strong>Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision Transformers</strong><br><button class=copy-to-clipboard title="Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision Transformers" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 90<br>Keywords: Vision Transformer, Few-shot, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Self-supervised Learning, Supervised Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09326v1.pdf filename=2404.09326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> <b>knowledge</b> <b>distillation</b> recently emerged as a viable approach to harness the <b>knowledge</b> <b>of</b> large-scale pre-trained models, using limited data and computational resources. In this paper, we propose a novel <b>few-shot</b> feature <b>distillation</b> approach for <b>vision</b> <b>transformers.</b> Our approach is based on two key steps. Leveraging the fact that <b>vision</b> <b>transformers</b> have a consistent depth-wise structure, we first copy the weights from intermittent layers of existing pre-trained <b>vision</b> <b>transformers</b> (teachers) into shallower architectures (students), where the intermittence factor controls the complexity of the student <b>transformer</b> with respect to its teacher. Next, we employ an enhanced version of Low-Rank Adaptation (LoRA) to <b>distill</b> <b>knowledge</b> <b>into</b> the student in a <b>few-shot</b> scenario, aiming to recover the information processing carried out by the skipped teacher layers. We present comprehensive experiments with <b>supervised</b> and <b>self-supervised</b> <b>transformers</b> as teachers, on five data sets from various domains, including natural, medical and satellite images. The empirical results confirm the superiority of our approach over competitive baselines. Moreover, the ablation results demonstrate the usefulness of each component of the proposed pipeline.</p></p class="citation"></blockquote><h3 id=225--37102-exploring-generative-ai-for-sim2real-in-driving-data-synthesis-haonan-zhao-et-al-2024>(2/25 | 37/102) Exploring Generative AI for Sim2Real in Driving Data Synthesis (Haonan Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haonan Zhao, Yiting Wang, Thomas Bashford-Rogers, Valentina Donzella, Kurt Debattista. (2024)<br><strong>Exploring Generative AI for Sim2Real in Driving Data Synthesis</strong><br><button class=copy-to-clipboard title="Exploring Generative AI for Sim2Real in Driving Data Synthesis" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: ControlNet, Generative AI, Generative Adversarial Network, Probabilistic Model, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09111v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09111v1.pdf filename=2404.09111v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Datasets are essential for training and testing vehicle perception algorithms. However, the collection and annotation of real-world images is time-consuming and expensive. Driving simulators offer a solution by automatically generating various driving scenarios with corresponding annotations, but the <b>simulation-to-reality</b> (Sim2Real) domain gap remains a challenge. While most of the <b>Generative</b> <b>Artificial</b> Intelligence (AI) follows the de facto <b>Generative</b> <b>Adversarial</b> Nets <b>(GANs)-based</b> methods, the recent emerging diffusion <b>probabilistic</b> <b>models</b> have not been fully explored in mitigating Sim2Real challenges for driving data synthesis. To explore the performance, this paper applied three different <b>generative</b> <b>AI</b> methods to leverage semantic label maps from a driving simulator as a bridge for the creation of realistic datasets. A comparative analysis of these methods is presented from the perspective of image quality and perception. New synthetic datasets, which include driving images and auto-generated high-quality annotations, are produced with low costs and high scene variability. The experimental results show that although <b>GAN-based</b> methods are adept at generating high-quality images when provided with manually annotated labels, <b>ControlNet</b> produces synthetic datasets with fewer artefacts and more structural fidelity when using simulator-generated labels. This suggests that the diffusion-based approach may provide improved stability and an alternative method for addressing Sim2Real challenges.</p></p class="citation"></blockquote><h3 id=325--38102-detclipv3-towards-versatile-generative-open-vocabulary-object-detection-lewei-yao-et-al-2024>(3/25 | 38/102) DetCLIPv3: Towards Versatile Generative Open-vocabulary Object Detection (Lewei Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lewei Yao, Renjie Pi, Jianhua Han, Xiaodan Liang, Hang Xu, Wei Zhang, Zhenguo Li, Dan Xu. (2024)<br><strong>DetCLIPv3: Towards Versatile Generative Open-vocabulary Object Detection</strong><br><button class=copy-to-clipboard title="DetCLIPv3: Towards Versatile Generative Open-vocabulary Object Detection" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Object Detection, Benchmarking, Fine-tuning, Zero-shot, Image2text, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09216v1.pdf filename=2404.09216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing open-vocabulary <b>object</b> <b>detectors</b> typically require a predefined set of categories from users, significantly confining their application scenarios. In this paper, we introduce DetCLIPv3, a high-performing detector that excels not only at both open-vocabulary <b>object</b> <b>detection,</b> but also generating hierarchical labels for detected <b>objects.</b> <b>DetCLIPv3</b> is characterized by three core designs: 1. Versatile model architecture: we derive a robust open-set detection framework which is further empowered with generation ability via the integration of a caption head. 2. High information density data: we develop an auto-annotation pipeline leveraging visual <b>large</b> <b>language</b> <b>model</b> to refine captions for <b>large-scale</b> <b>image-text</b> <b>pairs,</b> providing rich, multi-granular <b>object</b> <b>labels</b> to enhance the training. 3. Efficient training strategy: we employ a pre-training stage with low-resolution inputs that enables the <b>object</b> <b>captioner</b> to efficiently learn a broad spectrum of visual concepts from extensive <b>image-text</b> paired data. This is followed by a <b>fine-tuning</b> stage that leverages a small number of high-resolution samples to further enhance detection performance. With these effective designs, DetCLIPv3 demonstrates superior open-vocabulary detection performance, \eg, our Swin-T backbone model achieves a notable 47.0 <b>zero-shot</b> fixed AP on the LVIS minival <b>benchmark,</b> outperforming GLIPv2, GroundingDINO, and DetCLIPv2 by 18.0/19.6/6.6 AP, respectively. DetCLIPv3 also achieves a state-of-the-art 19.7 AP in dense captioning task on VG dataset, showcasing its strong generative capability.</p></p class="citation"></blockquote><h3 id=425--39102-text2taste-a-versatile-egocentric-vision-system-for-intelligent-reading-assistance-using-large-language-model-wiktor-mucha-et-al-2024>(4/25 | 39/102) TEXT2TASTE: A Versatile Egocentric Vision System for Intelligent Reading Assistance Using Large Language Model (Wiktor Mucha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wiktor Mucha, Florin Cuconasu, Naome A. Etori, Valia Kalokyri, Giovanni Trappolini. (2024)<br><strong>TEXT2TASTE: A Versatile Egocentric Vision System for Intelligent Reading Assistance Using Large Language Model</strong><br><button class=copy-to-clipboard title="TEXT2TASTE: A Versatile Egocentric Vision System for Intelligent Reading Assistance Using Large Language Model" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Object Detection, Optical Character Recognition, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09254v1.pdf filename=2404.09254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability to read, understand and find important information from written text is a critical skill in our daily lives for our independence, comfort and safety. However, a significant part of our society is affected by partial vision impairment, which leads to discomfort and dependency in daily activities. To address the limitations of this part of society, we propose an intelligent reading assistant based on smart glasses with embedded RGB cameras and a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM),</b> whose functionality goes beyond corrective lenses. The video recorded from the egocentric perspective of a person wearing the glasses is processed to localise text information using <b>object</b> <b>detection</b> and <b>optical</b> <b>character</b> <b>recognition</b> methods. The <b>LLM</b> processes the data and allows the user to interact with the text and responds to a given query, thus extending the functionality of corrective lenses with the ability to find and <b>summarize</b> knowledge from the text. To evaluate our method, we create a chat-based application that allows the user to interact with the system. The evaluation is conducted in a real-world setting, such as reading menus in a restaurant, and involves four participants. The results show robust accuracy in text retrieval. The system not only provides accurate meal suggestions but also achieves high user satisfaction, highlighting the potential of smart glasses and <b>LLMs</b> in assisting people with special needs.</p></p class="citation"></blockquote><h3 id=525--40102-dreamscape-3d-scene-creation-via-gaussian-splatting-joint-correlation-modeling-xuening-yuan-et-al-2024>(5/25 | 40/102) DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation Modeling (Xuening Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuening Yuan, Hongyu Yang, Yueming Zhao, Di Huang. (2024)<br><strong>DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation Modeling</strong><br><button class=copy-to-clipboard title="DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation Modeling" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Text2image, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09227v1.pdf filename=2404.09227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent progress in text-to-3D creation has been propelled by integrating the potent prior of <b>Diffusion</b> <b>Models</b> from <b>text-to-image</b> generation into the 3D domain. Nevertheless, generating 3D scenes characterized by multiple instances and intricate arrangements remains challenging. In this study, we present DreamScape, a method for creating highly consistent 3D scenes solely from textual descriptions, leveraging the strong 3D representation capabilities of Gaussian Splatting and the complex arrangement abilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Our approach involves a 3D Gaussian Guide ($3{DG^2}$) for scene representation, consisting of semantic primitives (objects) and their spatial transformations and relationships derived directly from text <b>prompts</b> using <b>LLMs.</b> This compositional representation allows for local-to-global optimization of the entire scene. A progressive scale control is tailored during local object generation, ensuring that objects of different sizes and densities adapt to the scene, which addresses training instability issue arising from simple blending in the subsequent global optimization stage. To mitigate potential biases of <b>LLM</b> priors, we model collision relationships between objects at the global level, enhancing physical correctness and overall realism. Additionally, to generate pervasive objects like rain and snow distributed extensively across the scene, we introduce a sparse initialization and densification strategy. Experiments demonstrate that DreamScape offers high usability and controllability, enabling the generation of high-fidelity 3D scenes from only text <b>prompts</b> and achieving state-of-the-art performance compared to other methods.</p></p class="citation"></blockquote><h3 id=625--41102-texthawk-exploring-efficient-fine-grained-perception-of-multimodal-large-language-models-ya-qi-yu-et-al-2024>(6/25 | 41/102) TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models (Ya-Qi Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ya-Qi Yu, Minghui Liao, Jihao Wu, Yongxin Liao, Xiaoyu Zheng, Wei Zeng. (2024)<br><strong>TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 49<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Gemini, Information Compression, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09204v1.pdf filename=2404.09204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) have shown impressive results on various <b>multimodal</b> tasks. However, most existing MLLMs are not well suited for document-oriented tasks, which require fine-grained image perception and <b>information</b> <b>compression.</b> In this paper, we present TextHawk, a MLLM that is specifically designed for document-oriented tasks, while preserving the general capabilities of MLLMs. TextHawk is aimed to explore efficient fine-grained perception by designing four dedicated components. Firstly, a ReSampling and ReArrangement (ReSA) module is proposed to reduce the redundancy in the document texts and lower the computational cost of the MLLM. We explore encoding the positions of each local feature by presenting Scalable Positional Embeddings (SPEs), which can preserve the scalability of various image sizes. A Query Proposal Network (QPN) is then adopted to initialize the queries dynamically among different sub-images. To further enhance the fine-grained visual perceptual ability of the MLLM, we design a Multi-Level Cross-Attention (MLCA) mechanism that captures the hierarchical structure and semantic relations of document images. Furthermore, we create a new <b>instruction-tuning</b> <b>dataset</b> for document-oriented tasks by enriching the <b>multimodal</b> document data with <b>Gemini</b> Pro. We conduct extensive experiments on both general and document-oriented MLLM <b>benchmarks,</b> and show that TextHawk outperforms the state-of-the-art methods, demonstrating its effectiveness and superiority in fine-grained document perception and general abilities.</p></p class="citation"></blockquote><h3 id=725--42102-roofdiffusion-constructing-roofs-from-severely-corrupted-point-data-via-diffusion-kyle-shih-huang-lo-et-al-2024>(7/25 | 42/102) RoofDiffusion: Constructing Roofs from Severely Corrupted Point Data via Diffusion (Kyle Shih-Huang Lo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyle Shih-Huang Lo, Jörg Peters, Eric Spellman. (2024)<br><strong>RoofDiffusion: Constructing Roofs from Severely Corrupted Point Data via Diffusion</strong><br><button class=copy-to-clipboard title="RoofDiffusion: Constructing Roofs from Severely Corrupted Point Data via Diffusion" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 46<br>Keywords: Benchmarking, Benchmarking, Data Augmentation, Self-supervised Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09290v1.pdf filename=2404.09290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate completion and denoising of roof height maps are crucial to reconstructing high-quality 3D buildings. Repairing sparse points can enhance low-cost sensor use and reduce UAV flight overlap. RoofDiffusion is a new end-to-end <b>self-supervised</b> diffusion technique for robustly completing, in particular difficult, roof height maps. RoofDiffusion leverages widely-available curated footprints and can so handle up to 99% point sparsity and 80% roof area occlusion (regional incompleteness). A variant, No-FP RoofDiffusion, simultaneously predicts building footprints and heights. Both quantitatively outperform state-of-the-art unguided depth completion and representative inpainting methods for Digital Elevation Models (DEM), on both a roof-specific <b>benchmark</b> and the BuildingNet dataset. Qualitative assessments show the effectiveness of RoofDiffusion for datasets with real-world scans including AHN3, Dales3D, and USGS 3DEP LiDAR. Tested with the leading City3D algorithm, preprocessing height maps with RoofDiffusion noticeably improves 3D building reconstruction. RoofDiffusion is complemented by a new dataset of 13k complex roof geometries, focusing on long-tail issues in remote sensing; a novel <b>simulation</b> of tree occlusion; and a wide variety of large-area roof cut-outs for <b>data</b> <b>augmentation</b> and <b>benchmarking.</b></p></p class="citation"></blockquote><h3 id=825--43102-facecat-enhancing-face-recognition-security-with-a-unified-generative-model-framework-jiawei-chen-et-al-2024>(8/25 | 43/102) FaceCat: Enhancing Face Recognition Security with a Unified Generative Model Framework (Jiawei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Chen, Xiao Yang, Yinpeng Dong, Hang Su, Jianteng Peng, Zhaoxia Yin. (2024)<br><strong>FaceCat: Enhancing Face Recognition Security with a Unified Generative Model Framework</strong><br><button class=copy-to-clipboard title="FaceCat: Enhancing Face Recognition Security with a Unified Generative Model Framework" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Face Recognition, Adversarial Detection, Benchmarking, Multi-modal, Prompt, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09193v1.pdf filename=2404.09193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Face</b> <b>anti-spoofing</b> (FAS) and <b>adversarial</b> <b>detection</b> (FAD) have been regarded as critical technologies to ensure the safety of <b>face</b> <b>recognition</b> systems. As a consequence of their limited practicality and generalization, some existing methods aim to devise a framework capable of concurrently detecting both threats to address the challenge. Nevertheless, these methods still encounter challenges of insufficient generalization and suboptimal robustness, potentially owing to the inherent drawback of discriminative models. Motivated by the rich structural and detailed features of <b>face</b> <b>generative</b> models, we propose FaceCat which utilizes the <b>face</b> <b>generative</b> model as a pre-trained model to improve the performance of FAS and FAD. Specifically, FaceCat elaborately designs a hierarchical fusion mechanism to capture rich <b>face</b> <b>semantic</b> features of the generative model. These features then serve as a robust foundation for a lightweight head, designed to execute FAS and FAD tasks simultaneously. As relying solely on single-modality data often leads to suboptimal performance, we further propose a novel text-guided <b>multi-modal</b> alignment strategy that utilizes text <b>prompts</b> to enrich feature representation, thereby enhancing performance. For fair evaluations, we build a comprehensive protocol with a wide range of 28 attack types to <b>benchmark</b> the performance. Extensive experiments validate the effectiveness of FaceCat generalizes significantly better and obtains excellent robustness against input transformations.</p></p class="citation"></blockquote><h3 id=925--44102-gcc-generative-calibration-clustering-haifeng-xia-et-al-2024>(9/25 | 44/102) GCC: Generative Calibration Clustering (Haifeng Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haifeng Xia, Hai Huang, Zhengming Ding. (2024)<br><strong>GCC: Generative Calibration Clustering</strong><br><button class=copy-to-clipboard title="GCC: Generative Calibration Clustering" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 41<br>Keywords: Benchmarking, Clustering, Contrastive Learning, Representation Learning, Self-supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09115v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09115v1.pdf filename=2404.09115v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>clustering</b> as an important branch of <b>unsupervised</b> <b>representation</b> <b>learning</b> focuses on embedding semantically similar samples into the identical feature space. This core demand inspires the exploration of <b>contrastive</b> <b>learning</b> and subspace <b>clustering.</b> However, these solutions always rely on the basic assumption that there are sufficient and category-balanced samples for generating valid high-level <b>representation.</b> <b>This</b> hypothesis actually is too strict to be satisfied for real-world applications. To overcome such a challenge, the natural strategy is utilizing generative models to augment considerable instances. How to use these novel samples to effectively fulfill <b>clustering</b> performance improvement is still difficult and under-explored. In this paper, we propose a novel Generative Calibration <b>Clustering</b> (GCC) method to delicately incorporate feature learning and augmentation into <b>clustering</b> procedure. First, we develop a discriminative feature alignment mechanism to discover intrinsic relationship across real and generated samples. Second, we design a <b>self-supervised</b> metric learning to generate more reliable cluster assignment to boost the conditional diffusion generation. Extensive experimental results on three <b>benchmarks</b> validate the effectiveness and advantage of our proposed method over the state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=1025--45102-change-guiding-network-incorporating-change-prior-to-guide-change-detection-in-remote-sensing-imagery-chengxi-han-et-al-2024>(10/25 | 45/102) Change Guiding Network: Incorporating Change Prior to Guide Change Detection in Remote Sensing Imagery (Chengxi Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengxi Han, Chen Wu, Haonan Guo, Meiqi Hu, Jiepan Li, Hongruixuan Chen. (2024)<br><strong>Change Guiding Network: Incorporating Change Prior to Guide Change Detection in Remote Sensing Imagery</strong><br><button class=copy-to-clipboard title="Change Guiding Network: Incorporating Change Prior to Guide Change Detection in Remote Sensing Imagery" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 35<br>Keywords: Convolutional Neural Network, Convolution, Convolutional Neural Network, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09179v1.pdf filename=2404.09179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of automated artificial intelligence algorithms and remote sensing instruments has benefited change detection (CD) tasks. However, there is still a lot of space to study for precise detection, especially the edge integrity and internal holes phenomenon of change features. In order to solve these problems, we design the Change Guiding Network (CGNet), to tackle the insufficient expression problem of change features in the conventional U-Net structure adopted in previous methods, which causes inaccurate edge detection and internal holes. Change maps from deep features with rich semantic information are generated and used as prior information to guide multi-scale feature fusion, which can improve the expression ability of change features. Meanwhile, we propose a <b>self-attention</b> module named Change Guide Module (CGM), which can effectively capture the long-distance dependency among pixels and effectively overcome the problem of the insufficient receptive field of traditional <b>convolutional</b> <b>neural</b> <b>networks.</b> On four major CD datasets, we verify the usefulness and efficiency of the CGNet, and a large number of experiments and ablation studies demonstrate the effectiveness of CGNet. We&rsquo;re going to open-source our code at <a href=https://github.com/ChengxiHAN/CGNet-CD>https://github.com/ChengxiHAN/CGNet-CD</a>.</p></p class="citation"></blockquote><h3 id=1125--46102-tri-modal-confluence-with-temporal-dynamics-for-scene-graph-generation-in-operating-rooms-diandian-guo-et-al-2024>(11/25 | 46/102) Tri-modal Confluence with Temporal Dynamics for Scene Graph Generation in Operating Rooms (Diandian Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Diandian Guo, Manxi Lin, Jialun Pei, He Tang, Yueming Jin, Pheng-Ann Heng. (2024)<br><strong>Tri-modal Confluence with Temporal Dynamics for Scene Graph Generation in Operating Rooms</strong><br><button class=copy-to-clipboard title="Tri-modal Confluence with Temporal Dynamics for Scene Graph Generation in Operating Rooms" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Graph, Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09231v1.pdf filename=2404.09231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A comprehensive understanding of surgical scenes allows for monitoring of the surgical process, reducing the occurrence of accidents and enhancing efficiency for medical professionals. Semantic modeling within operating rooms, as a scene <b>graph</b> generation (SGG) task, is challenging since it involves consecutive recognition of subtle surgical actions over prolonged periods. To address this challenge, we propose a Tri-modal (i.e., images, point clouds, and language) confluence with Temporal dynamics framework, termed TriTemp-OR. Diverging from previous approaches that integrated temporal information via memory <b>graphs,</b> our method embraces two advantages: 1) we directly exploit bi-modal temporal information from the video streaming for hierarchical feature interaction, and 2) the prior knowledge from <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is embedded to alleviate the class-imbalance problem in the operating theatre. Specifically, our model performs temporal interactions across 2D frames and 3D point clouds, including a scale-adaptive multi-view temporal interaction (ViewTemp) and a geometric-temporal point aggregation (PointTemp). Furthermore, we transfer knowledge from the biomedical <b>LLM,</b> LLaVA-Med, to deepen the comprehension of intraoperative relations. The proposed TriTemp-OR enables the aggregation of tri-modal features through relation-aware unification to predict relations so as to generate scene <b>graphs.</b> Experimental results on the 4D-OR <b>benchmark</b> demonstrate the superior performance of our model for long-term OR streaming.</p></p class="citation"></blockquote><h3 id=1225--47102-loopanimate-loopable-salient-object-animation-fanyi-wang-et-al-2024>(12/25 | 47/102) LoopAnimate: Loopable Salient Object Animation (Fanyi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fanyi Wang, Peng Liu, Haotian Hu, Dan Meng, Jingwen Su, Jinjin Xu, Yanhao Zhang, Xiaoming Ren, Zhiwang Zhang. (2024)<br><strong>LoopAnimate: Loopable Salient Object Animation</strong><br><button class=copy-to-clipboard title="LoopAnimate: Loopable Salient Object Animation" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09172v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09172v1.pdf filename=2404.09172v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Research on <b>diffusion</b> <b>model-based</b> video generation has advanced rapidly. However, limitations in object fidelity and generation length hinder its practical applications. Additionally, specific domains like animated wallpapers require seamless looping, where the first and last frames of the video match seamlessly. To address these challenges, this paper proposes LoopAnimate, a novel method for generating videos with consistent start and end frames. To enhance object fidelity, we introduce a framework that decouples multi-level image appearance and textual semantic information. Building upon an image-to-image <b>diffusion</b> <b>model,</b> our approach incorporates both pixel-level and feature-level information from the input image, injecting image appearance and textual semantic embeddings at different positions of the <b>diffusion</b> <b>model.</b> Existing UNet-based video generation models require to input the entire videos during training to encode temporal and positional information at once. However, due to limitations in GPU memory, the number of frames is typically restricted to 16. To address this, this paper proposes a three-stage training strategy with progressively increasing frame numbers and reducing <b>fine-tuning</b> modules. Additionally, we introduce the Temporal E nhanced Motion Module(TEMM) to extend the capacity for encoding temporal and positional information up to 36 frames. The proposed LoopAnimate, which for the first time extends the single-pass generation length of UNet-based video generation models to 35 frames while maintaining high-quality video generation. Experiments demonstrate that LoopAnimate achieves state-of-the-art performance in both objective metrics, such as fidelity and temporal consistency, and subjective evaluation results.</p></p class="citation"></blockquote><h3 id=1325--48102-fusion-mamba-for-cross-modality-object-detection-wenhao-dong-et-al-2024>(13/25 | 48/102) Fusion-Mamba for Cross-modality Object Detection (Wenhao Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenhao Dong, Haodong Zhu, Shaohui Lin, Xiaoyan Luo, Yunhang Shen, Xuhui Liu, Juan Zhang, Guodong Guo, Baochang Zhang. (2024)<br><strong>Fusion-Mamba for Cross-modality Object Detection</strong><br><button class=copy-to-clipboard title="Fusion-Mamba for Cross-modality Object Detection" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Graph Attention Networks<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09146v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09146v1.pdf filename=2404.09146v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-modality fusing complementary information from different modalities effectively improves <b>object</b> <b>detection</b> performance, making it more useful and robust for a wider range of applications. Existing fusion strategies combine different types of images or merge different backbone features through elaborated neural network modules. However, these methods neglect that modality disparities affect cross-modality fusion performance, as different modalities with different camera focal lengths, placements, and angles are hardly fused. In this paper, we investigate cross-modality fusion by associating cross-modal features in a hidden state space based on an improved Mamba with a <b>gating</b> mechanism. We design a Fusion-Mamba block (FMB) to map cross-modal features into a hidden state space for interaction, thereby reducing disparities between cross-modal features and enhancing the representation consistency of fused features. FMB contains two modules: the State Space Channel Swapping (SSCS) module facilitates shallow feature fusion, and the Dual State Space Fusion (DSSF) enables deep fusion in a hidden state space. Through extensive experiments on public datasets, our proposed approach outperforms the state-of-the-art methods on $m$AP with 5.9% on $M^3FD$ and 4.9% on FLIR-Aligned datasets, demonstrating superior <b>object</b> <b>detection</b> performance. To the best of our knowledge, this is the first work to explore the potential of Mamba for cross-modal fusion and establish a new baseline for cross-modality <b>object</b> <b>detection.</b></p></p class="citation"></blockquote><h3 id=1425--49102-syntstereo2real-edge-aware-gan-for-remote-sensing-image-to-image-translation-while-maintaining-stereo-constraint-vasudha-venkatesan-et-al-2024>(14/25 | 49/102) SyntStereo2Real: Edge-Aware GAN for Remote Sensing Image-to-Image Translation while Maintaining Stereo Constraint (Vasudha Venkatesan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vasudha Venkatesan, Daniel Panangian, Mario Fuentes Reyes, Ksenia Bittner. (2024)<br><strong>SyntStereo2Real: Edge-Aware GAN for Remote Sensing Image-to-Image Translation while Maintaining Stereo Constraint</strong><br><button class=copy-to-clipboard title="SyntStereo2Real: Edge-Aware GAN for Remote Sensing Image-to-Image Translation while Maintaining Stereo Constraint" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Deep Neural Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09277v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09277v1.pdf filename=2404.09277v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of remote sensing, the scarcity of stereo-matched and particularly lack of accurate ground truth data often hinders the training of <b>deep</b> <b>neural</b> <b>networks.</b> The use of synthetically generated images as an alternative, alleviates this problem but suffers from the problem of domain generalization. Unifying the capabilities of image-to-image translation and stereo-matching presents an effective solution to address the issue of domain generalization. Current methods involve combining two networks, an unpaired image-to-image translation network and a stereo-matching network, while jointly optimizing them. We propose an edge-aware <b>GAN-based</b> network that effectively tackles both tasks simultaneously. We obtain edge maps of input images from the Sobel operator and use it as an additional input to the encoder in the generator to enforce geometric consistency during translation. We additionally include a warping loss calculated from the translated images to maintain the stereo consistency. We demonstrate that our model produces qualitatively and quantitatively superior results than existing models, and its applicability extends to diverse domains, including autonomous driving.</p></p class="citation"></blockquote><h3 id=1525--50102-in-my-perspective-in-my-hands-accurate-egocentric-2d-hand-pose-and-action-recognition-wiktor-mucha-et-al-2024>(15/25 | 50/102) In My Perspective, In My Hands: Accurate Egocentric 2D Hand Pose and Action Recognition (Wiktor Mucha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wiktor Mucha, Martin Kampel. (2024)<br><strong>In My Perspective, In My Hands: Accurate Egocentric 2D Hand Pose and Action Recognition</strong><br><button class=copy-to-clipboard title="In My Perspective, In My Hands: Accurate Egocentric 2D Hand Pose and Action Recognition" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09308v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09308v1.pdf filename=2404.09308v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Action recognition is essential for egocentric video understanding, allowing automatic and continuous monitoring of Activities of Daily Living (ADLs) without user effort. Existing literature focuses on 3D hand pose input, which requires computationally intensive depth estimation networks or wearing an uncomfortable depth sensor. In contrast, there has been insufficient research in understanding 2D hand pose for egocentric action recognition, despite the availability of user-friendly smart glasses in the market capable of capturing a single RGB image. Our study aims to fill this research gap by exploring the field of 2D hand pose estimation for egocentric action recognition, making two contributions. Firstly, we introduce two novel approaches for 2D hand pose estimation, namely EffHandNet for single-hand estimation and EffHandEgoNet, tailored for an egocentric perspective, capturing interactions between hands and objects. Both methods outperform state-of-the-art models on H2O and FPHA public <b>benchmarks.</b> Secondly, we present a robust action recognition architecture from 2D hand and object poses. This method incorporates EffHandEgoNet, and a <b>transformer-based</b> action recognition method. Evaluated on H2O and FPHA datasets, our architecture has a faster inference time and achieves an accuracy of 91.32% and 94.43%, respectively, surpassing state of the art, including 3D-based methods. Our work demonstrates that using 2D skeletal data is a robust approach for egocentric action understanding. Extensive evaluation and ablation studies show the impact of the hand pose estimation approach, and how each input affects the overall performance.</p></p class="citation"></blockquote><h3 id=1625--51102-trafficvlm-a-controllable-visual-language-model-for-traffic-video-captioning-quang-minh-dinh-et-al-2024>(16/25 | 51/102) TrafficVLM: A Controllable Visual Language Model for Traffic Video Captioning (Quang Minh Dinh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quang Minh Dinh, Minh Khoi Ho, Anh Quan Dang, Hung Phong Tran. (2024)<br><strong>TrafficVLM: A Controllable Visual Language Model for Traffic Video Captioning</strong><br><button class=copy-to-clipboard title="TrafficVLM: A Controllable Visual Language Model for Traffic Video Captioning" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Fine-tuning, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09275v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09275v1.pdf filename=2404.09275v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traffic video description and analysis have received much attention recently due to the growing demand for efficient and reliable urban surveillance systems. Most existing methods only focus on locating traffic event segments, which severely lack descriptive details related to the behaviour and context of all the subjects of interest in the events. In this paper, we present TrafficVLM, a novel <b>multi-modal</b> dense video captioning model for vehicle ego camera view. TrafficVLM models traffic video events at different levels of analysis, both spatially and temporally, and generates long fine-grained descriptions for the vehicle and pedestrian at different phases of the event. We also propose a conditional component for TrafficVLM to control the generation outputs and a multi-task <b>fine-tuning</b> paradigm to enhance TrafficVLM&rsquo;s learning capability. Experiments show that TrafficVLM performs well on both vehicle and overhead camera views. Our solution achieved outstanding results in Track 2 of the AI City Challenge 2024, ranking us third in the challenge standings. Our code is publicly available at <a href=https://github.com/quangminhdinh/TrafficVLM>https://github.com/quangminhdinh/TrafficVLM</a>.</p></p class="citation"></blockquote><h3 id=1725--52102-fedccl-federated-dual-clustered-feature-contrast-under-domain-heterogeneity-yu-qiao-et-al-2024>(17/25 | 52/102) FedCCL: Federated Dual-Clustered Feature Contrast Under Domain Heterogeneity (Yu Qiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Qiao, Huy Q. Le, Mengchun Zhang, Apurba Adhikary, Chaoning Zhang, Choong Seon Hong. (2024)<br><strong>FedCCL: Federated Dual-Clustered Feature Contrast Under Domain Heterogeneity</strong><br><button class=copy-to-clipboard title="FedCCL: Federated Dual-Clustered Feature Contrast Under Domain Heterogeneity" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Clustering, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09259v1.pdf filename=2404.09259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) facilitates a privacy-preserving neural network training paradigm through collaboration between edge clients and a central server. One significant challenge is that the distributed data is not independently and identically distributed (non-IID), typically including both intra-domain and inter-domain heterogeneity. However, recent research is limited to simply using averaged signals as a form of regularization and only focusing on one aspect of these non-IID challenges. Given these limitations, this paper clarifies these two non-IID challenges and attempts to introduce cluster representation to address them from both local and global perspectives. Specifically, we propose a dual-clustered feature contrast-based FL framework with dual focuses. First, we employ <b>clustering</b> on the local representations of each client, aiming to capture intra-class information based on these local clusters at a high level of granularity. Then, we facilitate cross-client knowledge sharing by pulling the local representation closer to clusters shared by clients with similar semantics while pushing them away from clusters with dissimilar semantics. Second, since the sizes of local clusters belonging to the same class may differ for each client, we further utilize <b>clustering</b> on the global side and conduct averaging to create a consistent global signal for guiding each local training in a contrastive manner. Experimental results on multiple datasets demonstrate that our proposal achieves comparable or superior performance gain under intra-domain and inter-domain heterogeneity.</p></p class="citation"></blockquote><h3 id=1825--53102-textitsweet----an-open-source-modular-platform-for-contactless-hand-vascular-biometric-experiments-david-geissbühler-et-al-2024>(18/25 | 53/102) \textit{sweet} &ndash; An Open Source Modular Platform for Contactless Hand Vascular Biometric Experiments (David Geissbühler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Geissbühler, Sushil Bhattacharjee, Ketan Kotwal, Guillaume Clivaz, Sébastien Marcel. (2024)<br><strong>\textit{sweet} &ndash; An Open Source Modular Platform for Contactless Hand Vascular Biometric Experiments</strong><br><button class=copy-to-clipboard title="\textit{sweet} -- An Open Source Modular Platform for Contactless Hand Vascular Biometric Experiments" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-2, I-4, I-5, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: PaLM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09376v1.pdf filename=2404.09376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current finger-vein or <b>palm-vein</b> recognition systems usually require direct contact of the subject with the apparatus. This can be problematic in environments where hygiene is of primary importance. In this work we present a contactless vascular biometrics sensor platform named \sweet which can be used for hand vascular biometrics studies (wrist-, palm- and finger-vein) and surface features such as palmprint. It supports several acquisition modalities such as multi-spectral Near-Infrared (NIR), RGB-color, Stereo Vision (SV) and Photometric Stereo (PS). Using this platform we collect a dataset consisting of the fingers, <b>palm</b> and wrist vascular data of 120 subjects and develop a powerful 3D pipeline for the pre-processing of this data. We then present biometric experimental results, focusing on Finger-Vein Recognition (FVR). Finally, we discuss fusion of multiple modalities, such <b>palm-vein</b> combined with <b>palm-print</b> biometrics. The acquisition software, parts of the hardware design, the new FV dataset, as well as source-code for our experiments are publicly available for research purposes.</p></p class="citation"></blockquote><h3 id=1925--54102-bridging-data-islands-geographic-heterogeneity-aware-federated-learning-for-collaborative-remote-sensing-semantic-segmentation-jieyi-tan-et-al-2024>(19/25 | 54/102) Bridging Data Islands: Geographic Heterogeneity-Aware Federated Learning for Collaborative Remote Sensing Semantic Segmentation (Jieyi Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jieyi Tan, Yansheng Li, Sergey A. Bartalev, Bo Dang, Wei Chen, Yongjun Zhang, Liangqi Yuan. (2024)<br><strong>Bridging Data Islands: Geographic Heterogeneity-Aware Federated Learning for Collaborative Remote Sensing Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Bridging Data Islands: Geographic Heterogeneity-Aware Federated Learning for Collaborative Remote Sensing Semantic Segmentation" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09292v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09292v1.pdf filename=2404.09292v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remote sensing semantic segmentation (RSS) is an essential task in Earth Observation missions. Due to data privacy concerns, high-quality remote sensing images with annotations cannot be well shared among institutions, making it difficult to fully utilize RSS data to train a generalized model. <b>Federated</b> <b>Learning</b> (FL), a privacy-preserving collaborative learning technology, is a potential solution. However, the current research on how to effectively apply FL in RSS is still scarce and requires further investigation. Remote sensing images in various institutions often exhibit strong geographical heterogeneity. More specifically, it is reflected in terms of class-distribution heterogeneity and object-appearance heterogeneity. Unfortunately, most existing FL studies show inadequate focus on geographical heterogeneity, thus leading to performance degradation in the global model. Considering the aforementioned issues, we propose a novel Geographic Heterogeneity-Aware <b>Federated</b> <b>Learning</b> (GeoFed) framework to address privacy-preserving RSS. Through Global Feature Extension and Tail Regeneration modules, class-distribution heterogeneity is alleviated. Additionally, we design an Essential Feature Mining strategy to alleviate object-appearance heterogeneity by constructing essential features. Extensive experiments on three datasets (i.e., FBP, CASID, Inria) show that our GeoFed consistently outperforms the current state-of-the-art methods. The code will be available publicly.</p></p class="citation"></blockquote><h3 id=2025--55102-vrs-nerf-visual-relocalization-with-sparse-neural-radiance-field-fei-xue-et-al-2024>(20/25 | 55/102) VRS-NeRF: Visual Relocalization with Sparse Neural Radiance Field (Fei Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fei Xue, Ignas Budvytis, Daniel Olmeda Reino, Roberto Cipolla. (2024)<br><strong>VRS-NeRF: Visual Relocalization with Sparse Neural Radiance Field</strong><br><button class=copy-to-clipboard title="VRS-NeRF: Visual Relocalization with Sparse Neural Radiance Field" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 10<br>Keywords: Virtual Reality (VR)<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09271v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09271v1.pdf filename=2404.09271v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual relocalization is a key technique to autonomous driving, robotics, and virtual/augmented reality. After decades of explorations, absolute pose regression (APR), scene coordinate regression (SCR), and hierarchical methods (HMs) have become the most popular frameworks. However, in spite of high efficiency, APRs and SCRs have limited accuracy especially in large-scale outdoor scenes; HMs are accurate but need to store a large number of 2D descriptors for matching, resulting in poor efficiency. In this paper, we propose an efficient and accurate framework, called <b>VRS-NeRF,</b> for visual relocalization with sparse neural radiance field. Precisely, we introduce an explicit geometric map (EGM) for 3D map representation and an implicit learning map (ILM) for sparse patches rendering. In this localization process, EGP provides priors of spare 2D points and ILM utilizes these sparse points to render patches with sparse NeRFs for matching. This allows us to discard a large number of 2D descriptors so as to reduce the map size. Moreover, rendering patches only for useful points rather than all pixels in the whole image reduces the rendering time significantly. This framework inherits the accuracy of HMs and discards their low efficiency. Experiments on 7Scenes, CambridgeLandmarks, and Aachen datasets show that our method gives much better accuracy than APRs and SCRs, and close performance to HMs but is much more efficient.</p></p class="citation"></blockquote><h3 id=2125--56102-hanet-a-hierarchical-attention-network-for-change-detection-with-bitemporal-very-high-resolution-remote-sensing-images-chengxi-han-et-al-2024>(21/25 | 56/102) HANet: A Hierarchical Attention Network for Change Detection With Bitemporal Very-High-Resolution Remote Sensing Images (Chengxi Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengxi Han, Chen Wu, Haonan Guo, Meiqi Hu, Hongruixuan Chen. (2024)<br><strong>HANet: A Hierarchical Attention Network for Change Detection With Bitemporal Very-High-Resolution Remote Sensing Images</strong><br><button class=copy-to-clipboard title="HANet: A Hierarchical Attention Network for Change Detection With Bitemporal Very-High-Resolution Remote Sensing Images" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09178v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09178v1.pdf filename=2404.09178v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Benefiting from the developments in deep learning technology, deep-learning-based algorithms employing automatic feature extraction have achieved remarkable performance on the change detection (CD) task. However, the performance of existing deep-learning-based CD methods is hindered by the imbalance between changed and unchanged pixels. To tackle this problem, a progressive foreground-balanced sampling strategy on the basis of not adding change information is proposed in this article to help the model accurately learn the features of the changed pixels during the early training process and thereby improve detection performance.Furthermore, we design a discriminative Siamese network, hierarchical attention network (HANet), which can integrate multiscale features and refine detailed features. The main part of HANet is the HAN module, which is a lightweight and effective <b>self-attention</b> mechanism. Extensive experiments and ablation studies on two CDdatasets with extremely unbalanced labels validate the effectiveness and efficiency of the proposed method.</p></p class="citation"></blockquote><h3 id=2225--57102-coreset-selection-for-object-detection-hojun-lee-et-al-2024>(22/25 | 57/102) Coreset Selection for Object Detection (Hojun Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hojun Lee, Suyoung Kim, Junhoo Lee, Jaeyoung Yoo, Nojun Kwak. (2024)<br><strong>Coreset Selection for Object Detection</strong><br><button class=copy-to-clipboard title="Coreset Selection for Object Detection" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09161v1.pdf filename=2404.09161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Coreset selection is a method for selecting a small, representative subset of an entire dataset. It has been primarily researched in image classification, assuming there is only one <b>object</b> <b>per</b> image. However, coreset selection for <b>object</b> <b>detection</b> is more challenging as an image can contain multiple <b>objects.</b> <b>As</b> a result, much research has yet to be done on this topic. Therefore, we introduce a new approach, Coreset Selection for <b>Object</b> <b>Detection</b> (CSOD). CSOD generates imagewise and classwise representative feature vectors for multiple <b>objects</b> <b>of</b> the same class within each image. Subsequently, we adopt submodular optimization for considering both representativeness and diversity and utilize the representative vectors in the submodular optimization process to select a subset. When we evaluated CSOD on the Pascal VOC dataset, CSOD outperformed random selection by +6.4%p in AP$_{50}$ when selecting 200 images.</p></p class="citation"></blockquote><h3 id=2325--58102-streaknet-arch-an-anti-scattering-network-based-architecture-for-underwater-carrier-lidar-radar-imaging-xuelong-li-et-al-2024>(23/25 | 58/102) StreakNet-Arch: An Anti-scattering Network-based Architecture for Underwater Carrier LiDAR-Radar Imaging (Xuelong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuelong Li, Hongjun An, Guangying Li, Xing Wang, Guanghua Cheng, Zhe Sun. (2024)<br><strong>StreakNet-Arch: An Anti-scattering Network-based Architecture for Underwater Carrier LiDAR-Radar Imaging</strong><br><button class=copy-to-clipboard title="StreakNet-Arch: An Anti-scattering Network-based Architecture for Underwater Carrier LiDAR-Radar Imaging" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09158v1.pdf filename=2404.09158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce StreakNet-Arch, a novel signal processing architecture designed for Underwater Carrier LiDAR-Radar (UCLR) imaging systems, to address the limitations in scatter suppression and real-time imaging. StreakNet-Arch formulates the signal processing as a real-time, end-to-end binary classification task, enabling real-time image acquisition. To achieve this, we leverage <b>Self-Attention</b> networks and propose a novel Double Branch Cross Attention (DBC-Attention) mechanism that surpasses the performance of traditional methods. Furthermore, we present a method for embedding streak-tube camera images into attention networks, effectively acting as a learned bandpass filter. To facilitate further research, we contribute a publicly available streak-tube camera image dataset. The dataset contains 2,695,168 real-world underwater 3D point cloud data. These advancements significantly improve UCLR capabilities, enhancing its performance and applicability in underwater imaging tasks. The source code and dataset can be found at <a href=https://github.com/BestAnHongjun/StreakNet>https://github.com/BestAnHongjun/StreakNet</a> .</p></p class="citation"></blockquote><h3 id=2425--59102-face-voice-association-in-multilingual-environments-fame-challenge-2024-evaluation-plan-muhammad-saad-saeed-et-al-2024>(24/25 | 59/102) Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan (Muhammad Saad Saeed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Saad Saeed, Shah Nawaz, Muhammad Salman Tahir, Rohan Kumar Das, Muhammad Zaigham Zaheer, Marta Moscati, Markus Schedl, Muhammad Haris Khan, Karthik Nandakumar, Muhammad Haroon Yousaf. (2024)<br><strong>Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan</strong><br><button class=copy-to-clipboard title="Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-SD, cs.CV, eess-AS<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09342v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09342v1.pdf filename=2404.09342v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancements of technology have led to the use of <b>multimodal</b> systems in various real-world applications. Among them, the audio-visual systems are one of the widely used <b>multimodal</b> systems. In the recent years, associating face and voice of a person has gained attention due to presence of unique correlation between them. The Face-voice Association in Multilingual Environments (FAME) Challenge 2024 focuses on exploring face-voice association under a unique condition of multilingual scenario. This condition is inspired from the fact that half of the world&rsquo;s population is bilingual and most often people communicate under multilingual scenario. The challenge uses a dataset namely, Multilingual Audio-Visual (MAV-Celeb) for exploring face-voice association in multilingual environments. This report provides the details of the challenge, dataset, baselines and task details for the FAME Challenge.</p></p class="citation"></blockquote><h3 id=2525--60102-panet-a-physics-guided-parametric-augmentation-net-for-image-dehazing-by-hazing-chih-ling-chang-et-al-2024>(25/25 | 60/102) PANet: A Physics-guided Parametric Augmentation Net for Image Dehazing by Hazing (Chih-Ling Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chih-Ling Chang, Fu-Jen Tsai, Zi-Ling Huang, Lin Gu, Chia-Wen Lin. (2024)<br><strong>PANet: A Physics-guided Parametric Augmentation Net for Image Dehazing by Hazing</strong><br><button class=copy-to-clipboard title="PANet: A Physics-guided Parametric Augmentation Net for Image Dehazing by Hazing" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09269v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09269v1.pdf filename=2404.09269v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image dehazing faces challenges when dealing with hazy images in real-world scenarios. A huge domain gap between synthetic and real-world haze images degrades dehazing performance in practical settings. However, collecting real-world image datasets for training dehazing models is challenging since both hazy and clean pairs must be captured under the same conditions. In this paper, we propose a Physics-guided Parametric Augmentation Network (PANet) that generates photo-realistic hazy and clean training pairs to effectively enhance real-world dehazing performance. PANet comprises a Haze-to-Parameter Mapper (HPM) to project hazy images into a parameter space and a Parameter-to-Haze Mapper (PHM) to map the resampled haze parameters back to hazy images. In the parameter space, we can pixel-wisely resample individual haze parameter maps to generate diverse hazy images with physically-explainable haze conditions unseen in the training set. Our experimental results demonstrate that PANet can augment diverse realistic hazy images to enrich existing hazy image <b>benchmarks</b> so as to effectively boost the performances of state-of-the-art image dehazing models.</p></p class="citation"></blockquote><h2 id=csmm-1>cs.MM (1)</h2><h3 id=11--61102-arena-a-patch-of-interest-vit-inference-acceleration-system-for-edge-assisted-video-analytics-haosong-peng-et-al-2024>(1/1 | 61/102) Arena: A Patch-of-Interest ViT Inference Acceleration System for Edge-Assisted Video Analytics (Haosong Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haosong Peng, Wei Feng, Hao Li, Yufeng Zhan, Qihua Zhou, Yuanqing Xia. (2024)<br><strong>Arena: A Patch-of-Interest ViT Inference Acceleration System for Edge-Assisted Video Analytics</strong><br><button class=copy-to-clipboard title="Arena: A Patch-of-Interest ViT Inference Acceleration System for Edge-Assisted Video Analytics" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-CV, cs-MM, cs.MM<br>Keyword Score: 75<br>Keywords: Convolutional Neural Network, Vision Transformer, Convolutional Neural Network, Foundation Model, Pruning, Recurrent Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09245v1.pdf filename=2404.09245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of edge computing has made real-time intelligent video analytics feasible. Previous works, based on traditional model architecture (e.g., <b>CNN,</b> <b>RNN,</b> etc.), employ various strategies to filter out non-region-of-interest content to minimize bandwidth and computation consumption but show inferior performance in adverse environments. Recently, visual <b>foundation</b> <b>models</b> based on <b>transformers</b> have shown great performance in adverse environments due to their amazing generalization capability. However, they require a large amount of computation power, which limits their applications in real-time intelligent video analytics. In this paper, we find visual <b>foundation</b> <b>models</b> like <b>Vision</b> <b>Transformer</b> (ViT) also have a dedicated acceleration mechanism for video analytics. To this end, we introduce Arena, an end-to-end edge-assisted video inference acceleration system based on ViT. We leverage the capability of ViT that can be accelerated through token <b>pruning</b> by only offloading and feeding Patches-of-Interest (PoIs) to the downstream models. Additionally, we employ probability-based patch sampling, which provides a simple but efficient mechanism for determining PoIs where the probable locations of objects are in subsequent frames. Through extensive evaluations on public datasets, our findings reveal that Arena can boost inference speeds by up to $1.58\times$ and $1.82\times$ on average while consuming only 54% and 34% of the bandwidth, respectively, all with high inference accuracy.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--62102-prior-agnostic-multi-scale-contrastive-text-audio-pre-training-for-parallelized-tts-frontend-modeling-quanxiu-wang-et-al-2024>(1/2 | 62/102) Prior-agnostic Multi-scale Contrastive Text-Audio Pre-training for Parallelized TTS Frontend Modeling (Quanxiu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quanxiu Wang, Hui Huang, Mingjie Wang, Yong Dai, Jinzuomu Zhong, Benlai Tang. (2024)<br><strong>Prior-agnostic Multi-scale Contrastive Text-Audio Pre-training for Parallelized TTS Frontend Modeling</strong><br><button class=copy-to-clipboard title="Prior-agnostic Multi-scale Contrastive Text-Audio Pre-training for Parallelized TTS Frontend Modeling" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 70<br>Keywords: Supervised Learning, Supervised Learning, Unsupervised Learning, Disambiguation, Text Normalization, Text-to-speech, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09192v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09192v1.pdf filename=2404.09192v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the past decade, a series of unflagging efforts have been dedicated to developing highly expressive and controllable <b>text-to-speech</b> <b>(TTS)</b> systems. In general, the holistic <b>TTS</b> comprises two interconnected components: the frontend module and the backend module. The frontend excels in capturing linguistic representations from the raw <b>text</b> <b>input,</b> while the backend module converts linguistic cues to speech. The research community has shown growing interest in the study of the frontend component, recognizing its pivotal role in <b>text-to-speech</b> <b>systems,</b> including <b>Text</b> <b>Normalization</b> (TN), Prosody Boundary Prediction (PBP), and Polyphone <b>Disambiguation</b> (PD). Nonetheless, the limitations posed by insufficient annotated textual data and the reliance on homogeneous <b>text</b> <b>signals</b> significantly undermine the effectiveness of its <b>supervised</b> <b>learning.</b> To evade this obstacle, a novel two-stage <b>TTS</b> frontend prediction pipeline, named TAP-FM, is proposed in this paper. Specifically, during the first learning phase, we present a Multi-scale Contrastive <b>Text-audio</b> <b>Pre-training</b> protocol (MC-TAP), which hammers at acquiring richer insights via multi-granularity contrastive pre-training in an <b>unsupervised</b> manner. Instead of mining homogeneous features in prior pre-training approaches, our framework demonstrates the ability to delve deep into both global and local <b>text-audio</b> <b>semantic</b> and acoustic representations. Furthermore, a parallelized <b>TTS</b> frontend model is delicately devised to execute TN, PD, and PBP prediction tasks, respectively in the second stage. Finally, extensive experiments illustrate the superiority of our proposed method, achieving state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=22--63102-an-experimental-comparison-of-multi-view-self-supervised-methods-for-music-tagging-gabriel-meseguer-brocal-et-al-2024>(2/2 | 63/102) An Experimental Comparison Of Multi-view Self-supervised Methods For Music Tagging (Gabriel Meseguer-Brocal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriel Meseguer-Brocal, Dorian Desblancs, Romain Hennequin. (2024)<br><strong>An Experimental Comparison Of Multi-view Self-supervised Methods For Music Tagging</strong><br><button class=copy-to-clipboard title="An Experimental Comparison Of Multi-view Self-supervised Methods For Music Tagging" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 50<br>Keywords: Contrastive Learning, Fine-tuning, Self-supervised Learning, Self-supervised Learning, Self-supervised Pre-training<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09177v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09177v1.pdf filename=2404.09177v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> has emerged as a powerful way to pre-train generalizable machine learning models on large amounts of unlabeled data. It is particularly compelling in the music domain, where obtaining labeled data is time-consuming, error-prone, and ambiguous. During the <b>self-supervised</b> <b>process,</b> models are trained on pretext tasks, with the primary objective of acquiring robust and informative features that can later be <b>fine-tuned</b> for specific downstream tasks. The choice of the pretext task is critical as it guides the model to shape the feature space with meaningful constraints for information encoding. In the context of music, most works have relied on <b>contrastive</b> <b>learning</b> or masking techniques. In this study, we expand the scope of pretext tasks applied to music by investigating and comparing the performance of new <b>self-supervised</b> <b>methods</b> for music tagging. We open-source a simple ResNet model trained on a diverse catalog of millions of tracks. Our results demonstrate that, although most of these pre-training methods result in similar downstream results, <b>contrastive</b> <b>learning</b> consistently results in better downstream performance compared to other <b>self-supervised</b> <b>pre-training</b> methods. This holds true in a limited-data downstream context.</p></p class="citation"></blockquote><h2 id=cscr-5>cs.CR (5)</h2><h3 id=15--64102-counteracting-concept-drift-by-learning-with-future-malware-predictions-branislav-bosansky-et-al-2024>(1/5 | 64/102) Counteracting Concept Drift by Learning with Future Malware Predictions (Branislav Bosansky et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Branislav Bosansky, Lada Hospodkova, Michal Najman, Maria Rigaki, Elnaz Babayeva, Viliam Lisy. (2024)<br><strong>Counteracting Concept Drift by Learning with Future Malware Predictions</strong><br><button class=copy-to-clipboard title="Counteracting Concept Drift by Learning with Future Malware Predictions" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: Adversarial Learning, Generative Adversarial Network, Generative Adversarial Network, Malware<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09352v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09352v1.pdf filename=2404.09352v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The accuracy of deployed <b>malware-detection</b> classifiers degrades over time due to changes in data distributions and increasing discrepancies between training and testing data. This phenomenon is known as the concept drift. While the concept drift can be caused by various reasons in general, new malicious files are created by <b>malware</b> authors with a clear intention of avoiding detection. The existence of the intention opens a possibility for predicting such future samples. Including predicted samples in training data should consequently increase the accuracy of the classifiers on new testing data. We compare two methods for predicting future samples: (1) <b>adversarial</b> <b>training</b> and (2) <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs).</b> The first method explicitly seeks for <b>adversarial</b> <b>examples</b> against the classifier that are then used as a part of training data. Similarly, <b>GANs</b> also generate synthetic training data. We use <b>GANs</b> to learn changes in data distributions within different time periods of training data and then apply these changes to generate samples that could be in testing data. We compare these prediction methods on two different datasets: (1) Ember public dataset and (2) the internal dataset of files incoming to Avast. We show that while <b>adversarial</b> <b>training</b> yields more robust classifiers, this method is not a good predictor of future <b>malware</b> in general. This is in contrast with previously reported positive results in different domains (including natural language processing and spam detection). On the other hand, we show that <b>GANs</b> can be successfully used as predictors of future <b>malware.</b> We specifically examine <b>malware</b> families that exhibit significant changes in their data distributions over time and the experimental results confirm that <b>GAN-based</b> predictions can significantly improve the accuracy of the classifier on new, previously unseen data.</p></p class="citation"></blockquote><h3 id=25--65102-new-class-of-ciphers-using-hardware-entropy-source-jan-j-tatarkiewicz-et-al-2024>(2/5 | 65/102) New Class of Ciphers Using Hardware Entropy Source (Jan J. Tatarkiewicz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan J. Tatarkiewicz, Wieslaw B. Kuzmicz. (2024)<br><strong>New Class of Ciphers Using Hardware Entropy Source</strong><br><button class=copy-to-clipboard title="New Class of Ciphers Using Hardware Entropy Source" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Brute-Force Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09288v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09288v1.pdf filename=2404.09288v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel, computationally simple method of hiding any message in the stream of random bits by using a secret key. The method is called Bury Among Random Numbers (BARN). A stream of random bits is produced by extracting the entropy of a physical process in a hardware-based true random number generator (TRNG). The process of placing bits of a message into the stream of random bits is governed by the number of random bits skipped between subsequent insertions. The set of numbers that correspond to the steps of BARN is derived from a random number also provided by TRNG. Hence BARN cipher does not depend on any arithmetic function. We propose an effective method of computing random keys from a given number of random bits. We estimate the number of permutations that need to be tested during a <b>brute-force</b> <b>attack</b> on the new cipher for various key lengths. Some practical applications for the new class of symmetrical ciphers are discussed.</p></p class="citation"></blockquote><h3 id=35--66102-artificial-intelligence-enhanced-security-problems-in-real-time-scenario-using-blowfish-algorithm-yuvaraju-chinnam-et-al-2024>(3/5 | 66/102) Artificial Intelligence enhanced Security Problems in Real-Time Scenario using Blowfish Algorithm (Yuvaraju Chinnam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuvaraju Chinnam, Bosubabu Sambana. (2024)<br><strong>Artificial Intelligence enhanced Security Problems in Real-Time Scenario using Blowfish Algorithm</strong><br><button class=copy-to-clipboard title="Artificial Intelligence enhanced Security Problems in Real-Time Scenario using Blowfish Algorithm" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-IR, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09286v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09286v1.pdf filename=2404.09286v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In a nutshell, &ldquo;the cloud&rdquo; refers to a collection of interconnected computing resources made possible by an extensive, real-time communication network like the internet. Because of its potential to reduce processing costs, the emerging paradigm of cloud computing has recently attracted a large number of academics. The exponential expansion of cloud computing has made the rapid expansion of cloud services very remarkable. Ensuring the <b>security</b> of personal information in today&rsquo;s interconnected world is no easy task. These days, <b>security</b> is really crucial. Models of <b>security</b> that are relevant to cloud computing include confidentiality, authenticity, accessibility, data integrity, and recovery. Using the Hybrid Encryption this study, we cover all the <b>security</b> issues and leaks in cloud infrastructure.</p></p class="citation"></blockquote><h3 id=45--67102-make-split-not-hijack-preventing-feature-space-hijacking-attacks-in-split-learning-tanveer-khan-et-al-2024>(4/5 | 67/102) Make Split, not Hijack: Preventing Feature-Space Hijacking Attacks in Split Learning (Tanveer Khan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanveer Khan, Mindaugas Budzys, Antonis Michalas. (2024)<br><strong>Make Split, not Hijack: Preventing Feature-Space Hijacking Attacks in Split Learning</strong><br><button class=copy-to-clipboard title="Make Split, not Hijack: Preventing Feature-Space Hijacking Attacks in Split Learning" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09265v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09265v1.pdf filename=2404.09265v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The popularity of Machine Learning (ML) makes the privacy of sensitive data more imperative than ever. Collaborative learning techniques like Split Learning (SL) aim to protect client data while enhancing ML processes. Though promising, SL has been proved to be vulnerable to a plethora of attacks, thus raising concerns about its effectiveness on data privacy. In this work, we introduce a hybrid approach combining SL and Function Secret Sharing (FSS) to ensure client data privacy. The client adds a random mask to the activation map before sending it to the servers. The servers cannot access the original function but instead work with shares generated using FSS. Consequently, during both forward and backward propagation, the servers cannot reconstruct the client&rsquo;s raw data from the activation map. Furthermore, through visual invertibility, we demonstrate that the server is incapable of reconstructing the raw image data from the activation map when using FSS. It enhances privacy by reducing privacy leakage compared to other SL-based approaches where the server can access client input information. Our approach also ensures <b>security</b> against feature space hijacking attack, protecting sensitive information from potential manipulation. Our protocols yield promising results, reducing communication overhead by over 2x and training time by over 7x compared to the same model with FSS, without any SL. Also, we show that our approach achieves >96% accuracy and remains equivalent to the plaintext models.</p></p class="citation"></blockquote><h3 id=55--68102-printlistener-uncovering-the-vulnerability-of-fingerprint-authentication-via-the-finger-friction-sound-man-zhou-et-al-2024>(5/5 | 68/102) PrintListener: Uncovering the Vulnerability of Fingerprint Authentication via the Finger Friction Sound (Man Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Man Zhou, Shuao Su, Qian Wang, Qi Li, Yuting Zhou, Xiaojing Ma, Zhengxiong Li. (2024)<br><strong>PrintListener: Uncovering the Vulnerability of Fingerprint Authentication via the Finger Friction Sound</strong><br><button class=copy-to-clipboard title="PrintListener: Uncovering the Vulnerability of Fingerprint Authentication via the Finger Friction Sound" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09214v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09214v1.pdf filename=2404.09214v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fingerprint authentication has been extensively employed in contemporary identity verification systems owing to its rapidity and cost-effectiveness. Due to its widespread use, fingerprint leakage may cause sensitive information theft, enormous economic and personnel losses, and even a potential compromise of national <b>security.</b> As a fingerprint that can coincidentally match a specific proportion of the overall fingerprint population, MasterPrint rings the alarm bells for the <b>security</b> of fingerprint authentication. In this paper, we propose a new side-channel attack on the minutiae-based Automatic Fingerprint Identification System (AFIS), called PrintListener, which leverages users&rsquo; fingertip swiping actions on the screen to extract fingerprint pattern features (the first-level features) and synthesizes a stronger targeted PatternMasterPrint with potential second-level features. The attack scenario of PrintListener is extensive and covert. It only needs to record users&rsquo; fingertip friction sound and can be launched by leveraging a large number of social media platforms. Extensive experimental results in realworld scenarios show that Printlistener can significantly improve the attack potency of MasterPrint.</p></p class="citation"></blockquote><h2 id=cshc-3>cs.HC (3)</h2><h3 id=13--69102-evaluating-the-efficacy-of-haptic-feedback-360-treadmill-integrated-virtual-reality-framework-and-longitudinal-training-on-decision-making-performance-in-a-complex-search-and-shoot-simulation-akash-k-rao-et-al-2024>(1/3 | 69/102) Evaluating the efficacy of haptic feedback, 360° treadmill-integrated Virtual Reality framework and longitudinal training on decision-making performance in a complex search-and-shoot simulation (Akash K Rao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akash K Rao, Arnav Bhavsar, Shubhajit Roy Chowdhury, Sushil Chandra, Ramsingh Negi, Prakash Duraisamy, Varun Dutt. (2024)<br><strong>Evaluating the efficacy of haptic feedback, 360° treadmill-integrated Virtual Reality framework and longitudinal training on decision-making performance in a complex search-and-shoot simulation</strong><br><button class=copy-to-clipboard title="Evaluating the efficacy of haptic feedback, 360° treadmill-integrated Virtual Reality framework and longitudinal training on decision-making performance in a complex search-and-shoot simulation" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Virtual Reality (VR), Virtual Reality (VR)<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09147v1.pdf filename=2404.09147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Virtual</b> <b>Reality</b> <b>(VR)</b> has made significant strides, offering users a multitude of ways to interact with <b>virtual</b> <b>environments.</b> Each sensory modality in <b>VR</b> provides distinct inputs and interactions, enhancing the user&rsquo;s immersion and presence. However, the potential of additional sensory modalities, such as haptic feedback and 360{\deg} locomotion, to improve decision-making performance has not been thoroughly investigated. This study addresses this gap by evaluating the impact of a haptic feedback, 360{\deg} locomotion-integrated <b>VR</b> framework and longitudinal, heterogeneous training on decision-making performance in a complex search-and-shoot <b>simulation.</b> The study involved 32 participants from a defence <b>simulation</b> base in India, who were randomly divided into two groups: experimental (haptic feedback, 360{\deg} locomotion-integrated <b>VR</b> framework with longitudinal, heterogeneous training) and placebo control (longitudinal, heterogeneous <b>VR</b> training without extrasensory modalities). The experiment lasted 10 days. On Day 1, all subjects executed a search-and-shoot <b>simulation</b> closely replicating the elements/situations in the real world. From Day 2 to Day 9, the subjects underwent heterogeneous training, imparted by the design of various complexity levels in the <b>simulation</b> using changes in behavioral attributes/artificial intelligence of the enemies. On Day 10, they repeated the search-and-shoot <b>simulation</b> executed on Day 1. The results showed that the experimental group experienced a gradual increase in presence, immersion, and engagement compared to the placebo control group. However, there was no significant difference in decision-making performance between the two groups on day 10. We intend to use these findings to design multisensory <b>VR</b> training frameworks that enhance engagement levels and decision-making performance.</p></p class="citation"></blockquote><h3 id=23--70102-deceptive-patterns-of-intelligent-and-interactive-writing-assistants-karim-benharrak-et-al-2024>(2/3 | 70/102) Deceptive Patterns of Intelligent and Interactive Writing Assistants (Karim Benharrak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Karim Benharrak, Tim Zindulka, Daniel Buschek. (2024)<br><strong>Deceptive Patterns of Intelligent and Interactive Writing Assistants</strong><br><button class=copy-to-clipboard title="Deceptive Patterns of Intelligent and Interactive Writing Assistants" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CL, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: ChatGPT, Chatbot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09375v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09375v1.pdf filename=2404.09375v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> have become an integral part of new intelligent and interactive writing assistants. Many are offered commercially with a <b>chatbot-like</b> UI, such as <b>ChatGPT,</b> and provide little information about their inner workings. This makes this new type of widespread system a potential target for deceptive design patterns. For example, such assistants might exploit hidden costs by providing guidance up until a certain point before asking for a fee to see the rest. As another example, they might sneak unwanted content/edits into longer generated or revised text pieces (e.g. to influence the expressed opinion). With these and other examples, we conceptually transfer several deceptive patterns from the literature to the new context of AI writing assistants. Our goal is to raise awareness and encourage future research into how the UI and interaction design of such systems can impact people and their writing.</p></p class="citation"></blockquote><h3 id=33--71102-investigating-the-impact-of-virtual-element-misalignment-in-collaborative-augmented-reality-experiences-francesco-vona-et-al-2024>(3/3 | 71/102) Investigating the impact of virtual element misalignment in collaborative Augmented Reality experiences (Francesco Vona et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Vona, Sina Hinzmann, Michael Stern, Tanja Kojić, Navid Ashrafi, David Grieshammer, Jan-Niklas Voigt-Antons. (2024)<br><strong>Investigating the impact of virtual element misalignment in collaborative Augmented Reality experiences</strong><br><button class=copy-to-clipboard title="Investigating the impact of virtual element misalignment in collaborative Augmented Reality experiences" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Augmented Reality (AR), Augmented Reality (AR)<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09174v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09174v1.pdf filename=2404.09174v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The collaboration in co-located shared environments has sparked an increased interest in immersive technologies, including <b>Augmented</b> <b>Reality</b> <b>(AR).</b> Since research in this field has primarily focused on individual user experiences in <b>AR,</b> the collaborative aspects within shared <b>AR</b> spaces remain less explored, and fewer studies can provide guidelines for designing this type of experience. This article investigates how the user experience in a collaborative shared <b>AR</b> space is affected by divergent perceptions of virtual objects and the effects of positional synchrony and avatars. For this purpose, we developed an <b>AR</b> app and used two distinct experimental conditions to study the influencing factors. Forty-eight participants, organized into 24 pairs, participated in the experiment and jointly interacted with shared virtual objects. Results indicate that divergent perceptions of virtual objects did not directly influence communication and collaboration dynamics. Conversely, positional synchrony emerged as a critical factor, significantly enhancing the quality of the collaborative experience. On the contrary, while not negligible, avatars played a relatively less pronounced role in influencing these dynamics. The findings can potentially offer valuable practical insights, guiding the development of future collaborative AR/VR environments.</p></p class="citation"></blockquote><h2 id=csro-6>cs.RO (6)</h2><h3 id=16--72102-a-survey-on-integration-of-large-language-models-with-intelligent-robots-yeseung-kim-et-al-2024>(1/6 | 72/102) A Survey on Integration of Large Language Models with Intelligent Robots (Yeseung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yeseung Kim, Dohyun Kim, Jieun Choi, Jisang Park, Nayoung Oh, Daehyung Park. (2024)<br><strong>A Survey on Integration of Large Language Models with Intelligent Robots</strong><br><button class=copy-to-clipboard title="A Survey on Integration of Large Language Models with Intelligent Robots" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09228v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09228v1.pdf filename=2404.09228v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the integration of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has revolutionized the field of robotics, enabling robots to communicate, understand, and reason with human-like proficiency. This paper explores the multifaceted impact of <b>LLMs</b> on robotics, addressing key challenges and opportunities for leveraging these models across various domains. By categorizing and analyzing <b>LLM</b> applications within core robotics elements &ndash; communication, perception, planning, and control &ndash; we aim to provide actionable insights for researchers seeking to integrate <b>LLMs</b> into their robotic systems. Our investigation focuses on <b>LLMs</b> developed post-GPT-3.5, primarily in text-based modalities while also considering <b>multimodal</b> approaches for perception and control. We offer comprehensive guidelines and examples for <b>prompt</b> engineering, facilitating beginners&rsquo; access to <b>LLM-based</b> robotics solutions. Through tutorial-level examples and structured <b>prompt</b> construction, we illustrate how <b>LLM-guided</b> enhancements can be seamlessly integrated into robotics applications. This survey serves as a roadmap for researchers navigating the evolving landscape of <b>LLM-driven</b> robotics, offering a comprehensive overview and practical guidance for harnessing the power of language models in robotics development.</p></p class="citation"></blockquote><h3 id=26--73102-tube-rrt-efficient-homotopic-path-planning-for-swarm-robotics-passing-through-large-scale-obstacle-environments-pengda-mao-et-al-2024>(2/6 | 73/102) Tube-RRT*: Efficient Homotopic Path Planning for Swarm Robotics Passing-Through Large-Scale Obstacle Environments (Pengda Mao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengda Mao, Quan Quan. (2024)<br><em><em>Tube-RRT</em>: Efficient Homotopic Path Planning for Swarm Robotics Passing-Through Large-Scale Obstacle Environments</em>*<br><button class=copy-to-clipboard title="Tube-RRT*: Efficient Homotopic Path Planning for Swarm Robotics Passing-Through Large-Scale Obstacle Environments" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09200v1.pdf filename=2404.09200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the concept of optimal virtual tube has emerged as a novel solution to the challenging task of navigating obstacle-dense environments for swarm robotics, offering a wide ranging of applications. However, it lacks an efficient homotopic path planning method in obstacle-dense environments. This paper introduces Tube-RRT*, an innovative homotopic path planning method that builds upon and improves the Rapidly-exploring Random Tree (RRT) algorithm. Tube-RRT* is specifically designed to generate homotopic paths for the trajectories in the virtual tube, strategically considering opening volume and tube length to mitigate swarm congestion and ensure agile navigation. Through comprehensive comparative <b>simulations</b> conducted within complex, large-scale obstacle environments, we demonstrate the effectiveness of Tube-RRT*.</p></p class="citation"></blockquote><h3 id=36--74102-beatle----self-reconfigurable-aerial-robot-design-control-and-experimental-validation-junichiro-sugihara-et-al-2024>(3/6 | 74/102) BEATLE &ndash; Self-Reconfigurable Aerial Robot: Design, Control and Experimental Validation (Junichiro Sugihara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junichiro Sugihara, Moju Zhao, Takuzumi Nishio, Kei Okada, Masayuki Inaba. (2024)<br><strong>BEATLE &ndash; Self-Reconfigurable Aerial Robot: Design, Control and Experimental Validation</strong><br><button class=copy-to-clipboard title="BEATLE -- Self-Reconfigurable Aerial Robot: Design, Control and Experimental Validation" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09153v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09153v1.pdf filename=2404.09153v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modular self-reconfigurable robots (MSRRs) offer enhanced task flexibility by constructing various structures suitable for each task. However, conventional terrestrial MSRRs equipped with wheels face critical challenges, including limitations in the size of constructible structures and system robustness due to elevated wrench loads applied to each module. In this work, we introduce an Aerial MSRR (A-MSRR) system named BEATLE, capable of merging and separating in-flight. BEATLE can merge without applying wrench loads to adjacent modules, thereby expanding the scalability and robustness of conventional terrestrial MSRRs. In this article, we propose a system configuration for BEATLE, including mechanical design, a control framework for multi-connected flight, and a motion planner for reconfiguration motion. The design of a docking mechanism and housing structure aims to balance the durability of the constructed structure with ease of separation. Furthermore, the proposed flight control framework achieves stable multi-connected flight based on contact wrench control. Moreover, the proposed motion planner based on a finite state machine (FSM) achieves precise and robust reconfiguration motion. We also introduce the actual implementation of the prototype and validate the robustness and scalability of the proposed system design through experiments and <b>simulation</b> studies.</p></p class="citation"></blockquote><h3 id=46--75102-learning-cross-hand-policies-for-high-dof-reaching-and-grasping-qijin-she-et-al-2024>(4/6 | 75/102) Learning Cross-hand Policies for High-DOF Reaching and Grasping (Qijin She et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qijin She, Shishun Zhang, Yunfan Ye, Min Liu, Ruizhen Hu, Kai Xu. (2024)<br><strong>Learning Cross-hand Policies for High-DOF Reaching and Grasping</strong><br><button class=copy-to-clipboard title="Learning Cross-hand Policies for High-DOF Reaching and Grasping" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-GR, cs-RO, cs.RO<br>Keyword Score: 15<br>Keywords: Geometry, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09150v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09150v1.pdf filename=2404.09150v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reaching-and-grasping is a fundamental skill for robotic manipulation, but existing methods usually train models on a specific gripper and cannot be reused on another gripper without retraining. In this paper, we propose a novel method that can learn a unified policy model that can be easily transferred to different dexterous grippers. Our method consists of two stages: a gripper-agnostic policy model that predicts the displacements of predefined key points on the gripper, and a gripper specific adaptation model that translates these displacements into adjustments for controlling the grippers&rsquo; joints. The gripper state and interactions with objects are captured at the finger level using robust geometric representations, integrated with a <b>transformer-based</b> network to address variations in gripper morphology and <b>geometry.</b> In the experimental part, we evaluate our method on several dexterous grippers and objects of diverse shapes, and the result shows that our method significantly outperforms the baseline methods. Pioneering the transfer of grasp policies across different dexterous grippers, our method effectively demonstrates its potential for learning generalizable and transferable manipulation skills for various robotic hands</p></p class="citation"></blockquote><h3 id=56--76102-snn4agents-a-framework-for-developing-energy-efficient-embodied-spiking-neural-networks-for-autonomous-agents-rachmad-vidya-wicaksana-putra-et-al-2024>(5/6 | 76/102) SNN4Agents: A Framework for Developing Energy-Efficient Embodied Spiking Neural Networks for Autonomous Agents (Rachmad Vidya Wicaksana Putra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rachmad Vidya Wicaksana Putra, Alberto Marchisio, Muhammad Shafique. (2024)<br><strong>SNN4Agents: A Framework for Developing Energy-Efficient Embodied Spiking Neural Networks for Autonomous Agents</strong><br><button class=copy-to-clipboard title="SNN4Agents: A Framework for Developing Energy-Efficient Embodied Spiking Neural Networks for Autonomous Agents" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-NE, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09331v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09331v1.pdf filename=2404.09331v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent trends have shown that autonomous agents, such as Autonomous Ground Vehicles (AGVs), Unmanned Aerial Vehicles (UAVs), and mobile robots, effectively improve human productivity in solving diverse tasks. However, since these agents are typically powered by portable batteries, they require extremely low power/energy consumption to operate in a long lifespan. To solve this challenge, neuromorphic computing has emerged as a promising solution, where bio-inspired Spiking Neural Networks (SNNs) use spikes from event-based cameras or data conversion pre-processing to perform sparse computations efficiently. However, the studies of SNN deployments for autonomous agents are still at an early stage. Hence, the optimization stages for enabling efficient embodied SNN deployments for autonomous agents have not been defined systematically. Toward this, we propose a novel framework called SNN4Agents that consists of a set of optimization techniques for designing energy-efficient embodied SNNs targeting autonomous agent applications. Our SNN4Agents employs weight <b>quantization,</b> timestep reduction, and attention window reduction to jointly improve the energy efficiency, reduce the memory footprint, optimize the processing latency, while maintaining high accuracy. In the evaluation, we investigate use cases of event-based car recognition, and explore the trade-offs among accuracy, latency, memory, and energy consumption. The experimental results show that our proposed framework can maintain high accuracy (i.e., 84.12% accuracy) with 68.75% memory saving, 3.58x speed-up, and 4.03x energy efficiency improvement as compared to the state-of-the-art work for NCARS dataset, thereby enabling energy-efficient embodied SNN deployments for autonomous agents.</p></p class="citation"></blockquote><h3 id=66--77102-increasing-slam-pose-accuracy-by-ground-to-satellite-image-registration-yanhao-zhang-et-al-2024>(6/6 | 77/102) Increasing SLAM Pose Accuracy by Ground-to-Satellite Image Registration (Yanhao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanhao Zhang, Yujiao Shi, Shan Wang, Ankit Vora, Akhil Perincherry, Yongbo Chen, Hongdong Li. (2024)<br><strong>Increasing SLAM Pose Accuracy by Ground-to-Satellite Image Registration</strong><br><button class=copy-to-clipboard title="Increasing SLAM Pose Accuracy by Ground-to-Satellite Image Registration" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09169v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09169v1.pdf filename=2404.09169v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vision-based localization for autonomous driving has been of great interest among researchers. When a pre-built 3D map is not available, the techniques of visual simultaneous localization and mapping (SLAM) are typically adopted. Due to error accumulation, visual SLAM (vSLAM) usually suffers from long-term drift. This paper proposes a framework to increase the localization accuracy by fusing the vSLAM with a deep-learning-based ground-to-satellite (G2S) image registration method. In this framework, a coarse (spatial correlation bound check) to fine (visual odometry consistency check) method is designed to select the valid G2S prediction. The selected prediction is then fused with the SLAM measurement by solving a scaled pose <b>graph</b> problem. To further increase the localization accuracy, we provide an iterative trajectory fusion pipeline. The proposed framework is evaluated on two well-known autonomous driving datasets, and the results demonstrate the accuracy and robustness in terms of vehicle localization.</p></p class="citation"></blockquote><h2 id=csse-4>cs.SE (4)</h2><h3 id=14--78102-tasks-people-prompt-a-taxonomy-of-llm-downstream-tasks-in-software-verification-and-falsification-approaches-víctor-a-braberman-et-al-2024>(1/4 | 78/102) Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software Verification and Falsification Approaches (Víctor A. Braberman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Víctor A. Braberman, Flavia Bonomo-Braberman, Yiannis Charalambous, Juan G. Colonna, Lucas C. Cordeiro, Rosiane de Freitas. (2024)<br><strong>Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software Verification and Falsification Approaches</strong><br><button class=copy-to-clipboard title="Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software Verification and Falsification Approaches" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: F-3-1; D-2-4; D-2-5; I-2-7, cs-AI, cs-CL, cs-LG, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09384v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09384v1.pdf filename=2404.09384v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompting</b> has become one of the main approaches to leverage emergent capabilities of <b>Large</b> <b>Language</b> <b>Models</b> [Brown et al. NeurIPS 2020, Wei et al. TMLR 2022, Wei et al. NeurIPS 2022]. During the last year, researchers and practitioners have been playing with <b>prompts</b> to see how to make the most of <b>LLMs.</b> By homogeneously dissecting 80 papers, we investigate in deep how software testing and verification research communities have been abstractly architecting their <b>LLM-enabled</b> solutions. More precisely, first, we want to validate whether downstream tasks are an adequate concept to convey the blueprint of <b>prompt-based</b> solutions. We also aim at identifying number and nature of such tasks in solutions. For such goal, we develop a novel downstream task taxonomy that enables pinpointing some engineering patterns in a rather varied spectrum of Software Engineering problems that encompasses testing, fuzzing, debugging, vulnerability detection, static analysis and program verification approaches.</p></p class="citation"></blockquote><h3 id=24--79102-test-code-generation-for-telecom-software-systems-using-two-stage-generative-model-mohamad-nabeel-et-al-2024>(2/4 | 79/102) Test Code Generation for Telecom Software Systems using Two-Stage Generative Model (Mohamad Nabeel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamad Nabeel, Doumitrou Daniil Nimara, Tahar Zanouda. (2024)<br><strong>Test Code Generation for Telecom Software Systems using Two-Stage Generative Model</strong><br><button class=copy-to-clipboard title="Test Code Generation for Telecom Software Systems using Two-Stage Generative Model" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-LG, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Code Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09249v1.pdf filename=2404.09249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the evolution of Telecom towards achieving intelligent, autonomous, and open networks has led to an increasingly complex Telecom Software system, supporting various heterogeneous deployment scenarios, with multi-standard and multi-vendor support. As a result, it becomes a challenge for <b>large-scale</b> <b>Telecom</b> <b>software</b> companies to develop and test software for all deployment scenarios. To address these challenges, we propose a framework for Automated Test Generation for <b>large-scale</b> <b>Telecom</b> <b>Software</b> systems. We begin by generating Test Case Input data for test scenarios observed using a time-series Generative model trained on historical Telecom Network data during field trials. Additionally, the time-series Generative model helps in preserving the privacy of Telecom data. The generated time-series software performance data are then utilized with test descriptions written in natural language to generate Test Script using the Generative <b>Large</b> <b>Language</b> <b>Model.</b> Our comprehensive experiments on public datasets and Telecom datasets obtained from operational Telecom Networks demonstrate that the framework can effectively generate comprehensive test case data input and useful test code.</p></p class="citation"></blockquote><h3 id=34--80102-emerging-platforms-meet-emerging-llms-a-year-long-journey-of-top-down-development-siyuan-feng-et-al-2024>(3/4 | 80/102) Emerging Platforms Meet Emerging LLMs: A Year-Long Journey of Top-Down Development (Siyuan Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyuan Feng, Jiawei Liu, Ruihang Lai, Charlie F. Ruan, Yong Yu, Lingming Zhang, Tianqi Chen. (2024)<br><strong>Emerging Platforms Meet Emerging LLMs: A Year-Long Journey of Top-Down Development</strong><br><button class=copy-to-clipboard title="Emerging Platforms Meet Emerging LLMs: A Year-Long Journey of Top-Down Development" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-LG, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09151v1.pdf filename=2404.09151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deploying machine learning (ML) on diverse computing platforms is crucial to accelerate and broaden their applications. However, it presents significant software engineering challenges due to the fast evolution of models, especially the recent \llmfull{s} (\llm{s}), and the emergence of new computing platforms. Current ML frameworks are primarily engineered for CPU and CUDA platforms, leaving a big gap in enabling emerging ones like Metal, Vulkan, and WebGPU. While a traditional bottom-up development pipeline fails to close the gap timely, we introduce TapML, a top-down approach and tooling designed to streamline the deployment of ML systems on diverse platforms, optimized for developer productivity. Unlike traditional bottom-up methods, which involve extensive manual testing and debugging, TapML automates unit testing through test carving and adopts a migration-based strategy for gradually offloading model computations from mature source platforms to emerging target platforms. By leveraging realistic inputs and remote connections for gradual target offloading, TapML accelerates the validation and minimizes debugging scopes, significantly optimizing development efforts. TapML was developed and applied through a year-long, real-world effort that successfully deployed significant emerging models and platforms. Through serious deployments of 82 emerging models in 17 distinct architectures across 5 emerging platforms, we showcase the effectiveness of TapML in enhancing developer productivity while ensuring model reliability and efficiency. Furthermore, we <b>summarize</b> comprehensive case studies from our real-world development, offering best practices for developing emerging ML systems.</p></p class="citation"></blockquote><h3 id=44--81102-service-weaver-a-promising-direction-for-cloud-native-systems-jacoby-johnson-et-al-2024>(4/4 | 81/102) Service Weaver: A Promising Direction for Cloud-native Systems? (Jacoby Johnson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacoby Johnson, Subash Kharel, Alan Mannamplackal, Amr S. Abdelfattah, Tomas Cerny. (2024)<br><strong>Service Weaver: A Promising Direction for Cloud-native Systems?</strong><br><button class=copy-to-clipboard title="Service Weaver: A Promising Direction for Cloud-native Systems?" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-ET, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09357v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09357v1.pdf filename=2404.09357v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cloud-native and microservice architectures have taken over the development world by storm. While being incredibly scalable and resilient, microservice architectures also come at the cost of increased overhead to build and maintain. Google&rsquo;s Service Weaver aims to simplify the complexities associated with implementing cloud-native systems by introducing the concept of a single modular binary composed of agent-like components, thereby abstracting away the microservice architecture notion of individual services. While Service Weaver presents a promising approach to streamline the development of cloud-native applications and addresses nearly all significant aspects of conventional cloud-native systems, there are existing tradeoffs affecting the overall functionality of the system. Notably, Service Weaver&rsquo;s straightforward implementation and deployment of components alleviate the overhead of constructing a complex microservice architecture. However, it is important to acknowledge that certain features, including separate code bases, routing mechanisms, resiliency, and <b>security,</b> are presently lacking in the framework.</p></p class="citation"></blockquote><h2 id=csit-2>cs.IT (2)</h2><h3 id=12--82102-joint-near-field-uplink-communication-and-localization-using-message-passing-based-sparse-bayesian-learning-fei-liu-et-al-2024>(1/2 | 82/102) Joint Near Field Uplink Communication and Localization Using Message Passing-Based Sparse Bayesian Learning (Fei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fei Liu, Zhengdao Yuan, Qinghua Guo, Yuanyuan Zhang, Zhongyong Wang, J. Andrew Zhang. (2024)<br><strong>Joint Near Field Uplink Communication and Localization Using Message Passing-Based Sparse Bayesian Learning</strong><br><button class=copy-to-clipboard title="Joint Near Field Uplink Communication and Localization Using Message Passing-Based Sparse Bayesian Learning" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 30<br>Keywords: Message-Passing, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09201v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09201v1.pdf filename=2404.09201v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work deals with the problem of uplink communication and localization in an integrated sensing and communication system, where users are in the near field (NF) of antenna aperture due to the use of high carrier frequency and large antenna arrays at base stations. We formulate joint NF signal detection and localization as a problem of recovering signals with a sparse pattern. To solve the problem, we develop a message passing based sparse Bayesian learning (SBL) algorithm, where multiple unitary approximate message passing (UAMP)-based sparse signal estimators work jointly to recover the sparse signals with low complexity. <b>Simulation</b> results demonstrate the effectiveness of the proposed method.</p></p class="citation"></blockquote><h3 id=22--83102-unsourced-random-access-in-mimo-quasi-static-rayleigh-fading-channels-with-finite-blocklength-junyuan-gao-et-al-2024>(2/2 | 83/102) Unsourced Random Access in MIMO Quasi-Static Rayleigh Fading Channels with Finite Blocklength (Junyuan Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyuan Gao, Yongpeng Wu, Giuseppe Caire, Wei Yang, Wenjun Zhang. (2024)<br><strong>Unsourced Random Access in MIMO Quasi-Static Rayleigh Fading Channels with Finite Blocklength</strong><br><button class=copy-to-clipboard title="Unsourced Random Access in MIMO Quasi-Static Rayleigh Fading Channels with Finite Blocklength" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09198v1.pdf filename=2404.09198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the fundamental limits of unsourced random access (URA) with a random and unknown number ${\rm{K}}_a$ of active users in MIMO quasi-static Rayleigh fading channels. First, we derive an upper bound on the probability of incorrectly estimating the number of active users. We prove that it exponentially decays with the number of receive antennas and eventually vanishes, whereas reaches a plateau as the power and blocklength increase. Then, we derive non-asymptotic achievability and converse bounds on the minimum energy-per-bit required by each active user to reliably transmit $J$ bits with blocklength $n$. Numerical results verify the tightness of our bounds, suggesting that they provide <b>benchmarks</b> to evaluate existing schemes. The extra required energy-per-bit due to the uncertainty of the number of active users decreases as $\mathbb{E}[{\rm{K}}_a]$ increases. Compared to random access with individual codebooks, the URA paradigm achieves higher spectral and energy efficiency. Moreover, using codewords distributed on a sphere is shown to outperform the Gaussian random coding scheme in the non-asymptotic regime.</p></p class="citation"></blockquote><h2 id=physicsao-ph-1>physics.ao-ph (1)</h2><h3 id=11--84102-impact-of-curved-elements-for-flows-over-orography-with-a-discontinuous-galerkin-scheme-giuseppe-orlando-et-al-2024>(1/1 | 84/102) Impact of curved elements for flows over orography with a Discontinuous Galerkin scheme (Giuseppe Orlando et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giuseppe Orlando, Tommaso Benacchio, Luca Bonaventura. (2024)<br><strong>Impact of curved elements for flows over orography with a Discontinuous Galerkin scheme</strong><br><button class=copy-to-clipboard title="Impact of curved elements for flows over orography with a Discontinuous Galerkin scheme" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.ao-ph<br>Categories: cs-NA, math-NA, physics-ao-ph, physics.ao-ph<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09319v1.pdf filename=2404.09319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a quantitative assessment of the impact of high-order mappings on the <b>simulation</b> of flows over complex orography. Curved boundaries were not used in earlier numerical methods, whereas they are employed nowadays to an increasing extent in combination with high-order methods, such as the Finite Element Method (FEM) and the Spectral Element Method (SEM). We consider here a specific Discontinuous Galerkin (DG) method implemented in the framework of the deal.II library, which natively supports high-order mappings. A number of numerical experiments based on classical <b>benchmarks</b> over idealized orographic profiles demonstrate the positive impact of curved boundaries on the accuracy of the results. These findings are also supported by results of the application of this approach to non-smooth and realistic orographic profiles.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=11--85102-qandle-accelerating-state-vector-simulation-using-gate-matrix-caching-and-circuit-splitting-gerhard-stenzel-et-al-2024>(1/1 | 85/102) Qandle: Accelerating State Vector Simulation Using Gate-Matrix Caching and Circuit Splitting (Gerhard Stenzel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gerhard Stenzel, Sebastian Zielinski, Michael Kölle, Philipp Altmann, Jonas Nüßlein, Thomas Gabor. (2024)<br><strong>Qandle: Accelerating State Vector Simulation Using Gate-Matrix Caching and Circuit Splitting</strong><br><button class=copy-to-clipboard title="Qandle: Accelerating State Vector Simulation Using Gate-Matrix Caching and Circuit Splitting" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, quant-ph, quant-ph<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09213v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09213v1.pdf filename=2404.09213v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To address the computational complexity associated with state-vector <b>simulation</b> for quantum circuits, we propose a combination of advanced techniques to accelerate circuit execution. Quantum gate matrix caching reduces the overhead of repeated applications of the Kronecker product when applying a gate matrix to the state vector by storing decomposed partial matrices for each gate. Circuit splitting divides the circuit into sub-circuits with fewer gates by constructing a dependency <b>graph,</b> enabling parallel or sequential execution on disjoint subsets of the state vector. These techniques are implemented using the PyTorch machine learning framework. We demonstrate the performance of our approach by comparing it to other PyTorch-compatible quantum state-vector simulators. Our implementation, named Qandle, is designed to seamlessly integrate with existing machine learning workflows, providing a user-friendly API and compatibility with the OpenQASM format. Qandle is an open-source project hosted on GitHub <a href=https://github.com/gstenzel/qandle>https://github.com/gstenzel/qandle</a> and PyPI <a href=https://pypi.org/project/qandle/>https://pypi.org/project/qandle/</a> .</p></p class="citation"></blockquote><h2 id=cssi-1>cs.SI (1)</h2><h3 id=11--86102-survey-on-embedding-models-for-knowledge-graph-and-its-applications-manita-pote-2024>(1/1 | 86/102) Survey on Embedding Models for Knowledge Graph and its Applications (Manita Pote, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manita Pote. (2024)<br><strong>Survey on Embedding Models for Knowledge Graph and its Applications</strong><br><button class=copy-to-clipboard title="Survey on Embedding Models for Knowledge Graph and its Applications" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-SI, cs.SI<br>Keyword Score: 23<br>Keywords: Graph, Graph Embedding, Knowledge Graph, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09167v1.pdf filename=2404.09167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>Graph</b> <b>(KG)</b> is a <b>graph</b> <b>based</b> data structure to represent facts of the world where nodes represent real world entities or abstract concept and edges represent relation between the entities. <b>Graph</b> <b>as</b> representation for <b>knowledge</b> <b>has</b> several drawbacks like data sparsity, computational complexity and manual feature engineering. <b>Knowledge</b> <b>Graph</b> <b>embedding</b> tackles the drawback by representing entities and relation in low dimensional vector space by capturing the semantic relation between them. There are different <b>KG</b> embedding models. Here, we discuss translation based and neural network based embedding models which differ based on semantic property, scoring function and architecture they use. Further, we discuss application of <b>KG</b> in some domains that use deep learning models and leverage social media data.</p></p class="citation"></blockquote><h2 id=cscy-2>cs.CY (2)</h2><h3 id=12--87102-llempower-understanding-disparities-in-the-control-and-access-of-large-language-models-vishwas-sathish-et-al-2024>(1/2 | 87/102) LLeMpower: Understanding Disparities in the Control and Access of Large Language Models (Vishwas Sathish et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vishwas Sathish, Hannah Lin, Aditya K Kamath, Anish Nyayachavadi. (2024)<br><strong>LLeMpower: Understanding Disparities in the Control and Access of Large Language Models</strong><br><button class=copy-to-clipboard title="LLeMpower: Understanding Disparities in the Control and Access of Large Language Models" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: K-4-0; K-7-4, cs-AI, cs-CL, cs-CY, cs-ET, cs.CY<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09356v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09356v1.pdf filename=2404.09356v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are a powerful technology that augment human skill to create new opportunities, akin to the development of steam engines and the internet. However, <b>LLMs</b> come with a high cost. They require significant computing resources and energy to train and serve. Inequity in their control and access has led to concentration of ownership and power to a small collection of corporations. In our study, we collect training and inference requirements for various <b>LLMs.</b> We then analyze the economic strengths of nations and organizations in the context of developing and serving these models. Additionally, we also look at whether individuals around the world can access and use this emerging technology. We compare and contrast these groups to show that these technologies are monopolized by a surprisingly few entities. We conclude with a qualitative study on the ethical implications of our findings and discuss future directions towards equity in <b>LLM</b> access.</p></p class="citation"></blockquote><h3 id=22--88102-a-computational-model-for-gender-asset-gap-management-with-a-focus-on-gender-disparity-in-land-acquisition-and-land-tenure-security-oluwatosin-ogundare-et-al-2024>(2/2 | 88/102) A computational model for gender asset gap management with a focus on gender disparity in land acquisition and land tenure security (Oluwatosin Ogundare et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oluwatosin Ogundare, Lewis Njualem. (2024)<br><strong>A computational model for gender asset gap management with a focus on gender disparity in land acquisition and land tenure security</strong><br><button class=copy-to-clipboard title="A computational model for gender asset gap management with a focus on gender disparity in land acquisition and land tenure security" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Recommendation, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09164v1.pdf filename=2404.09164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gender inequality is a significant concern in many cultures, as women face significant barriers to asset acquisition particularly land ownership and control. Land acquisition and land tenure <b>security</b> are complex issues that affect various cultural groups differently, leading to disparities in access and ownership especially when superimposed with other socio-economic issues like gender inequality. Measuring the severity of these issues across different cultural groups is challenging due to variations in cultural norms, expectations and effectiveness of the measurement framework to correctly assess the level of severity. While nominal measures of gender asset gap provide valuable insights into land acquisition and tenure <b>security</b> issues, they do not fully capture the nuances of cultural differences and the impact of governmental and corporate policies that influence gender disparity in land ownership and control. The proposed framework aims to fill this gap by incorporating cultural and policy factors in developing a new measurement framework equipped with a more robust, comprehensive metric to standardize the approach to assessing the severity of gender asset disparity in a general sense but with a focus on land acquisition and tenure <b>security</b> to engender more effective interventions and policy <b>recommendations.</b></p></p class="citation"></blockquote><h2 id=csdc-3>cs.DC (3)</h2><h3 id=13--89102-the-intelligent-prediction-and-assessment-of-financial-information-risk-in-the-cloud-computing-model-yufu-wang-et-al-2024>(1/3 | 89/102) The intelligent prediction and assessment of financial information risk in the cloud computing model (Yufu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufu Wang, Mingwei Zhu, Jiaqiang Yuan, Guanghui Wang, Hong Zhou. (2024)<br><strong>The intelligent prediction and assessment of financial information risk in the cloud computing model</strong><br><button class=copy-to-clipboard title="The intelligent prediction and assessment of financial information risk in the cloud computing model" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-AI, cs-DC, cs.DC<br>Keyword Score: 20<br>Keywords: Recommendation, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09322v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09322v1.pdf filename=2404.09322v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cloud computing (cloud computing) is a kind of distributed computing, referring to the network &ldquo;cloud&rdquo; will be a huge data calculation and processing program into countless small programs, and then, through the system composed of multiple servers to process and analyze these small programs to get the results and return to the user. This report explores the intersection of cloud computing and financial information processing, identifying risks and challenges faced by financial institutions in adopting cloud technology. It discusses the need for intelligent solutions to enhance data processing efficiency and accuracy while addressing <b>security</b> and privacy concerns. Drawing on regulatory frameworks, the report proposes policy <b>recommendations</b> to mitigate concentration risks associated with cloud computing in the financial industry. By combining intelligent forecasting and evaluation technologies with cloud computing models, the study aims to provide effective solutions for financial data processing and management, facilitating the industry&rsquo;s transition towards digital transformation.</p></p class="citation"></blockquote><h3 id=23--90102-egret-reinforcement-mechanism-for-sequential-computation-offloading-in-edge-computing-haosong-peng-et-al-2024>(2/3 | 90/102) Egret: Reinforcement Mechanism for Sequential Computation Offloading in Edge Computing (Haosong Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haosong Peng, Yufeng Zhan, DiHua Zhai, Xiaopu Zhang, Yuanqing Xia. (2024)<br><strong>Egret: Reinforcement Mechanism for Sequential Computation Offloading in Edge Computing</strong><br><button class=copy-to-clipboard title="Egret: Reinforcement Mechanism for Sequential Computation Offloading in Edge Computing" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09285v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09285v1.pdf filename=2404.09285v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As an emerging computing paradigm, edge computing offers computing resources closer to the data sources, helping to improve the service quality of many real-time applications. A crucial problem is designing a rational pricing mechanism to maximize the revenue of the edge computing service provider (ECSP). However, prior works have considerable limitations: clients are static and are required to disclose their preferences, which is impractical in reality. However, previous works assume user privacy information to be known or consider the number of users in edge scenarios to be static. To address this issue, we propose a novel sequential computation offloading mechanism, where the ECSP posts prices of computing resources with different configurations to clients in turn. Clients independently choose which computing resources to purchase and how to offload based on their prices. Then Egret, a deep <b>reinforcement</b> <b>learning-based</b> approach that achieves maximum revenue, is proposed. Egret determines the optimal price and visiting orders online without considering clients&rsquo; preferences. Experimental results show that the revenue of ECSP in Egret is only 1.29% lower than Oracle and 23.43% better than the state-of-the-art when the client arrives dynamically.</p></p class="citation"></blockquote><h3 id=33--91102-a-reinforcement-learning-based-backfilling-strategy-for-hpc-batch-jobs-elliot-kolker-hicks-et-al-2024>(3/3 | 91/102) A Reinforcement Learning Based Backfilling Strategy for HPC Batch Jobs (Elliot Kolker-Hicks et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elliot Kolker-Hicks, Di Zhang, Dong Dai. (2024)<br><strong>A Reinforcement Learning Based Backfilling Strategy for HPC Batch Jobs</strong><br><button class=copy-to-clipboard title="A Reinforcement Learning Based Backfilling Strategy for HPC Batch Jobs" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09264v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09264v1.pdf filename=2404.09264v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High Performance Computing (HPC) systems are used across a wide range of disciplines for both large and complex computations. HPC systems often receive many thousands of computational tasks at a time, colloquially referred to as jobs. These jobs must then be scheduled as optimally as possible so they can be completed within a reasonable timeframe. HPC scheduling systems often employ a technique called backfilling, wherein low-priority jobs are scheduled earlier to use the available resources that are waiting for the pending high-priority jobs. To make it work, backfilling largely relies on job runtime to calculate the start time of the ready-to-schedule jobs and avoid delaying them. It is a common belief that better estimations of job runtime will lead to better backfilling and more effective scheduling. However, our experiments show a different conclusion: there is a missing trade-off between prediction accuracy and backfilling opportunities. To learn how to achieve the best trade-off, we believe <b>reinforcement</b> <b>learning</b> (RL) can be effectively leveraged. <b>Reinforcement</b> <b>Learning</b> relies on an agent which makes decisions from observing the environment, and gains rewards or punishments based on the quality of its decision-making. Based on this idea, we designed RLBackfilling, a <b>reinforcement</b> <b>learning-based</b> backfilling algorithm. We show how RLBackfilling can learn effective backfilling strategies via trial-and-error on existing job traces. Our evaluation results show up to 59% better scheduling performance (based on average bounded job slowdown) compared to EASY backfilling using user-provided job runtime and 30% better performance compared with EASY using the ideal predicted job runtime (the actual job runtime).</p></p class="citation"></blockquote><h2 id=eesssy-3>eess.SY (3)</h2><h3 id=13--92102-mpc-based-linear-equivalence-with-control-barrier-functions-for-vtol-uavs-ali-mohamed-ali-et-al-2024>(1/3 | 92/102) MPC Based Linear Equivalence with Control Barrier Functions for VTOL-UAVs (Ali Mohamed Ali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Mohamed Ali, Hashim A. Hashim, Chao Shen. (2024)<br><strong>MPC Based Linear Equivalence with Control Barrier Functions for VTOL-UAVs</strong><br><button class=copy-to-clipboard title="MPC Based Linear Equivalence with Control Barrier Functions for VTOL-UAVs" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09320v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09320v1.pdf filename=2404.09320v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we propose a cascaded scheme of linear Model prediction Control (MPC) based on Control Barrier Functions (CBF) with Dynamic Feedback Linearization (DFL) for Vertical Take-off and Landing (VTOL) Unmanned Aerial Vehicles (UAVs). CBF is a tool that allows enforcement of forward invariance of a set using Lyapunov-like functions to ensure safety. The First control synthesis that employed CBF was based on Quadratic Program (QP) that modifies the existing controller to satisfy the safety requirements. However, the CBF-QP-based controllers leading to longer detours and undesirable transient performance. Recent contributions utilize the framework of MPC benefiting from the prediction capabilities and constraints imposed on the state and control inputs. Due to the intrinsic nonlinearities of the dynamics of robotics systems, all the existing MPC-CBF solutions rely on nonlinear MPC formulations or operate on less accurate linear models. In contrast, our novel solution unlocks the benefits of linear MPC-CBF while considering the full underactuated dynamics without any linear approximations. The cascaded scheme converts the problem of safe VTOL-UAV navigation to a Quadratic Constraint Quadratic Programming (QCQP) problem solved efficiently by off-the-shelf solvers. The closed-loop stability and recursive feasibility is proved along with numerical <b>simulations</b> showing the effective and robust solutions. Keywords: Unmanned Aerial Vehicles, Vertical Take-off and Landing, Model Predictive Control, MPC, Nonlinearity, Dynamic Feedback Linearization, Optimal Control.</p></p class="citation"></blockquote><h3 id=23--93102-numerical-discretization-methods-for-the-extended-linear-quadratic-control-problem-zhanhao-zhang-et-al-2024>(2/3 | 93/102) Numerical Discretization Methods for the Extended Linear Quadratic Control Problem (Zhanhao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhanhao Zhang, Jan Lorenz Svensen, Morten Wahlgreen Kaysfeld, Anders Hilmar Damm Christensen, Steen Hørsholt, John Bagterp Jørgensen. (2024)<br><strong>Numerical Discretization Methods for the Extended Linear Quadratic Control Problem</strong><br><button class=copy-to-clipboard title="Numerical Discretization Methods for the Extended Linear Quadratic Control Problem" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Continuous Time, Continuous Time, Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09316v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09316v1.pdf filename=2404.09316v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we introduce numerical methods for discretizing <b>continuous-time</b> <b>linear-quadratic</b> optimal control problems (LQ-OCPs). The discretization of <b>continuous-time</b> <b>LQ-OCPs</b> is formulated into differential equation systems, and we can obtain the <b>discrete</b> <b>equivalent</b> by solving these systems. We present the ordinary differential equation (ODE), matrix exponential, and a novel step-doubling method for the discretization of LQ-OCPs. Utilizing Euler-Maruyama discretization with a fine step, we reformulate the costs of <b>continuous-time</b> <b>stochastic</b> LQ-OCPs into a quadratic form, and show that the stochastic cost follows the $\chi^2$ distribution. In the numerical experiment, we test and compare the proposed numerical methods. The results ensure that the <b>discrete-time</b> <b>LQ-OCP</b> derived using the proposed numerical methods is equivalent to the original problem.</p></p class="citation"></blockquote><h3 id=33--94102-on-joint-convergence-of-traffic-state-and-weight-vector-in-learning-based-dynamic-routing-with-value-function-approximation-yidan-wu-et-al-2024>(3/3 | 94/102) On Joint Convergence of Traffic State and Weight Vector in Learning-Based Dynamic Routing with Value Function Approximation (Yidan Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yidan Wu, Jianan Zhang, Li Jin. (2024)<br><strong>On Joint Convergence of Traffic State and Weight Vector in Learning-Based Dynamic Routing with Value Function Approximation</strong><br><button class=copy-to-clipboard title="On Joint Convergence of Traffic State and Weight Vector in Learning-Based Dynamic Routing with Value Function Approximation" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09188v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09188v1.pdf filename=2404.09188v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning-based approaches are increasingly popular for traffic control problems. However, these approaches are applied typically as black boxes with limited theoretical guarantees and interpretability. In this paper, we consider the theory of dynamic routing over parallel servers, a representative traffic control task, using semi-gradient on-policy control algorithm, a representative <b>reinforcement</b> <b>learning</b> method. We consider a linear value function approximation on an infinite state space; a Lyapunov function is also derived from the approximator. In particular, the structure of the approximator naturally makes possible idling policies, which is an interesting and useful advantage over existing dynamic routing schemes. We show that the convergence of the approximation weights is coupled with the convergence of the traffic state. We show that if the system is stabilizable, then (i) the weight vector converges to a bounded region, and (ii) the traffic state is bounded in the mean. We also empirically show that the proposed algorithm is computationally efficient with an insignificant optimality gap.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--95102-characterizing-soft-error-resiliency-in-arms-ethos-u55-embedded-machine-learning-accelerator-abhishek-tyagi-et-al-2024>(1/1 | 95/102) Characterizing Soft-Error Resiliency in Arm&rsquo;s Ethos-U55 Embedded Machine Learning Accelerator (Abhishek Tyagi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhishek Tyagi, Reiley Jeyapaul, Chuteng Zhu, Paul Whatmough, Yuhao Zhu. (2024)<br><strong>Characterizing Soft-Error Resiliency in Arm&rsquo;s Ethos-U55 Embedded Machine Learning Accelerator</strong><br><button class=copy-to-clipboard title="Characterizing Soft-Error Resiliency in Arm's Ethos-U55 Embedded Machine Learning Accelerator" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AI, cs-AR, cs.AR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09317v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09317v1.pdf filename=2404.09317v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As Neural Processing Units (NPU) or accelerators are increasingly deployed in a variety of applications including safety critical applications such as autonomous vehicle, and medical imaging, it is critical to understand the fault-tolerance nature of the NPUs. We present a reliability study of Arm&rsquo;s Ethos-U55, an important industrial-scale NPU being utilised in embedded and IoT applications. We perform large scale RTL-level fault injections to characterize Ethos-U55 against the Automotive Safety Integrity Level D (ASIL-D) resiliency standard commonly used for safety-critical applications such as autonomous vehicles. We show that, under soft errors, all four configurations of the NPU fall short of the required level of resiliency for a variety of neural networks running on the NPU. We show that it is possible to meet the ASIL-D level resiliency without resorting to conventional strategies like Dual Core Lock Step (DCLS) that has an area overhead of 100%. We achieve so through selective protection, where hardware structures are selectively protected (e.g., duplicated, hardened) based on their sensitivity to soft errors and their silicon areas. To identify the optimal configuration that minimizes the area overhead while meeting the ASIL-D standard, the main challenge is the large search space associated with the time-consuming RTL <b>simulation.</b> To address this challenge, we present a statistical analysis tool that is validated against Arm silicon and that allows us to quickly navigate hundreds of billions of fault sites without exhaustive RTL fault injections. We show that by carefully duplicating a small fraction of the functional blocks and hardening the Flops in other blocks meets the ASIL-D safety standard while introducing an area overhead of only 38%.</p></p class="citation"></blockquote><h2 id=eessiv-1>eess.IV (1)</h2><h3 id=11--96102-breast-cancer-image-classification-method-based-on-deep-transfer-learning-weimin-wang-et-al-2024>(1/1 | 96/102) Breast Cancer Image Classification Method Based on Deep Transfer Learning (Weimin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weimin Wang, Min Gao, Mingxuan Xiao, Xu Yan, Yufeng Li. (2024)<br><strong>Breast Cancer Image Classification Method Based on Deep Transfer Learning</strong><br><button class=copy-to-clipboard title="Breast Cancer Image Classification Method Based on Deep Transfer Learning" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 15<br>Keywords: Deep Neural Network, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09226v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09226v1.pdf filename=2404.09226v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To address the issues of limited samples, time-consuming feature design, and low accuracy in detection and classification of breast cancer pathological images, a breast cancer image classification model algorithm combining <b>deep</b> <b>learning</b> <b>and</b> <b>transfer</b> <b>learning</b> is proposed. This algorithm is based on the DenseNet structure of <b>deep</b> <b>neural</b> <b>networks,</b> and constructs a network model by introducing attention mechanisms, and trains the enhanced dataset using multi-level <b>transfer</b> <b>learning.</b> Experimental results demonstrate that the algorithm achieves an efficiency of over 84.0% in the test set, with a significantly improved classification accuracy compared to previous models, making it applicable to medical breast cancer detection tasks.</p></p class="citation"></blockquote><h2 id=astro-phep-1>astro-ph.EP (1)</h2><h3 id=11--97102-machine-learning-based-identification-of-gaia-astrometric-exoplanet-orbits-johannes-sahlmann-et-al-2024>(1/1 | 97/102) Machine learning-based identification of Gaia astrometric exoplanet orbits (Johannes Sahlmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johannes Sahlmann, Pablo Gómez. (2024)<br><strong>Machine learning-based identification of Gaia astrometric exoplanet orbits</strong><br><button class=copy-to-clipboard title="Machine learning-based identification of Gaia astrometric exoplanet orbits" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.EP<br>Categories: astro-ph-EP, astro-ph-IM, astro-ph-SR, astro-ph.EP, cs-LG<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09350v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09350v1.pdf filename=2404.09350v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The third Gaia data release (DR3) contains $\sim$170 000 astrometric orbit solutions of two-body systems located within $\sim$500 pc of the Sun. Determining component masses in these systems, in particular of stars hosting exoplanets, usually hinges on incorporating complementary observations in addition to the astrometry, e.g. spectroscopy and radial velocities. Several DR3 two-body systems with exoplanet, brown-dwarf, stellar, and black-hole components have been confirmed in this way. We developed an alternative machine learning approach that uses only the DR3 orbital solutions with the aim of identifying the best candidates for exoplanets and brown-dwarf companions. Based on confirmed substellar companions in the literature, we use semi-supervised <b>anomaly</b> <b>detection</b> methods in combination with extreme gradient boosting and random forest classifiers to determine likely low-mass outliers in the population of non-single sources. We employ and study feature importance to investigate the method&rsquo;s plausibility and produced a list of 22 best candidates of which four are exoplanet candidates and another five are either very-massive brown dwarfs or very-low mass stars. Three candidates, including one initial exoplanet candidate, correspond to false-positive solutions where longer-period binary star motion was fitted with a biased shorter-period orbit. We highlight nine candidates with brown-dwarf companions for preferential follow-up. One candidate companion around the Sun-like star G 15-6 could be confirmed as a genuine brown dwarf using external radial-velocity data. This new approach is a powerful complement to the traditional identification methods for substellar companions among Gaia astrometric orbits. It is particularly relevant in the context of Gaia DR4 and its expected exoplanet discovery yield.</p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=11--98102-correlated-mean-field-imitation-learning-zhiyu-zhao-et-al-2024>(1/1 | 98/102) Correlated Mean Field Imitation Learning (Zhiyu Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyu Zhao, Ning Yang, Xue Yan, Haifeng Zhang, Jun Wang, Yaodong Yang. (2024)<br><strong>Correlated Mean Field Imitation Learning</strong><br><button class=copy-to-clipboard title="Correlated Mean Field Imitation Learning" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09324v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09324v1.pdf filename=2404.09324v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate multi-agent imitation learning (IL) within the framework of mean field games (MFGs), considering the presence of time-varying correlated signals. Existing MFG IL algorithms assume demonstrations are sampled from Mean Field Nash Equilibria (MFNE), limiting their adaptability to real-world scenarios. For example, in the traffic network equilibrium influenced by public routing <b>recommendations,</b> <b>recommendations</b> introduce time-varying correlated signals into the game, not captured by MFNE and other existing correlated equilibrium concepts. To address this gap, we propose Adaptive Mean Field Correlated Equilibrium (AMFCE), a general equilibrium incorporating time-varying correlated signals. We establish the existence of AMFCE under mild conditions and prove that MFNE is a subclass of AMFCE. We further propose Correlated Mean Field Imitation Learning (CMFIL), a novel IL framework designed to recover the AMFCE, accompanied by a theoretical guarantee on the quality of the recovered policy. Experimental results, including a real-world traffic flow prediction problem, demonstrate the superiority of CMFIL over state-of-the-art IL baselines, highlighting the potential of CMFIL in understanding large population behavior under correlated signals.</p></p class="citation"></blockquote><h2 id=csai-1>cs.AI (1)</h2><h3 id=11--99102-owloop-interfaces-for-mapping-owl-axioms-into-oop-hierarchies-luca-buoncompagni-et-al-2024>(1/1 | 99/102) OWLOOP: Interfaces for Mapping OWL Axioms into OOP Hierarchies (Luca Buoncompagni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Buoncompagni, Fulvio Mastrogiovanni. (2024)<br><strong>OWLOOP: Interfaces for Mapping OWL Axioms into OOP Hierarchies</strong><br><button class=copy-to-clipboard title="OWLOOP: Interfaces for Mapping OWL Axioms into OOP Hierarchies" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: 68T27 (Primary) 68T30, 68N19, 68T40 (Secondary), D-2-11; D-1-5; D-1-6; E-2; I-2-4, cs-AI, cs-LO, cs-RO, cs-SE, cs.AI<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09305v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09305v1.pdf filename=2404.09305v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper tackles the issue of mapping logic axioms formalised in the Ontology Web Language (OWL) within the Object-Oriented Programming (OOP) paradigm. The issues of mapping OWL axioms hierarchies and OOP objects hierarchies are due to OWL-based <b>reasoning</b> algorithms, which might change an OWL hierarchy at runtime; instead, OOP hierarchies are usually defined as static structures. Although programming paradigms based on reflection allow changing the OOP hierarchies at runtime and mapping OWL axioms dynamically, there are no currently available mechanisms that do not limit the <b>reasoning</b> algorithms. Thus, the factory-based paradigm is typically used since it decouples the OWL and OOP hierarchies. However, the factory inhibits OOP polymorphism and introduces a paradigm shift with respect to widely accepted OOP paradigms. We present the OWLOOP API, which exploits the factory to not limit <b>reasoning</b> algorithms, and it provides novel OOP interfaces concerning the axioms in an ontology. OWLOOP is designed to limit the paradigm shift required for using ontologies while improving, through OOP-like polymorphism, the modularity of software architectures that exploit logic <b>reasoning.</b> The paper details our OWL to OOP mapping mechanism, and it shows the benefits and limitations of OWLOOP through examples concerning a robot in a smart environment.</p></p class="citation"></blockquote><h2 id=csne-1>cs.NE (1)</h2><h3 id=11--100102-advanced-intelligent-optimization-algorithms-for-multi-objective-optimal-power-flow-in-future-power-systems-a-review-yuyan-li-2024>(1/1 | 100/102) Advanced Intelligent Optimization Algorithms for Multi-Objective Optimal Power Flow in Future Power Systems: A Review (Yuyan Li, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuyan Li. (2024)<br><strong>Advanced Intelligent Optimization Algorithms for Multi-Objective Optimal Power Flow in Future Power Systems: A Review</strong><br><button class=copy-to-clipboard title="Advanced Intelligent Optimization Algorithms for Multi-Objective Optimal Power Flow in Future Power Systems: A Review" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs-SY, cs.NE, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09203v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09203v1.pdf filename=2404.09203v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This review explores the application of intelligent optimization algorithms to Multi-Objective Optimal Power Flow (MOPF) in enhancing modern power systems. It delves into the challenges posed by the integration of renewables, smart grids, and increasing energy demands, focusing on evolutionary algorithms, swarm intelligence, and deep <b>reinforcement</b> <b>learning.</b> The effectiveness, scalability, and application of these algorithms are analyzed, with findings suggesting that algorithm selection is contingent on the specific MOPF problem at hand, and hybrid approaches offer significant promise. The importance of standard test systems for verifying solutions and the role of software tools in facilitating analysis are emphasized. Future research is directed towards exploiting machine learning for dynamic optimization, embracing decentralized energy systems, and adapting to evolving policy frameworks to improve power system efficiency and sustainability. This review aims to advance MOPF research by highlighting state-of-the-art methodologies and encouraging the development of innovative solutions for future energy challenges.</p></p class="citation"></blockquote><h2 id=cscc-1>cs.CC (1)</h2><h3 id=11--101102-on-the-complexity-of-some-cycle-convexity-parameters-carlos-v-g-c-lima-et-al-2024>(1/1 | 101/102) On the complexity of some cycle convexity parameters (Carlos V. G. C. Lima et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carlos V. G. C. Lima, Thiago Marcilon, Pedro Paulo de Medeiros. (2024)<br><strong>On the complexity of some cycle convexity parameters</strong><br><button class=copy-to-clipboard title="On the complexity of some cycle convexity parameters" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs.CC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09236v1.pdf filename=2404.09236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The subject of <b>graph</b> convexity is well explored in the literature, the so-called interval convexities above all. In this work, we explore the cycle convexity, whose interval function is $I(S) = S \cup {u \mid G[S \cup {u}]$ has a cycle containing $u}$. In this convexity, we prove that the decision problems associated to the parameters rank and convexity number are in \NP-complete and \W[1]-hard when parameterized by the solution size. We also prove that to determine whether the percolation time of a <b>graph</b> is at least $k$ is \NP-complete, but polynomial for cacti or when $k\leq2$</p></p class="citation"></blockquote><h2 id=mathna-1>math.NA (1)</h2><h3 id=11--102102-asymptotic-preserving-approximations-for-stochastic-incompressible-viscous-fluids-and-spdes-on-group-jianbo-cui-et-al-2024>(1/1 | 102/102) Asymptotic-preserving approximations for stochastic incompressible viscous fluids and SPDEs on group (Jianbo Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianbo Cui, Derui Sheng. (2024)<br><strong>Asymptotic-preserving approximations for stochastic incompressible viscous fluids and SPDEs on group</strong><br><button class=copy-to-clipboard title="Asymptotic-preserving approximations for stochastic incompressible viscous fluids and SPDEs on group" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math-PR, math.NA<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09168v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09168v1.pdf filename=2404.09168v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The long-term dynamics of particles involved in an incompressible flow with a small viscosity ($\epsilon>0$) and slow chemical reactions, is depicted by a class of stochastic reaction-diffusion-advection (RDA) equations with a fast advection term of magnitude $1/\epsilon$. It has been shown in [7] the fast advection asymptotics of stochastic RDA equation in $\mathbb{R}^2$ can be characterized through a stochastic partial differential equation (SPDE) on the <b>graph</b> associated with certain Hamiltonian. To simulate such fast advection asymptotics, we introduce and study an asymptotic-preserving (AP) exponential Euler approximation for the multiscale stochastic RDA equation. There are three key ingredients in proving asymptotic-preserving property of the proposed approximation. First, a strong error estimate, which depends on $1/\epsilon$ linearly, is obtained via a variational argument. Second, we prove the consistency of exponential Euler approximations on the fast advection asymptotics between the original problem and the SPDE on <b>graph.</b> Last, a <b>graph</b> weighted space is introduced to quantify the approximation error for SPDE on <b>graph,</b> which avoids the possible singularity near the vertices. Numerical experiments are carried out to support the theoretical results.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/ title="arXiv @ 2024.04.15" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.04.15</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Bandit Algorithm Basic</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-20>cs.CL (20)</a><ul><li><a href=#120--1102-gemquad--generating-multilingual-question-answering-datasets-from-large-language-models-using-few-shot-learning-amani-namboori-et-al-2024>(1/20 | 1/102) GeMQuAD : Generating Multilingual Question Answering Datasets from Large Language Models using Few Shot Learning (Amani Namboori et al., 2024)</a></li><li><a href=#220--2102-cross-data-knowledge-graph-construction-for-llm-enabled-educational-question-answering-system-acasestudyathcmut-tuan-bui-et-al-2024>(2/20 | 2/102) Cross-Data Knowledge Graph Construction for LLM-enabled Educational Question-Answering System: A~Case~Study~at~HCMUT (Tuan Bui et al., 2024)</a></li><li><a href=#320--3102-compass-large-multilingual-language-model-for-south-east-asia-sophia-maria-2024>(3/20 | 3/102) Compass: Large Multilingual Language Model for South-east Asia (Sophia Maria, 2024)</a></li><li><a href=#420--4102-from-bytes-to-borsch-fine-tuning-gemma-and-mistral-for-the-ukrainian-language-representation-artur-kiulian-et-al-2024>(4/20 | 4/102) From Bytes to Borsch: Fine-Tuning Gemma and Mistral for the Ukrainian Language Representation (Artur Kiulian et al., 2024)</a></li><li><a href=#520--5102-post-semantic-thinking-a-robust-strategy-to-distill-reasoning-capacity-from-large-language-models-xiao-chen-et-al-2024>(5/20 | 5/102) Post-Semantic-Thinking: A Robust Strategy to Distill Reasoning Capacity from Large Language Models (Xiao Chen et al., 2024)</a></li><li><a href=#620--6102-unveiling-llm-evaluation-focused-on-metrics-challenges-and-solutions-taojun-hu-et-al-2024>(6/20 | 6/102) Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions (Taojun Hu et al., 2024)</a></li><li><a href=#720--7102-confidence-calibration-and-rationalization-for-llms-via-multi-agent-deliberation-ruixin-yang-et-al-2024>(7/20 | 7/102) Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation (Ruixin Yang et al., 2024)</a></li><li><a href=#820--8102-self-selected-attention-span-for-accelerating-large-language-model-inference-tian-jin-et-al-2024>(8/20 | 8/102) Self-Selected Attention Span for Accelerating Large Language Model Inference (Tian Jin et al., 2024)</a></li><li><a href=#920--9102-jafin-japanese-financial-instruction-dataset-kota-tanabe-et-al-2024>(9/20 | 9/102) JaFIn: Japanese Financial Instruction Dataset (Kota Tanabe et al., 2024)</a></li><li><a href=#1020--10102-dke-research-at-semeval-2024-task-2-incorporating-data-augmentation-with-generative-models-and-biomedical-knowledge-to-enhance-inference-robustness-yuqi-wang-et-al-2024>(10/20 | 10/102) DKE-Research at SemEval-2024 Task 2: Incorporating Data Augmentation with Generative Models and Biomedical Knowledge to Enhance Inference Robustness (Yuqi Wang et al., 2024)</a></li><li><a href=#1120--11102-toner-type-oriented-named-entity-recognition-with-generative-language-model-guochao-jiang-et-al-2024>(11/20 | 11/102) ToNER: Type-oriented Named Entity Recognition with Generative Language Model (Guochao Jiang et al., 2024)</a></li><li><a href=#1220--12102-low-resource-named-entity-recognition-with-cross-lingual-character-level-neural-conditional-random-fields-ryan-cotterell-et-al-2024>(12/20 | 12/102) Low-Resource Named Entity Recognition with Cross-Lingual, Character-Level Neural Conditional Random Fields (Ryan Cotterell et al., 2024)</a></li><li><a href=#1320--13102-when-hindsight-is-not-2020-testing-limits-on-reflective-thinking-in-large-language-models-yanhong-li-et-al-2024>(13/20 | 13/102) When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models (Yanhong Li et al., 2024)</a></li><li><a href=#1420--14102-towards-practical-tool-usage-for-continually-learning-llms-jerry-huang-et-al-2024>(14/20 | 14/102) Towards Practical Tool Usage for Continually Learning LLMs (Jerry Huang et al., 2024)</a></li><li><a href=#1520--15102-understanding-the-role-of-temperature-in-diverse-question-generation-by-gpt-4-arav-agarwal-et-al-2024>(15/20 | 15/102) Understanding the Role of Temperature in Diverse Question Generation by GPT-4 (Arav Agarwal et al., 2024)</a></li><li><a href=#1620--16102-entropy-guided-extrapolative-decoding-to-improve-factuality-in-large-language-models-souvik-das-et-al-2024>(16/20 | 16/102) Entropy Guided Extrapolative Decoding to Improve Factuality in Large Language Models (Souvik Das et al., 2024)</a></li><li><a href=#1720--17102-reap-the-wild-wind-detecting-media-storms-in-large-scale-news-corpora-dror-k-markus-et-al-2024>(17/20 | 17/102) Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora (Dror K. Markus et al., 2024)</a></li><li><a href=#1820--18102-tldr-at-semeval-2024-task-2-t5-generated-clinical-language-summaries-for-deberta-report-analysis-spandan-das-et-al-2024>(18/20 | 18/102) TLDR at SemEval-2024 Task 2: T5-generated clinical-Language summaries for DeBERTa Report Analysis (Spandan Das et al., 2024)</a></li><li><a href=#1920--19102-large-language-models-are-as-persuasive-as-humans-but-why-about-the-cognitive-effort-and-moral-emotional-language-of-llm-arguments-carlos-carrasco-farre-2024>(19/20 | 19/102) Large Language Models are as persuasive as humans, but why? About the cognitive effort and moral-emotional language of LLM arguments (Carlos Carrasco-Farre, 2024)</a></li><li><a href=#2020--20102-towards-fast-inference-exploring-and-improving-blockwise-parallel-drafts-taehyeon-kim-et-al-2024>(20/20 | 20/102) Towards Fast Inference: Exploring and Improving Blockwise Parallel Drafts (Taehyeon Kim et al., 2024)</a></li></ul></li><li><a href=#cslg-13>cs.LG (13)</a><ul><li><a href=#113--21102-hierarchical-attention-models-for-multi-relational-graphs-roshni-g-iyer-et-al-2024>(1/13 | 21/102) Hierarchical Attention Models for Multi-Relational Graphs (Roshni G. Iyer et al., 2024)</a></li><li><a href=#213--22102-foundational-gpt-model-for-meg-richard-csaky-et-al-2024>(2/13 | 22/102) Foundational GPT Model for MEG (Richard Csaky et al., 2024)</a></li><li><a href=#313--23102-knowledgeable-agents-by-offline-reinforcement-learning-from-large-language-model-rollouts-jing-cheng-pang-et-al-2024>(3/13 | 23/102) Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts (Jing-Cheng Pang et al., 2024)</a></li><li><a href=#413--24102-feddistill-global-model-distillation-for-local-model-de-biasing-in-non-iid-federated-learning-changlin-song-et-al-2024>(4/13 | 24/102) FedDistill: Global Model Distillation for Local Model De-Biasing in Non-IID Federated Learning (Changlin Song et al., 2024)</a></li><li><a href=#513--25102-degnn-dual-experts-graph-neural-network-handling-both-edge-and-node-feature-noise-tai-hasegawa-et-al-2024>(5/13 | 25/102) DEGNN: Dual Experts Graph Neural Network Handling Both Edge and Node Feature Noise (Tai Hasegawa et al., 2024)</a></li><li><a href=#613--26102-mitigating-heterogeneity-among-factor-tensors-via-lie-group-manifolds-for-tensor-decomposition-based-temporal-knowledge-graph-embedding-jiang-li-et-al-2024>(6/13 | 26/102) Mitigating Heterogeneity among Factor Tensors via Lie Group Manifolds for Tensor Decomposition Based Temporal Knowledge Graph Embedding (Jiang Li et al., 2024)</a></li><li><a href=#713--27102-fault-detection-in-mobile-networks-using-diffusion-models-mohamad-nabeel-et-al-2024>(7/13 | 27/102) Fault Detection in Mobile Networks Using Diffusion Models (Mohamad Nabeel et al., 2024)</a></li><li><a href=#813--28102-transformerfam-feedback-attention-is-working-memory-dongseong-hwang-et-al-2024>(8/13 | 28/102) TransformerFAM: Feedback attention is working memory (Dongseong Hwang et al., 2024)</a></li><li><a href=#913--29102-rf-diffusion-radio-signal-generation-via-time-frequency-diffusion-guoxuan-chi-et-al-2024>(9/13 | 29/102) RF-Diffusion: Radio Signal Generation via Time-Frequency Diffusion (Guoxuan Chi et al., 2024)</a></li><li><a href=#1013--30102-adversarial-robustness-limits-via-scaling-law-and-human-alignment-studies-brian-r-bartoldson-et-al-2024>(10/13 | 30/102) Adversarial Robustness Limits via Scaling-Law and Human-Alignment Studies (Brian R. Bartoldson et al., 2024)</a></li><li><a href=#1113--31102-map-model-aggregation-and-personalization-in-federated-learning-with-incomplete-classes-xin-chun-li-et-al-2024>(11/13 | 31/102) MAP: Model Aggregation and Personalization in Federated Learning with Incomplete Classes (Xin-Chun Li et al., 2024)</a></li><li><a href=#1213--32102-intelligent-chemical-purification-technique-based-on-machine-learning-wenchao-wu-et-al-2024>(12/13 | 32/102) Intelligent Chemical Purification Technique Based on Machine Learning (Wenchao Wu et al., 2024)</a></li><li><a href=#1313--33102-lsrom-learning-self-refined-organizing-map-for-fast-imbalanced-streaming-data-clustering-yongqi-xu-et-al-2024>(13/13 | 33/102) LSROM: Learning Self-Refined Organizing Map for Fast Imbalanced Streaming Data Clustering (Yongqi Xu et al., 2024)</a></li></ul></li><li><a href=#csni-2>cs.NI (2)</a><ul><li><a href=#12--34102-interactive-generative-ai-agents-for-satellite-networks-through-a-mixture-of-experts-transmission-ruichen-zhang-et-al-2024>(1/2 | 34/102) Interactive Generative AI Agents for Satellite Networks through a Mixture of Experts Transmission (Ruichen Zhang et al., 2024)</a></li><li><a href=#22--35102-a-paradigm-for-collaborative-pervasive-fog-computing-ecosystems-at-the-network-edge-abderrahmen-mtibaa-2024>(2/2 | 35/102) A Paradigm For Collaborative Pervasive Fog Computing Ecosystems at the Network Edge (Abderrahmen Mtibaa, 2024)</a></li></ul></li><li><a href=#cscv-25>cs.CV (25)</a><ul><li><a href=#125--36102-weight-copy-and-low-rank-adaptation-for-few-shot-distillation-of-vision-transformers-diana-nicoleta-grigore-et-al-2024>(1/25 | 36/102) Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision Transformers (Diana-Nicoleta Grigore et al., 2024)</a></li><li><a href=#225--37102-exploring-generative-ai-for-sim2real-in-driving-data-synthesis-haonan-zhao-et-al-2024>(2/25 | 37/102) Exploring Generative AI for Sim2Real in Driving Data Synthesis (Haonan Zhao et al., 2024)</a></li><li><a href=#325--38102-detclipv3-towards-versatile-generative-open-vocabulary-object-detection-lewei-yao-et-al-2024>(3/25 | 38/102) DetCLIPv3: Towards Versatile Generative Open-vocabulary Object Detection (Lewei Yao et al., 2024)</a></li><li><a href=#425--39102-text2taste-a-versatile-egocentric-vision-system-for-intelligent-reading-assistance-using-large-language-model-wiktor-mucha-et-al-2024>(4/25 | 39/102) TEXT2TASTE: A Versatile Egocentric Vision System for Intelligent Reading Assistance Using Large Language Model (Wiktor Mucha et al., 2024)</a></li><li><a href=#525--40102-dreamscape-3d-scene-creation-via-gaussian-splatting-joint-correlation-modeling-xuening-yuan-et-al-2024>(5/25 | 40/102) DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation Modeling (Xuening Yuan et al., 2024)</a></li><li><a href=#625--41102-texthawk-exploring-efficient-fine-grained-perception-of-multimodal-large-language-models-ya-qi-yu-et-al-2024>(6/25 | 41/102) TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models (Ya-Qi Yu et al., 2024)</a></li><li><a href=#725--42102-roofdiffusion-constructing-roofs-from-severely-corrupted-point-data-via-diffusion-kyle-shih-huang-lo-et-al-2024>(7/25 | 42/102) RoofDiffusion: Constructing Roofs from Severely Corrupted Point Data via Diffusion (Kyle Shih-Huang Lo et al., 2024)</a></li><li><a href=#825--43102-facecat-enhancing-face-recognition-security-with-a-unified-generative-model-framework-jiawei-chen-et-al-2024>(8/25 | 43/102) FaceCat: Enhancing Face Recognition Security with a Unified Generative Model Framework (Jiawei Chen et al., 2024)</a></li><li><a href=#925--44102-gcc-generative-calibration-clustering-haifeng-xia-et-al-2024>(9/25 | 44/102) GCC: Generative Calibration Clustering (Haifeng Xia et al., 2024)</a></li><li><a href=#1025--45102-change-guiding-network-incorporating-change-prior-to-guide-change-detection-in-remote-sensing-imagery-chengxi-han-et-al-2024>(10/25 | 45/102) Change Guiding Network: Incorporating Change Prior to Guide Change Detection in Remote Sensing Imagery (Chengxi Han et al., 2024)</a></li><li><a href=#1125--46102-tri-modal-confluence-with-temporal-dynamics-for-scene-graph-generation-in-operating-rooms-diandian-guo-et-al-2024>(11/25 | 46/102) Tri-modal Confluence with Temporal Dynamics for Scene Graph Generation in Operating Rooms (Diandian Guo et al., 2024)</a></li><li><a href=#1225--47102-loopanimate-loopable-salient-object-animation-fanyi-wang-et-al-2024>(12/25 | 47/102) LoopAnimate: Loopable Salient Object Animation (Fanyi Wang et al., 2024)</a></li><li><a href=#1325--48102-fusion-mamba-for-cross-modality-object-detection-wenhao-dong-et-al-2024>(13/25 | 48/102) Fusion-Mamba for Cross-modality Object Detection (Wenhao Dong et al., 2024)</a></li><li><a href=#1425--49102-syntstereo2real-edge-aware-gan-for-remote-sensing-image-to-image-translation-while-maintaining-stereo-constraint-vasudha-venkatesan-et-al-2024>(14/25 | 49/102) SyntStereo2Real: Edge-Aware GAN for Remote Sensing Image-to-Image Translation while Maintaining Stereo Constraint (Vasudha Venkatesan et al., 2024)</a></li><li><a href=#1525--50102-in-my-perspective-in-my-hands-accurate-egocentric-2d-hand-pose-and-action-recognition-wiktor-mucha-et-al-2024>(15/25 | 50/102) In My Perspective, In My Hands: Accurate Egocentric 2D Hand Pose and Action Recognition (Wiktor Mucha et al., 2024)</a></li><li><a href=#1625--51102-trafficvlm-a-controllable-visual-language-model-for-traffic-video-captioning-quang-minh-dinh-et-al-2024>(16/25 | 51/102) TrafficVLM: A Controllable Visual Language Model for Traffic Video Captioning (Quang Minh Dinh et al., 2024)</a></li><li><a href=#1725--52102-fedccl-federated-dual-clustered-feature-contrast-under-domain-heterogeneity-yu-qiao-et-al-2024>(17/25 | 52/102) FedCCL: Federated Dual-Clustered Feature Contrast Under Domain Heterogeneity (Yu Qiao et al., 2024)</a></li><li><a href=#1825--53102-textitsweet----an-open-source-modular-platform-for-contactless-hand-vascular-biometric-experiments-david-geissbühler-et-al-2024>(18/25 | 53/102) \textit{sweet} &ndash; An Open Source Modular Platform for Contactless Hand Vascular Biometric Experiments (David Geissbühler et al., 2024)</a></li><li><a href=#1925--54102-bridging-data-islands-geographic-heterogeneity-aware-federated-learning-for-collaborative-remote-sensing-semantic-segmentation-jieyi-tan-et-al-2024>(19/25 | 54/102) Bridging Data Islands: Geographic Heterogeneity-Aware Federated Learning for Collaborative Remote Sensing Semantic Segmentation (Jieyi Tan et al., 2024)</a></li><li><a href=#2025--55102-vrs-nerf-visual-relocalization-with-sparse-neural-radiance-field-fei-xue-et-al-2024>(20/25 | 55/102) VRS-NeRF: Visual Relocalization with Sparse Neural Radiance Field (Fei Xue et al., 2024)</a></li><li><a href=#2125--56102-hanet-a-hierarchical-attention-network-for-change-detection-with-bitemporal-very-high-resolution-remote-sensing-images-chengxi-han-et-al-2024>(21/25 | 56/102) HANet: A Hierarchical Attention Network for Change Detection With Bitemporal Very-High-Resolution Remote Sensing Images (Chengxi Han et al., 2024)</a></li><li><a href=#2225--57102-coreset-selection-for-object-detection-hojun-lee-et-al-2024>(22/25 | 57/102) Coreset Selection for Object Detection (Hojun Lee et al., 2024)</a></li><li><a href=#2325--58102-streaknet-arch-an-anti-scattering-network-based-architecture-for-underwater-carrier-lidar-radar-imaging-xuelong-li-et-al-2024>(23/25 | 58/102) StreakNet-Arch: An Anti-scattering Network-based Architecture for Underwater Carrier LiDAR-Radar Imaging (Xuelong Li et al., 2024)</a></li><li><a href=#2425--59102-face-voice-association-in-multilingual-environments-fame-challenge-2024-evaluation-plan-muhammad-saad-saeed-et-al-2024>(24/25 | 59/102) Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan (Muhammad Saad Saeed et al., 2024)</a></li><li><a href=#2525--60102-panet-a-physics-guided-parametric-augmentation-net-for-image-dehazing-by-hazing-chih-ling-chang-et-al-2024>(25/25 | 60/102) PANet: A Physics-guided Parametric Augmentation Net for Image Dehazing by Hazing (Chih-Ling Chang et al., 2024)</a></li></ul></li><li><a href=#csmm-1>cs.MM (1)</a><ul><li><a href=#11--61102-arena-a-patch-of-interest-vit-inference-acceleration-system-for-edge-assisted-video-analytics-haosong-peng-et-al-2024>(1/1 | 61/102) Arena: A Patch-of-Interest ViT Inference Acceleration System for Edge-Assisted Video Analytics (Haosong Peng et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--62102-prior-agnostic-multi-scale-contrastive-text-audio-pre-training-for-parallelized-tts-frontend-modeling-quanxiu-wang-et-al-2024>(1/2 | 62/102) Prior-agnostic Multi-scale Contrastive Text-Audio Pre-training for Parallelized TTS Frontend Modeling (Quanxiu Wang et al., 2024)</a></li><li><a href=#22--63102-an-experimental-comparison-of-multi-view-self-supervised-methods-for-music-tagging-gabriel-meseguer-brocal-et-al-2024>(2/2 | 63/102) An Experimental Comparison Of Multi-view Self-supervised Methods For Music Tagging (Gabriel Meseguer-Brocal et al., 2024)</a></li></ul></li><li><a href=#cscr-5>cs.CR (5)</a><ul><li><a href=#15--64102-counteracting-concept-drift-by-learning-with-future-malware-predictions-branislav-bosansky-et-al-2024>(1/5 | 64/102) Counteracting Concept Drift by Learning with Future Malware Predictions (Branislav Bosansky et al., 2024)</a></li><li><a href=#25--65102-new-class-of-ciphers-using-hardware-entropy-source-jan-j-tatarkiewicz-et-al-2024>(2/5 | 65/102) New Class of Ciphers Using Hardware Entropy Source (Jan J. Tatarkiewicz et al., 2024)</a></li><li><a href=#35--66102-artificial-intelligence-enhanced-security-problems-in-real-time-scenario-using-blowfish-algorithm-yuvaraju-chinnam-et-al-2024>(3/5 | 66/102) Artificial Intelligence enhanced Security Problems in Real-Time Scenario using Blowfish Algorithm (Yuvaraju Chinnam et al., 2024)</a></li><li><a href=#45--67102-make-split-not-hijack-preventing-feature-space-hijacking-attacks-in-split-learning-tanveer-khan-et-al-2024>(4/5 | 67/102) Make Split, not Hijack: Preventing Feature-Space Hijacking Attacks in Split Learning (Tanveer Khan et al., 2024)</a></li><li><a href=#55--68102-printlistener-uncovering-the-vulnerability-of-fingerprint-authentication-via-the-finger-friction-sound-man-zhou-et-al-2024>(5/5 | 68/102) PrintListener: Uncovering the Vulnerability of Fingerprint Authentication via the Finger Friction Sound (Man Zhou et al., 2024)</a></li></ul></li><li><a href=#cshc-3>cs.HC (3)</a><ul><li><a href=#13--69102-evaluating-the-efficacy-of-haptic-feedback-360-treadmill-integrated-virtual-reality-framework-and-longitudinal-training-on-decision-making-performance-in-a-complex-search-and-shoot-simulation-akash-k-rao-et-al-2024>(1/3 | 69/102) Evaluating the efficacy of haptic feedback, 360° treadmill-integrated Virtual Reality framework and longitudinal training on decision-making performance in a complex search-and-shoot simulation (Akash K Rao et al., 2024)</a></li><li><a href=#23--70102-deceptive-patterns-of-intelligent-and-interactive-writing-assistants-karim-benharrak-et-al-2024>(2/3 | 70/102) Deceptive Patterns of Intelligent and Interactive Writing Assistants (Karim Benharrak et al., 2024)</a></li><li><a href=#33--71102-investigating-the-impact-of-virtual-element-misalignment-in-collaborative-augmented-reality-experiences-francesco-vona-et-al-2024>(3/3 | 71/102) Investigating the impact of virtual element misalignment in collaborative Augmented Reality experiences (Francesco Vona et al., 2024)</a></li></ul></li><li><a href=#csro-6>cs.RO (6)</a><ul><li><a href=#16--72102-a-survey-on-integration-of-large-language-models-with-intelligent-robots-yeseung-kim-et-al-2024>(1/6 | 72/102) A Survey on Integration of Large Language Models with Intelligent Robots (Yeseung Kim et al., 2024)</a></li><li><a href=#26--73102-tube-rrt-efficient-homotopic-path-planning-for-swarm-robotics-passing-through-large-scale-obstacle-environments-pengda-mao-et-al-2024>(2/6 | 73/102) Tube-RRT*: Efficient Homotopic Path Planning for Swarm Robotics Passing-Through Large-Scale Obstacle Environments (Pengda Mao et al., 2024)</a></li><li><a href=#36--74102-beatle----self-reconfigurable-aerial-robot-design-control-and-experimental-validation-junichiro-sugihara-et-al-2024>(3/6 | 74/102) BEATLE &ndash; Self-Reconfigurable Aerial Robot: Design, Control and Experimental Validation (Junichiro Sugihara et al., 2024)</a></li><li><a href=#46--75102-learning-cross-hand-policies-for-high-dof-reaching-and-grasping-qijin-she-et-al-2024>(4/6 | 75/102) Learning Cross-hand Policies for High-DOF Reaching and Grasping (Qijin She et al., 2024)</a></li><li><a href=#56--76102-snn4agents-a-framework-for-developing-energy-efficient-embodied-spiking-neural-networks-for-autonomous-agents-rachmad-vidya-wicaksana-putra-et-al-2024>(5/6 | 76/102) SNN4Agents: A Framework for Developing Energy-Efficient Embodied Spiking Neural Networks for Autonomous Agents (Rachmad Vidya Wicaksana Putra et al., 2024)</a></li><li><a href=#66--77102-increasing-slam-pose-accuracy-by-ground-to-satellite-image-registration-yanhao-zhang-et-al-2024>(6/6 | 77/102) Increasing SLAM Pose Accuracy by Ground-to-Satellite Image Registration (Yanhao Zhang et al., 2024)</a></li></ul></li><li><a href=#csse-4>cs.SE (4)</a><ul><li><a href=#14--78102-tasks-people-prompt-a-taxonomy-of-llm-downstream-tasks-in-software-verification-and-falsification-approaches-víctor-a-braberman-et-al-2024>(1/4 | 78/102) Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software Verification and Falsification Approaches (Víctor A. Braberman et al., 2024)</a></li><li><a href=#24--79102-test-code-generation-for-telecom-software-systems-using-two-stage-generative-model-mohamad-nabeel-et-al-2024>(2/4 | 79/102) Test Code Generation for Telecom Software Systems using Two-Stage Generative Model (Mohamad Nabeel et al., 2024)</a></li><li><a href=#34--80102-emerging-platforms-meet-emerging-llms-a-year-long-journey-of-top-down-development-siyuan-feng-et-al-2024>(3/4 | 80/102) Emerging Platforms Meet Emerging LLMs: A Year-Long Journey of Top-Down Development (Siyuan Feng et al., 2024)</a></li><li><a href=#44--81102-service-weaver-a-promising-direction-for-cloud-native-systems-jacoby-johnson-et-al-2024>(4/4 | 81/102) Service Weaver: A Promising Direction for Cloud-native Systems? (Jacoby Johnson et al., 2024)</a></li></ul></li><li><a href=#csit-2>cs.IT (2)</a><ul><li><a href=#12--82102-joint-near-field-uplink-communication-and-localization-using-message-passing-based-sparse-bayesian-learning-fei-liu-et-al-2024>(1/2 | 82/102) Joint Near Field Uplink Communication and Localization Using Message Passing-Based Sparse Bayesian Learning (Fei Liu et al., 2024)</a></li><li><a href=#22--83102-unsourced-random-access-in-mimo-quasi-static-rayleigh-fading-channels-with-finite-blocklength-junyuan-gao-et-al-2024>(2/2 | 83/102) Unsourced Random Access in MIMO Quasi-Static Rayleigh Fading Channels with Finite Blocklength (Junyuan Gao et al., 2024)</a></li></ul></li><li><a href=#physicsao-ph-1>physics.ao-ph (1)</a><ul><li><a href=#11--84102-impact-of-curved-elements-for-flows-over-orography-with-a-discontinuous-galerkin-scheme-giuseppe-orlando-et-al-2024>(1/1 | 84/102) Impact of curved elements for flows over orography with a Discontinuous Galerkin scheme (Giuseppe Orlando et al., 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#11--85102-qandle-accelerating-state-vector-simulation-using-gate-matrix-caching-and-circuit-splitting-gerhard-stenzel-et-al-2024>(1/1 | 85/102) Qandle: Accelerating State Vector Simulation Using Gate-Matrix Caching and Circuit Splitting (Gerhard Stenzel et al., 2024)</a></li></ul></li><li><a href=#cssi-1>cs.SI (1)</a><ul><li><a href=#11--86102-survey-on-embedding-models-for-knowledge-graph-and-its-applications-manita-pote-2024>(1/1 | 86/102) Survey on Embedding Models for Knowledge Graph and its Applications (Manita Pote, 2024)</a></li></ul></li><li><a href=#cscy-2>cs.CY (2)</a><ul><li><a href=#12--87102-llempower-understanding-disparities-in-the-control-and-access-of-large-language-models-vishwas-sathish-et-al-2024>(1/2 | 87/102) LLeMpower: Understanding Disparities in the Control and Access of Large Language Models (Vishwas Sathish et al., 2024)</a></li><li><a href=#22--88102-a-computational-model-for-gender-asset-gap-management-with-a-focus-on-gender-disparity-in-land-acquisition-and-land-tenure-security-oluwatosin-ogundare-et-al-2024>(2/2 | 88/102) A computational model for gender asset gap management with a focus on gender disparity in land acquisition and land tenure security (Oluwatosin Ogundare et al., 2024)</a></li></ul></li><li><a href=#csdc-3>cs.DC (3)</a><ul><li><a href=#13--89102-the-intelligent-prediction-and-assessment-of-financial-information-risk-in-the-cloud-computing-model-yufu-wang-et-al-2024>(1/3 | 89/102) The intelligent prediction and assessment of financial information risk in the cloud computing model (Yufu Wang et al., 2024)</a></li><li><a href=#23--90102-egret-reinforcement-mechanism-for-sequential-computation-offloading-in-edge-computing-haosong-peng-et-al-2024>(2/3 | 90/102) Egret: Reinforcement Mechanism for Sequential Computation Offloading in Edge Computing (Haosong Peng et al., 2024)</a></li><li><a href=#33--91102-a-reinforcement-learning-based-backfilling-strategy-for-hpc-batch-jobs-elliot-kolker-hicks-et-al-2024>(3/3 | 91/102) A Reinforcement Learning Based Backfilling Strategy for HPC Batch Jobs (Elliot Kolker-Hicks et al., 2024)</a></li></ul></li><li><a href=#eesssy-3>eess.SY (3)</a><ul><li><a href=#13--92102-mpc-based-linear-equivalence-with-control-barrier-functions-for-vtol-uavs-ali-mohamed-ali-et-al-2024>(1/3 | 92/102) MPC Based Linear Equivalence with Control Barrier Functions for VTOL-UAVs (Ali Mohamed Ali et al., 2024)</a></li><li><a href=#23--93102-numerical-discretization-methods-for-the-extended-linear-quadratic-control-problem-zhanhao-zhang-et-al-2024>(2/3 | 93/102) Numerical Discretization Methods for the Extended Linear Quadratic Control Problem (Zhanhao Zhang et al., 2024)</a></li><li><a href=#33--94102-on-joint-convergence-of-traffic-state-and-weight-vector-in-learning-based-dynamic-routing-with-value-function-approximation-yidan-wu-et-al-2024>(3/3 | 94/102) On Joint Convergence of Traffic State and Weight Vector in Learning-Based Dynamic Routing with Value Function Approximation (Yidan Wu et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--95102-characterizing-soft-error-resiliency-in-arms-ethos-u55-embedded-machine-learning-accelerator-abhishek-tyagi-et-al-2024>(1/1 | 95/102) Characterizing Soft-Error Resiliency in Arm&rsquo;s Ethos-U55 Embedded Machine Learning Accelerator (Abhishek Tyagi et al., 2024)</a></li></ul></li><li><a href=#eessiv-1>eess.IV (1)</a><ul><li><a href=#11--96102-breast-cancer-image-classification-method-based-on-deep-transfer-learning-weimin-wang-et-al-2024>(1/1 | 96/102) Breast Cancer Image Classification Method Based on Deep Transfer Learning (Weimin Wang et al., 2024)</a></li></ul></li><li><a href=#astro-phep-1>astro-ph.EP (1)</a><ul><li><a href=#11--97102-machine-learning-based-identification-of-gaia-astrometric-exoplanet-orbits-johannes-sahlmann-et-al-2024>(1/1 | 97/102) Machine learning-based identification of Gaia astrometric exoplanet orbits (Johannes Sahlmann et al., 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#11--98102-correlated-mean-field-imitation-learning-zhiyu-zhao-et-al-2024>(1/1 | 98/102) Correlated Mean Field Imitation Learning (Zhiyu Zhao et al., 2024)</a></li></ul></li><li><a href=#csai-1>cs.AI (1)</a><ul><li><a href=#11--99102-owloop-interfaces-for-mapping-owl-axioms-into-oop-hierarchies-luca-buoncompagni-et-al-2024>(1/1 | 99/102) OWLOOP: Interfaces for Mapping OWL Axioms into OOP Hierarchies (Luca Buoncompagni et al., 2024)</a></li></ul></li><li><a href=#csne-1>cs.NE (1)</a><ul><li><a href=#11--100102-advanced-intelligent-optimization-algorithms-for-multi-objective-optimal-power-flow-in-future-power-systems-a-review-yuyan-li-2024>(1/1 | 100/102) Advanced Intelligent Optimization Algorithms for Multi-Objective Optimal Power Flow in Future Power Systems: A Review (Yuyan Li, 2024)</a></li></ul></li><li><a href=#cscc-1>cs.CC (1)</a><ul><li><a href=#11--101102-on-the-complexity-of-some-cycle-convexity-parameters-carlos-v-g-c-lima-et-al-2024>(1/1 | 101/102) On the complexity of some cycle convexity parameters (Carlos V. G. C. Lima et al., 2024)</a></li></ul></li><li><a href=#mathna-1>math.NA (1)</a><ul><li><a href=#11--102102-asymptotic-preserving-approximations-for-stochastic-incompressible-viscous-fluids-and-spdes-on-group-jianbo-cui-et-al-2024>(1/1 | 102/102) Asymptotic-preserving approximations for stochastic incompressible viscous fluids and SPDEs on group (Jianbo Cui et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>