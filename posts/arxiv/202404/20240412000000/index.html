<!doctype html><html><head><title>arXiv @ 2024.04.12</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.04.12"><meta property="og:description" content="Primary Categories cond-mat.mtrl-sci (2) cs.AI (5) cs.CE (1) cs.CG (1) cs.CL (37) cs.CR (5) cs.CV (53) cs.CY (4) cs.DB (1) cs.DC (1) cs.DL (1) cs.DM (1) cs.DS (3) cs.ET (1) cs.GT (1) cs.HC (9) cs.IR (4) cs.IT (8) cs.LG (30) cs.NE (3) cs.NI (4) cs.PF (1) cs.RO (12) cs.SD (2) cs.SE (2) eess.AS (5) eess.IV (3) eess.SY (7) math.CO (1) math.NA (2) math.OC (1) physics.optics (1) q-fin.ST (1) quant-ph (5) Keywords keyword cs."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202404/20240412000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-12T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-12T00:00:00+00:00"><meta name=description content="arXiv @ 2024.04.12"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240408000000/ title="arXiv @ 2024.04.08">arXiv @ 2024.04.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/ title="arXiv @ 2024.04.09">arXiv @ 2024.04.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240410000000/ title="arXiv @ 2024.04.10">arXiv @ 2024.04.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/ title="arXiv @ 2024.04.11">arXiv @ 2024.04.11</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/ title="arXiv @ 2024.04.12">arXiv @ 2024.04.12</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202404/20240412000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Friday, Apr 12, 2024</p></div><div class=title><h1>arXiv @ 2024.04.12</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#cond-matmtrl-sci-2>cond-mat.mtrl-sci (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#csai-5>cs.AI (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#cscl-37>cs.CL (37)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#cscr-5>cs.CR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#cscv-53>cs.CV (53)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#cscy-4>cs.CY (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#csdl-1>cs.DL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#csdm-1>cs.DM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#csds-3>cs.DS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#cset-1>cs.ET (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#cshc-9>cs.HC (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#csir-4>cs.IR (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#csit-8>cs.IT (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#cslg-30>cs.LG (30)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#csne-3>cs.NE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#csni-4>cs.NI (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#cspf-1>cs.PF (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#csro-12>cs.RO (12)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#csse-2>cs.SE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#eessas-5>eess.AS (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#eessiv-3>eess.IV (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#eesssy-7>eess.SY (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#physicsoptics-1>physics.optics (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#q-finst-1>q-fin.ST (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/#quant-ph-5>quant-ph (5)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Attack</td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Adversarial Purification</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Augmented Reality (AR)</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td>2</td><td>1</td><td>1</td></tr><tr><td>BERT</td><td>4</td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Benchmarking</td><td>10</td><td>18</td><td>3</td><td></td></tr><tr><td>Black Box</td><td>1</td><td></td><td></td><td></td></tr><tr><td>ChatGPT</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Code Generation</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Common-sense Reasoning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Content Detection</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Continuous Time</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Contrastive Learning</td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>Convolution</td><td></td><td>3</td><td>2</td><td>1</td></tr><tr><td>Convolutional Neural Network</td><td>1</td><td>1</td><td>4</td><td>2</td></tr><tr><td>Curriculum Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>4</td><td>2</td><td></td></tr><tr><td>Dialogue System</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Diffusion Model</td><td>1</td><td>13</td><td>1</td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Explainable AI</td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Federated Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Few-shot</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>7</td><td>2</td><td></td><td>1</td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>GPT</td><td>3</td><td></td><td></td><td>1</td></tr><tr><td>GPT-3</td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Gemini</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Generative AI</td><td></td><td>1</td><td>3</td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>2</td><td>3</td><td></td></tr><tr><td>Geometry</td><td></td><td>2</td><td></td><td>1</td></tr><tr><td>Graph</td><td>4</td><td>3</td><td>4</td><td></td></tr><tr><td>Graph Neural Network</td><td>2</td><td>1</td><td>4</td><td></td></tr><tr><td>Grounding</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Image2text</td><td></td><td>2</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>3</td><td>1</td><td>2</td><td></td></tr><tr><td>Instruction Tuning</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Knowledge Graph</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>LLaMA</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td></td><td>6</td></tr><tr><td>Language Generation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>44</td><td>5</td><td>4</td><td>2</td></tr><tr><td>Low-Resource</td><td>1</td><td></td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Meta Learning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Metaphor Detection</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Mistral</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td>2</td><td>11</td><td>3</td><td>2</td></tr><tr><td>Mutual Information</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Named Entity Recognition</td><td>4</td><td></td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>5</td><td></td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>4</td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Pre-trained Language Model</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Prompt</td><td>3</td><td>6</td><td>1</td><td></td></tr><tr><td>Quantization</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Question Answering</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Reasoning</td><td>2</td><td>1</td><td>2</td><td>1</td></tr><tr><td>Recommendation</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Reconstruction Loss</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td>1</td><td>1</td><td>3</td><td>2</td></tr><tr><td>Representation Learning</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Retrieval Augmentation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>15</td><td></td><td></td><td></td></tr><tr><td>Rouge</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Scaling Law</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Security</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Self-Attention</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>3</td><td>4</td><td>2</td></tr><tr><td>Simulation</td><td>2</td><td>1</td><td>1</td><td>3</td></tr><tr><td>Simulator</td><td>2</td><td>1</td><td>1</td><td>3</td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Style Transfer</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Summarization</td><td>4</td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td></td><td>4</td><td></td><td>2</td></tr><tr><td>T5</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Text Generation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Summarization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text-to-speech</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>6</td><td></td><td></td></tr><tr><td>Tokenization</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>Transformer</td><td>6</td><td>8</td><td>4</td><td>1</td></tr><tr><td>Unsupervised Learning</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Virtual Reality (VR)</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>4</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>2</td><td>1</td><td>2</td></tr><tr><td>Visual Question Answering</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Yolo</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>2</td><td>3</td><td>2</td><td></td></tr><tr><td>human-in-the-loop</td><td></td><td>1</td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-37>cs.CL (37)</h2><h3 id=137--1218-llms-in-biomedicine-a-study-on-clinical-named-entity-recognition-masoud-monajatipoor-et-al-2024>(1/37 | 1/218) LLMs in Biomedicine: A study on clinical Named Entity Recognition (Masoud Monajatipoor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masoud Monajatipoor, Jiaxin Yang, Joel Stremmel, Melika Emami, Fazlolah Mohaghegh, Mozhdeh Rouhsedaghat, Kai-Wei Chang. (2024)<br><strong>LLMs in Biomedicine: A study on clinical Named Entity Recognition</strong><br><button class=copy-to-clipboard title="LLMs in Biomedicine: A study on clinical Named Entity Recognition" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 113<br>Keywords: Benchmarking, Few-shot, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Zero-shot, Named Entity Recognition, Named Entity Recognition, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07376v1.pdf filename=2404.07376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> demonstrate remarkable versatility in various NLP tasks but encounter distinct challenges in biomedicine due to medical language complexities and data scarcity. This paper investigates the application of <b>LLMs</b> in the medical domain by exploring strategies to enhance their performance for the <b>Named-Entity</b> <b>Recognition</b> <b>(NER)</b> task. Specifically, our study reveals the importance of meticulously designed <b>prompts</b> in biomedicine. Strategic selection of <b>in-context</b> examples yields a notable improvement, showcasing ~15-20% increase in F1 score across all <b>benchmark</b> datasets for <b>few-shot</b> clinical <b>NER.</b> Additionally, our findings suggest that integrating external resources through <b>prompting</b> strategies can bridge the gap between general-purpose <b>LLM</b> proficiency and the specialized demands of medical <b>NER.</b> Leveraging a medical knowledge base, our proposed method inspired by <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> can boost the F1 score of <b>LLMs</b> for <b>zero-shot</b> clinical <b>NER.</b> We will release the code upon publication.</p></p class="citation"></blockquote><h3 id=237--2218-superposition-prompting-improving-and-accelerating-retrieval-augmented-generation-thomas-merth-et-al-2024>(2/37 | 2/218) Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation (Thomas Merth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Merth, Qichen Fu, Mohammad Rastegari, Mahyar Najibi. (2024)<br><strong>Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation</strong><br><button class=copy-to-clipboard title="Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 93<br>Keywords: Benchmarking, Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Transformer, Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06910v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06910v1.pdf filename=2404.06910v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the successes of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> they exhibit significant drawbacks, particularly when processing long contexts. Their inference cost scales quadratically with respect to sequence length, making it expensive for deployment in some real-world text processing applications, such as <b>retrieval-augmented</b> <b>generation</b> <b>(RAG).</b> Additionally, <b>LLMs</b> also exhibit the &ldquo;distraction phenomenon,&rdquo; where irrelevant context in the <b>prompt</b> degrades output quality. To address these drawbacks, we propose a novel <b>RAG</b> <b>prompting</b> methodology, superposition <b>prompting,</b> which can be directly applied to pre-trained <b>transformer-based</b> <b>LLMs</b> without the need for <b>fine-tuning.</b> At a high level, superposition <b>prompting</b> allows the <b>LLM</b> to process input documents in parallel <b>prompt</b> paths, discarding paths once they are deemed irrelevant. We demonstrate the capability of our method to simultaneously enhance time efficiency across a variety of <b>question-answering</b> <b>benchmarks</b> using multiple pre-trained <b>LLMs.</b> Furthermore, our technique significantly improves accuracy when the retrieved context is <b>large</b> <b>relative</b> <b>the</b> context the model was trained on. For example, our approach facilitates an 93x reduction in compute time while improving accuracy by 43% on the NaturalQuestions-Open dataset with the MPT-7B instruction-tuned model over naive <b>RAG.</b></p></p class="citation"></blockquote><h3 id=337--3218-onco-retriever-generative-classifier-for-retrieval-of-ehr-records-in-oncology-shashi-kant-gupta-et-al-2024>(3/37 | 3/218) Onco-Retriever: Generative Classifier for Retrieval of EHR Records in Oncology (Shashi Kant Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shashi Kant Gupta, Aditya Basu, Bradley Taylor, Anai Kothari, Hrituraj Singh. (2024)<br><strong>Onco-Retriever: Generative Classifier for Retrieval of EHR Records in Oncology</strong><br><button class=copy-to-clipboard title="Onco-Retriever: Generative Classifier for Retrieval of EHR Records in Oncology" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Mistral, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06680v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06680v1.pdf filename=2404.06680v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Retrieving information from EHR systems is essential for answering specific questions about patient journeys and improving the delivery of clinical care. Despite this fact, most EHR systems still rely on keyword-based searches. With the advent of generative <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> retrieving information can lead to better search and <b>summarization</b> capabilities. Such retrievers can also feed <b>Retrieval-augmented</b> <b>generation</b> <b>(RAG)</b> pipelines to answer any query. However, the task of retrieving information from EHR real-world clinical data contained within EHR systems in order to solve several downstream use cases is challenging due to the difficulty in creating query-document support pairs. We provide a blueprint for creating such datasets in an affordable manner using <b>large</b> <b>language</b> <b>models.</b> Our method results in a retriever that is 30-50 F-1 points better than propriety counterparts such as Ada and <b>Mistral</b> for oncology data elements. We further compare our model, called Onco-Retriever, against <b>fine-tuned</b> PubMedBERT model as well. We conduct an extensive manual evaluation on real-world EHR data along with latency analysis of the different models and provide a path forward for healthcare organizations to build domain-specific retrievers.</p></p class="citation"></blockquote><h3 id=437--4218-xnlieu-a-dataset-for-cross-lingual-nli-in-basque-maite-heredia-et-al-2024>(4/37 | 4/218) XNLIeu: a dataset for cross-lingual NLI in Basque (Maite Heredia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maite Heredia, Julen Etxaniz, Muitze Zulaika, Xabier Saralegi, Jeremy Barnes, Aitor Soroa. (2024)<br><strong>XNLIeu: a dataset for cross-lingual NLI in Basque</strong><br><button class=copy-to-clipboard title="XNLIeu: a dataset for cross-lingual NLI in Basque" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Low-Resource, Transfer Learning, Natural Language Inference, Natural Language Inference, Natural Language Understanding, Neural Machine Translation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06996v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06996v1.pdf filename=2404.06996v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>XNLI is a popular <b>Natural</b> <b>Language</b> <b>Inference</b> <b>(NLI)</b> <b>benchmark</b> widely used to evaluate cross-lingual <b>Natural</b> <b>Language</b> <b>Understanding</b> (NLU) capabilities across languages. In this paper, we expand XNLI to include Basque, a <b>low-resource</b> language that can greatly benefit from <b>transfer-learning</b> <b>approaches.</b> The new dataset, dubbed XNLIeu, has been developed by first machine-translating the English XNLI corpus into Basque, followed by a manual post-edition step. We have conducted a series of experiments using mono- and multilingual <b>LLMs</b> to assess a) the effect of professional post-edition on the <b>MT</b> system; b) the best cross-lingual strategy for <b>NLI</b> in Basque; and c) whether the choice of the best cross-lingual strategy is influenced by the fact that the dataset is built by translation. The results show that post-edition is necessary and that the translate-train cross-lingual strategy obtains better results overall, although the gain is lower when tested in a dataset that has been built natively from scratch. Our code and datasets are publicly available under open licenses.</p></p class="citation"></blockquote><h3 id=537--5218-dynamic-generation-of-personalities-with-large-language-models-jianzhi-liu-et-al-2024>(5/37 | 5/218) Dynamic Generation of Personalities with Large Language Models (Jianzhi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianzhi Liu, Hexiang Gu, Tianyu Zheng, Liuyu Xiang, Huijia Wu, Jie Fu, Zhaofeng He. (2024)<br><strong>Dynamic Generation of Personalities with Large Language Models</strong><br><button class=copy-to-clipboard title="Dynamic Generation of Personalities with Large Language Models" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Fine-tuning, GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07084v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07084v1.pdf filename=2404.07084v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of mimicking human deliberation, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> show promising performance, thereby amplifying the importance of this research area. Deliberation is influenced by both logic and personality. However, previous studies predominantly focused on the logic of <b>LLMs,</b> neglecting the exploration of personality aspects. In this work, we introduce Dynamic Personality Generation (DPG), a dynamic personality generation method based on Hypernetworks. Initially, we embed the Big Five personality theory into <b>GPT-4</b> to form a personality assessment machine, enabling it to evaluate characters&rsquo; personality traits from dialogues automatically. We propose a new metric to assess personality generation capability based on this evaluation method. Then, we use this personality assessment machine to evaluate dialogues in script data, resulting in a personality-dialogue dataset. Finally, we <b>fine-tune</b> DPG on the personality-dialogue dataset. Experiments prove that DPG&rsquo;s personality generation capability is stronger after <b>fine-tuning</b> on this dataset than traditional <b>fine-tuning</b> methods, surpassing <b>prompt-based</b> <b>GPT-4.</b></p></p class="citation"></blockquote><h3 id=637--6218-hybrid-multi-stage-decoding-for-few-shot-ner-with-entity-aware-contrastive-learning-peipei-liu-et-al-2024>(6/37 | 6/218) Hybrid Multi-stage Decoding for Few-shot NER with Entity-aware Contrastive Learning (Peipei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peipei Liu, Gaosheng Wang, Ying Tong, Jian Liang, Zhenquan Ding, Hongsong Zhu. (2024)<br><strong>Hybrid Multi-stage Decoding for Few-shot NER with Entity-aware Contrastive Learning</strong><br><button class=copy-to-clipboard title="Hybrid Multi-stage Decoding for Few-shot NER with Entity-aware Contrastive Learning" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Contrastive Learning, Few-shot, Fine-tuning, Fine-tuning, Meta Learning, Named Entity Recognition, Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06970v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06970v1.pdf filename=2404.06970v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> <b>named</b> <b>entity</b> <b>recognition</b> can identify new types of <b>named</b> <b>entities</b> <b>based</b> on a few labeled examples. Previous methods employing token-level or span-level metric learning suffer from the computational burden and a large number of negative sample spans. In this paper, we propose the Hybrid Multi-stage Decoding for <b>Few-shot</b> <b>NER</b> with Entity-aware <b>Contrastive</b> <b>Learning</b> (MsFNER), which splits the general <b>NER</b> into two stages: entity-span detection and entity classification. There are 3 processes for introducing MsFNER: training, <b>finetuning,</b> and inference. In the training process, we train and get the best entity-span detection model and the entity classification model separately on the source domain using <b>meta-learning,</b> <b>where</b> we create a <b>contrastive</b> <b>learning</b> module to enhance entity representations for entity classification. During <b>finetuning,</b> we <b>finetune</b> the both models on the support dataset of target domain. In the inference process, for the unlabeled data, we first detect the entity-spans, then the entity-spans are jointly determined by the entity classification model and the KNN. We conduct experiments on the open FewNERD dataset and the results demonstrate the advance of MsFNER.</p></p class="citation"></blockquote><h3 id=737--7218-control-dag-constrained-decoding-for-non-autoregressive-directed-acyclic-t5-using-weighted-finite-state-automata-jinghong-chen-et-al-2024>(7/37 | 7/218) Control-DAG: Constrained Decoding for Non-Autoregressive Directed Acyclic T5 using Weighted Finite State Automata (Jinghong Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinghong Chen, Weizhe Lin, Jingbiao Mei, Bill Byrne. (2024)<br><strong>Control-DAG: Constrained Decoding for Non-Autoregressive Directed Acyclic T5 using Weighted Finite State Automata</strong><br><button class=copy-to-clipboard title="Control-DAG: Constrained Decoding for Non-Autoregressive Directed Acyclic T5 using Weighted Finite State Automata" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: T5, Transformer, Language Generation, Natural Language Generation, Natural Language Generation, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06854v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06854v1.pdf filename=2404.06854v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Directed Acyclic <b>Transformer</b> is a fast non-autoregressive (NAR) model that performs well in <b>Neural</b> <b>Machine</b> <b>Translation.</b> Two issues prevent its application to general <b>Natural</b> <b>Language</b> <b>Generation</b> <b>(NLG)</b> tasks: frequent Out-Of-Vocabulary (OOV) errors and the inability to faithfully generate entity names. We introduce Control-DAG, a constrained decoding algorithm for our Directed Acyclic <b>T5</b> (DA-T5) model which offers lexical, vocabulary and length control. We show that Control-DAG significantly enhances DA-T5 on the Schema Guided Dialogue and the DART datasets, establishing strong NAR results for Task-Oriented Dialogue and Data-to-Text <b>NLG.</b></p></p class="citation"></blockquote><h3 id=837--8218-not-all-contexts-are-equal-teaching-llms-credibility-aware-generation-ruotong-pan-et-al-2024>(8/37 | 8/218) Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation (Ruotong Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruotong Pan, Boxi Cao, Hongyu Lin, Xianpei Han, Jia Zheng, Sirui Wang, Xunliang Cai, Le Sun. (2024)<br><strong>Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation</strong><br><button class=copy-to-clipboard title="Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model, Retrieval Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06809v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06809v1.pdf filename=2404.06809v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid development of <b>large</b> <b>language</b> <b>models</b> has led to the widespread adoption of <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG),</b> which integrates external knowledge to alleviate knowledge bottlenecks and mitigate hallucinations. However, the existing <b>RAG</b> paradigm inevitably suffers from the impact of flawed information introduced during the <b>retrieval</b> <b>phrase,</b> <b>thereby</b> diminishing the reliability and correctness of the generated outcomes. In this paper, we propose Credibility-aware Generation (CAG), a universally applicable framework designed to mitigate the impact of flawed information in <b>RAG.</b> At its core, CAG aims to equip models with the ability to discern and process information based on its credibility. To this end, we propose an innovative data transformation framework that generates data based on credibility, thereby effectively endowing models with the capability of CAG. Furthermore, to accurately evaluate the models&rsquo; capabilities of CAG, we construct a comprehensive <b>benchmark</b> covering three critical real-world scenarios. Experimental results demonstrate that our model can effectively understand and utilize credibility for generation, significantly outperform other models with <b>retrieval</b> <b>augmentation,</b> <b>and</b> exhibit resilience against the disruption caused by noisy documents, thereby maintaining robust performance. Moreover, our model supports customized credibility, offering a wide range of potential applications.</p></p class="citation"></blockquote><h3 id=937--9218-simpler-becomes-harder-do-llms-exhibit-a-coherent-behavior-on-simplified-corpora-miriam-anschütz-et-al-2024>(9/37 | 9/218) Simpler becomes Harder: Do LLMs Exhibit a Coherent Behavior on Simplified Corpora? (Miriam Anschütz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miriam Anschütz, Edoardo Mosca, Georg Groh. (2024)<br><strong>Simpler becomes Harder: Do LLMs Exhibit a Coherent Behavior on Simplified Corpora?</strong><br><button class=copy-to-clipboard title="Simpler becomes Harder: Do LLMs Exhibit a Coherent Behavior on Simplified Corpora?" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: BERT, GPT, GPT-3, GPT-3.5, Large Language Model, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06838v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06838v1.pdf filename=2404.06838v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text simplification seeks to improve readability while retaining the original content and meaning. Our study investigates whether pre-trained classifiers also maintain such coherence by comparing their predictions on both original and simplified inputs. We conduct experiments using 11 pre-trained models, including <b>BERT</b> and OpenAI&rsquo;s <b>GPT</b> 3.5, across six datasets spanning three languages. Additionally, we conduct a detailed analysis of the correlation between prediction change rates and simplification types/strengths. Our findings reveal alarming inconsistencies across all languages and models. If not promptly addressed, simplified inputs can be easily exploited to craft zero-iteration model-agnostic <b>adversarial</b> <b>attacks</b> with success rates of up to 50%</p></p class="citation"></blockquote><h3 id=1037--10218-grasame-injecting-token-level-structural-information-to-pretrained-language-models-via-graph-guided-self-attention-mechanism-shuzhou-yuan-et-al-2024>(10/37 | 10/218) GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism (Shuzhou Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuzhou Yuan, Michael Färber. (2024)<br><strong>GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism</strong><br><button class=copy-to-clipboard title="GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 59<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Multi-modal, Multi-modal, Pre-trained Language Model, Pre-trained Language Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06911v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06911v1.pdf filename=2404.06911v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pretrained</b> <b>Language</b> <b>Models</b> <b>(PLMs)</b> benefit from external knowledge stored in <b>graph</b> <b>structures</b> <b>for</b> various downstream tasks. However, bridging the modality gap between <b>graph</b> <b>structures</b> <b>and</b> text remains a significant challenge. Traditional methods like linearizing <b>graphs</b> <b>for</b> <b>PLMs</b> lose vital <b>graph</b> <b>connectivity,</b> <b>whereas</b> <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> require cumbersome processes for integration into <b>PLMs.</b> In this work, we propose a novel <b>graph-guided</b> <b>self-attention</b> <b>mechanism,</b> GraSAME. GraSAME seamlessly incorporates token-level structural information into <b>PLMs</b> without necessitating additional alignment or concatenation efforts. As an end-to-end, lightweight <b>multimodal</b> module, GraSAME follows a multi-task learning strategy and effectively bridges the gap between <b>graph</b> <b>and</b> <b>textual</b> modalities, facilitating dynamic interactions between <b>GNNs</b> and <b>PLMs.</b> Our experiments on the <b>graph-to-text</b> <b>generation</b> <b>task</b> demonstrate that GraSAME outperforms baseline models and achieves results comparable to state-of-the-art (SOTA) models on WebNLG datasets. Furthermore, compared to SOTA models, GraSAME eliminates the need for extra pre-training tasks to adjust <b>graph</b> <b>inputs</b> <b>and</b> reduces the number of trainable parameters by over 100 million.</p></p class="citation"></blockquote><h3 id=1137--11218-metacheckgpt----a-multi-task-hallucination-detector-using-llm-uncertainty-and-meta-models-rahul-mehta-et-al-2024>(11/37 | 11/218) MetaCheckGPT &ndash; A Multi-task Hallucination Detector Using LLM Uncertainty and Meta-models (Rahul Mehta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rahul Mehta, Andrew Hoblitzell, Jack O&rsquo;Keefe, Hyeju Jang, Vasudeva Varma. (2024)<br><strong>MetaCheckGPT &ndash; A Multi-task Hallucination Detector Using LLM Uncertainty and Meta-models</strong><br><button class=copy-to-clipboard title="MetaCheckGPT -- A Multi-task Hallucination Detector Using LLM Uncertainty and Meta-models" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T07, 68T50, I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 55<br>Keywords: Black Box, ChatGPT, GPT-4, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06948v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06948v2.pdf filename=2404.06948v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hallucinations in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have recently become a significant problem. A recent effort in this direction is a shared task at Semeval 2024 Task 6, SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. This paper describes our winning solution ranked 1st and 2nd in the 2 sub-tasks of model agnostic and model aware tracks respectively. We propose a meta-regressor framework of <b>LLMs</b> for model evaluation and integration that achieves the highest scores on the leaderboard. We also experiment with various <b>transformer-based</b> models and <b>black</b> <b>box</b> methods like <b>ChatGPT,</b> Vectara, and others. In addition, we perform an error analysis comparing <b>GPT4</b> against our best model which shows the limitations of the former.</p></p class="citation"></blockquote><h3 id=1237--12218-transferable-and-efficient-non-factual-content-detection-via-probe-training-with-offline-consistency-checking-xiaokang-zhang-et-al-2024>(12/37 | 12/218) Transferable and Efficient Non-Factual Content Detection via Probe Training with Offline Consistency Checking (Xiaokang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaokang Zhang, Zijun Yao, Jing Zhang, Kaifeng Yun, Jifan Yu, Juanzi Li, Jie Tang. (2024)<br><strong>Transferable and Efficient Non-Factual Content Detection via Probe Training with Offline Consistency Checking</strong><br><button class=copy-to-clipboard title="Transferable and Efficient Non-Factual Content Detection via Probe Training with Offline Consistency Checking" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Out-of-distribution, Content Detection, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06742v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06742v1.pdf filename=2404.06742v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting non-factual <b>content</b> <b>is</b> a longstanding goal to increase the trustworthiness of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> generations. Current factuality probes, trained using humanannotated labels, exhibit limited transferability to <b>out-of-distribution</b> <b>content,</b> <b>while</b> online selfconsistency checking imposes extensive computation burden due to the necessity of generating multiple outputs. This paper proposes PINOSE, which trains a probing model on offline self-consistency checking results, thereby circumventing the need for human-annotated data and achieving transferability across diverse data distributions. As the consistency check process is offline, PINOSE reduces the computational burden of generating multiple responses by online consistency verification. Additionally, it examines various aspects of internal states prior to response decoding, contributing to more effective detection of factual inaccuracies. Experiment results on both factuality detection and <b>question</b> <b>answering</b> <b>benchmarks</b> show that PINOSE achieves surpassing results than existing factuality detection methods. Our code and datasets are publicly available on this anonymized repository.</p></p class="citation"></blockquote><h3 id=1337--13218-from-model-centered-to-human-centered-revision-distance-as-a-metric-for-text-evaluation-in-llms-based-applications-yongqiang-ma-et-al-2024>(13/37 | 13/218) From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications (Yongqiang Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongqiang Ma, Lizhi Qing, Jiawei Liu, Yangyang Kang, Yue Zhang, Wei Lu, Xiaozhong Liu, Qikai Cheng. (2024)<br><strong>From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications</strong><br><button class=copy-to-clipboard title="From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 50<br>Keywords: BERT, GPT, Large Language Model, Large Language Model, Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07108v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07108v2.pdf filename=2404.07108v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluating <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> is fundamental, particularly in the context of practical applications. Conventional evaluation methods, typically designed primarily for <b>LLM</b> development, yield numerical scores that ignore the user experience. Therefore, our study shifts the focus from model-centered to human-centered evaluation in the context of AI-powered writing assistance applications. Our proposed metric, termed <code>Revision Distance,'' utilizes &lt;b>LLMs&lt;/b> to suggest revision edits that mimic the human writing process. It is determined by counting the revision edits generated by &lt;b>LLMs.&lt;/b> Benefiting from the generated revision edit details, our metric can provide a self-explained text evaluation result in a human-understandable manner beyond the context-independent score. Our results show that for the easy-writing task, </code>Revision Distance&rsquo;&rsquo; is consistent with established metrics <b>(ROUGE,</b> <b>Bert-score,</b> and <b>GPT-score),</b> but offers more insightful, detailed feedback and better distinguishes between texts. Moreover, in the context of challenging academic writing tasks, our metric still delivers reliable evaluations where other metrics tend to struggle. Furthermore, our metric also holds significant potential for scenarios lacking reference texts.</p></p class="citation"></blockquote><h3 id=1437--14218-accelerating-inference-in-large-language-models-with-a-unified-layer-skipping-strategy-yijin-liu-et-al-2024>(14/37 | 14/218) Accelerating Inference in Large Language Models with a Unified Layer Skipping Strategy (Yijin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijin Liu, Fandong Meng, Jie Zhou. (2024)<br><strong>Accelerating Inference in Large Language Models with a Unified Layer Skipping Strategy</strong><br><button class=copy-to-clipboard title="Accelerating Inference in Large Language Models with a Unified Layer Skipping Strategy" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Neural Machine Translation, Text Summarization, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06954v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06954v1.pdf filename=2404.06954v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, dynamic computation methods have shown notable acceleration for <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> by skipping several layers of computations through elaborate heuristics or additional predictors. However, in the decoding process of existing approaches, different samples are assigned different computational budgets, which cannot guarantee a stable and precise acceleration effect. Furthermore, existing approaches generally skip multiple contiguous layers at the bottom or top of the layers, leading to a drastic change in the model&rsquo;s layer-wise representations, and thus a consequent performance degeneration. Therefore, we propose a Unified Layer Skipping strategy, which selects the number of layers to skip computation based solely on the target speedup ratio, and then skips the corresponding number of intermediate layer computations in a balanced manner. Since the Unified Layer Skipping strategy is independent of input samples, it naturally supports popular acceleration techniques such as batch decoding and KV caching, thus demonstrating more practicality for real-world applications. Experimental results on two common tasks, i.e., <b>machine</b> <b>translation</b> and <b>text</b> <b>summarization,</b> indicate that given a target speedup ratio, the Unified Layer Skipping strategy significantly enhances both the inference performance and the actual model throughput over existing dynamic approaches.</p></p class="citation"></blockquote><h3 id=1537--15218-llama-vits-enhancing-tts-synthesis-with-semantic-awareness-xincan-feng-et-al-2024>(15/37 | 15/218) Llama-VITS: Enhancing TTS Synthesis with Semantic Awareness (Xincan Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xincan Feng, Akifumi Yoshimoto. (2024)<br><strong>Llama-VITS: Enhancing TTS Synthesis with Semantic Awareness</strong><br><button class=copy-to-clipboard title="Llama-VITS: Enhancing TTS Synthesis with Semantic Awareness" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 50<br>Keywords: BERT, LLaMA, Text-to-speech, Text-to-speech, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06714v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06714v1.pdf filename=2404.06714v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in Natural Language Processing (NLP) have seen Large-scale Language Models <b>(LLMs)</b> excel at producing high-quality text for various purposes. Notably, in <b>Text-To-Speech</b> <b>(TTS)</b> systems, the integration of <b>BERT</b> for semantic token generation has underscored the importance of semantic content in producing coherent speech outputs. Despite this, the specific utility of <b>LLMs</b> in enhancing <b>TTS</b> synthesis remains considerably limited. This research introduces an innovative approach, <b>Llama-VITS,</b> which enhances <b>TTS</b> synthesis by enriching the semantic content of text using <b>LLM.</b> <b>Llama-VITS</b> integrates semantic embeddings from Llama2 with the VITS model, a leading end-to-end <b>TTS</b> framework. By leveraging Llama2 for the primary speech synthesis process, our experiments demonstrate that <b>Llama-VITS</b> matches the naturalness of the original VITS (ORI-VITS) and those incorporate <b>BERT</b> <b>(BERT-VITS),</b> on the LJSpeech dataset, a substantial collection of neutral, clear speech. Moreover, our method significantly enhances emotive expressiveness on the EmoV_DB_bea_sem dataset, a curated selection of emotionally consistent speech from the EmoV_DB dataset, highlighting its potential to generate emotive speech.</p></p class="citation"></blockquote><h3 id=1637--16218-leave-no-context-behind-efficient-infinite-context-transformers-with-infini-attention-tsendsuren-munkhdalai-et-al-2024>(16/37 | 16/218) Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (Tsendsuren Munkhdalai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tsendsuren Munkhdalai, Manaal Faruqui, Siddharth Gopal. (2024)<br><strong>Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</strong><br><button class=copy-to-clipboard title="Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs-NE, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Transformer, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07143v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07143v1.pdf filename=2404.07143v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work introduces an efficient method to scale <b>Transformer-based</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single <b>Transformer</b> block. We demonstrate the effectiveness of our approach on long-context language modeling <b>benchmarks,</b> 1M sequence length passkey context block retrieval and 500K length book <b>summarization</b> tasks with 1B and 8B <b>LLMs.</b> Our approach introduces minimal bounded memory parameters and enables fast streaming inference for <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1737--17218-continuous-language-model-interpolation-for-dynamic-and-controllable-text-generation-sara-kangaslahti-et-al-2024>(17/37 | 17/218) Continuous Language Model Interpolation for Dynamic and Controllable Text Generation (Sara Kangaslahti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sara Kangaslahti, David Alvarez-Melis. (2024)<br><strong>Continuous Language Model Interpolation for Dynamic and Controllable Text Generation</strong><br><button class=copy-to-clipboard title="Continuous Language Model Interpolation for Dynamic and Controllable Text Generation" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07117v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07117v1.pdf filename=2404.07117v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have gained popularity for a variety of use cases, making them adaptable and controllable has become increasingly important, especially for user-facing applications. While the existing literature on <b>LLM</b> adaptation primarily focuses on finding a model (or models) that optimizes a single predefined objective, here we focus on the challenging case where the model must dynamically adapt to diverse &ndash; and often changing &ndash; user preferences. For this, we leverage adaptation methods based on linear weight interpolation, casting them as continuous multi-domain interpolators that produce models with specific prescribed generation characteristics on-the-fly. Specifically, we use low-rank updates to <b>fine-tune</b> a base model to various different domains, yielding a set of anchor models with distinct generation profiles. Then, we use the weight updates of these anchor models to parametrize the entire (infinite) class of models contained within their convex hull. We empirically show that varying the interpolation weights yields predictable and consistent change in the model outputs with respect to all of the controlled attributes. We find that there is little entanglement between most attributes and identify and discuss the pairs of attributes for which this is not the case. Our results suggest that linearly interpolating between the weights of <b>fine-tuned</b> models facilitates predictable, fine-grained control of model outputs with respect to multiple stylistic characteristics simultaneously.</p></p class="citation"></blockquote><h3 id=1837--18218-groundedness-in-retrieval-augmented-long-form-generation-an-empirical-study-alessandro-stolfo-2024>(18/37 | 18/218) Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study (Alessandro Stolfo, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alessandro Stolfo. (2024)<br><strong>Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study</strong><br><button class=copy-to-clipboard title="Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Question Answering, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07060v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07060v1.pdf filename=2404.07060v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present an empirical study of groundedness in long-form <b>question</b> <b>answering</b> (LFQA) by retrieval-augmented <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> In particular, we evaluate whether every generated sentence is grounded in the retrieved documents or the model&rsquo;s pre-training data. Across 3 datasets and 4 model families, our findings reveal that a significant fraction of generated sentences are consistently ungrounded, even when those sentences contain correct ground-truth answers. Additionally, we examine the impacts of factors such as model size, decoding strategy, and <b>instruction</b> <b>tuning</b> on groundedness. Our results show that while larger models tend to ground their outputs more effectively, a significant portion of correct answers remains compromised by hallucinations. This study provides novel insights into the groundedness challenges in LFQA and underscores the necessity for more robust mechanisms in <b>LLMs</b> to mitigate the generation of ungrounded content.</p></p class="citation"></blockquote><h3 id=1937--19218-personality-aware-student-simulation-for-conversational-intelligent-tutoring-systems-zhengyuan-liu-et-al-2024>(19/37 | 19/218) Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems (Zhengyuan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengyuan Liu, Stella Xin Yin, Geyu Lin, Nancy F. Chen. (2024)<br><strong>Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems</strong><br><button class=copy-to-clipboard title="Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs.CL<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06762v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06762v1.pdf filename=2404.06762v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced learning experience. The emergence of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> further enables better human-machine interaction, and facilitates the development of conversational ITSs in various disciplines such as math and language learning. In dialogic teaching, recognizing and adapting to individual characteristics can significantly enhance student engagement and learning efficiency. However, characterizing and simulating student&rsquo;s persona remain challenging in training and evaluating conversational ITSs. In this work, we propose a framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects, and leverage <b>LLMs</b> for personality-aware student <b>simulation</b> in a language learning scenario. We further enhance the framework with multi-aspect validation, and conduct extensive analysis from both teacher and student perspectives. Our experimental results show that state-of-the-art <b>LLMs</b> can produce diverse student responses according to the given language ability and personality traits, and trigger teacher&rsquo;s adaptive scaffolding strategies.</p></p class="citation"></blockquote><h3 id=2037--20218-mathvc-an-llm-simulated-multi-character-virtual-classroom-for-mathematics-education-murong-yue-et-al-2024>(20/37 | 20/218) MathVC: An LLM-Simulated Multi-Character Virtual Classroom for Mathematics Education (Murong Yue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Murong Yue, Wijdane Mifdal, Yixuan Zhang, Jennifer Suh, Ziyu Yao. (2024)<br><strong>MathVC: An LLM-Simulated Multi-Character Virtual Classroom for Mathematics Education</strong><br><button class=copy-to-clipboard title="MathVC: An LLM-Simulated Multi-Character Virtual Classroom for Mathematics Education" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs.CL<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06711v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06711v1.pdf filename=2404.06711v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mathematical modeling (MM) is considered a fundamental skill for students in STEM disciplines. Practicing the MM skill is often the most effective when students can engage in group discussion and collaborative problem-solving. However, due to unevenly distributed teachers and educational resources needed to monitor such group activities, students do not always receive equal opportunities for this practice. Excitingly, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have recently demonstrated strong capability in both modeling mathematical problems and simulating characters with different traits and properties. Drawing inspiration from the advancement of <b>LLMs,</b> in this work, we present MATHVC, the very first <b>LLM-powered</b> virtual classroom containing multiple <b>LLM-simulated</b> student characters, with whom a human student can practice their MM skill. To encourage each <b>LLM</b> character&rsquo;s behaviors to be aligned with their specified math-relevant properties (termed &ldquo;characteristics alignment&rdquo;) and the overall conversational procedure to be close to an authentic student MM discussion (termed &ldquo;conversational procedural alignment&rdquo;), we proposed three innovations: integrating MM domain knowledge into the <b>simulation,</b> defining a symbolic schema as the ground for character <b>simulation,</b> and designing a meta planner at the platform level to drive the conversational procedure. Through experiments and ablation studies, we confirmed the effectiveness of our <b>simulation</b> approach and showed the promise for MATHVC to benefit real-life students in the future.</p></p class="citation"></blockquote><h3 id=2137--21218-whats-mine-becomes-yours-defining-annotating-and-detecting-context-dependent-paraphrases-in-news-interview-dialogs-anna-wegmann-et-al-2024>(21/37 | 21/218) What&rsquo;s Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs (Anna Wegmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna Wegmann, Tijs van den Broek, Dong Nguyen. (2024)<br><strong>What&rsquo;s Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs</strong><br><button class=copy-to-clipboard title="What's Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Convolutional Neural Network, Recommendation, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06670v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06670v1.pdf filename=2404.06670v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Best practices for high conflict conversations like counseling or customer support almost always include <b>recommendations</b> to paraphrase the previous speaker. Although paraphrase classification has received widespread attention in NLP, paraphrases are usually considered independent from context, and common models and datasets are not applicable to dialog settings. In this work, we investigate paraphrases in dialog (e.g., Speaker 1: &ldquo;That book is mine.&rdquo; becomes Speaker 2: &ldquo;That book is yours.&rdquo;). We provide an operationalization of context-dependent paraphrases, and develop a training for crowd-workers to classify paraphrases in dialog. We introduce a dataset with utterance pairs from NPR and <b>CNN</b> news interviews annotated for context-dependent paraphrases. To enable analyses on label variation, the dataset contains 5,581 annotations on 600 utterance pairs. We present promising results with <b>in-context</b> <b>learning</b> and with token classification models for automatic paraphrase detection in dialog.</p></p class="citation"></blockquote><h3 id=2237--22218-graph-chain-of-thought-augmenting-large-language-models-by-reasoning-on-graphs-bowen-jin-et-al-2024>(22/37 | 22/218) Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs (Bowen Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Suhang Wang, Yu Meng, Jiawei Han. (2024)<br><strong>Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs</strong><br><button class=copy-to-clipboard title="Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 36<br>Keywords: Graph, Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07103v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07103v1.pdf filename=2404.07103v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> while exhibiting exceptional performance, suffer from hallucinations, especially on knowledge-intensive tasks. Existing works propose to augment <b>LLMs</b> with individual text units retrieved from external knowledge corpora to alleviate the issue. However, in many domains, texts are interconnected (e.g., academic papers in a bibliographic <b>graph</b> are linked by citations and co-authorships) which form a (text-attributed) <b>graph.</b> The knowledge in such <b>graphs</b> is encoded not only in single texts/nodes but also in their associated connections. To facilitate the research of augmenting <b>LLMs</b> with <b>graphs,</b> we manually construct a <b>Graph</b> <b>Reasoning</b> <b>Benchmark</b> dataset called GRBench, containing 1,740 questions that can be answered with the knowledge from 10 domain <b>graphs.</b> Then, we propose a simple and effective framework called <b>Graph</b> Chain-of-thought <b>(Graph-CoT)</b> to augment <b>LLMs</b> with <b>graphs</b> by encouraging <b>LLMs</b> to reason on the <b>graph</b> iteratively. Each <b>Graph-CoT</b> iteration consists of three sub-steps: <b>LLM</b> <b>reasoning,</b> <b>LLM-graph</b> interaction, and <b>graph</b> execution. We conduct systematic experiments with three <b>LLM</b> backbones on GRBench, where <b>Graph-CoT</b> outperforms the baselines consistently. The code is available at <a href=https://github.com/PeterGriffinJin/Graph-CoT>https://github.com/PeterGriffinJin/Graph-CoT</a>.</p></p class="citation"></blockquote><h3 id=2337--23218-towards-robustness-of-text-to-visualization-translation-against-lexical-and-phrasal-variability-jinwei-lu-et-al-2024>(23/37 | 23/218) Towards Robustness of Text-to-Visualization Translation against Lexical and Phrasal Variability (Jinwei Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinwei Lu, Yuanfeng Song, Haodi Zhang, Chen Zhang, Raymond Chi-Wing Wong. (2024)<br><strong>Towards Robustness of Text-to-Visualization Translation against Lexical and Phrasal Variability</strong><br><button class=copy-to-clipboard title="Towards Robustness of Text-to-Visualization Translation against Lexical and Phrasal Variability" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07135v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07135v2.pdf filename=2404.07135v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-to-Vis is an emerging task in the natural language processing (NLP) area that aims to automatically generate data visualizations from natural language questions (NLQs). Despite their progress, existing text-to-vis models often heavily rely on lexical matching between words in the questions and tokens in data schemas. This overreliance on lexical matching may lead to a diminished level of model robustness against input variations. In this study, we thoroughly examine the robustness of current text-to-vis models, an area that has not previously been explored. In particular, we construct the first robustness dataset nvBench-Rob, which contains diverse lexical and phrasal variations based on the original text-to-vis <b>benchmark</b> nvBench. Then, we found that the performance of existing text-to-vis models on this new dataset dramatically drops, implying that these methods exhibit inadequate robustness overall. Finally, we propose a novel framework based on <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> technique, named GRED, specifically designed to address input perturbations in these two variants. The framework consists of three parts: NLQ-Retrieval Generator, Visualization Query-Retrieval Retuner and Annotation-based Debugger, which are used to tackle the challenges posed by natural language variants, programming style differences and data schema variants, respectively. Extensive experimental evaluations show that, compared to the state-of-the-art model RGVisNet in the Text-to-Vis field, GRED performs better in terms of model robustness, with a 32% increase in accuracy on the proposed nvBench-Rob dataset.</p></p class="citation"></blockquote><h3 id=2437--24218-a-mathematical-theory-for-learning-semantic-languages-by-abstract-learners-kuo-yu-liao-et-al-2024>(24/37 | 24/218) A Mathematical Theory for Learning Semantic Languages by Abstract Learners (Kuo-Yu Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kuo-Yu Liao, Cheng-Shang Chang, Y. -W. Peter Hong. (2024)<br><strong>A Mathematical Theory for Learning Semantic Languages by Abstract Learners</strong><br><button class=copy-to-clipboard title="A Mathematical Theory for Learning Semantic Languages by Abstract Learners" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IT, cs-LG, cs.CL, math-IT<br>Keyword Score: 33<br>Keywords: Graph, Large Language Model, Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07009v1.pdf filename=2404.07009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated the emergence of capabilities (learned skills) when the number of system parameters and the size of training data surpass certain thresholds. The exact mechanisms behind such phenomena are not fully understood and remain a topic of active research. Inspired by the skill-text bipartite <b>graph</b> model presented in [1] for modeling semantic language, we develop a mathematical theory to explain the emergence of learned skills, taking the learning (or training) process into account. Our approach models the learning process for skills in the skill-text bipartite <b>graph</b> as an iterative decoding process in Low-Density Parity Check (LDPC) codes and Irregular Repetition Slotted ALOHA (IRSA). Using density evolution analysis, we demonstrate the emergence of learned skills when the ratio of the size of training texts to the number of skills exceeds a certain threshold. Our analysis also yields a <b>scaling</b> <b>law</b> for testing errors relative to the size of training texts. Upon completion of the training, we propose a method for semantic compression and discuss its application in semantic communication.</p></p class="citation"></blockquote><h3 id=2537--25218-emotion-cause-pair-extraction-method-based-on-multi-granularity-information-and-multi-module-interaction-mingrui-fu-et-al-2024>(25/37 | 25/218) Emotion-cause pair extraction method based on multi-granularity information and multi-module interaction (Mingrui Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingrui Fu, Weijiang Li. (2024)<br><strong>Emotion-cause pair extraction method based on multi-granularity information and multi-module interaction</strong><br><button class=copy-to-clipboard title="Emotion-cause pair extraction method based on multi-granularity information and multi-module interaction" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 31<br>Keywords: Graph, Benchmarking, Knowledge Graph, BERT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06812v1.pdf filename=2404.06812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The purpose of emotion-cause pair extraction is to extract the pair of emotion clauses and cause clauses. On the one hand, the existing methods do not take fully into account the relationship between the emotion extraction of two auxiliary tasks. On the other hand, the existing two-stage model has the problem of error propagation. In addition, existing models do not adequately address the emotion and cause-induced locational imbalance of samples. To solve these problems, an end-to-end multitasking model (MM-ECPE) based on shared interaction between GRU, <b>knowledge</b> <b>graph</b> and <b>transformer</b> modules is proposed. Furthermore, based on MM-ECPE, in order to use the encoder layer to better solve the problem of imbalanced distribution of clause distances between clauses and emotion clauses, we propose a novel encoding based on <b>BERT,</b> sentiment lexicon, and position-aware interaction module layer of emotion motif pair retrieval model (MM-ECPE(BERT)). The model first fully models the interaction between different tasks through the multi-level sharing module, and mines the shared information between emotion-cause pair extraction and the emotion extraction and cause extraction. Second, to solve the imbalanced distribution of emotion clauses and cause clauses problem, suitable labels are screened out according to the <b>knowledge</b> <b>graph</b> path length and task-specific features are constructed so that the model can focus on extracting pairs with corresponding emotion-cause relationships. Experimental results on the ECPE <b>benchmark</b> dataset show that the proposed model achieves good performance, especially on position-imbalanced samples.</p></p class="citation"></blockquote><h3 id=2637--26218-goex-perspectives-and-designs-towards-a-runtime-for-autonomous-llm-applications-shishir-g-patil-et-al-2024>(26/37 | 26/218) GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications (Shishir G. Patil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shishir G. Patil, Tianjun Zhang, Vivian Fang, Noppapon C., Roy Huang, Aaron Hao, Martin Casado, Joseph E. Gonzalez, Raluca Ada Popa, Ion Stoica. (2024)<br><strong>GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications</strong><br><button class=copy-to-clipboard title="GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Dialogue System, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06921v1.pdf filename=2404.06921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are evolving beyond their classical role of providing information within <b>dialogue</b> <b>systems</b> to actively engaging with tools and performing actions on real-world applications and services. Today, humans verify the correctness and appropriateness of the <b>LLM-generated</b> outputs (e.g., code, functions, or actions) before putting them into real-world execution. This poses significant challenges as code comprehension is well known to be notoriously difficult. In this paper, we study how humans can efficiently collaborate with, delegate to, and supervise autonomous <b>LLMs</b> in the future. We argue that in many cases, &ldquo;post-facto validation&rdquo; - verifying the correctness of a proposed action after seeing the output - is much easier than the aforementioned &ldquo;pre-facto validation&rdquo; setting. The core concept behind enabling a post-facto validation system is the integration of an intuitive undo feature, and establishing a damage confinement for the <b>LLM-generated</b> actions as effective strategies to mitigate the associated risks. Using this, a human can now either revert the effect of an <b>LLM-generated</b> output or be confident that the potential risk is bounded. We believe this is critical to unlock the potential for <b>LLM</b> agents to interact with applications and services with limited (post-facto) human involvement. We describe the design and implementation of our open-source runtime for executing <b>LLM</b> actions, Gorilla Execution Engine (GoEX), and present open research questions towards realizing the goal of <b>LLMs</b> and applications interacting with each other with minimal human supervision. We release GoEX at <a href=https://github.com/ShishirPatil/gorilla/>https://github.com/ShishirPatil/gorilla/</a>.</p></p class="citation"></blockquote><h3 id=2737--27218-cqil-inference-latency-optimization-with-concurrent-computation-of-quasi-independent-layers-longwei-zou-et-al-2024>(27/37 | 27/218) CQIL: Inference Latency Optimization with Concurrent Computation of Quasi-Independent Layers (Longwei Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Longwei Zou, Qingyang Wang, Han Zhao, Jiangang Kong, Yi Yang, Yangdong Deng. (2024)<br><strong>CQIL: Inference Latency Optimization with Concurrent Computation of Quasi-Independent Layers</strong><br><button class=copy-to-clipboard title="CQIL: Inference Latency Optimization with Concurrent Computation of Quasi-Independent Layers" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Quantization, LLaMA, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06709v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06709v1.pdf filename=2404.06709v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The fast-growing <b>large</b> <b>scale</b> <b>language</b> models are delivering unprecedented performance on almost all natural language processing tasks. However, the effectiveness of <b>large</b> <b>language</b> <b>models</b> are reliant on an exponentially increasing number of parameters. The overwhelming computation complexity incurs a high inference latency that negatively affects user experience. Existing methods to improve inference efficiency, such as tensor parallelism and <b>quantization,</b> target to reduce per-layer computing latency, yet overlook the cumulative latency due to the number of layers. Recent works on reducing the cumulative latency through layer removing, however, lead to significant performance drop. Motivated by the similarity of inputs among adjacent layers, we propose to identify quasi-independent layers, which can be concurrently computed to significantly decrease inference latency. We also introduce a bypassing technique to mitigate the effect of information loss. Empirical experiments of the proposed approach on the <b>LLaMA</b> models confirm that Concurrent Computation of Quasi-Independent Layers (CQIL) can reduce latency by up to 48.3% on the <b>LLaMA-33B</b> model, while maintaining a close level of performance.</p></p class="citation"></blockquote><h3 id=2837--28218-culturalteaming-ai-assisted-interactive-red-teaming-for-challenging-llms-lack-of-multicultural-knowledge-yu-ying-chiu-et-al-2024>(28/37 | 28/218) CulturalTeaming: AI-Assisted Interactive Red-Teaming for Challenging LLMs&rsquo; (Lack of) Multicultural Knowledge (Yu Ying Chiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Ying Chiu, Liwei Jiang, Maria Antoniak, Chan Young Park, Shuyue Stella Li, Mehar Bhatia, Sahithya Ravi, Yulia Tsvetkov, Vered Shwartz, Yejin Choi. (2024)<br><strong>CulturalTeaming: AI-Assisted Interactive Red-Teaming for Challenging LLMs&rsquo; (Lack of) Multicultural Knowledge</strong><br><button class=copy-to-clipboard title="CulturalTeaming: AI-Assisted Interactive Red-Teaming for Challenging LLMs' (Lack of) Multicultural Knowledge" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06664v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06664v1.pdf filename=2404.06664v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Frontier <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are developed by researchers and practitioners with skewed cultural backgrounds and on datasets with skewed sources. However, <b>LLMs&rsquo;</b> (lack of) multicultural knowledge cannot be effectively assessed with current methods for developing <b>benchmarks.</b> Existing multicultural evaluations primarily rely on expensive and restricted human annotations or potentially outdated internet resources. Thus, they struggle to capture the intricacy, dynamics, and diversity of cultural norms. <b>LLM-generated</b> <b>benchmarks</b> are promising, yet risk propagating the same biases they are meant to measure. To synergize the creativity and expert cultural knowledge of human annotators and the scalability and standardizability of <b>LLM-based</b> automation, we introduce CulturalTeaming, an interactive red-teaming system that leverages human-AI collaboration to build truly challenging evaluation dataset for assessing the multicultural knowledge of <b>LLMs,</b> while improving annotators&rsquo; capabilities and experiences. Our study reveals that CulturalTeaming&rsquo;s various modes of AI assistance support annotators in creating cultural questions, that modern <b>LLMs</b> fail at, in a gamified manner. Importantly, the increased level of AI assistance (e.g., <b>LLM-generated</b> revision hints) empowers users to create more difficult questions with enhanced perceived creativity of themselves, shedding light on the promises of involving heavier AI assistance in modern evaluation dataset creation procedures. Through a series of 1-hour workshop sessions, we gather CULTURALBENCH-V0.1, a compact yet high-quality evaluation dataset with users&rsquo; red-teaming attempts, that different families of modern <b>LLMs</b> perform with accuracy ranging from 37.7% to 72.2%, revealing a notable gap in <b>LLMs&rsquo;</b> multicultural proficiency.</p></p class="citation"></blockquote><h3 id=2937--29218-improving-language-model-reasoning-with-self-motivated-learning-yunlong-feng-et-al-2024>(29/37 | 29/218) Improving Language Model Reasoning with Self-motivated Learning (Yunlong Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunlong Feng, Yang Xu, Libo Qin, Yasheng Wang, Wanxiang Che. (2024)<br><strong>Improving Language Model Reasoning with Self-motivated Learning</strong><br><button class=copy-to-clipboard title="Improving Language Model Reasoning with Self-motivated Learning" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07017v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07017v1.pdf filename=2404.07017v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale high-quality training data is important for improving the performance of models. After trained with data that has rationales <b>(reasoning</b> steps), models gain <b>reasoning</b> capability. However, the dataset with high-quality rationales is relatively scarce due to the high annotation cost. To address this issue, we propose \textit{Self-motivated Learning} framework. The framework motivates the model itself to automatically generate rationales on existing datasets. Based on the inherent rank from correctness across multiple rationales, the model learns to generate better rationales, leading to higher <b>reasoning</b> capability. Specifically, we train a reward model with the rank to evaluate the quality of rationales, and improve the performance of <b>reasoning</b> through <b>reinforcement</b> <b>learning.</b> Experiment results of Llama2 7B on multiple <b>reasoning</b> datasets show that our method significantly improves the <b>reasoning</b> ability of models, even outperforming text-davinci-002 in some datasets.</p></p class="citation"></blockquote><h3 id=3037--30218-event-grounded-criminal-court-view-generation-withcooperative-large-language-models-linan-yue-et-al-2024>(30/37 | 30/218) Event Grounded Criminal Court View Generation withCooperative (Large) Language Models (Linan Yue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linan Yue, Qi Liu, Lili Zhao, Li Wang, Weibo Gao, Yanqing An. (2024)<br><strong>Event Grounded Criminal Court View Generation withCooperative (Large) Language Models</strong><br><button class=copy-to-clipboard title="Event Grounded Criminal Court View Generation withCooperative (Large) Language Models" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07001v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07001v1.pdf filename=2404.07001v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the development of legal intelligence, Criminal Court View Generation has attracted much attention as a crucial task of legal intelligence, which aims to generate concise and coherent texts that <b>summarize</b> case facts and provide explanations for verdicts. Existing researches explore the key information in case facts to yield the court views. Most of them employ a coarse-grained approach that partitions the facts into broad segments (e.g., verdict-related sentences) to make predictions. However, this approach fails to capture the complex details present in the case facts, such as various criminal elements and legal events. To this end, in this paper, we propose an Event Grounded Generation (EGG) method for criminal court view generation with cooperative (Large) Language Models, which introduces the fine-grained event information into the generation. Specifically, we first design a <b>LLMs-based</b> extraction method that can extract events in case facts without massive annotated events. Then, we incorporate the extracted events into court view generation by merging case facts and events. Besides, considering the computational burden posed by the use of <b>LLMs</b> in the extraction phase of EGG, we propose a <b>LLMs-free</b> EGG method that can eliminate the requirement for event extraction using <b>LLMs</b> in the inference phase. Extensive experimental results on a real-world dataset clearly validate the effectiveness of our proposed method.</p></p class="citation"></blockquote><h3 id=3137--31218-does-mapo-tofu-contain-coffee-probing-llms-for-food-related-cultural-knowledge-li-zhou-et-al-2024>(31/37 | 31/218) Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural Knowledge (Li Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Zhou, Taelin Karidi, Nicolas Garneau, Yong Cao, Wanlong Liu, Wenyu Chen, Daniel Hershcovich. (2024)<br><strong>Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural Knowledge</strong><br><button class=copy-to-clipboard title="Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural Knowledge" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06833v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06833v1.pdf filename=2404.06833v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have highlighted the presence of cultural biases in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> yet often lack a robust methodology to dissect these phenomena comprehensively. Our work aims to bridge this gap by delving into the Food domain, a universally relevant yet culturally diverse aspect of human life. We introduce FmLAMA, a multilingual dataset centered on food-related cultural facts and variations in food practices. We analyze <b>LLMs</b> across various architectures and configurations, evaluating their performance in both monolingual and multilingual settings. By leveraging templates in six different languages, we investigate how <b>LLMs</b> interact with language-specific and cultural knowledge. Our findings reveal that (1) <b>LLMs</b> demonstrate a pronounced bias towards food knowledge prevalent in the United States; (2) Incorporating relevant cultural context significantly improves <b>LLMs&rsquo;</b> ability to access cultural knowledge; (3) The efficacy of <b>LLMs</b> in capturing cultural nuances is highly dependent on the interplay between the probing language, the specific model architecture, and the cultural context in question. This research underscores the complexity of integrating cultural understanding into <b>LLMs</b> and emphasizes the importance of culturally diverse datasets to mitigate biases and enhance model performance across different cultural domains.</p></p class="citation"></blockquote><h3 id=3237--32218-exploring-concept-depth-how-large-language-models-acquire-knowledge-at-different-layers-mingyu-jin-et-al-2024>(32/37 | 32/218) Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers? (Mingyu Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, Fan Yang, Mengnan Du, Yongfeng Zhang. (2024)<br><strong>Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?</strong><br><button class=copy-to-clipboard title="Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07066v1.pdf filename=2404.07066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies the phenomenon that different concepts are learned in different layers of <b>large</b> <b>language</b> <b>models,</b> i.e. more difficult concepts are fully acquired with deeper layers. We define the difficulty of concepts by the level of abstraction, and here it is crudely categorized by factual, emotional, and inferential. Each category contains a spectrum of tasks, arranged from simple to complex. For example, within the factual dimension, tasks range from lie detection to categorizing mathematical problems. We employ a probing technique to extract representations from different layers of the model and apply these to classification tasks. Our findings reveal that models tend to efficiently classify simpler tasks, indicating that these concepts are learned in shallower layers. Conversely, more complex tasks may only be discernible at deeper layers, if at all. This paper explores the implications of these findings for our understanding of model learning processes and internal representations. Our implementation is available at \url{https://github.com/Luckfort/CD}.</p></p class="citation"></blockquote><h3 id=3337--33218-meta4xnli-a-crosslingual-parallel-corpus-for-metaphor-detection-and-interpretation-elisa-sanchez-bayona-et-al-2024>(33/37 | 33/218) Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation (Elisa Sanchez-Bayona et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elisa Sanchez-Bayona, Rodrigo Agerri. (2024)<br><strong>Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation</strong><br><button class=copy-to-clipboard title="Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Metaphor Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07053v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07053v1.pdf filename=2404.07053v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Metaphors,</b> <b>although</b> occasionally unperceived, are ubiquitous in our everyday language. Thus, it is crucial for Language Models to be able to grasp the underlying meaning of this kind of figurative language. In this work, we present Meta4XNLI, a novel parallel dataset for the tasks of <b>metaphor</b> <b>detection</b> and interpretation that contains <b>metaphor</b> <b>annotations</b> in both Spanish and English. We investigate language models&rsquo; <b>metaphor</b> <b>identification</b> and understanding abilities through a series of monolingual and cross-lingual experiments by leveraging our proposed corpus. In order to comprehend how these non-literal expressions affect models&rsquo; performance, we look over the results and perform an error analysis. Additionally, parallel data offers many potential opportunities to investigate <b>metaphor</b> <b>transferability</b> between these languages and the impact of translation on the development of multilingual annotated resources.</p></p class="citation"></blockquote><h3 id=3437--34218-a-computational-analysis-of-the-dehumanisation-of-migrants-from-syria-and-ukraine-in-slovene-news-media-jaya-caporusso-et-al-2024>(34/37 | 34/218) A Computational Analysis of the Dehumanisation of Migrants from Syria and Ukraine in Slovene News Media (Jaya Caporusso et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaya Caporusso, Damar Hoogland, Mojca Brglez, Boshko Koloski, Matthew Purver, Senja Pollak. (2024)<br><strong>A Computational Analysis of the Dehumanisation of Migrants from Syria and Ukraine in Slovene News Media</strong><br><button class=copy-to-clipboard title="A Computational Analysis of the Dehumanisation of Migrants from Syria and Ukraine in Slovene News Media" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07036v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07036v1.pdf filename=2404.07036v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dehumanisation involves the perception and or treatment of a social group&rsquo;s members as less than human. This phenomenon is rarely addressed with computational linguistic techniques. We adapt a recently proposed approach for English, making it easier to transfer to other languages and to evaluate, introducing a new sentiment resource, the use of <b>zero-shot</b> cross-lingual valence and arousal detection, and a new method for statistical significance testing. We then apply it to study attitudes to migration expressed in Slovene newspapers, to examine changes in the Slovene discourse on migration between the 2015-16 migration crisis following the war in Syria and the 2022-23 period following the war in Ukraine. We find that while this discourse became more negative and more intense over time, it is less dehumanising when specifically addressing Ukrainian migrants compared to others.</p></p class="citation"></blockquote><h3 id=3537--35218-lm-transparency-tool-interactive-tool-for-analyzing-transformer-language-models-igor-tufanov-et-al-2024>(35/37 | 35/218) LM Transparency Tool: Interactive Tool for Analyzing Transformer Language Models (Igor Tufanov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Igor Tufanov, Karen Hambardzumyan, Javier Ferrando, Elena Voita. (2024)<br><strong>LM Transparency Tool: Interactive Tool for Analyzing Transformer Language Models</strong><br><button class=copy-to-clipboard title="LM Transparency Tool: Interactive Tool for Analyzing Transformer Language Models" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07004v1.pdf filename=2404.07004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the LM Transparency Tool (LM-TT), an open-source interactive toolkit for analyzing the internal workings of <b>Transformer-based</b> language models. Differently from previously existing tools that focus on isolated parts of the decision-making process, our framework is designed to make the entire prediction process transparent, and allows tracing back model behavior from the top-layer representation to very fine-grained parts of the model. Specifically, it (1) shows the important part of the whole input-to-output information flow, (2) allows attributing any changes done by a model block to individual attention heads and feed-forward neurons, (3) allows interpreting the functions of those heads or neurons. A crucial part of this pipeline is showing the importance of specific model components at each step. As a result, we are able to look at the roles of model components only in cases where they are important for a prediction. Since knowing which components should be inspected is key for analyzing large models where the number of these components is extremely high, we believe our tool will greatly support the interpretability community both in research settings and in practical applications.</p></p class="citation"></blockquote><h3 id=3637--36218-charles-translator-a-machine-translation-system-between-ukrainian-and-czech-martin-popel-et-al-2024>(36/37 | 36/218) Charles Translator: A Machine Translation System between Ukrainian and Czech (Martin Popel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Popel, Lucie Poláková, Michal Novák, Jindřich Helcl, Jindřich Libovický, Pavel Straňák, Tomáš Krabač, Jaroslava Hlaváčová, Mariia Anisimova, Tereza Chlaňová. (2024)<br><strong>Charles Translator: A Machine Translation System between Ukrainian and Czech</strong><br><button class=copy-to-clipboard title="Charles Translator: A Machine Translation System between Ukrainian and Czech" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06964v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06964v1.pdf filename=2404.06964v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Charles Translator, a <b>machine</b> <b>translation</b> system between Ukrainian and Czech, developed as part of a society-wide effort to mitigate the impact of the Russian-Ukrainian war on individuals and society. The system was developed in the spring of 2022 with the help of many language data providers in order to quickly meet the demand for such a service, which was not available at the time in the required quality. The translator was later implemented as an online web interface and as an Android app with speech input, both featuring Cyrillic-Latin script transliteration. The system translates directly, compared to other available systems that use English as a pivot, and thus take advantage of the typological similarity of the two languages. It uses the block back-translation method, which allows for efficient use of monolingual training data. The paper describes the development process, including data collection and implementation, evaluation, mentions several use cases, and outlines possibilities for the further development of the system for educational purposes.</p></p class="citation"></blockquote><h3 id=3737--37218-diffusiondialog-a-diffusion-model-for-diverse-dialog-generation-with-latent-space-jianxiang-xiang-et-al-2024>(37/37 | 37/218) DiffusionDialog: A Diffusion Model for Diverse Dialog Generation with Latent Space (Jianxiang Xiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianxiang Xiang, Zhenhua Liu, Haodong Liu, Yin Bai, Jia Cheng, Wenliang Chen. (2024)<br><strong>DiffusionDialog: A Diffusion Model for Diverse Dialog Generation with Latent Space</strong><br><button class=copy-to-clipboard title="DiffusionDialog: A Diffusion Model for Diverse Dialog Generation with Latent Space" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06760v1.pdf filename=2404.06760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In real-life conversations, the content is diverse, and there exists the one-to-many problem that requires diverse generation. Previous studies attempted to introduce discrete or Gaussian-based continuous latent variables to address the one-to-many problem, but the diversity is limited. Recently, <b>diffusion</b> <b>models</b> have made breakthroughs in computer vision, and some attempts have been made in natural language processing. In this paper, we propose DiffusionDialog, a novel approach to enhance the diversity of dialogue generation with the help of <b>diffusion</b> <b>model.</b> In our approach, we introduce continuous latent variables into the <b>diffusion</b> <b>model.</b> The problem of using latent variables in the dialog task is how to build both an effective prior of the latent space and an inferring process to obtain the proper latent given the context. By combining the encoder and latent-based <b>diffusion</b> <b>model,</b> we encode the response&rsquo;s latent representation in a continuous space as the prior, instead of fixed Gaussian distribution or simply discrete ones. We then infer the latent by denoising step by step with the <b>diffusion</b> <b>model.</b> The experimental results show that our model greatly enhances the diversity of dialog responses while maintaining coherence. Furthermore, in further analysis, we find that our <b>diffusion</b> <b>model</b> achieves high inference efficiency, which is the main challenge of applying <b>diffusion</b> <b>models</b> in natural language processing.</p></p class="citation"></blockquote><h2 id=cscv-53>cs.CV (53)</h2><h3 id=153--38218-adapting-llama-decoder-to-vision-transformer-jiahao-wang-et-al-2024>(1/53 | 38/218) Adapting LLaMA Decoder to Vision Transformer (Jiahao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahao Wang, Wenqi Shao, Mengzhao Chen, Chengyue Wu, Yong Liu, Kaipeng Zhang, Songyang Zhang, Kai Chen, Ping Luo. (2024)<br><strong>Adapting LLaMA Decoder to Vision Transformer</strong><br><button class=copy-to-clipboard title="Adapting LLaMA Decoder to Vision Transformer" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 110<br>Keywords: Vision Transformer, Quantization, Supervised Learning, Supervised Learning, Transfer Learning, LLaMA, Transformer, Large Language Model, Large Language Model, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06773v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06773v1.pdf filename=2404.06773v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work examines whether decoder-only <b>Transformers</b> such as <b>LLaMA,</b> which were originally designed for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> can be adapted to the computer <b>vision</b> <b>field.</b> We first &ldquo;LLaMAfy&rdquo; a standard ViT step-by-step to align with <b>LLaMA&rsquo;s</b> architecture, and find that directly applying a casual mask to the <b>self-attention</b> brings an attention collapse issue, resulting in the failure to the network training. We suggest to reposition the class token behind the image tokens with a post-sequence class token technique to overcome this challenge, enabling causal <b>self-attention</b> to efficiently capture the entire image&rsquo;s information. Additionally, we develop a soft mask strategy that gradually introduces a casual mask to the <b>self-attention</b> at the onset of training to facilitate the optimization behavior. The tailored model, dubbed as image <b>LLaMA</b> (iLLaMA), is akin to <b>LLaMA</b> in architecture and enables direct <b>supervised</b> <b>learning.</b> Its causal <b>self-attention</b> boosts computational efficiency and learns complex representation by elevating attention map ranks. iLLaMA rivals the performance with its encoder-only counterparts, achieving 75.1% ImageNet top-1 accuracy with only 5.7M parameters. Scaling the model to ~310M and pre-training on ImageNet-21K further enhances the accuracy to 86.0%. Extensive experiments demonstrate iLLaMA&rsquo;s reliable properties: calibration, shape-texture bias, <b>quantization</b> compatibility, ADE20K segmentation and CIFAR <b>transfer</b> <b>learning.</b> We hope our study can kindle fresh views to visual model design in the wave of <b>LLMs.</b> Pre-trained models and codes are available here.</p></p class="citation"></blockquote><h3 id=253--39218-vllms-provide-better-context-for-emotion-understanding-through-common-sense-reasoning-alexandros-xenos-et-al-2024>(2/53 | 39/218) VLLMs Provide Better Context for Emotion Understanding Through Common Sense Reasoning (Alexandros Xenos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandros Xenos, Niki Maria Foteinopoulou, Ioanna Ntinou, Ioannis Patras, Georgios Tzimiropoulos. (2024)<br><strong>VLLMs Provide Better Context for Emotion Understanding Through Common Sense Reasoning</strong><br><button class=copy-to-clipboard title="VLLMs Provide Better Context for Emotion Understanding Through Common Sense Reasoning" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-HC, cs.CV<br>Keyword Score: 50<br>Keywords: Transformer, Common-sense Reasoning, Reasoning, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07078v1.pdf filename=2404.07078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recognising emotions in context involves identifying the apparent emotions of an individual, taking into account contextual cues from the surrounding scene. Previous approaches to this task have involved the design of explicit scene-encoding architectures or the incorporation of external scene-related information, such as captions. However, these methods often utilise limited contextual information or rely on intricate training pipelines. In this work, we leverage the groundbreaking capabilities of Vision-and-Large-Language Models (VLLMs) to enhance <b>in-context</b> emotion classification without introducing complexity to the training process in a two-stage approach. In the first stage, we propose <b>prompting</b> VLLMs to generate descriptions in natural language of the subject&rsquo;s apparent emotion relative to the visual context. In the second stage, the descriptions are used as contextual information and, along with the image input, are used to train a <b>transformer-based</b> architecture that fuses text and visual features before the final classification task. Our experimental results show that the text and image features have complementary information, and our fused architecture significantly outperforms the individual modalities without any complex training methods. We evaluate our approach on three different datasets, namely, EMOTIC, CAER-S, and BoLD, and achieve state-of-the-art or comparable accuracy across all datasets and metrics compared to much more complex approaches. The code will be made publicly available on github: <a href=https://github.com/NickyFot/EmoCommonSense.git>https://github.com/NickyFot/EmoCommonSense.git</a></p></p class="citation"></blockquote><h3 id=353--40218-implicit-multi-spectral-transformer-an-lightweight-and-effective-visible-to-infrared-image-translation-model-yijia-chen-et-al-2024>(3/53 | 40/218) Implicit Multi-Spectral Transformer: An Lightweight and Effective Visible to Infrared Image Translation Model (Yijia Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijia Chen, Pinghua Chen, Xiangxin Zhou, Yingtie Lei, Ziyang Zhou, Mingxian Li. (2024)<br><strong>Implicit Multi-Spectral Transformer: An Lightweight and Effective Visible to Infrared Image Translation Model</strong><br><button class=copy-to-clipboard title="Implicit Multi-Spectral Transformer: An Lightweight and Effective Visible to Infrared Image Translation Model" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Benchmarking, Benchmarking, Generative Adversarial Network, Generative Adversarial Network, Transformer, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07072v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07072v1.pdf filename=2404.07072v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of computer vision, visible light <b>images</b> <b>often</b> exhibit low contrast in low-light conditions, presenting a significant challenge. While infrared imagery provides a potential solution, its utilization entails high costs and practical limitations. Recent advancements in deep learning, particularly the deployment of <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs),</b> have facilitated the transformation of visible light <b>images</b> <b>to</b> infrared <b>images.</b> <b>However,</b> these methods often experience unstable training phases and may produce suboptimal outputs. To address these issues, we propose a novel end-to-end <b>Transformer-based</b> model that efficiently converts visible light <b>images</b> <b>into</b> high-fidelity infrared <b>images.</b> <b>Initially,</b> the Texture Mapping Module and Color Perception Adapter collaborate to extract texture and color features from the visible light <b>image.</b> <b>The</b> Dynamic Fusion Aggregation Module subsequently integrates these features. Finally, the transformation into an infrared <b>image</b> <b>is</b> refined through the synergistic action of the Color Perception Adapter and the Enhanced Perception Attention mechanism. Comprehensive <b>benchmarking</b> experiments confirm that our model outperforms existing methods, producing infrared <b>images</b> <b>of</b> markedly superior quality, both qualitatively and quantitatively. Furthermore, the proposed model enables more effective downstream applications for infrared <b>images</b> <b>than</b> other methods.</p></p class="citation"></blockquote><h3 id=453--41218-realmdreamer-text-driven-3d-scene-generation-with-inpainting-and-depth-diffusion-jaidev-shriram-et-al-2024>(4/53 | 41/218) RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion (Jaidev Shriram et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi. (2024)<br><strong>RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion</strong><br><button class=copy-to-clipboard title="RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Fine-tuning, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07199v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07199v1.pdf filename=2404.07199v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce RealmDreamer, a technique for generation of general forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D Gaussian Splatting representation to match complex text <b>prompts.</b> We initialize these splats by utilizing the state-of-the-art <b>text-to-image</b> generators, lifting their samples into 3D, and computing the occlusion volume. We then optimize this representation across multiple views as a 3D inpainting task with image-conditional <b>diffusion</b> <b>models.</b> To learn correct geometric structure, we incorporate a depth <b>diffusion</b> <b>model</b> by conditioning on the samples from the inpainting model, giving rich geometric structure. Finally, we <b>finetune</b> the model using sharpened samples from image generators. Notably, our technique does not require video or multi-view data and can synthesize a variety of high-quality 3D scenes in different styles, consisting of multiple objects. Its generality additionally allows 3D synthesis from a single image.</p></p class="citation"></blockquote><h3 id=553--42218-tuning-free-adaptive-style-incorporation-for-structure-consistent-text-driven-style-transfer-yanqi-ge-et-al-2024>(5/53 | 42/218) Tuning-Free Adaptive Style Incorporation for Structure-Consistent Text-Driven Style Transfer (Yanqi Ge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanqi Ge, Jiaqi Liu, Qingnan Fan, Xi Jiang, Ye Huang, Shuai Qin, Hong Gu, Wen Li, Lixin Duan. (2024)<br><strong>Tuning-Free Adaptive Style Incorporation for Structure-Consistent Text-Driven Style Transfer</strong><br><button class=copy-to-clipboard title="Tuning-Free Adaptive Style Incorporation for Structure-Consistent Text-Driven Style Transfer" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Style Transfer, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06835v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06835v1.pdf filename=2404.06835v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we target the task of text-driven <b>style</b> <b>transfer</b> in the context of <b>text-to-image</b> (T2I) <b>diffusion</b> <b>models.</b> The main challenge is consistent structure preservation while enabling effective <b>style</b> <b>transfer</b> effects. The past approaches in this field directly concatenate the content and <b>style</b> <b>prompts</b> for a <b>prompt-level</b> <b>style</b> <b>injection,</b> leading to unavoidable structure distortions. In this work, we propose a novel solution to the text-driven <b>style</b> <b>transfer</b> task, namely, Adaptive <b>Style</b> <b>Incorporation~(ASI),</b> to achieve fine-grained feature-level <b>style</b> <b>incorporation.</b> It consists of the Siamese Cross-Attention~(SiCA) to decouple the single-track cross-attention to a dual-track structure to obtain separate content and <b>style</b> <b>features,</b> and the Adaptive Content-Style Blending (AdaBlending) module to couple the content and <b>style</b> <b>information</b> from a structure-consistent manner. Experimentally, our method exhibits much better performance in both structure preservation and stylized effects.</p></p class="citation"></blockquote><h3 id=653--43218-ai-guided-defect-detection-techniques-to-model-single-crystal-diamond-growth-rohan-reddy-mekala-et-al-2024>(6/53 | 43/218) AI-Guided Defect Detection Techniques to Model Single Crystal Diamond Growth (Rohan Reddy Mekala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohan Reddy Mekala, Elias Garratt, Matthias Muehle, Arjun Srinivasan, Adam Porter, Mikael Lindvall. (2024)<br><strong>AI-Guided Defect Detection Techniques to Model Single Crystal Diamond Growth</strong><br><button class=copy-to-clipboard title="AI-Guided Defect Detection Techniques to Model Single Crystal Diamond Growth" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: Active Learning, Data Augmentation, Geometry, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07306v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07306v1.pdf filename=2404.07306v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>From a process development perspective, diamond growth via chemical vapor deposition has made significant strides. However, challenges persist in achieving high quality and large-area material production. These difficulties include controlling conditions to maintain uniform growth rates for the entire growth surface. As growth progresses, various factors or defect states emerge, altering the uniform conditions. These changes affect the growth rate and result in the formation of crystalline defects at the microscale. However, there is a distinct lack of methods to identify these defect states and their <b>geometry</b> using images taken during the growth process. This paper details seminal work on defect segmentation pipeline using in-situ optical images to identify features that indicate defective states that are visible at the macroscale. Using a semantic segmentation approach as applied in our previous work, these defect states and corresponding derivative features are isolated and classified by their pixel masks. Using an annotation focused <b>human-in-the-loop</b> software architecture to produce training datasets, with modules for selective <b>data</b> <b>labeling</b> using <b>active</b> <b>learning,</b> <b>data</b> <b>augmentations,</b> and model-assisted labeling, our approach achieves effective annotation accuracy and drastically reduces the time and cost of labeling by orders of magnitude. On the model development front, we found that deep learning-based algorithms are the most efficient. They can accurately learn complex representations from feature-rich datasets. Our best-performing model, based on the YOLOV3 and DeeplabV3plus architectures, achieved excellent accuracy for specific features of interest. Specifically, it reached 93.35% accuracy for center defects, 92.83% for polycrystalline defects, and 91.98% for edge defects.</p></p class="citation"></blockquote><h3 id=753--44218-dreamscene360-unconstrained-text-to-3d-scene-generation-with-panoramic-gaussian-splatting-shijie-zhou-et-al-2024>(7/53 | 44/218) DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting (Shijie Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi. (2024)<br><strong>DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting</strong><br><button class=copy-to-clipboard title="DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: Diffusion Model, Geometry, Virtual Reality (VR), Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06903v1.pdf filename=2404.06903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing demand for <b>virtual</b> <b>reality</b> applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360$^{\circ}$ scene generation pipeline that facilitates the creation of comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D <b>diffusion</b> <b>model</b> and <b>prompt</b> self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary &ldquo;flat&rdquo; (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D <b>geometry,</b> our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360$^{\circ}$ perspective, providing an enhanced immersive experience over existing techniques. Project website at: <a href=http://dreamscene360.github.io/>http://dreamscene360.github.io/</a></p></p class="citation"></blockquote><h3 id=853--45218-object-conditioned-energy-based-attention-map-alignment-in-text-to-image-diffusion-models-yasi-zhang-et-al-2024>(8/53 | 45/218) Object-Conditioned Energy-Based Attention Map Alignment in Text-to-Image Diffusion Models (Yasi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasi Zhang, Peiyu Yu, Ying Nian Wu. (2024)<br><strong>Object-Conditioned Energy-Based Attention Map Alignment in Text-to-Image Diffusion Models</strong><br><button class=copy-to-clipboard title="Object-Conditioned Energy-Based Attention Map Alignment in Text-to-Image Diffusion Models" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Diffusion Model, Benchmarking, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07389v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07389v1.pdf filename=2404.07389v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> <b>diffusion</b> <b>models</b> have shown great success in generating high-quality text-guided images. Yet, these models may still fail to semantically align generated images with the provided text <b>prompts,</b> leading to problems like incorrect attribute binding and/or catastrophic object neglect. Given the pervasive object-oriented structure underlying text <b>prompts,</b> we introduce a novel object-conditioned Energy-Based Attention Map Alignment (EBAMA) method to address the aforementioned problems. We show that an object-centric attribute binding loss naturally emerges by approximately maximizing the log-likelihood of a $z$-parameterized energy-based model with the help of the negative sampling technique. We further propose an object-centric intensity regularizer to prevent excessive shifts of objects attention towards their attributes. Extensive qualitative and quantitative experiments, including human evaluation, on several challenging <b>benchmarks</b> demonstrate the superior performance of our method over previous strong counterparts. With better aligned attention maps, our approach shows great promise in further enhancing the text-controlled image editing ability of <b>diffusion</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=953--46218-scaling-multi-camera-3d-object-detection-through-weak-to-strong-eliciting-hao-lu-et-al-2024>(9/53 | 46/218) Scaling Multi-Camera 3D Object Detection through Weak-to-Strong Eliciting (Hao Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Lu, Jiaqi Tang, Xinli Xu, Xu Cao, Yunpeng Zhang, Guoqing Wang, Dalong Du, Hao Chen, Yingcong Chen. (2024)<br><strong>Scaling Multi-Camera 3D Object Detection through Weak-to-Strong Eliciting</strong><br><button class=copy-to-clipboard title="Scaling Multi-Camera 3D Object Detection through Weak-to-Strong Eliciting" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Object Detection, Benchmarking, Foundation Model, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06700v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06700v1.pdf filename=2404.06700v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emergence of Multi-Camera 3D <b>Object</b> <b>Detection</b> (MC3D-Det), facilitated by bird&rsquo;s-eye view (BEV) representation, signifies a notable progression in 3D <b>object</b> <b>detection.</b> Scaling MC3D-Det training effectively accommodates varied camera parameters and urban landscapes, paving the way for the MC3D-Det <b>foundation</b> <b>model.</b> However, the multi-view fusion stage of the MC3D-Det method relies on the ill-posed monocular perception during training rather than surround refinement ability, leading to what we term &ldquo;surround refinement degradation&rdquo;. To this end, our study presents a weak-to-strong eliciting framework aimed at enhancing surround refinement while maintaining robust monocular perception. Specifically, our framework employs weakly tuned experts trained on distinct subsets, and each is inherently biased toward specific camera configurations and scenarios. These biased experts can learn the perception of monocular degeneration, which can help the multi-view fusion stage to enhance surround refinement abilities. Moreover, a composite <b>distillation</b> strategy is proposed to integrate the universal knowledge of 2D <b>foundation</b> <b>models</b> and task-specific information. Finally, for MC3D-Det joint training, the elaborate dataset merge strategy is designed to solve the problem of inconsistent camera numbers and camera parameters. We set up a multiple dataset joint training <b>benchmark</b> for MC3D-Det and adequately evaluated existing methods. Further, we demonstrate the proposed framework brings a generalized and significant boost over multiple baselines. Our code is at \url{https://github.com/EnVision-Research/Scale-BEV}.</p></p class="citation"></blockquote><h3 id=1053--47218-safegen-mitigating-unsafe-content-generation-in-text-to-image-models-xinfeng-li-et-al-2024>(10/53 | 47/218) SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models (Xinfeng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinfeng Li, Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu. (2024)<br><strong>SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models</strong><br><button class=copy-to-clipboard title="SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CR, cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Text2image, Prompt, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06666v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06666v1.pdf filename=2404.06666v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> <b>(T2I)</b> models, such as Stable Diffusion, have exhibited remarkable performance in generating high-quality images from <b>text</b> <b>descriptions</b> in recent years. However, <b>text-to-image</b> <b>models</b> may be tricked into generating not-safe-for-work (NSFW) content, particularly in sexual scenarios. Existing countermeasures mostly focus on filtering inappropriate inputs and outputs, or suppressing improper <b>text</b> <b>embeddings,</b> which can block explicit NSFW-related content (e.g., naked or sexy) but may still be vulnerable to adversarial <b>prompts</b> inputs that appear innocent but are ill-intended. In this paper, we present SafeGen, a framework to mitigate unsafe content generation by <b>text-to-image</b> <b>models</b> in a <b>text-agnostic</b> <b>manner.</b> The key idea is to eliminate unsafe visual representations from the model regardless of the <b>text</b> <b>input.</b> In this way, the <b>text-to-image</b> <b>model</b> is resistant to adversarial <b>prompts</b> since unsafe visual representations are obstructed from within. Extensive experiments conducted on four datasets demonstrate SafeGen&rsquo;s effectiveness in mitigating unsafe content generation while preserving the high-fidelity of benign images. SafeGen outperforms eight state-of-the-art baseline methods and achieves 99.1% sexual content removal performance. Furthermore, our constructed <b>benchmark</b> of adversarial <b>prompts</b> provides a basis for future development and evaluation of anti-NSFW-generation methods.</p></p class="citation"></blockquote><h3 id=1153--48218-gooddrag-towards-good-practices-for-drag-editing-with-diffusion-models-zewei-zhang-et-al-2024>(11/53 | 48/218) GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models (Zewei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zewei Zhang, Huan Liu, Jun Chen, Xiangyu Xu. (2024)<br><strong>GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models</strong><br><button class=copy-to-clipboard title="GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs-LG, cs-MM, cs.CV<br>Keyword Score: 32<br>Keywords: Diffusion Model, Benchmarking, Benchmarking, Multi-modal, Multi-modal, Gemini<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07206v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07206v1.pdf filename=2404.07206v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce GoodDrag, a novel approach to improve the stability and image quality of drag editing. Unlike existing methods that struggle with accumulated perturbations and often result in distortions, GoodDrag introduces an AlDD framework that alternates between drag and denoising operations within the <b>diffusion</b> <b>process,</b> effectively improving the fidelity of the result. We also propose an information-preserving motion supervision operation that maintains the original features of the starting point for precise manipulation and artifact reduction. In addition, we contribute to the <b>benchmarking</b> of drag editing by introducing a new dataset, Drag100, and developing dedicated quality assessment metrics, Dragging Accuracy Index and <b>Gemini</b> Score, utilizing Large <b>Multimodal</b> Models. Extensive experiments demonstrate that the proposed GoodDrag compares favorably against the state-of-the-art approaches both qualitatively and quantitatively. The project page is <a href=https://gooddrag.github.io>https://gooddrag.github.io</a>.</p></p class="citation"></blockquote><h3 id=1253--49218-solving-masked-jigsaw-puzzles-with-diffusion-vision-transformers-jinyang-liu-et-al-2024>(12/53 | 49/218) Solving Masked Jigsaw Puzzles with Diffusion Vision Transformers (Jinyang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinyang Liu, Wondmgezahu Teshome, Sandesh Ghimire, Mario Sznaier, Octavia Camps. (2024)<br><strong>Solving Masked Jigsaw Puzzles with Diffusion Vision Transformers</strong><br><button class=copy-to-clipboard title="Solving Masked Jigsaw Puzzles with Diffusion Vision Transformers" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07292v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07292v1.pdf filename=2404.07292v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Solving image and video jigsaw puzzles poses the challenging task of rearranging image fragments or video frames from unordered sequences to restore meaningful images and video sequences. Existing approaches often hinge on discriminative models tasked with predicting either the absolute positions of puzzle elements or the permutation actions applied to the original data. Unfortunately, these methods face limitations in effectively solving puzzles with a large number of elements. In this paper, we propose JPDVT, an innovative approach that harnesses diffusion <b>transformers</b> to address this challenge. Specifically, we generate positional information for image patches or video frames, conditioned on their underlying visual content. This information is then employed to accurately assemble the puzzle pieces in their correct positions, even in scenarios involving missing pieces. Our method achieves state-of-the-art performance on several datasets.</p></p class="citation"></blockquote><h3 id=1353--50218-trajpred-trajectory-prediction-with-region-based-relation-learning-chen-zhou-et-al-2024>(13/53 | 50/218) TrajPRed: Trajectory Prediction with Region-based Relation Learning (Chen Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Zhou, Ghassan AlRegib, Armin Parchami, Kunjan Singh. (2024)<br><strong>TrajPRed: Trajectory Prediction with Region-based Relation Learning</strong><br><button class=copy-to-clipboard title="TrajPRed: Trajectory Prediction with Region-based Relation Learning" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Autoencoder, Convolution, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06971v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06971v1.pdf filename=2404.06971v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Forecasting human trajectories in traffic scenes is critical for safety within mixed or fully autonomous systems. Human future trajectories are driven by two major stimuli, social interactions, and stochastic goals. Thus, reliable forecasting needs to capture these two stimuli. Edge-based relation modeling represents social interactions using pairwise correlations from precise individual states. Nevertheless, edge-based relations can be vulnerable under perturbations. To alleviate these issues, we propose a region-based relation learning paradigm that models social interactions via region-wise dynamics of joint states, i.e., the changes in the density of crowds. In particular, region-wise agent joint information is encoded within <b>convolutional</b> feature grids. Social relations are modeled by relating the temporal changes of local joint information from a global perspective. We show that region-based relations are less susceptible to perturbations. In order to account for the stochastic individual goals, we exploit a conditional <b>variational</b> <b>autoencoder</b> to realize multi-goal estimation and diverse future prediction. Specifically, we perform <b>variational</b> <b>inference</b> via the latent distribution, which is conditioned on the correlation between input states and associated target goals. Sampling from the latent distribution enables the framework to reliably capture the stochastic behavior in test data. We integrate multi-goal estimation and region-based relation learning to model the two stimuli, social interactions, and stochastic goals, in a prediction framework. We evaluate our framework on the ETH-UCY dataset and Stanford Drone Dataset (SDD). We show that the diverse prediction better fits the ground truth when incorporating the relation module. Our framework outperforms the state-of-the-art models on SDD by $27.61%$/$18.20%$ of ADE/FDE metrics.</p></p class="citation"></blockquote><h3 id=1453--51218-urban-architect-steerable-3d-urban-scene-generation-with-layout-prior-fan-lu-et-al-2024>(14/53 | 51/218) Urban Architect: Steerable 3D Urban Scene Generation with Layout Prior (Fan Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fan Lu, Kwan-Yee Lin, Yan Xu, Hongsheng Li, Guang Chen, Changjun Jiang. (2024)<br><strong>Urban Architect: Steerable 3D Urban Scene Generation with Layout Prior</strong><br><button class=copy-to-clipboard title="Urban Architect: Steerable 3D Urban Scene Generation with Layout Prior" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Knowledge Distillation, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06780v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06780v1.pdf filename=2404.06780v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-to-3D generation has achieved remarkable success via large-scale <b>text-to-image</b> <b>diffusion</b> <b>models.</b> Nevertheless, there is no paradigm for scaling up the methodology to urban scale. Urban scenes, characterized by numerous elements, intricate arrangement relationships, and vast scale, present a formidable barrier to the interpretability of ambiguous textual descriptions for effective model optimization. In this work, we surmount the limitations by introducing a compositional 3D layout representation into text-to-3D paradigm, serving as an additional prior. It comprises a set of semantic primitives with simple geometric structures and explicit arrangement relationships, complementing textual descriptions and enabling steerable generation. Upon this, we propose two modifications &ndash; (1) We introduce Layout-Guided Variational Score <b>Distillation</b> to address model optimization inadequacies. It conditions the score <b>distillation</b> sampling process with geometric and semantic constraints of 3D layouts. (2) To handle the unbounded nature of urban scenes, we represent 3D scene with a Scalable Hash Grid structure, incrementally adapting to the growing scale of urban scenes. Extensive experiments substantiate the capability of our framework to scale text-to-3D generation to large-scale urban scenes that cover over 1000m driving distance for the first time. We also present various scene editing demonstrations, showing the powers of steerable urban scene generation. Website: <a href=https://urbanarchitect.github.io>https://urbanarchitect.github.io</a>.</p></p class="citation"></blockquote><h3 id=1553--52218-monoselfrecon-purely-self-supervised-explicit-generalizable-3d-reconstruction-of-indoor-scenes-from-monocular-rgb-views-runfa-li-et-al-2024>(15/53 | 52/218) MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D Reconstruction of Indoor Scenes from Monocular RGB Views (Runfa Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runfa Li, Upal Mahbub, Vasudev Bhaskaran, Truong Nguyen. (2024)<br><strong>MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D Reconstruction of Indoor Scenes from Monocular RGB Views</strong><br><button class=copy-to-clipboard title="MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D Reconstruction of Indoor Scenes from Monocular RGB Views" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Autoencoder, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06753v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06753v1.pdf filename=2404.06753v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current monocular 3D scene reconstruction (3DR) works are either fully-supervised, or not generalizable, or implicit in 3D representation. We propose a novel framework - MonoSelfRecon that for the first time achieves explicit 3D mesh reconstruction for generalizable indoor scenes with monocular RGB views by purely self-supervision on voxel-SDF (signed distance function). MonoSelfRecon follows an <b>Autoencoder-based</b> architecture, decodes voxel-SDF and a generalizable Neural Radiance Field (NeRF), which is used to guide voxel-SDF in self-supervision. We propose novel <b>self-supervised</b> losses, which not only support pure self-supervision, but can be used together with <b>supervised</b> signals to further boost <b>supervised</b> training. Our experiments show that &ldquo;MonoSelfRecon&rdquo; trained in pure self-supervision outperforms current best <b>self-supervised</b> indoor depth estimation models and is comparable to 3DR models trained in fully supervision with depth annotations. MonoSelfRecon is not restricted by specific model design, which can be used to any models with voxel-SDF for purely <b>self-supervised</b> manner.</p></p class="citation"></blockquote><h3 id=1653--53218-umbrae-unified-multimodal-decoding-of-brain-signals-weihao-xia-et-al-2024>(16/53 | 53/218) UMBRAE: Unified Multimodal Decoding of Brain Signals (Weihao Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weihao Xia, Raoul de Charette, Cengiz Öztireli, Jing-Hao Xue. (2024)<br><strong>UMBRAE: Unified Multimodal Decoding of Brain Signals</strong><br><button class=copy-to-clipboard title="UMBRAE: Unified Multimodal Decoding of Brain Signals" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 29<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Weakly-supervised Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07202v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07202v1.pdf filename=2404.07202v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address prevailing challenges of the brain-powered research, departing from the observation that the literature hardly recover accurate spatial information and require subject-specific models. To address these challenges, we propose UMBRAE, a unified <b>multimodal</b> decoding of brain signals. First, to extract instance-level conceptual and spatial details from neural signals, we introduce an efficient universal brain encoder for <b>multimodal-brain</b> alignment and recover object descriptions at multiple levels of granularity from subsequent <b>multimodal</b> <b>large</b> <b>language</b> <b>model</b> (MLLM). Second, we introduce a cross-subject training strategy mapping subject-specific features to a common feature space. This allows a model to be trained on multiple subjects without extra resources, even yielding superior results compared to subject-specific models. Further, we demonstrate this supports <b>weakly-supervised</b> adaptation to new subjects, with only a fraction of the total training data. Experiments demonstrate that UMBRAE not only achieves superior results in the newly introduced tasks but also outperforms methods in well established tasks. To assess our method, we construct and share with the community a comprehensive brain understanding <b>benchmark</b> BrainHub. Our code and <b>benchmark</b> are available at <a href=https://weihaox.github.io/UMBRAE>https://weihaox.github.io/UMBRAE</a>.</p></p class="citation"></blockquote><h3 id=1753--54218-hrvda-high-resolution-visual-document-assistant-chaohu-liu-et-al-2024>(17/53 | 54/218) HRVDA: High-Resolution Visual Document Assistant (Chaohu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaohu Liu, Kun Yin, Haoyu Cao, Xinghua Jiang, Xin Li, Yinsong Liu, Deqiang Jiang, Xing Sun, Linli Xu. (2024)<br><strong>HRVDA: High-Resolution Visual Document Assistant</strong><br><button class=copy-to-clipboard title="HRVDA: High-Resolution Visual Document Assistant" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06918v1.pdf filename=2404.06918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Leveraging vast training data, <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) have demonstrated formidable general visual comprehension capabilities and achieved remarkable performance across various tasks. However, their performance in visual document understanding still leaves much room for improvement. This discrepancy is primarily attributed to the fact that visual document understanding is a fine-grained prediction task. In natural scenes, MLLMs typically use low-resolution images, leading to a substantial loss of visual information. Furthermore, general-purpose MLLMs do not excel in handling document-oriented <b>instructions.</b> <b>In</b> this paper, we propose a High-Resolution Visual Document Assistant (HRVDA), which bridges the gap between MLLMs and visual document understanding. This model employs a content filtering mechanism and an <b>instruction</b> <b>filtering</b> module to separately filter out the content-agnostic visual tokens and <b>instruction-agnostic</b> <b>visual</b> tokens, thereby achieving efficient model training and inference for high-resolution images. In addition, we construct a document-oriented visual <b>instruction</b> <b>tuning</b> dataset and apply a multi-stage training strategy to enhance the model&rsquo;s document modeling capabilities. Extensive experiments demonstrate that our model achieves state-of-the-art performance across multiple document understanding datasets, while maintaining training efficiency and inference speed comparable to low-resolution models.</p></p class="citation"></blockquote><h3 id=1853--55218-unsupervised-visible-infrared-reid-via-pseudo-label-correction-and-modality-level-alignment-yexin-liu-et-al-2024>(18/53 | 55/218) Unsupervised Visible-Infrared ReID via Pseudo-label Correction and Modality-level Alignment (Yexin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yexin Liu, Weiming Zhang, Athanasios V. Vasilakos, Lin Wang. (2024)<br><strong>Unsupervised Visible-Infrared ReID via Pseudo-label Correction and Modality-level Alignment</strong><br><button class=copy-to-clipboard title="Unsupervised Visible-Infrared ReID via Pseudo-label Correction and Modality-level Alignment" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Benchmarking, Clustering, Contrastive Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06683v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06683v1.pdf filename=2404.06683v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> visible-infrared person re-identification (UVI-ReID) has recently gained great attention due to its potential for enhancing human detection in diverse environments without labeling. Previous methods utilize intra-modality <b>clustering</b> and cross-modality feature matching to achieve UVI-ReID. However, there exist two challenges: 1) noisy pseudo labels might be generated in the <b>clustering</b> process, and 2) the cross-modality feature alignment via matching the marginal distribution of visible and infrared modalities may misalign the different identities from two modalities. In this paper, we first conduct a theoretic analysis where an interpretable generalization upper bound is introduced. Based on the analysis, we then propose a novel <b>unsupervised</b> cross-modality person re-identification framework (PRAISE). Specifically, to address the first challenge, we propose a pseudo-label correction strategy that utilizes a Beta Mixture Model to predict the probability of mis-clustering based network&rsquo;s memory effect and rectifies the correspondence by adding a perceptual term to <b>contrastive</b> <b>learning.</b> Next, we introduce a modality-level alignment strategy that generates paired visible-infrared latent features and reduces the modality gap by aligning the labeling function of visible and infrared features to learn identity discriminative and modality-invariant features. Experimental results on two <b>benchmark</b> datasets demonstrate that our method achieves state-of-the-art performance than the <b>unsupervised</b> visible-ReID methods.</p></p class="citation"></blockquote><h3 id=1953--56218-deep-generative-data-assimilation-in-multimodal-setting-yongquan-qu-et-al-2024>(19/53 | 56/218) Deep Generative Data Assimilation in Multimodal Setting (Yongquan Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongquan Qu, Juan Nathaniel, Shuolin Li, Pierre Gentine. (2024)<br><strong>Deep Generative Data Assimilation in Multimodal Setting</strong><br><button class=copy-to-clipboard title="Deep Generative Data Assimilation in Multimodal Setting" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06665v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06665v1.pdf filename=2404.06665v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robust integration of physical knowledge and data is key to improve computational <b>simulations,</b> such as Earth system models. Data assimilation is crucial for achieving this goal because it provides a systematic framework to calibrate model outputs with observations, which can include remote sensing imagery and ground station measurements, with uncertainty quantification. Conventional methods, including Kalman filters and variational approaches, inherently rely on simplifying linear and Gaussian assumptions, and can be computationally expensive. Nevertheless, with the rapid adoption of data-driven methods in many areas of computational sciences, we see the potential of emulating traditional data assimilation with deep learning, especially generative models. In particular, the diffusion-based probabilistic framework has large overlaps with data assimilation principles: both allows for conditional generation of samples with a Bayesian inverse framework. These models have shown remarkable success in text-conditioned image generation or image-controlled video synthesis. Likewise, one can frame data assimilation as observation-conditioned state calibration. In this work, we propose SLAMS: Score-based Latent Assimilation in <b>Multimodal</b> Setting. Specifically, we assimilate in-situ weather station data and ex-situ satellite imagery to calibrate the vertical temperature profiles, globally. Through extensive ablation, we demonstrate that SLAMS is robust even in low-resolution, noisy, and sparse data settings. To our knowledge, our work is the first to apply deep generative framework for <b>multimodal</b> data assimilation using real-world datasets; an important step for building robust computational simulators, including the next-generation Earth system models. Our code is available at: <a href=https://github.com/yongquan-qu/SLAMS>https://github.com/yongquan-qu/SLAMS</a></p></p class="citation"></blockquote><h3 id=2053--57218-multi-modal-document-presentation-attack-detection-with-forensics-trace-disentanglement-changsheng-chen-et-al-2024>(20/53 | 57/218) Multi-modal Document Presentation Attack Detection With Forensics Trace Disentanglement (Changsheng Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changsheng Chen, Yongyi Deng, Liangwei Lin, Zitong Yu, Zhimao Lai. (2024)<br><strong>Multi-modal Document Presentation Attack Detection With Forensics Trace Disentanglement</strong><br><button class=copy-to-clipboard title="Multi-modal Document Presentation Attack Detection With Forensics Trace Disentanglement" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Benchmarking, Multi-modal, Self-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06663v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06663v1.pdf filename=2404.06663v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Document Presentation Attack Detection (DPAD) is an important measure in protecting the authenticity of a document image. However, recent DPAD methods demand additional resources, such as manual effort in collecting additional data or knowing the parameters of acquisition devices. This work proposes a DPAD method based on <b>multi-modal</b> disentangled traces (MMDT) without the above drawbacks. We first disentangle the recaptured traces by a <b>self-supervised</b> disentanglement and synthesis network to enhance the generalization capacity in document images with different contents and layouts. Then, unlike the existing DPAD approaches that rely only on data in the RGB domain, we propose to explicitly employ the disentangled recaptured traces as new modalities in the <b>transformer</b> backbone through adaptive <b>multi-modal</b> adapters to fuse RGB/trace features efficiently. Visualization of the disentangled traces confirms the effectiveness of the proposed method in different document contents. Extensive experiments on three <b>benchmark</b> datasets demonstrate the superiority of our MMDT method on representing forensic traces of recapturing distortion.</p></p class="citation"></blockquote><h3 id=2153--58218-unified-language-driven-zero-shot-domain-adaptation-senqiao-yang-et-al-2024>(21/53 | 58/218) Unified Language-driven Zero-shot Domain Adaptation (Senqiao Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Senqiao Yang, Zhuotao Tian, Li Jiang, Jiaya Jia. (2024)<br><strong>Unified Language-driven Zero-shot Domain Adaptation</strong><br><button class=copy-to-clipboard title="Unified Language-driven Zero-shot Domain Adaptation" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Representation Learning, Zero-shot, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07155v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07155v1.pdf filename=2404.07155v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces Unified Language-driven <b>Zero-shot</b> <b>Domain</b> <b>Adaptation</b> (ULDA), a novel task setting that enables a single model to adapt to diverse target <b>domains</b> <b>without</b> explicit <b>domain-ID</b> <b>knowledge.</b> We identify the constraints in the existing language-driven <b>zero-shot</b> <b>domain</b> <b>adaptation</b> task, particularly the requirement for <b>domain</b> <b>IDs</b> and <b>domain-specific</b> <b>models,</b> which may restrict flexibility and scalability. To overcome these issues, we propose a new framework for ULDA, consisting of Hierarchical Context Alignment (HCA), <b>Domain</b> <b>Consistent</b> <b>Representation</b> <b>Learning</b> (DCRL), and Text-Driven Rectifier (TDR). These components work synergistically to align simulated features with target text across multiple visual levels, retain semantic correlations between different regional <b>representations,</b> <b>and</b> rectify biases between simulated and real target visual features, respectively. Our extensive empirical evaluations demonstrate that this framework achieves competitive performance in both settings, surpassing even the model that requires <b>domain-ID,</b> <b>showcasing</b> its superiority and generalization ability. The proposed method is not only effective but also maintains practicality and efficiency, as it does not introduce additional computational costs during inference. Our project page is <a href=https://senqiaoyang.com/project/ULDA>https://senqiaoyang.com/project/ULDA</a> .</p></p class="citation"></blockquote><h3 id=2253--59218-brave-broadening-the-visual-encoding-of-vision-language-models-oğuzhan-fatih-kar-et-al-2024>(22/53 | 59/218) BRAVE: Broadening the visual encoding of vision-language models (Oğuzhan Fatih Kar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oğuzhan Fatih Kar, Alessio Tonioni, Petra Poklukar, Achin Kulshrestha, Amir Zamir, Federico Tombari. (2024)<br><strong>BRAVE: Broadening the visual encoding of vision-language models</strong><br><button class=copy-to-clipboard title="BRAVE: Broadening the visual encoding of vision-language models" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Visual Question Answering, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07204v1.pdf filename=2404.07204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-language</b> models (VLMs) are typically composed of a vision encoder, e.g. CLIP, and a language model (LM) that interprets the encoded features to solve downstream tasks. Despite remarkable progress, VLMs are subject to several shortcomings due to the limited capabilities of vision encoders, e.g. &ldquo;blindness&rdquo; to certain image features, visual hallucination, etc. To address these issues, we study broadening the visual encoding capabilities of VLMs. We first comprehensively <b>benchmark</b> several vision encoders with different inductive biases for solving VLM tasks. We observe that there is no single encoding configuration that consistently achieves top performance across different tasks, and encoders with different biases can perform surprisingly similarly. Motivated by this, we introduce a method, named BRAVE, that consolidates features from multiple frozen encoders into a more versatile representation that can be directly fed as the input to a frozen LM. BRAVE achieves state-of-the-art performance on a broad range of captioning and <b>VQA</b> <b>benchmarks</b> and significantly reduces the aforementioned issues of VLMs, while requiring a smaller number of trainable parameters than existing methods and having a more compressed representation. Our results highlight the potential of incorporating different visual biases for a more broad and contextualized visual understanding of VLMs.</p></p class="citation"></blockquote><h3 id=2353--60218-oracle-large-vision-language-models-for-knowledge-guided-holistic-or-domain-modeling-ege-özsoy-et-al-2024>(23/53 | 60/218) ORacle: Large Vision-Language Models for Knowledge-Guided Holistic OR Domain Modeling (Ege Özsoy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ege Özsoy, Chantal Pellegrini, Matthias Keicher, Nassir Navab. (2024)<br><strong>ORacle: Large Vision-Language Models for Knowledge-Guided Holistic OR Domain Modeling</strong><br><button class=copy-to-clipboard title="ORacle: Large Vision-Language Models for Knowledge-Guided Holistic OR Domain Modeling" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Graph, Data Augmentation, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07031v1.pdf filename=2404.07031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Every day, countless surgeries are performed worldwide, each within the distinct settings of operating rooms (ORs) that vary not only in their setups but also in the personnel, tools, and equipment used. This inherent diversity poses a substantial challenge for achieving a holistic understanding of the OR, as it requires models to generalize beyond their initial training datasets. To reduce this gap, we introduce ORacle, an advanced <b>vision-language</b> model designed for holistic OR domain modeling, which incorporates multi-view and temporal capabilities and can leverage external knowledge during inference, enabling it to adapt to previously unseen surgical scenarios. This capability is further enhanced by our novel <b>data</b> <b>augmentation</b> framework, which significantly diversifies the training dataset, ensuring ORacle&rsquo;s proficiency in applying the provided knowledge effectively. In rigorous testing, in scene <b>graph</b> generation, and downstream tasks on the 4D-OR dataset, ORacle not only demonstrates state-of-the-art performance but does so requiring less <b>data</b> <b>than</b> existing models. Furthermore, its adaptability is displayed through its ability to interpret unseen views, actions, and appearances of tools and equipment. This demonstrates ORacle&rsquo;s potential to significantly enhance the scalability and affordability of OR domain modeling and opens a pathway for future advancements in surgical <b>data</b> <b>science.</b> We will release our code and <b>data</b> <b>upon</b> acceptance.</p></p class="citation"></blockquote><h3 id=2453--61218-diffusion-based-inpainting-of-incomplete-euclidean-distance-matrices-of-trajectories-generated-by-a-fractional-brownian-motion-alexander-lobashev-et-al-2024>(24/53 | 61/218) Diffusion-based inpainting of incomplete Euclidean distance matrices of trajectories generated by a fractional Brownian motion (Alexander Lobashev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Lobashev, Kirill Polovnikov. (2024)<br><strong>Diffusion-based inpainting of incomplete Euclidean distance matrices of trajectories generated by a fractional Brownian motion</strong><br><button class=copy-to-clipboard title="Diffusion-based inpainting of incomplete Euclidean distance matrices of trajectories generated by a fractional Brownian motion" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68T07, I-2-0, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Diffusion Model, Graph, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07029v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07029v1.pdf filename=2404.07029v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fractional Brownian trajectories (fBm) feature both randomness and strong scale-free correlations, challenging generative models to reproduce the intrinsic memory characterizing the underlying process. Here we test a <b>diffusion</b> <b>probabilistic</b> <b>model</b> on a specific dataset of corrupted images corresponding to incomplete Euclidean distance matrices of fBm at various memory exponents $H$. Our dataset implies uniqueness of the data imputation in the regime of low missing ratio, where the remaining partial <b>graph</b> is rigid, providing the ground truth for the inpainting. We find that the conditional <b>diffusion</b> <b>generation</b> stably reproduces the statistics of missing fBm-distributed distances for different values of $H$ exponent. Furthermore, while <b>diffusion</b> <b>models</b> have been recently shown to remember samples from the training database, we show that <b>diffusion-based</b> <b>inpainting</b> behaves qualitatively different from the database search with the increasing database size. Finally, we apply our fBm-trained <b>diffusion</b> <b>model</b> with $H=1/3$ for completion of chromosome distance matrices obtained in single-cell microscopy experiments, showing its superiority over the standard bioinformatics algorithms. Our source code is available on GitHub at <a href=https://github.com/alobashev/diffusion_fbm>https://github.com/alobashev/diffusion_fbm</a>.</p></p class="citation"></blockquote><h3 id=2553--62218-medrg-medical-report-grounding-with-multi-modal-large-language-model-ke-zou-et-al-2024>(25/53 | 62/218) MedRG: Medical Report Grounding with Multi-modal Large Language Model (Ke Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ke Zou, Yang Bai, Zhihao Chen, Yang Zhou, Yidi Chen, Kai Ren, Meng Wang, Xuedong Yuan, Xiaojing Shen, Huazhu Fu. (2024)<br><strong>MedRG: Medical Report Grounding with Multi-modal Large Language Model</strong><br><button class=copy-to-clipboard title="MedRG: Medical Report Grounding with Multi-modal Large Language Model" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Multi-modal, Grounding, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06798v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06798v1.pdf filename=2404.06798v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical Report <b>Grounding</b> is pivotal in identifying the most relevant regions in medical images based on a given phrase query, a critical aspect in medical image analysis and radiological diagnosis. However, prevailing visual <b>grounding</b> approaches necessitate the manual extraction of key phrases from medical reports, imposing substantial burdens on both system efficiency and physicians. In this paper, we introduce a novel framework, Medical Report <b>Grounding</b> (MedRG), an end-to-end solution for utilizing a <b>multi-modal</b> <b>Large</b> <b>Language</b> <b>Model</b> to predict key phrase by incorporating a unique token, BOX, into the vocabulary to serve as an embedding for unlocking detection capabilities. Subsequently, the vision encoder-decoder jointly decodes the hidden embedding and the input medical image, generating the corresponding <b>grounding</b> box. The experimental results validate the effectiveness of MedRG, surpassing the performance of the existing state-of-the-art medical phrase <b>grounding</b> methods. This study represents a pioneering exploration of the medical report <b>grounding</b> task, marking the first-ever endeavor in this domain.</p></p class="citation"></blockquote><h3 id=2653--63218-sparse-points-to-dense-clouds-enhancing-3d-detection-with-limited-lidar-data-aakash-kumar-et-al-2024>(26/53 | 63/218) Sparse Points to Dense Clouds: Enhancing 3D Detection with Limited LiDAR Data (Aakash Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aakash Kumar, Chen Chen, Ajmal Mian, Neils Lobo, Mubarak Shah. (2024)<br><strong>Sparse Points to Dense Clouds: Enhancing 3D Detection with Limited LiDAR Data</strong><br><button class=copy-to-clipboard title="Sparse Points to Dense Clouds: Enhancing 3D Detection with Limited LiDAR Data" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Object Detection, Augmented Reality (AR), Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06715v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06715v1.pdf filename=2404.06715v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D detection is a critical task that enables machines to identify and locate <b>objects</b> <b>in</b> three-dimensional space. It has a broad range of applications in several fields, including autonomous driving, robotics and <b>augmented</b> <b>reality.</b> Monocular 3D detection is attractive as it requires only a single camera, however, it lacks the accuracy and robustness required for real world applications. High resolution LiDAR on the other hand, can be expensive and lead to interference problems in heavy traffic given their active transmissions. We propose a balanced approach that combines the advantages of monocular and point cloud-based 3D detection. Our method requires only a small number of 3D points, that can be obtained from a low-cost, low-resolution sensor. Specifically, we use only 512 points, which is just 1% of a full LiDAR frame in the KITTI dataset. Our method reconstructs a complete 3D point cloud from this limited 3D information combined with a single image. The reconstructed 3D point cloud and corresponding image can be used by any <b>multi-modal</b> off-the-shelf detector for 3D <b>object</b> <b>detection.</b> By using the proposed network architecture with an off-the-shelf <b>multi-modal</b> 3D detector, the accuracy of 3D detection improves by 20% compared to the state-of-the-art monocular detection methods and 6% to 9% compare to the baseline <b>multi-modal</b> methods on KITTI and JackRabbot datasets.</p></p class="citation"></blockquote><h3 id=2753--64218-a-transformer-based-model-for-the-prediction-of-human-gaze-behavior-on-videos-suleyman-ozdel-et-al-2024>(27/53 | 64/218) A Transformer-Based Model for the Prediction of Human Gaze Behavior on Videos (Suleyman Ozdel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suleyman Ozdel, Yao Rong, Berat Mert Albaba, Yen-Ling Kuo, Xi Wang. (2024)<br><strong>A Transformer-Based Model for the Prediction of Human Gaze Behavior on Videos</strong><br><button class=copy-to-clipboard title="A Transformer-Based Model for the Prediction of Human Gaze Behavior on Videos" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-HC, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07351v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07351v1.pdf filename=2404.07351v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Eye-tracking applications that utilize the human gaze in video understanding tasks have become increasingly important. To effectively automate the process of video analysis based on eye-tracking data, it is important to accurately replicate human gaze behavior. However, this task presents significant challenges due to the inherent complexity and ambiguity of human gaze patterns. In this work, we introduce a novel method for simulating human gaze behavior. Our approach uses a <b>transformer-based</b> <b>reinforcement</b> <b>learning</b> algorithm to train an agent that acts as a human observer, with the primary role of watching videos and simulating human gaze behavior. We employed an eye-tracking dataset gathered from videos generated by the VirtualHome simulator, with a primary focus on activity recognition. Our experimental results demonstrate the effectiveness of our gaze prediction method by highlighting its capability to replicate human gaze behavior and its applicability for downstream tasks where real human-gaze is used as input.</p></p class="citation"></blockquote><h3 id=2853--65218-instantmesh-efficient-3d-mesh-generation-from-a-single-image-with-sparse-view-large-reconstruction-models-jiale-xu-et-al-2024>(28/53 | 65/218) InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models (Jiale Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, Ying Shan. (2024)<br><strong>InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models</strong><br><button class=copy-to-clipboard title="InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07191v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07191v1.pdf filename=2404.07191v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability. By synergizing the strengths of an off-the-shelf multiview <b>diffusion</b> <b>model</b> and a sparse-view reconstruction model based on the LRM architecture, InstantMesh is able to create diverse 3D assets within 10 seconds. To enhance the training efficiency and exploit more geometric supervisions, e.g, depths and normals, we integrate a differentiable iso-surface extraction module into our framework and directly optimize on the mesh representation. Experimental results on public datasets demonstrate that InstantMesh significantly outperforms other latest image-to-3D baselines, both qualitatively and quantitatively. We release all the code, weights, and demo of InstantMesh, with the intention that it can make substantial contributions to the community of 3D <b>generative</b> <b>AI</b> and empower both researchers and content creators.</p></p class="citation"></blockquote><h3 id=2953--66218-move-anything-with-layered-scene-diffusion-jiawei-ren-et-al-2024>(29/53 | 66/218) Move Anything with Layered Scene Diffusion (Jiawei Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Ren, Mengmeng Xu, Jui-Chieh Wu, Ziwei Liu, Tao Xiang, Antoine Toisoul. (2024)<br><strong>Move Anything with Layered Scene Diffusion</strong><br><button class=copy-to-clipboard title="Move Anything with Layered Scene Diffusion" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07178v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07178v1.pdf filename=2404.07178v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> generate images with an unprecedented level of quality, but how can we freely rearrange image layouts? Recent works generate controllable scenes via learning spatially disentangled latent codes, but these methods do not apply to <b>diffusion</b> <b>models</b> due to their fixed forward process. In this work, we propose SceneDiffusion to optimize a layered scene representation during the <b>diffusion</b> <b>sampling</b> process. Our key insight is that spatial disentanglement can be obtained by jointly denoising scene renderings at different spatial layouts. Our generated scenes support a wide range of spatial editing operations, including moving, resizing, cloning, and layer-wise appearance editing operations, including object restyling and replacing. Moreover, a scene can be generated conditioned on a reference image, thus enabling object moving for in-the-wild images. Notably, this approach is training-free, compatible with general <b>text-to-image</b> <b>diffusion</b> <b>models,</b> and responsive in less than a second.</p></p class="citation"></blockquote><h3 id=3053--67218-lost-in-translation-modern-neural-networks-still-struggle-with-small-realistic-image-transformations-ofir-shifman-et-al-2024>(30/53 | 67/218) Lost in Translation: Modern Neural Networks Still Struggle With Small Realistic Image Transformations (Ofir Shifman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ofir Shifman, Yair Weiss. (2024)<br><strong>Lost in Translation: Modern Neural Networks Still Struggle With Small Realistic Image Transformations</strong><br><button class=copy-to-clipboard title="Lost in Translation: Modern Neural Networks Still Struggle With Small Realistic Image Transformations" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Data Augmentation, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07153v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07153v1.pdf filename=2404.07153v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks that achieve remarkable performance in <b>image</b> <b>classification</b> have previously been shown to be easily fooled by tiny transformations such as a one pixel translation of the input <b>image.</b> <b>In</b> order to address this problem, two approaches have been proposed in recent years. The first approach suggests using huge datasets together with <b>data</b> <b>augmentation</b> in the hope that a highly varied training set will teach the network to learn to be invariant. The second approach suggests using architectural modifications based on sampling theory to deal explicitly with <b>image</b> <b>translations.</b> In this paper, we show that these approaches still fall short in robustly handling &rsquo;natural&rsquo; <b>image</b> <b>translations</b> that simulate a subtle change in camera orientation. Our findings reveal that a mere one-pixel translation can result in a significant change in the predicted <b>image</b> <b>representation</b> for approximately 40% of the test <b>images</b> <b>in</b> state-of-the-art models (e.g. open-CLIP trained on LAION-2B or DINO-v2) , while models that are explicitly constructed to be robust to cyclic translations can still be fooled with 1 pixel realistic (non-cyclic) translations 11% of the time. We present Robust Inference by Crop Selection: a simple method that can be proven to achieve any desired level of consistency, although with a modest tradeoff with the model&rsquo;s accuracy. Importantly, we demonstrate how employing this method reduces the ability to fool state-of-the-art models with a 1 pixel translation to less than 5% while suffering from only a 1% drop in classification accuracy. Additionally, we show that our method can be easy adjusted to deal with circular shifts as well. In such case we achieve 100% robustness to integer shifts with state-of-the-art accuracy, and with no need for any further training.</p></p class="citation"></blockquote><h3 id=3153--68218-driver-attention-tracking-and-analysis-dat-viet-thanh-nguyen-et-al-2024>(31/53 | 68/218) Driver Attention Tracking and Analysis (Dat Viet Thanh Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dat Viet Thanh Nguyen, Anh Tran, Hoai Nam Vu, Cuong Pham, Minh Hoai. (2024)<br><strong>Driver Attention Tracking and Analysis</strong><br><button class=copy-to-clipboard title="Driver Attention Tracking and Analysis" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07122v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07122v2.pdf filename=2404.07122v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel method to estimate a driver&rsquo;s points-of-gaze using a pair of ordinary cameras mounted on the windshield and dashboard of a car. This is a challenging problem due to the dynamics of traffic environments with 3D scenes of unknown depths. This problem is further complicated by the volatile distance between the driver and the camera system. To tackle these challenges, we develop a novel <b>convolutional</b> <b>network</b> that simultaneously analyzes the image of the scene and the image of the driver&rsquo;s face. This network has a camera calibration module that can compute an embedding vector that represents the spatial configuration between the driver and the camera system. This calibration module improves the overall network&rsquo;s performance, which can be jointly trained end to end. We also address the lack of annotated data for training and evaluation by introducing a large-scale driving dataset with point-of-gaze annotations. This is an in situ dataset of real driving sessions in an urban city, containing synchronized images of the driving scene as well as the face and gaze of the driver. Experiments on this dataset show that the proposed method outperforms various baseline methods, having the mean prediction error of 29.69 pixels, which is relatively small compared to the $1280{\times}720$ resolution of the scene camera.</p></p class="citation"></blockquote><h3 id=3253--69218-adversarial-purification-for-no-reference-image-quality-metrics-applicability-study-and-new-methods-aleksandr-gushchin-et-al-2024>(32/53 | 69/218) Adversarial purification for no-reference image-quality metrics: applicability study and new methods (Aleksandr Gushchin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aleksandr Gushchin, Anna Chistyakova, Vladislav Minashkin, Anastasia Antsiferova, Dmitriy Vatolin. (2024)<br><strong>Adversarial purification for no-reference image-quality metrics: applicability study and new methods</strong><br><button class=copy-to-clipboard title="Adversarial purification for no-reference image-quality metrics: applicability study and new methods" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Adversarial Attack, Adversarial Purification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06957v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06957v1.pdf filename=2404.06957v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the area of <b>adversarial</b> <b>attacks</b> on image quality metrics has begun to be explored, whereas the area of defences remains under-researched. In this study, we aim to cover that case and check the transferability of <b>adversarial</b> <b>purification</b> defences from image classifiers to IQA methods. In this paper, we apply several widespread attacks on IQA models and examine the success of the defences against them. The purification methodologies covered different preprocessing techniques, including geometrical transformations, compression, denoising, and modern neural network-based methods. Also, we address the challenge of assessing the efficacy of a defensive methodology by proposing ways to estimate output visual quality and the success of neutralizing attacks. Defences were tested against attack on three IQA metrics &ndash; Linearity, MetaIQA and SPAQ. The code for attacks and defences is available at: (link is hidden for a blind review).</p></p class="citation"></blockquote><h3 id=3353--70218-fine-color-guidance-in-diffusion-models-and-its-application-to-image-compression-at-extremely-low-bitrates-tom-bordin-et-al-2024>(33/53 | 70/218) Fine color guidance in diffusion models and its application to image compression at extremely low bitrates (Tom Bordin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tom Bordin, Thomas Maugey. (2024)<br><strong>Fine color guidance in diffusion models and its application to image compression at extremely low bitrates</strong><br><button class=copy-to-clipboard title="Fine color guidance in diffusion models and its application to image compression at extremely low bitrates" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06865v1.pdf filename=2404.06865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study addresses the challenge of, without training or <b>fine-tuning,</b> controlling the global color aspect of images generated with a <b>diffusion</b> <b>model.</b> We rewrite the guidance equations to ensure that the outputs are closer to a known color map, and this without hindering the quality of the generation. Our method leads to new guidance equations. We show in the color guidance context that, the scaling of the guidance should not decrease but remains high throughout the <b>diffusion</b> <b>process.</b> In a second contribution, our guidance is applied in a compression framework, we combine both semantic and general color information on the image to decode the images at low cost. We show that our method is effective at improving fidelity and realism of compressed images at extremely low bit rates, when compared to other classical or more semantic oriented approaches.</p></p class="citation"></blockquote><h3 id=3453--71218-zero-shot-point-cloud-completion-via-2d-priors-tianxin-huang-et-al-2024>(34/53 | 71/218) Zero-shot Point Cloud Completion Via 2D Priors (Tianxin Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianxin Huang, Zhiwen Yan, Yuyang Zhao, Gim Hee Lee. (2024)<br><strong>Zero-shot Point Cloud Completion Via 2D Priors</strong><br><button class=copy-to-clipboard title="Zero-shot Point Cloud Completion Via 2D Priors" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06814v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06814v1.pdf filename=2404.06814v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D point cloud completion is designed to recover complete shapes from partially observed point clouds. Conventional completion methods typically depend on extensive point cloud data for training %, with their effectiveness often constrained to object categories similar to those seen during training. In contrast, we propose a <b>zero-shot</b> framework aimed at completing partially observed point clouds across any unseen categories. Leveraging point rendering via Gaussian Splatting, we develop techniques of Point Cloud Colorization and <b>Zero-shot</b> Fractal Completion that utilize 2D priors from pre-trained <b>diffusion</b> <b>models</b> to infer missing regions. Experimental results on both synthetic and real-world scanned point clouds demonstrate that our approach outperforms existing methods in completing a variety of objects without any requirement for specific training data.</p></p class="citation"></blockquote><h3 id=3553--72218-efficient-and-scalable-chinese-vector-font-generation-via-component-composition-jinyu-song-et-al-2024>(35/53 | 72/218) Efficient and Scalable Chinese Vector Font Generation via Component Composition (Jinyu Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinyu Song, Weitao You, Shuhui Shi, Shuxuan Guo, Lingyun Sun, Wei Wang. (2024)<br><strong>Efficient and Scalable Chinese Vector Font Generation via Component Composition</strong><br><button class=copy-to-clipboard title="Efficient and Scalable Chinese Vector Font Generation via Component Composition" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 20<br>Keywords: Zero-shot, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06779v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06779v1.pdf filename=2404.06779v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chinese vector font generation is challenging due to the complex structure and huge amount of Chinese characters. Recent advances remain limited to generating a small set of characters with simple structure. In this work, we first observe that most Chinese characters can be disassembled into frequently-reused components. Therefore, we introduce the first efficient and scalable Chinese vector font generation approach via component composition, allowing generating numerous vector characters from a small set of components. To achieve this, we collect a large-scale dataset that contains over \textit{90K} Chinese characters with their components and layout information. Upon the dataset, we propose a simple yet effective framework based on spatial <b>transformer</b> networks (STN) and multiple losses tailored to font characteristics to learn the affine transformation of the components, which can be directly applied to the B'ezier curves, resulting in Chinese characters in vector format. Our qualitative and quantitative experiments have demonstrated that our method significantly surpasses the state-of-the-art vector font generation methods in generating large-scale complex Chinese characters in both font generation and <b>zero-shot</b> font extension.</p></p class="citation"></blockquote><h3 id=3653--73218-gaze-guided-graph-neural-network-for-action-anticipation-conditioned-on-intention-suleyman-ozdel-et-al-2024>(36/53 | 73/218) Gaze-Guided Graph Neural Network for Action Anticipation Conditioned on Intention (Suleyman Ozdel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suleyman Ozdel, Yao Rong, Berat Mert Albaba, Yen-Ling Kuo, Xi Wang. (2024)<br><strong>Gaze-Guided Graph Neural Network for Action Anticipation Conditioned on Intention</strong><br><button class=copy-to-clipboard title="Gaze-Guided Graph Neural Network for Action Anticipation Conditioned on Intention" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-HC, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07347v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07347v1.pdf filename=2404.07347v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans utilize their gaze to concentrate on essential information while perceiving and interpreting intentions in videos. Incorporating human gaze into computational algorithms can significantly enhance model performance in video understanding tasks. In this work, we address a challenging and innovative task in video understanding: predicting the actions of an agent in a video based on a partial video. We introduce the Gaze-guided Action Anticipation algorithm, which establishes a visual-semantic <b>graph</b> <b>from</b> <b>the</b> video input. Our method utilizes a <b>Graph</b> <b>Neural</b> <b>Network</b> to recognize the agent&rsquo;s intention and predict the action sequence to fulfill this intention. To assess the efficiency of our approach, we collect a dataset containing household activities generated in the VirtualHome environment, accompanied by human gaze data of viewing videos. Our method outperforms state-of-the-art techniques, achieving a 7% improvement in accuracy for 18-class intention recognition. This highlights the efficiency of our method in learning important features from human gaze data.</p></p class="citation"></blockquote><h3 id=3753--74218-3dmambacomplete-exploring-structured-state-space-model-for-point-cloud-completion-yixuan-li-et-al-2024>(37/53 | 74/218) 3DMambaComplete: Exploring Structured State Space Model for Point Cloud Completion (Yixuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixuan Li, Weidong Yang, Ben Fei. (2024)<br><strong>3DMambaComplete: Exploring Structured State Space Model for Point Cloud Completion</strong><br><button class=copy-to-clipboard title="3DMambaComplete: Exploring Structured State Space Model for Point Cloud Completion" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07106v1.pdf filename=2404.07106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Point cloud completion aims to generate a complete and high-fidelity point cloud from an initially incomplete and low-quality input. A prevalent strategy involves leveraging <b>Transformer-based</b> models to encode global features and facilitate the reconstruction process. However, the adoption of pooling operations to obtain global feature representations often results in the loss of local details within the point cloud. Moreover, the attention mechanism inherent in <b>Transformers</b> introduces additional computational complexity, rendering it challenging to handle long sequences effectively. To address these issues, we propose 3DMambaComplete, a point cloud completion network built on the novel Mamba framework. It comprises three modules: HyperPoint Generation encodes point cloud features using Mamba&rsquo;s selection mechanism and predicts a set of Hyperpoints. A specific offset is estimated, and the down-sampled points become HyperPoints. The HyperPoint Spread module disperses these HyperPoints across different spatial locations to avoid concentration. Finally, a deformation method transforms the 2D mesh representation of HyperPoints into a fine-grained 3D structure for point cloud reconstruction. Extensive experiments conducted on various established <b>benchmarks</b> demonstrate that 3DMambaComplete surpasses state-of-the-art point cloud completion methods, as confirmed by qualitative and quantitative analyses.</p></p class="citation"></blockquote><h3 id=3853--75218-identification-of-fine-grained-systematic-errors-via-controlled-scene-generation-valentyn-boreiko-et-al-2024>(38/53 | 75/218) Identification of Fine-grained Systematic Errors via Controlled Scene Generation (Valentyn Boreiko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Valentyn Boreiko, Matthias Hein, Jan Hendrik Metzen. (2024)<br><strong>Identification of Fine-grained Systematic Errors via Controlled Scene Generation</strong><br><button class=copy-to-clipboard title="Identification of Fine-grained Systematic Errors via Controlled Scene Generation" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Object Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07045v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07045v1.pdf filename=2404.07045v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many safety-critical applications, especially in autonomous driving, require reliable <b>object</b> <b>detectors.</b> They can be very effectively assisted by a method to search for and identify potential failures and systematic errors before these detectors are deployed. Systematic errors are characterized by combinations of attributes such as <b>object</b> <b>location,</b> scale, orientation, and color, as well as the composition of their respective backgrounds. To identify them, one must rely on something other than real images from a test set because they do not account for very rare but possible combinations of attributes. To overcome this limitation, we propose a pipeline for generating realistic synthetic scenes with fine-grained control, allowing the creation of complex scenes with multiple <b>objects.</b> <b>Our</b> approach, BEV2EGO, allows for a realistic generation of the complete scene with road-contingent control that maps 2D bird&rsquo;s-eye view (BEV) scene configurations to a first-person view (EGO). In addition, we propose a <b>benchmark</b> for controlled scene generation to select the most appropriate generative outpainting model for BEV2EGO. We further use it to perform a systematic analysis of multiple state-of-the-art <b>object</b> <b>detection</b> models and discover differences between them.</p></p class="citation"></blockquote><h3 id=3953--76218-multi-label-continual-learning-for-the-medical-domain-a-novel-benchmark-marina-ceccon-et-al-2024>(39/53 | 76/218) Multi-Label Continual Learning for the Medical Domain: A Novel Benchmark (Marina Ceccon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marina Ceccon, Davide Dalle Pezze, Alessandro Fabris, Gian Antonio Susto. (2024)<br><strong>Multi-Label Continual Learning for the Medical Domain: A Novel Benchmark</strong><br><button class=copy-to-clipboard title="Multi-Label Continual Learning for the Medical Domain: A Novel Benchmark" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06859v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06859v2.pdf filename=2404.06859v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-label image classification in dynamic environments is a problem that poses significant challenges. Previous studies have primarily focused on scenarios such as Domain Incremental Learning and Class Incremental Learning, which do not fully capture the complexity of real-world applications. In this paper, we study the problem of classification of medical imaging in the scenario termed New Instances and New Classes, which combines the challenges of both new class arrivals and domain shifts in a single framework. Unlike traditional scenarios, it reflects the realistic nature of CL in domains such as medical imaging, where updates may introduce both new classes and changes in domain characteristics. To address the unique challenges posed by this complex scenario, we introduce a novel approach called Pseudo-Label Replay. This method aims to mitigate forgetting while adapting to new classes and domain shifts by combining the advantages of the Replay and Pseudo-Label methods and solving their limitations in the proposed scenario. We evaluate our proposed approach on a challenging <b>benchmark</b> consisting of two datasets, seven tasks, and nineteen classes, modeling a realistic <b>Continual</b> <b>Learning</b> scenario. Our experimental findings demonstrate the effectiveness of Pseudo-Label Replay in addressing the challenges posed by the complex scenario proposed. Our method surpasses existing approaches, exhibiting superior performance while showing minimal forgetting.</p></p class="citation"></blockquote><h3 id=4053--77218-udiff-generating-conditional-unsigned-distance-fields-with-optimal-wavelet-diffusion-junsheng-zhou-et-al-2024>(40/53 | 77/218) UDiFF: Generating Conditional Unsigned Distance Fields with Optimal Wavelet Diffusion (Junsheng Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junsheng Zhou, Weiqi Zhang, Baorui Ma, Kanle Shi, Yu-Shen Liu, Zhizhong Han. (2024)<br><strong>UDiFF: Generating Conditional Unsigned Distance Fields with Optimal Wavelet Diffusion</strong><br><button class=copy-to-clipboard title="UDiFF: Generating Conditional Unsigned Distance Fields with Optimal Wavelet Diffusion" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06851v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06851v1.pdf filename=2404.06851v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have shown remarkable results for image generation, editing and inpainting. Recent works explore <b>diffusion</b> <b>models</b> for 3D shape generation with neural implicit functions, i.e., signed distance function and occupancy function. However, they are limited to shapes with closed surfaces, which prevents them from generating diverse 3D real-world contents containing open surfaces. In this work, we present UDiFF, a 3D <b>diffusion</b> <b>model</b> for unsigned distance fields (UDFs) which is capable to generate textured 3D shapes with open surfaces from text conditions or unconditionally. Our key idea is to generate UDFs in spatial-frequency domain with an optimal wavelet transformation, which produces a compact representation space for UDF generation. Specifically, instead of selecting an appropriate wavelet transformation which requires expensive manual efforts and still leads to large information loss, we propose a data-driven approach to learn the optimal wavelet transformation for UDFs. We evaluate UDiFF to show our advantages by numerical and visual comparisons with the latest methods on widely used <b>benchmarks.</b> Page: <a href=https://weiqi-zhang.github.io/UDiFF>https://weiqi-zhang.github.io/UDiFF</a>.</p></p class="citation"></blockquote><h3 id=4153--78218-splatpose--detect-pose-agnostic-3d-anomaly-detection-mathis-kruse-et-al-2024>(41/53 | 78/218) SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection (Mathis Kruse et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn. (2024)<br><strong>SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection</strong><br><button class=copy-to-clipboard title="SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Anomaly Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06832v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06832v1.pdf filename=2404.06832v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting anomalies in images has become a well-explored problem in both academia and industry. State-of-the-art algorithms are able to detect defects in increasingly difficult settings and data modalities. However, most current methods are not suited to address 3D objects captured from differing poses. While solutions using Neural Radiance Fields (NeRFs) have been proposed, they suffer from excessive computation requirements, which hinder real-world usability. For this reason, we propose the novel 3D Gaussian splatting-based framework SplatPose which, given multi-view images of a 3D object, accurately estimates the pose of unseen views in a differentiable manner, and detects anomalies in them. We achieve state-of-the-art results in both training and inference speed, and detection performance, even when using less training data than competing methods. We thoroughly evaluate our framework using the recently proposed Pose-agnostic <b>Anomaly</b> <b>Detection</b> <b>benchmark</b> and its multi-pose <b>anomaly</b> <b>detection</b> (MAD) data set.</p></p class="citation"></blockquote><h3 id=4253--79218-self-supervised-monocular-depth-estimation-on-water-scenes-via-specular-reflection-prior-zhengyang-lu-et-al-2024>(42/53 | 79/218) Self-supervised Monocular Depth Estimation on Water Scenes via Specular Reflection Prior (Zhengyang Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengyang Lu, Ying Chen. (2024)<br><strong>Self-supervised Monocular Depth Estimation on Water Scenes via Specular Reflection Prior</strong><br><button class=copy-to-clipboard title="Self-supervised Monocular Depth Estimation on Water Scenes via Specular Reflection Prior" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07176v1.pdf filename=2404.07176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monocular depth estimation from a single image is an ill-posed problem for computer vision due to insufficient reliable cues as the prior knowledge. Besides the inter-frame supervision, namely stereo and adjacent frames, extensive prior information is available in the same frame. Reflections from specular surfaces, informative intra-frame priors, enable us to reformulate the ill-posed depth estimation task as a multi-view synthesis. This paper proposes the first self-supervision for deep-learning depth estimation on water scenes via intra-frame priors, known as reflection supervision and geometrical constraints. In the first stage, a water segmentation network is performed to separate the reflection components from the entire image. Next, we construct a <b>self-supervised</b> framework to predict the target appearance from reflections, perceived as other perspectives. The photometric re-projection error, incorporating SmoothL1 and a novel photometric adaptive SSIM, is formulated to optimize pose and depth estimation by aligning the transformed virtual depths and source ones. As a supplement, the water surface is determined from real and virtual camera positions, which complement the depth of the water area. Furthermore, to alleviate these laborious ground truth annotations, we introduce a large-scale water reflection scene (WRS) dataset rendered from Unreal Engine 4. Extensive experiments on the WRS dataset prove the feasibility of the proposed method compared to state-of-the-art depth estimation techniques.</p></p class="citation"></blockquote><h3 id=4353--80218-mocap-to-visual-domain-adaptation-for-efficient-human-mesh-estimation-from-2d-keypoints-bedirhan-uguz-et-al-2024>(43/53 | 80/218) MoCap-to-Visual Domain Adaptation for Efficient Human Mesh Estimation from 2D Keypoints (Bedirhan Uguz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bedirhan Uguz, Ozhan Suat, Batuhan Karagoz, Emre Akbas. (2024)<br><strong>MoCap-to-Visual Domain Adaptation for Efficient Human Mesh Estimation from 2D Keypoints</strong><br><button class=copy-to-clipboard title="MoCap-to-Visual Domain Adaptation for Efficient Human Mesh Estimation from 2D Keypoints" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07094v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07094v1.pdf filename=2404.07094v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents Key2Mesh, a model that takes a set of 2D human pose keypoints as input and estimates the corresponding body mesh. Since this process does not involve any visual (i.e. RGB image) data, the model can be trained on large-scale motion capture (MoCap) datasets, thereby overcoming the scarcity of image datasets with 3D labels. To enable the model&rsquo;s application on RGB images, we first run an off-the-shelf 2D pose estimator to obtain the 2D keypoints, and then feed these 2D keypoints to Key2Mesh. To improve the performance of our model on RGB images, we apply an adversarial <b>domain</b> <b>adaptation</b> (DA) method to bridge the gap between the MoCap and visual <b>domains.</b> <b>Crucially,</b> our DA method does not require 3D labels for visual data, which enables adaptation to target sets without the need for costly labels. We evaluate Key2Mesh for the task of estimating 3D human meshes from 2D keypoints, in the absence of RGB and mesh label pairs. Our results on widely used H3.6M and 3DPW datasets show that Key2Mesh sets the new state-of-the-art by outperforming other models in PA-MPJPE for both datasets, and in MPJPE and PVE for the 3DPW dataset. Thanks to our model&rsquo;s simple architecture, it operates at least 12x faster than the prior state-of-the-art model, LGD. Additional qualitative samples and code are available on the project website: <a href=https://key2mesh.github.io/>https://key2mesh.github.io/</a>.</p></p class="citation"></blockquote><h3 id=4453--81218-an-evidential-enhanced-tri-branch-consistency-learning-method-for-semi-supervised-medical-image-segmentation-zhenxi-zhang-et-al-2024>(44/53 | 81/218) An Evidential-enhanced Tri-Branch Consistency Learning Method for Semi-supervised Medical Image Segmentation (Zhenxi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenxi Zhang, Heng Zhou, Xiaoran Shi, Ran Ran, Chunna Tian, Feng Zhou. (2024)<br><strong>An Evidential-enhanced Tri-Branch Consistency Learning Method for Semi-supervised Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="An Evidential-enhanced Tri-Branch Consistency Learning Method for Semi-supervised Medical Image Segmentation" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07032v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07032v1.pdf filename=2404.07032v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semi-supervised segmentation presents a promising approach for large-scale medical image analysis, effectively reducing annotation burdens while achieving comparable performance. This methodology holds substantial potential for streamlining the segmentation process and enhancing its feasibility within clinical settings for translational investigations. While cross-supervised training, based on distinct co-training sub-networks, has become a prevalent paradigm for this task, addressing critical issues such as predication disagreement and label-noise suppression requires further attention and progress in cross-supervised training. In this paper, we introduce an Evidential Tri-Branch Consistency learning framework (ETC-Net) for semi-supervised medical image segmentation. ETC-Net employs three branches: an evidential conservative branch, an evidential progressive branch, and an evidential fusion branch. The first two branches exhibit complementary characteristics, allowing them to address prediction diversity and enhance training stability. We also integrate uncertainty estimation from the evidential learning into cross-supervised training, mitigating the negative impact of erroneous supervision signals. Additionally, the evidential fusion branch capitalizes on the complementary attributes of the first two branches and leverages an evidence-based Dempster-Shafer fusion strategy, <b>supervised</b> by more reliable and accurate pseudo-labels of unlabeled data. Extensive experiments conducted on LA, Pancreas-CT, and ACDC datasets demonstrate that ETC-Net surpasses other state-of-the-art methods for semi-supervised segmentation. The code will be made available in the near future at <a href=https://github.com/Medsemiseg>https://github.com/Medsemiseg</a>.</p></p class="citation"></blockquote><h3 id=4553--82218-accurate-tennis-court-line-detection-on-amateur-recorded-matches-sameer-agrawal-et-al-2024>(45/53 | 82/218) Accurate Tennis Court Line Detection on Amateur Recorded Matches (Sameer Agrawal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sameer Agrawal, Ragoth Sundararajan, Vishak Sagar. (2024)<br><strong>Accurate Tennis Court Line Detection on Amateur Recorded Matches</strong><br><button class=copy-to-clipboard title="Accurate Tennis Court Line Detection on Amateur Recorded Matches" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-4-6, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06977v1.pdf filename=2404.06977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Typically, tennis court line detection is done by running Hough-Line-Detection to find straight lines in the image, and then computing a transformation matrix from the detected lines to create the final court structure. We propose numerous improvements and enhancements to this algorithm, including using pretrained State-of-the-Art shadow-removal and <b>object-detection</b> <b>ML</b> models to make our line-detection more robust. Compared to the original algorithm, our method can accurately detect lines on amateur, dirty courts. When combined with a robust ball-tracking system, our method will enable accurate, automatic refereeing for amateur and professional tennis matches alike.</p></p class="citation"></blockquote><h3 id=4653--83218-yolo-based-ocean-eddy-localization-with-aws-sagemaker-seraj-al-mahmud-mostafa-et-al-2024>(46/53 | 83/218) YOLO based Ocean Eddy Localization with AWS SageMaker (Seraj Al Mahmud Mostafa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seraj Al Mahmud Mostafa, Jinbo Wang, Benjamin Holt, Jianwu Wang. (2024)<br><strong>YOLO based Ocean Eddy Localization with AWS SageMaker</strong><br><button class=copy-to-clipboard title="YOLO based Ocean Eddy Localization with AWS SageMaker" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Yolo<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06744v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06744v1.pdf filename=2404.06744v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ocean eddies play a significant role both on the sea surface and beneath it, contributing to the sustainability of marine life dependent on oceanic behaviors. Therefore, it is crucial to investigate ocean eddies to monitor changes in the Earth, particularly in the oceans, and their impact on climate. This study aims to pinpoint ocean eddies using AWS cloud services, specifically SageMaker. The primary objective is to detect small-scale (&lt;20km) ocean eddies from satellite remote images and assess the feasibility of utilizing SageMaker, which offers tools for deploying AI applications. Moreover, this research not only explores the deployment of cloud-based services for remote sensing of Earth data but also evaluates several <b>YOLO</b> (You Only Look Once) models using single and multi-GPU-based services in the cloud. Furthermore, this study underscores the potential of these services, their limitations, challenges related to deployment and resource management, and their user-riendliness for Earth science projects.</p></p class="citation"></blockquote><h3 id=4753--84218-an-animation-based-augmentation-approach-for-action-recognition-from-discontinuous-video-xingyu-song-et-al-2024>(47/53 | 84/218) An Animation-based Augmentation Approach for Action Recognition from Discontinuous Video (Xingyu Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyu Song, Zhan Li, Shi Chen, Xin-Qiang Cai, Kazuyuki Demachi. (2024)<br><strong>An Animation-based Augmentation Approach for Action Recognition from Discontinuous Video</strong><br><button class=copy-to-clipboard title="An Animation-based Augmentation Approach for Action Recognition from Discontinuous Video" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06741v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06741v1.pdf filename=2404.06741v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The study of action recognition has attracted considerable attention recently due to its broad applications in multiple areas. However, with the issue of discontinuous training video, which not only decreases the performance of action recognition model, but complicates the <b>data</b> <b>augmentation</b> process as well, still remains under-exploration. In this study, we introduce the 4A (Action Animation-based Augmentation Approach), an innovative pipeline for <b>data</b> <b>augmentation</b> to address the problem. The main contributions remain in our work includes: (1) we investigate the problem of severe decrease on performance of action recognition task training by discontinuous video, and the limitation of existing augmentation methods on solving this problem. (2) we propose a novel augmentation pipeline, 4A, to address the problem of discontinuous video for training, while achieving a smoother and natural-looking action representation than the latest <b>data</b> <b>augmentation</b> methodology. (3) We achieve the same performance with only 10% of the original <b>data</b> <b>for</b> training as with all of the original <b>data</b> <b>from</b> the real-world dataset, and a better performance on In-the-wild videos, by employing our <b>data</b> <b>augmentation</b> techniques.</p></p class="citation"></blockquote><h3 id=4853--85218-convolution-based-probability-gradient-loss-for-semantic-segmentation-guohang-shan-et-al-2024>(48/53 | 85/218) Convolution-based Probability Gradient Loss for Semantic Segmentation (Guohang Shan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guohang Shan, Shuangcheng Jia. (2024)<br><strong>Convolution-based Probability Gradient Loss for Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Convolution-based Probability Gradient Loss for Semantic Segmentation" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06704v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06704v1.pdf filename=2404.06704v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a novel <b>Convolution-based</b> Probability Gradient (CPG) loss for semantic segmentation. It employs <b>convolution</b> kernels similar to the Sobel operator, capable of computing the gradient of pixel intensity in an image. This enables the computation of gradients for both ground-truth and predicted category-wise probabilities. It enhances network performance by maximizing the similarity between these two probability gradients. Moreover, to specifically enhance accuracy near the object&rsquo;s boundary, we extract the object boundary based on the ground-truth probability gradient and exclusively apply the CPG loss to pixels belonging to boundaries. CPG loss proves to be highly convenient and effective. It establishes pixel relationships through <b>convolution,</b> calculating errors from a distinct dimension compared to pixel-wise loss functions such as cross-entropy loss. We conduct qualitative and quantitative analyses to evaluate the impact of the CPG loss on three well-established networks (DeepLabv3-Resnet50, HRNetV2-OCR, and LRASPP_MobileNet_V3_Large) across three standard segmentation datasets (Cityscapes, COCO-Stuff, ADE20K). Our extensive experimental results consistently and significantly demonstrate that the CPG loss enhances the mean Intersection over Union.</p></p class="citation"></blockquote><h3 id=4953--86218-perception-oriented-video-frame-interpolation-via-asymmetric-blending-guangyang-wu-et-al-2024>(49/53 | 86/218) Perception-Oriented Video Frame Interpolation via Asymmetric Blending (Guangyang Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangyang Wu, Xin Tao, Changlin Li, Wenyi Wang, Xiaohong Liu, Qingqing Zheng. (2024)<br><strong>Perception-Oriented Video Frame Interpolation via Asymmetric Blending</strong><br><button class=copy-to-clipboard title="Perception-Oriented Video Frame Interpolation via Asymmetric Blending" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Reconstruction Loss<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06692v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06692v1.pdf filename=2404.06692v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous methods for Video Frame Interpolation (VFI) have encountered challenges, notably the manifestation of blur and ghosting effects. These issues can be traced back to two pivotal factors: unavoidable motion errors and misalignment in supervision. In practice, motion estimates often prove to be error-prone, resulting in misaligned features. Furthermore, the <b>reconstruction</b> <b>loss</b> tends to bring blurry results, particularly in misaligned regions. To mitigate these challenges, we propose a new paradigm called PerVFI (Perception-oriented Video Frame Interpolation). Our approach incorporates an Asymmetric Synergistic Blending module (ASB) that utilizes features from both sides to synergistically blend intermediate features. One reference frame emphasizes primary content, while the other contributes complementary information. To impose a stringent constraint on the blending process, we introduce a self-learned sparse quasi-binary mask which effectively mitigates ghosting and blur artifacts in the output. Additionally, we employ a normalizing flow-based generator and utilize the negative log-likelihood loss to learn the conditional distribution of the output, which further facilitates the generation of clear and fine details. Experimental results validate the superiority of PerVFI, demonstrating significant improvements in perceptual quality compared to existing methods. Codes are available at \url{https://github.com/mulns/PerVFI}</p></p class="citation"></blockquote><h3 id=5053--87218-efficient-denoising-using-score-embedding-in-score-based-diffusion-models-andrew-s-na-et-al-2024>(50/53 | 87/218) Efficient Denoising using Score Embedding in Score-based Diffusion Models (Andrew S. Na et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew S. Na, William Gao, Justin W. L. Wan. (2024)<br><strong>Efficient Denoising using Score Embedding in Score-based Diffusion Models</strong><br><button class=copy-to-clipboard title="Efficient Denoising using Score Embedding in Score-based Diffusion Models" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06661v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06661v1.pdf filename=2404.06661v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is well known that training a denoising score-based <b>diffusion</b> <b>models</b> requires tens of thousands of epochs and a substantial number of image data to train the model. In this paper, we propose to increase the efficiency in training score-based <b>diffusion</b> <b>models.</b> Our method allows us to decrease the number of epochs needed to train the <b>diffusion</b> <b>model.</b> We accomplish this by solving the log-density Fokker-Planck (FP) Equation numerically to compute the score \textit{before} training. The pre-computed score is embedded into the image to encourage faster training under slice Wasserstein distance. Consequently, it also allows us to decrease the number of images we need to train the neural network to learn an accurate score. We demonstrate through our numerical experiments the improved performance of our proposed method compared to standard score-based <b>diffusion</b> <b>models.</b> Our proposed method achieves a similar quality to the standard method meaningfully faster.</p></p class="citation"></blockquote><h3 id=5153--88218-peavs-perceptual-evaluation-of-audio-visual-synchrony-grounded-in-viewers-opinion-scores-lucas-goncalves-et-al-2024>(51/53 | 88/218) PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers&rsquo; Opinion Scores (Lucas Goncalves et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucas Goncalves, Prashant Mathur, Chandrashekhar Lavania, Metehan Cekic, Marcello Federico, Kyu J. Han. (2024)<br><strong>PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers&rsquo; Opinion Scores</strong><br><button class=copy-to-clipboard title="PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers' Opinion Scores" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV, eess-AS<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07336v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07336v1.pdf filename=2404.07336v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in audio-visual generative modeling have been propelled by progress in deep learning and the availability of data-rich <b>benchmarks.</b> However, the growth is not attributed solely to models and <b>benchmarks.</b> Universally accepted evaluation metrics also play an important role in advancing the field. While there are many metrics available to evaluate audio and visual content separately, there is a lack of metrics that offer a quantitative and interpretable measure of audio-visual synchronization for videos &ldquo;in the wild&rdquo;. To address this gap, we first created a large scale human annotated dataset (100+ hrs) representing nine types of synchronization errors in audio-visual content and how human perceive them. We then developed a PEAVS (Perceptual Evaluation of Audio-Visual Synchrony) score, a novel automatic metric with a 5-point scale that evaluates the quality of audio-visual synchronization. We validate PEAVS using a newly generated dataset, achieving a Pearson correlation of 0.79 at the set level and 0.54 at the clip level when compared to human labels. In our experiments, we observe a relative gain 50% over a natural extension of Fr'echet based metrics for Audio-Visual synchrony, confirming PEAVS efficacy in objectively modeling subjective perceptions of audio-visual synchronization for videos &ldquo;in the wild&rdquo;.</p></p class="citation"></blockquote><h3 id=5253--89218-unfolding-admm-for-enhanced-subspace-clustering-of-hyperspectral-images-xianlu-li-et-al-2024>(52/53 | 89/218) Unfolding ADMM for Enhanced Subspace Clustering of Hyperspectral Images (Xianlu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xianlu Li, Nicolas Nadisic, Shaoguang Huang, Aleksandra Pižurica. (2024)<br><strong>Unfolding ADMM for Enhanced Subspace Clustering of Hyperspectral Images</strong><br><button class=copy-to-clipboard title="Unfolding ADMM for Enhanced Subspace Clustering of Hyperspectral Images" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07112v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07112v1.pdf filename=2404.07112v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep subspace <b>clustering</b> methods are now prominent in <b>clustering,</b> typically using fully connected networks and a self-representation loss function. However, these methods often struggle with overfitting and lack interpretability. In this paper, we explore an alternative <b>clustering</b> approach based on deep unfolding. By unfolding iterative optimization methods into neural networks, this approach offers enhanced interpretability and reliability compared to data-driven deep learning methods, and greater adaptability and generalization than model-based approaches. Hence, unfolding has become widely used in inverse imaging problems, such as image restoration, reconstruction, and super-resolution, but has not been sufficiently explored yet in the context of <b>clustering.</b> In this work, we introduce an innovative <b>clustering</b> architecture for hyperspectral images (HSI) by unfolding an iterative solver based on the Alternating Direction Method of Multipliers (ADMM) for sparse subspace <b>clustering.</b> To our knowledge, this is the first attempt to apply unfolding ADMM for computing the self-representation matrix in subspace <b>clustering.</b> Moreover, our approach captures well the structural characteristics of HSI data by employing the K nearest neighbors algorithm as part of a structure preservation module. Experimental evaluation of three established HSI datasets shows clearly the potential of the unfolding approach in HSI <b>clustering</b> and even demonstrates superior performance compared to state-of-the-art techniques.</p></p class="citation"></blockquote><h3 id=5353--90218-sparse-global-matching-for-video-frame-interpolation-with-large-motion-chunxu-liu-et-al-2024>(53/53 | 90/218) Sparse Global Matching for Video Frame Interpolation with Large Motion (Chunxu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chunxu Liu, Guozhen Zhang, Rui Zhao, Limin Wang. (2024)<br><strong>Sparse Global Matching for Video Frame Interpolation with Large Motion</strong><br><button class=copy-to-clipboard title="Sparse Global Matching for Video Frame Interpolation with Large Motion" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06913v1.pdf filename=2404.06913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large motion poses a critical challenge in Video Frame Interpolation (VFI) task. Existing methods are often constrained by limited receptive fields, resulting in sub-optimal performance when handling scenarios with large motion. In this paper, we introduce a new pipeline for VFI, which can effectively integrate global-level information to alleviate issues associated with large motion. Specifically, we first estimate a pair of initial intermediate flows using a high-resolution feature map for extracting local details. Then, we incorporate a sparse global matching branch to compensate for flow estimation, which consists of identifying flaws in initial flows and generating sparse flow compensation with a global receptive field. Finally, we adaptively merge the initial flow estimation with global flow compensation, yielding a more accurate intermediate flow. To evaluate the effectiveness of our method in handling large motion, we carefully curate a more challenging subset from commonly used <b>benchmarks.</b> Our method demonstrates the state-of-the-art performance on these VFI subsets with large motion.</p></p class="citation"></blockquote><h2 id=csro-12>cs.RO (12)</h2><h3 id=112--91218-vision-language-model-based-physical-reasoning-for-robot-liquid-perception-wenqiang-lai-et-al-2024>(1/12 | 91/218) Vision-Language Model-based Physical Reasoning for Robot Liquid Perception (Wenqiang Lai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenqiang Lai, Yuan Gao, Tin Lun Lam. (2024)<br><strong>Vision-Language Model-based Physical Reasoning for Robot Liquid Perception</strong><br><button class=copy-to-clipboard title="Vision-Language Model-based Physical Reasoning for Robot Liquid Perception" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 91<br>Keywords: Fine-tuning, Geometry, Multi-modal, Multi-modal, GPT, Grounding, Reasoning, Large Language Model, Large Language Model, Vision-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06904v1.pdf filename=2404.06904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is a growing interest in applying <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in robotic tasks, due to their remarkable <b>reasoning</b> ability and extensive knowledge learned from vast training corpora. <b>Grounding</b> <b>LLMs</b> in the physical world remains an open challenge as they can only process textual input. Recent advancements in <b>large</b> <b>vision-language</b> <b>models</b> (LVLMs) have enabled a more comprehensive understanding of the physical world by incorporating visual input, which provides richer contextual information than language alone. In this work, we proposed a novel paradigm that leveraged <b>GPT-4V(ision),</b> the state-of-the-art LVLM by OpenAI, to enable embodied agents to perceive liquid objects via image-based environmental feedback. Specifically, we exploited the physical understanding of <b>GPT-4V</b> to interpret the visual representation (e.g., time-series plot) of non-visual feedback (e.g., F/T sensor data), indirectly enabling <b>multimodal</b> perception beyond vision and language using images as proxies. We evaluated our method using 10 common household liquids with containers of various <b>geometry</b> and material. Without any training or <b>fine-tuning,</b> we demonstrated that our method can enable the robot to indirectly perceive the physical response of liquids and estimate their viscosity. We also showed that by jointly <b>reasoning</b> over the visual and physical attributes learned through interactions, our method could recognize liquid objects in the absence of strong visual cues (e.g., container labels with legible text or symbols), increasing the accuracy from 69.0% &ndash; achieved by the best-performing vision-only variant &ndash; to 86.0%.</p></p class="citation"></blockquote><h3 id=212--92218-beyond-gait-learning-knee-angle-for-seamless-prosthesis-control-in-multiple-scenarios-pengwei-wang-et-al-2024>(2/12 | 92/218) Beyond Gait: Learning Knee Angle for Seamless Prosthesis Control in Multiple Scenarios (Pengwei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengwei Wang, Yilong Chen, Wan Su, Jie Wang, Teng Ma, Haoyong Yu. (2024)<br><strong>Beyond Gait: Learning Knee Angle for Seamless Prosthesis Control in Multiple Scenarios</strong><br><button class=copy-to-clipboard title="Beyond Gait: Learning Knee Angle for Seamless Prosthesis Control in Multiple Scenarios" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 80<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Probabilistic Model, LSTM, LSTM, LSTM, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06772v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06772v1.pdf filename=2404.06772v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning models have become a powerful tool in knee angle estimation for lower limb prostheses, owing to their adaptability across various gait phases and locomotion modes. Current methods utilize Multi-Layer Perceptrons (MLP), <b>Long-Short</b> <b>Term</b> <b>Memory</b> <b>Networks</b> <b>(LSTM),</b> and <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNN),</b> predominantly analyzing motion information from the thigh. Contrary to these approaches, our study introduces a holistic perspective by integrating whole-body movements as inputs. We propose a <b>transformer-based</b> <b>probabilistic</b> <b>framework,</b> termed the Angle Estimation <b>Probabilistic</b> <b>Model</b> (AEPM), that offers precise angle estimations across extensive scenarios beyond walking. AEPM achieves an overall RMSE of 6.70 degrees, with an RMSE of 3.45 degrees in walking scenarios. Compared to the state of the art, AEPM has improved the prediction accuracy for walking by 11.31%. Our method can achieve seamless adaptation between different locomotion modes. Also, this model can be utilized to analyze the synergy between the knee and other joints. We reveal that the whole body movement has valuable information for knee movement, which can provide insights into designing sensors for prostheses. The code is available at <a href=https://github.com/penway/Beyond-Gait-AEPM>https://github.com/penway/Beyond-Gait-AEPM</a>.</p></p class="citation"></blockquote><h3 id=312--93218-using-neural-networks-to-model-hysteretic-kinematics-in-tendon-actuated-continuum-robots-yuan-wang-et-al-2024>(3/12 | 93/218) Using Neural Networks to Model Hysteretic Kinematics in Tendon-Actuated Continuum Robots (Yuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Wang, Max McCandless, Abdulhamit Donder, Giovanni Pittiglio, Behnam Moradkhani, Yash Chitalia, Pierre E. Dupont. (2024)<br><strong>Using Neural Networks to Model Hysteretic Kinematics in Tendon-Actuated Continuum Robots</strong><br><button class=copy-to-clipboard title="Using Neural Networks to Model Hysteretic Kinematics in Tendon-Actuated Continuum Robots" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07168v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07168v1.pdf filename=2404.07168v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability to accurately model mechanical hysteretic behavior in tendon-actuated continuum robots using deep learning approaches is a growing area of interest. In this paper, we investigate the hysteretic response of two types of tendon-actuated continuum robots and, ultimately, compare three types of neural network modeling approaches with both forward and inverse kinematic mappings: feedforward neural network (FNN), FNN with a history input buffer, and <b>long</b> <b>short-term</b> <b>memory</b> <b>(LSTM)</b> network. We seek to determine which model best captures temporal dependent behavior. We find that, depending on the robot&rsquo;s design, choosing different kinematic inputs can alter whether hysteresis is exhibited by the system. Furthermore, we present the results of the model fittings, revealing that, in contrast to the standard FNN, both FNN with a history input buffer and the <b>LSTM</b> model exhibit the capacity to model historical dependence with comparable performance in capturing rate-dependent hysteresis.</p></p class="citation"></blockquote><h3 id=412--94218-enhancing-safety-in-mixed-traffic-learning-based-modeling-and-efficient-control-of-autonomous-and-human-driven-vehicles-jie-wang-et-al-2024>(4/12 | 94/218) Enhancing Safety in Mixed Traffic: Learning-Based Modeling and Efficient Control of Autonomous and Human-Driven Vehicles (Jie Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Wang, Yash Vardhan Pant, Lei Zhao, Michał Antkiewicz, Krzysztof Czarnecki. (2024)<br><strong>Enhancing Safety in Mixed Traffic: Learning-Based Modeling and Efficient Control of Autonomous and Human-Driven Vehicles</strong><br><button class=copy-to-clipboard title="Enhancing Safety in Mixed Traffic: Learning-Based Modeling and Efficient Control of Autonomous and Human-Driven Vehicles" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06732v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06732v1.pdf filename=2404.06732v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasing presence of autonomous vehicles (AVs) on public roads, developing robust control strategies to navigate the uncertainty of human-driven vehicles (HVs) is crucial. This paper introduces an advanced method for modeling HV behavior, combining a first-principles model with <b>Gaussian</b> <b>process</b> (GP) learning to enhance velocity prediction accuracy and provide a measurable uncertainty. We validated this innovative HV model using real-world data from field experiments and applied it to develop a GP-enhanced model predictive control (GP-MPC) strategy. This strategy aims to improve safety in mixed vehicle platoons by integrating uncertainty assessment into distance constraints. Comparative <b>simulation</b> studies with a conventional model predictive control (MPC) approach demonstrated that our GP-MPC strategy ensures more reliable safe distancing and fosters efficient vehicular dynamics, achieving notably higher speeds within the platoon. By incorporating a sparse GP technique in HV modeling and adopting a dynamic GP prediction within the MPC framework, we significantly reduced the computation time of GP-MPC, marking it only 4.6% higher than that of the conventional MPC. This represents a substantial improvement, making the process about 100 times faster than our preliminary work without these approximations. Our findings underscore the effectiveness of learning-based HV modeling in enhancing both safety and operational efficiency in mixed-traffic environments, paving the way for more harmonious AV-HV interactions.</p></p class="citation"></blockquote><h3 id=512--95218-cbfkit-a-control-barrier-function-toolbox-for-robotics-applications-mitchell-black-et-al-2024>(5/12 | 95/218) CBFKIT: A Control Barrier Function Toolbox for Robotics Applications (Mitchell Black et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mitchell Black, Georgios Fainekos, Bardh Hoxha, Hideki Okamoto, Danil Prokhorov. (2024)<br><strong>CBFKIT: A Control Barrier Function Toolbox for Robotics Applications</strong><br><button class=copy-to-clipboard title="CBFKIT: A Control Barrier Function Toolbox for Robotics Applications" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07158v1.pdf filename=2404.07158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces CBFKit, a Python/ROS toolbox for safe robotics planning and control under uncertainty. The toolbox provides a general framework for designing control barrier functions for mobility systems within both deterministic and stochastic environments. It can be connected to the ROS open-source robotics middleware, allowing for the setup of multi-robot applications, encoding of environments and maps, and integrations with predictive motion planning algorithms. Additionally, it offers multiple CBF variations and algorithms for robot control. The CBFKit is demonstrated on the Toyota Human Support Robot (HSR) in both <b>simulation</b> and in physical experiments.</p></p class="citation"></blockquote><h3 id=612--96218-wild-visual-navigation-fast-traversability-learning-via-pre-trained-models-and-online-self-supervision-matías-mattamala-et-al-2024>(6/12 | 96/218) Wild Visual Navigation: Fast Traversability Learning via Pre-Trained Models and Online Self-Supervision (Matías Mattamala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matías Mattamala, Jonas Frey, Piotr Libera, Nived Chebrolu, Georg Martius, Cesar Cadena, Marco Hutter, Maurice Fallon. (2024)<br><strong>Wild Visual Navigation: Fast Traversability Learning via Pre-Trained Models and Online Self-Supervision</strong><br><button class=copy-to-clipboard title="Wild Visual Navigation: Fast Traversability Learning via Pre-Trained Models and Online Self-Supervision" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07110v1.pdf filename=2404.07110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Natural environments such as forests and grasslands are challenging for robotic navigation because of the false perception of rigid obstacles from high grass, twigs, or bushes. In this work, we present Wild Visual Navigation (WVN), an online <b>self-supervised</b> <b>learning</b> system for visual traversability estimation. The system is able to continuously adapt from a short human demonstration in the field, only using onboard sensing and computing. One of the key ideas to achieve this is the use of high-dimensional features from pre-trained <b>self-supervised</b> <b>models,</b> which implicitly encode semantic information that massively simplifies the learning task. Further, the development of an online scheme for supervision generator enables concurrent training and inference of the learned model in the wild. We demonstrate our approach through diverse real-world deployments in forests, parks, and grasslands. Our system is able to bootstrap the traversable terrain segmentation in less than 5 min of in-field training time, enabling the robot to navigate in complex, previously unseen outdoor terrains. Code: <a href=https://bit.ly/498b0CV>https://bit.ly/498b0CV</a> - Project page:https://bit.ly/3M6nMHH</p></p class="citation"></blockquote><h3 id=712--97218-laplass-latent-space-planning-for-stochastic-systems-marlyse-reeves-et-al-2024>(7/12 | 97/218) LaPlaSS: Latent Space Planning for Stochastic Systems (Marlyse Reeves et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marlyse Reeves, Brian C. Williams. (2024)<br><strong>LaPlaSS: Latent Space Planning for Stochastic Systems</strong><br><button class=copy-to-clipboard title="LaPlaSS: Latent Space Planning for Stochastic Systems" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Autoencoder, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07063v1.pdf filename=2404.07063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous mobile agents often operate in hazardous environments, necessitating an awareness of safety. These agents can have non-linear, stochastic dynamics that must be considered during planning to guarantee bounded risk. Most state of the art methods require closed-form dynamics to verify plan correctness and safety however modern robotic systems often have dynamics that are learned from data. Thus, there is a need to perform efficient trajectory planning with guarantees on risk for agents without known dynamics models. We propose a &ldquo;generate-and-test&rdquo; approach to risk-bounded planning in which a planner generates a candidate trajectory using an approximate linear dynamics model and a validator assesses the risk of the trajectory, computing additional safety constraints for the planner if the candidate does not satisfy the desired risk bound. To acquire the approximate model, we use a <b>variational</b> <b>autoencoder</b> to learn a latent linear dynamics model and encode the planning problem into the latent space to generate the candidate trajectory. The VAE also serves to sample trajectories around the candidate to use in the validator. We demonstrate that our algorithm, LaPlaSS, is able to generate trajectory plans with bounded risk for a real-world agent with learned dynamics and is an order of magnitude more efficient than the state of the art.</p></p class="citation"></blockquote><h3 id=812--98218-a-data-efficient-framework-for-learning-local-heuristics-rishi-veerapaneni-et-al-2024>(8/12 | 98/218) A Data Efficient Framework for Learning Local Heuristics (Rishi Veerapaneni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rishi Veerapaneni, Jonathan Park, Muhammad Suhail Saleem, Maxim Likhachev. (2024)<br><strong>A Data Efficient Framework for Learning Local Heuristics</strong><br><button class=copy-to-clipboard title="A Data Efficient Framework for Learning Local Heuristics" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06728v1.pdf filename=2404.06728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advent of machine learning, there have been several recent attempts to learn effective and generalizable heuristics. Local Heuristic A* (LoHA*) is one recent method that instead of learning the entire heuristic estimate, learns a &ldquo;local&rdquo; residual heuristic that estimates the cost to escape a region (Veerapaneni et al 2023). LoHA*, like other <b>supervised</b> <b>learning</b> methods, collects a dataset of target values by querying an oracle on many planning problems (in this case, local planning problems). This data collection process can become slow as the size of the local region increases or if the domain requires expensive collision checks. Our main insight is that when an A* search solves a start-goal planning problem it inherently ends up solving multiple local planning problems. We exploit this observation to propose an efficient data collection framework that does &lt;1/10th the amount of work (measured by expansions) to collect the same amount of data in comparison to baselines. This idea also enables us to run LoHA* in an online manner where we can iteratively collect data and improve our model while solving relevant start-goal tasks. We demonstrate the performance of our data collection and online framework on a 4D $(x, y, \theta, v)$ navigation domain.</p></p class="citation"></blockquote><h3 id=912--99218-fast-and-accurate-relative-motion-tracking-for-two-industrial-robots-honglu-he-et-al-2024>(9/12 | 99/218) Fast and Accurate Relative Motion Tracking for Two Industrial Robots (Honglu He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Honglu He, Chen-lung Lu, Glenn Saunders, Pinghai Yang, Jeffrey Schoonover, John Wason, Santiago Paternain, Agung Julius, John T. Wen. (2024)<br><strong>Fast and Accurate Relative Motion Tracking for Two Industrial Robots</strong><br><button class=copy-to-clipboard title="Fast and Accurate Relative Motion Tracking for Two Industrial Robots" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06687v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06687v1.pdf filename=2404.06687v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Industrial robotic applications such as spraying, welding, and additive manufacturing frequently require fast, accurate, and uniform motion along a 3D spatial curve. To increase process throughput, some manufacturers propose a dual-robot setup to overcome the speed limitation of a single robot. Industrial robot motion is programmed through waypoints connected by motion primitives (Cartesian linear and circular paths and linear joint paths at constant Cartesian speed). The actual robot motion is affected by the blending between these motion primitives and the pose of the robot (an outstretched/close to singularity pose tends to have larger path-tracking errors). Choosing the waypoints and the speed along each motion segment to achieve the performance requirement is challenging. At present, there is no automated solution, and laborious manual tuning by robot experts is needed to approach the desired performance. In this paper, we present a systematic three-step approach to designing and programming a dual-robot system to optimize system performance. The first step is to select the relative placement between the two robots based on the specified relative motion path. The second step is to select the relative waypoints and the motion primitives. The final step is to update the waypoints iteratively based on the actual relative motion. Waypoint iteration is first executed in <b>simulation</b> and then completed using the actual robots. For performance measures, we use the mean path speed subject to the relative position and orientation constraints and the path speed uniformity constraint. We have demonstrated the effectiveness of this method with ABB and FANUC robots on two challenging test curves. The performance improvement over the current industrial practice baseline is over 300%. Compared to the optimized single-arm case that we have previously reported, the improvement is over 14%.</p></p class="citation"></blockquote><h3 id=1012--100218-incorporating-explanations-into-human-machine-interfaces-for-trust-and-situation-awareness-in-autonomous-vehicles-shahin-atakishiyev-et-al-2024>(10/12 | 100/218) Incorporating Explanations into Human-Machine Interfaces for Trust and Situation Awareness in Autonomous Vehicles (Shahin Atakishiyev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shahin Atakishiyev, Mohammad Salameh, Randy Goebel. (2024)<br><strong>Incorporating Explanations into Human-Machine Interfaces for Trust and Situation Awareness in Autonomous Vehicles</strong><br><button class=copy-to-clipboard title="Incorporating Explanations into Human-Machine Interfaces for Trust and Situation Awareness in Autonomous Vehicles" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07383v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07383v1.pdf filename=2404.07383v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous vehicles often make complex decisions via machine learning-based predictive models applied to collected sensor data. While this combination of methods provides a foundation for real-time actions, self-driving behavior primarily remains opaque to end users. In this sense, explainability of real-time decisions is a crucial and natural requirement for building trust in autonomous vehicles. Moreover, as autonomous vehicles still cause serious traffic accidents for various reasons, timely conveyance of upcoming hazards to road users can help improve scene understanding and prevent potential risks. Hence, there is also a need to supply autonomous vehicles with user-friendly interfaces for effective human-machine teaming. Motivated by this problem, we study the role of <b>explainable</b> <b>AI</b> and human-machine interface jointly in building trust in vehicle autonomy. We first present a broad context of the explanatory human-machine systems with the &ldquo;3W1H&rdquo; (what, whom, when, how) approach. Based on these findings, we present a situation awareness framework for calibrating users&rsquo; trust in self-driving behavior. Finally, we perform an experiment on our framework, conduct a user study on it, and validate the empirical findings with hypothesis testing.</p></p class="citation"></blockquote><h3 id=1112--101218-reward-learning-from-suboptimal-demonstrations-with-applications-in-surgical-electrocautery-zohre-karimi-et-al-2024>(11/12 | 101/218) Reward Learning from Suboptimal Demonstrations with Applications in Surgical Electrocautery (Zohre Karimi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zohre Karimi, Shing-Hei Ho, Bao Thach, Alan Kuntz, Daniel S. Brown. (2024)<br><strong>Reward Learning from Suboptimal Demonstrations with Applications in Surgical Electrocautery</strong><br><button class=copy-to-clipboard title="Reward Learning from Suboptimal Demonstrations with Applications in Surgical Electrocautery" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07185v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07185v1.pdf filename=2404.07185v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automating robotic surgery via learning from demonstration (LfD) techniques is extremely challenging. This is because surgical tasks often involve sequential decision-making processes with complex interactions of physical objects and have low tolerance for mistakes. Prior works assume that all demonstrations are fully observable and optimal, which might not be practical in the real world. This paper introduces a sample-efficient method that learns a robust reward function from a limited amount of ranked suboptimal demonstrations consisting of partial-view point cloud observations. The method then learns a policy by optimizing the learned reward function using <b>reinforcement</b> <b>learning</b> (RL). We show that using a learned reward function to obtain a policy is more robust than pure imitation learning. We apply our approach on a physical surgical electrocautery task and demonstrate that our method can perform well even when the provided demonstrations are suboptimal and the observations are high-dimensional point clouds.</p></p class="citation"></blockquote><h3 id=1212--102218-deep-reinforcement-learning-for-mobile-robot-path-planning-hao-liu-et-al-2024>(12/12 | 102/218) Deep Reinforcement Learning for Mobile Robot Path Planning (Hao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Liu, Yi Shen, Shuangjiang Yu, Zijun Gao, Tong Wu. (2024)<br><strong>Deep Reinforcement Learning for Mobile Robot Path Planning</strong><br><button class=copy-to-clipboard title="Deep Reinforcement Learning for Mobile Robot Path Planning" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06974v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06974v1.pdf filename=2404.06974v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Path planning is an important problem with the the applications in many aspects, such as video games, robotics etc. This paper proposes a novel method to address the problem of Deep <b>Reinforcement</b> <b>Learning</b> (DRL) based path planning for a mobile robot. We design DRL-based algorithms, including reward functions, and parameter optimization, to avoid time-consuming work in a 2D environment. We also designed an Two-way search hybrid A* algorithm to improve the quality of local path planning. We transferred the designed algorithm to a simple embedded environment to test the computational load of the algorithm when running on a mobile robot. Experiments show that when deployed on a robot platform, the DRL-based algorithm in this article can achieve better planning results and consume less computing resources.</p></p class="citation"></blockquote><h2 id=csir-4>cs.IR (4)</h2><h3 id=14--103218-nfarec-a-negative-feedback-aware-recommender-model-xinfeng-wang-et-al-2024>(1/4 | 103/218) NFARec: A Negative Feedback-Aware Recommender Model (Xinfeng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinfeng Wang, Fumiyo Fukumoto, Jin Cui, Yoshimi Suzuki, Dongjin Yu. (2024)<br><strong>NFARec: A Negative Feedback-Aware Recommender Model</strong><br><button class=copy-to-clipboard title="NFARec: A Negative Feedback-Aware Recommender Model" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 63<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Convolution, Recommendation, Recommender System, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06900v1.pdf filename=2404.06900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>network</b> <b>(GNN)-based</b> models have been extensively studied for <b>recommendations,</b> as they can extract high-order collaborative signals accurately which is required for high-quality <b>recommender</b> <b>systems.</b> However, they neglect the valuable information gained through negative feedback in two aspects: (1) different users might hold opposite feedback on the same item, which hampers optimal information propagation in <b>GNNs,</b> and (2) even when an item vastly deviates from users&rsquo; preferences, they might still choose it and provide a negative rating. In this paper, we propose a negative feedback-aware <b>recommender</b> <b>model</b> (NFARec) that maximizes the leverage of negative feedback. To transfer information to multi-hop neighbors along an optimal path effectively, NFARec adopts a feedback-aware correlation that guides hypergraph <b>convolutions</b> (HGCs) to learn users&rsquo; structural representations. Moreover, NFARec incorporates an auxiliary task - predicting the feedback sentiment polarity (i.e., positive or negative) of the next interaction - based on the <b>Transformer</b> Hawkes Process. The task is beneficial for understanding users by learning the sentiment expressed in their previous sequential feedback patterns and predicting future interactions. Extensive experiments demonstrate that NFARec outperforms competitive baselines. Our source code and data are released at <a href=https://github.com/WangXFng/NFARec>https://github.com/WangXFng/NFARec</a>.</p></p class="citation"></blockquote><h3 id=24--104218-cadrec-contextualized-and-debiased-recommender-model-xinfeng-wang-et-al-2024>(2/4 | 104/218) CaDRec: Contextualized and Debiased Recommender Model (Xinfeng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinfeng Wang, Fumiyo Fukumoto, Jin Cui, Yoshimi Suzuki, Jiyi Li, Dongjin Yu. (2024)<br><strong>CaDRec: Contextualized and Debiased Recommender Model</strong><br><button class=copy-to-clipboard title="CaDRec: Contextualized and Debiased Recommender Model" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Node Embedding, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06895v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06895v1.pdf filename=2404.06895v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recommender models aimed at mining users&rsquo; behavioral patterns have raised great attention as one of the essential applications in daily life. Recent work on <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> or debiasing methods has attained remarkable gains. However, they still suffer from (1) over-smoothing <b>node</b> <b>embeddings</b> caused by recursive <b>convolutions</b> with <b>GNNs,</b> and (2) the skewed distribution of interactions due to popularity and user-individual biases. This paper proposes a contextualized and debiased recommender model (CaDRec). To overcome the over-smoothing issue, we explore a novel hypergraph <b>convolution</b> operator that can select effective neighbors during <b>convolution</b> by introducing both structural context and sequential context. To tackle the skewed distribution, we propose two strategies for disentangling interactions: (1) modeling individual biases to learn unbiased item embeddings, and (2) incorporating item popularity with positional encoding. Moreover, we mathematically show that the imbalance of the gradients to update item embeddings exacerbates the popularity bias, thus adopting regularization and weighting schemes as solutions. Extensive experiments on four datasets demonstrate the superiority of the CaDRec against state-of-the-art (SOTA) methods. Our source code and data are released at <a href=https://github.com/WangXFng/CaDRec>https://github.com/WangXFng/CaDRec</a>.</p></p class="citation"></blockquote><h3 id=34--105218-quati-a-brazilian-portuguese-information-retrieval-dataset-from-native-speakers-mirelle-bueno-et-al-2024>(3/4 | 105/218) Quati: A Brazilian Portuguese Information Retrieval Dataset from Native Speakers (Mirelle Bueno et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mirelle Bueno, Eduardo Seiti de Oliveira, Rodrigo Nogueira, Roberto A. Lotufo, Jayr Alencar Pereira. (2024)<br><strong>Quati: A Brazilian Portuguese Information Retrieval Dataset from Native Speakers</strong><br><button class=copy-to-clipboard title="Quati: A Brazilian Portuguese Information Retrieval Dataset from Native Speakers" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Information Retrieval, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06976v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06976v1.pdf filename=2404.06976v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite Portuguese being one of the most spoken languages in the world, there is a lack of high-quality <b>information</b> <b>retrieval</b> datasets in that language. We present Quati, a dataset specifically designed for the Brazilian Portuguese language. It comprises a collection of queries formulated by native speakers and a curated set of documents sourced from a selection of high-quality Brazilian Portuguese websites. These websites are frequented more likely by real users compared to those randomly scraped, ensuring a more representative and relevant corpus. To label the query-document pairs, we use a state-of-the-art <b>LLM,</b> which shows inter-annotator agreement levels comparable to human performance in our assessments. We provide a detailed description of our annotation methodology to enable others to create similar datasets for other languages, providing a cost-effective way of creating high-quality IR datasets with an arbitrary number of labeled documents per query. Finally, we evaluate a diverse range of open-source and commercial retrievers to serve as baseline systems. Quati is publicly available at <a href=https://huggingface.co/datasets/unicamp-dl/quati>https://huggingface.co/datasets/unicamp-dl/quati</a> and all scripts at <a href=https://github.com/unicamp-dl/quati>https://github.com/unicamp-dl/quati</a> .</p></p class="citation"></blockquote><h3 id=44--106218-transtarec-time-adaptive-translating-embedding-model-for-next-poi-recommendation-yiping-sun-2024>(4/4 | 106/218) TransTARec: Time-Adaptive Translating Embedding Model for Next POI Recommendation (Yiping Sun, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiping Sun. (2024)<br><strong>TransTARec: Time-Adaptive Translating Embedding Model for Next POI Recommendation</strong><br><button class=copy-to-clipboard title="TransTARec: Time-Adaptive Translating Embedding Model for Next POI Recommendation" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07096v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07096v1.pdf filename=2404.07096v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid growth of location acquisition technologies makes Point-of-Interest(POI) <b>recommendation</b> possible due to redundant user check-in records. In this paper, we focus on next POI <b>recommendation</b> in which next POI is based on previous POI. We observe that time plays an important role in next POI <b>recommendation</b> but is neglected in the recent proposed translating embedding methods. To tackle this shortage, we propose a time-adaptive translating embedding model (TransTARec) for next POI <b>recommendation</b> that naturally incorporates temporal influence, sequential dynamics, and user preference within a single component. Methodologically, we treat a (previous timestamp, user, next timestamp) triplet as a union translation vector and develop a neural-based fusion operation to fuse user preference and temporal influence. The superiority of TransTARec, which is confirmed by extensive experiments on real-world datasets, comes from not only the introduction of temporal influence but also the direct unification with user preference and sequential dynamics.</p></p class="citation"></blockquote><h2 id=cslg-30>cs.LG (30)</h2><h3 id=130--107218-forecasting-the-future-with-future-technologies-advancements-in-large-meteorological-models-hailong-shu-et-al-2024>(1/30 | 107/218) Forecasting the Future with Future Technologies: Advancements in Large Meteorological Models (Hailong Shu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hailong Shu, Yue Wang, Weiwei Song, Huichuang Guo, Zhen Song. (2024)<br><strong>Forecasting the Future with Future Technologies: Advancements in Large Meteorological Models</strong><br><button class=copy-to-clipboard title="Forecasting the Future with Future Technologies: Advancements in Large Meteorological Models" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, physics-ao-ph<br>Keyword Score: 63<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06668v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06668v1.pdf filename=2404.06668v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of meteorological forecasting has undergone a significant transformation with the integration of large models, especially those employing deep learning techniques. This paper reviews the advancements and applications of these models in weather prediction, emphasizing their role in transforming traditional forecasting methods. Models like FourCastNet, Pangu-Weather, GraphCast, ClimaX, and FengWu have made notable contributions by providing accurate, high-resolution forecasts, surpassing the capabilities of traditional Numerical Weather Prediction (NWP) models. These models utilize advanced neural network architectures, such as <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs),</b> <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs),</b> and <b>Transformers,</b> to process diverse meteorological data, enhancing predictive accuracy across various time scales and spatial resolutions. The paper addresses challenges in this domain, including data acquisition and computational demands, and explores future opportunities for model optimization and hardware advancements. It underscores the integration of artificial intelligence with conventional meteorological techniques, promising improved weather prediction accuracy and a significant contribution to addressing climate-related challenges. This synergy positions large models as pivotal in the evolving landscape of meteorological forecasting.</p></p class="citation"></blockquote><h3 id=230--108218-how-to-craft-backdoors-with-unlabeled-data-alone-yifei-wang-et-al-2024>(2/30 | 108/218) How to Craft Backdoors with Unlabeled Data Alone? (Yifei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Wang, Wenhan Ma, Yisen Wang. (2024)<br><strong>How to Craft Backdoors with Unlabeled Data Alone?</strong><br><button class=copy-to-clipboard title="How to Craft Backdoors with Unlabeled Data Alone?" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Clustering, Foundation Model, Mutual Information, Self-supervised Learning, Self-supervised Learning, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06694v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06694v1.pdf filename=2404.06694v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Relying only on unlabeled data, <b>Self-supervised</b> <b>learning</b> (SSL) can learn rich features in an economical and scalable way. As the drive-horse for building <b>foundation</b> <b>models,</b> SSL has received a lot of attention recently with wide applications, which also raises <b>security</b> concerns where backdoor attack is a major type of threat: if the released dataset is maliciously poisoned, backdoored SSL models can behave badly when triggers are injected to test samples. The goal of this work is to investigate this potential risk. We notice that existing backdoors all require a considerable amount of \emph{labeled} data that may not be available for SSL. To circumvent this limitation, we explore a more restrictive setting called no-label backdoors, where we only have access to the unlabeled data alone, where the key challenge is how to select the proper poison set without using label information. We propose two strategies for poison selection: <b>clustering-based</b> selection using pseudolabels, and contrastive selection derived from the <b>mutual</b> <b>information</b> principle. Experiments on CIFAR-10 and ImageNet-100 show that both no-label backdoors are effective on many SSL methods and outperform random poisoning by a large margin. Code will be available at <a href=https://github.com/PKU-ML/nlb>https://github.com/PKU-ML/nlb</a>.</p></p class="citation"></blockquote><h3 id=330--109218-advancing-real-time-pandemic-forecasting-using-large-language-models-a-covid-19-case-study-hongru-du-et-al-2024>(3/30 | 109/218) Advancing Real-time Pandemic Forecasting Using Large Language Models: A COVID-19 Case Study (Hongru Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongru Du, Jianan Zhao, Yang Zhao, Shaochong Xu, Xihong Lin, Yiran Chen, Lauren M. Gardner, Hao Frank Yang. (2024)<br><strong>Advancing Real-time Pandemic Forecasting Using Large Language Models: A COVID-19 Case Study</strong><br><button class=copy-to-clipboard title="Advancing Real-time Pandemic Forecasting Using Large Language Models: A COVID-19 Case Study" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 48<br>Keywords: Multi-modal, Representation Learning, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06962v1.pdf filename=2404.06962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Forecasting the short-term spread of an ongoing disease outbreak is a formidable challenge due to the complexity of contributing factors, some of which can be characterized through interlinked, multi-modality variables such as epidemiological time series data, viral biology, population demographics, and the intersection of public policy and human behavior. Existing forecasting model frameworks struggle with the multifaceted nature of relevant data and robust results translation, which hinders their performances and the provision of actionable insights for public health decision-makers. Our work introduces PandemicLLM, a novel framework with <b>multi-modal</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> that reformulates real-time forecasting of disease spread as a text <b>reasoning</b> problem, with the ability to incorporate real-time, complex, non-numerical information that previously unattainable in traditional forecasting models. This approach, through a unique AI-human cooperative <b>prompt</b> design and time series <b>representation</b> <b>learning,</b> encodes <b>multi-modal</b> data for <b>LLMs.</b> The model is applied to the COVID-19 pandemic, and trained to utilize textual public health policies, genomic surveillance, spatial, and epidemiological time series data, and is subsequently tested across all 50 states of the U.S. Empirically, PandemicLLM is shown to be a high-performing pandemic forecasting framework that effectively captures the impact of emerging variants and can provide timely and accurate predictions. The proposed PandemicLLM opens avenues for incorporating various pandemic-related data in heterogeneous formats and exhibits performance benefits over existing models. This study illuminates the potential of adapting <b>LLMs</b> and <b>representation</b> <b>learning</b> to enhance pandemic forecasting, illustrating how AI innovations can strengthen pandemic responses and crisis management in the future.</p></p class="citation"></blockquote><h3 id=430--110218-latim-longitudinal-representation-learning-in-continuous-time-models-to-predict-disease-progression-rachid-zeghlache-et-al-2024>(4/30 | 110/218) LaTiM: Longitudinal representation learning in continuous-time models to predict disease progression (Rachid Zeghlache et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rachid Zeghlache, Pierre-Henri Conze, Mostafa El Habib Daho, Yihao Li, Hugo Le Boité, Ramin Tadayoni, Pascal Massin, Béatrice Cochener, Alireza Rezaei, Ikram Brahim, Gwenolé Quellec, Mathieu Lamard. (2024)<br><strong>LaTiM: Longitudinal representation learning in continuous-time models to predict disease progression</strong><br><button class=copy-to-clipboard title="LaTiM: Longitudinal representation learning in continuous-time models to predict disease progression" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 45<br>Keywords: Continuous Time, Continuous Time, Data Augmentation, Representation Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07091v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07091v1.pdf filename=2404.07091v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work proposes a novel framework for analyzing disease progression using time-aware neural ordinary differential equations (NODE). We introduce a &ldquo;time-aware head&rdquo; in a framework trained through <b>self-supervised</b> <b>learning</b> (SSL) to leverage temporal information in latent space for <b>data</b> <b>augmentation.</b> This approach effectively integrates NODEs with SSL, offering significant performance improvements compared to traditional methods that lack explicit temporal integration. We demonstrate the effectiveness of our strategy for diabetic retinopathy progression prediction using the OPHDIAT database. Compared to the baseline, all NODE architectures achieve statistically significant improvements in area under the ROC curve (AUC) and Kappa metrics, highlighting the efficacy of pre-training with SSL-inspired approaches. Additionally, our framework promotes stable training for NODEs, a commonly encountered challenge in time-aware modeling.</p></p class="citation"></blockquote><h3 id=530--111218-global-contrastive-training-for-multimodal-electronic-health-records-with-language-supervision-yingbo-ma-et-al-2024>(5/30 | 111/218) Global Contrastive Training for Multimodal Electronic Health Records with Language Supervision (Yingbo Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingbo Ma, Suraj Kolla, Zhenhong Hu, Dhruv Kaliraman, Victoria Nolan, Ziyuan Guan, Yuanfang Ren, Brooke Armfield, Tezcan Ozrazgat-Baslanti, Jeremy A. Balch, Tyler J. Loftus, Parisa Rashidi, Azra Bihorac, Benjamin Shickel. (2024)<br><strong>Global Contrastive Training for Multimodal Electronic Health Records with Language Supervision</strong><br><button class=copy-to-clipboard title="Global Contrastive Training for Multimodal Electronic Health Records with Language Supervision" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 36<br>Keywords: Contrastive Learning, Multi-modal, Multi-modal, Transformer, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06723v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06723v1.pdf filename=2404.06723v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern electronic health records (EHRs) hold immense promise in tracking personalized patient health trajectories through sequential deep learning, owing to their extensive breadth, scale, and temporal granularity. Nonetheless, how to effectively leverage multiple modalities from EHRs poses significant challenges, given its complex characteristics such as high dimensionality, multimodality, sparsity, varied recording frequencies, and temporal irregularities. To this end, this paper introduces a novel <b>multimodal</b> <b>contrastive</b> <b>learning</b> framework, specifically focusing on medical time series and clinical notes. To tackle the challenge of sparsity and irregular time intervals in medical time series, the framework integrates temporal cross-attention <b>transformers</b> with a dynamic embedding and <b>tokenization</b> scheme for learning <b>multimodal</b> feature representations. To harness the interconnected relationships between medical time series and clinical notes, the framework equips a global <b>contrastive</b> <b>loss,</b> aligning a patient&rsquo;s <b>multimodal</b> feature representations with the corresponding discharge summaries. Since discharge summaries uniquely pertain to individual patients and represent a holistic view of the patient&rsquo;s hospital stay, machine learning models are led to learn discriminative <b>multimodal</b> features via global contrasting. Extensive experiments with a real-world EHR dataset demonstrated that our framework outperformed state-of-the-art approaches on the exemplar task of predicting the occurrence of nine postoperative complications for more than 120,000 major inpatient surgeries using <b>multimodal</b> data from UF health system split among three hospitals (UF Health Gainesville, UF Health Jacksonville, and UF Health Jacksonville-North).</p></p class="citation"></blockquote><h3 id=630--112218-vn-egnn-e3-equivariant-graph-neural-networks-with-virtual-nodes-enhance-protein-binding-site-identification-florian-sestak-et-al-2024>(6/30 | 112/218) VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification (Florian Sestak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian Sestak, Lisa Schneckenreiter, Johannes Brandstetter, Sepp Hochreiter, Andreas Mayr, Günter Klambauer. (2024)<br><strong>VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification</strong><br><button class=copy-to-clipboard title="VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, q-bio-BM<br>Keyword Score: 33<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07194v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07194v1.pdf filename=2404.07194v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Being able to identify regions within or around proteins, to which ligands can potentially bind, is an essential step to develop new drugs. Binding site identification methods can now profit from the availability of large amounts of 3D structures in protein structure databases or from AlphaFold predictions. Current binding site identification methods heavily rely on <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs),</b> usually designed to output E(3)-equivariant predictions. Such methods turned out to be very beneficial for physics-related tasks like binding energy or motion trajectory prediction. However, the performance of <b>GNNs</b> at binding site identification is still limited potentially due to the lack of dedicated nodes that model hidden geometric entities, such as binding pockets. In this work, we extend E(n)-Equivariant <b>Graph</b> <b>Neural</b> <b>Networks</b> (EGNNs) by adding virtual nodes and applying an extended message passing scheme. The virtual nodes in these <b>graphs</b> <b>are</b> <b>dedicated</b> quantities to learn representations of binding sites, which leads to improved predictive performance. In our experiments, we show that our proposed method VN-EGNN sets a new state-of-the-art at locating binding site centers on COACH420, HOLO4K and PDBbind2020.</p></p class="citation"></blockquote><h3 id=730--113218-fip-a-fixed-point-approach-for-causal-generative-modeling-meyer-scetbon-et-al-2024>(7/30 | 113/218) FiP: a Fixed-Point Approach for Causal Generative Modeling (Meyer Scetbon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meyer Scetbon, Joel Jennings, Agrin Hilmkil, Cheng Zhang, Chao Ma. (2024)<br><strong>FiP: a Fixed-Point Approach for Causal Generative Modeling</strong><br><button class=copy-to-clipboard title="FiP: a Fixed-Point Approach for Causal Generative Modeling" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 33<br>Keywords: Graph, Out-of-distribution, Zero-shot, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06969v1.pdf filename=2404.06969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modeling true world data-generating processes lies at the heart of empirical science. Structural Causal Models (SCMs) and their associated Directed Acyclic <b>Graphs</b> (DAGs) provide an increasingly popular answer to such problems by defining the causal generative process that transforms random noise into observations. However, learning them from observational data poses an ill-posed and NP-hard inverse problem in general. In this work, we propose a new and equivalent formalism that do not require DAGs to describe them, viewed as fixed-point problems on the causally ordered variables, and show three important cases where they can be uniquely recovered given the topological ordering (TO). To the best of our knowledge, we obtain the most general recovery results when the TO is known. Based on our theoretical findings, we design a two-stage causal generative model that first infers the causal order from observations in a <b>zero-shot</b> manner, thus by-passing the search, and then learns the generative fixed-point SCM on the ordered variables. To infer TOs from observations, we propose to amortize the learning of TOs on generated datasets by sequentially predicting the leaves of <b>graphs</b> seen during training. To learn fixed-point SCMs, we design a <b>transformer-based</b> architecture that exploits a new attention mechanism enabling the modeling of causal structures, and show that this parameterization is consistent with our formalism. Finally, we conduct an extensive evaluation of each method individually, and show that when combined, our model outperforms various baselines on generated <b>out-of-distribution</b> problems.</p></p class="citation"></blockquote><h3 id=830--114218-gansemble-for-small-and-imbalanced-data-sets-a-baseline-for-synthetic-microplastics-data-daniel-platnick-et-al-2024>(8/30 | 114/218) GANsemble for Small and Imbalanced Data Sets: A Baseline for Synthetic Microplastics Data (Daniel Platnick et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Platnick, Sourena Khanzadeh, Alireza Sadeghian, Richard Anthony Valenzano. (2024)<br><strong>GANsemble for Small and Imbalanced Data Sets: A Baseline for Synthetic Microplastics Data</strong><br><button class=copy-to-clipboard title="GANsemble for Small and Imbalanced Data Sets: A Baseline for Synthetic Microplastics Data" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Data Augmentation, Generative AI, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07356v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07356v1.pdf filename=2404.07356v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Microplastic particle ingestion or inhalation by humans is a problem of growing concern. Unfortunately, current research methods that use machine learning to understand their potential harms are obstructed by a lack of available <b>data.</b> <b>Deep</b> learning techniques in particular are challenged by such domains where only small or imbalanced <b>data</b> <b>sets</b> are available. Overcoming this challenge often involves oversampling underrepresented classes or augmenting the existing <b>data</b> <b>to</b> improve model performance. This paper proposes GANsemble: a two-module framework connecting <b>data</b> <b>augmentation</b> with conditional <b>generative</b> <b>adversarial</b> <b>networks</b> (cGANs) to generate class-conditioned synthetic <b>data.</b> <b>First,</b> the <b>data</b> <b>chooser</b> module automates augmentation strategy selection by searching for the best <b>data</b> <b>augmentation</b> strategy. Next, the cGAN module uses this strategy to train a cGAN for generating enhanced synthetic <b>data.</b> <b>We</b> experiment with the GANsemble framework on a small and imbalanced microplastics <b>data</b> <b>set.</b> A Microplastic-cGAN (MPcGAN) algorithm is introduced, and baselines for synthetic microplastics (SYMP) <b>data</b> <b>are</b> established in terms of Frechet Inception Distance (FID) and Inception Scores (IS). We also provide a synthetic microplastics filter (SYMP-Filter) algorithm to increase the quality of generated SYMP. Additionally, we show the best amount of oversampling with augmentation to fix class imbalance in small microplastics <b>data</b> <b>sets.</b> To our knowledge, this study is the first application of <b>generative</b> <b>AI</b> <b>to</b> synthetically create microplastics data.</p></p class="citation"></blockquote><h3 id=930--115218-sequential-decision-making-with-expert-demonstrations-under-unobserved-heterogeneity-vahid-balazadeh-et-al-2024>(9/30 | 115/218) Sequential Decision Making with Expert Demonstrations under Unobserved Heterogeneity (Vahid Balazadeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vahid Balazadeh, Keertana Chidambaram, Viet Nguyen, Rahul G. Krishnan, Vasilis Syrgkanis. (2024)<br><strong>Sequential Decision Making with Expert Demonstrations under Unobserved Heterogeneity</strong><br><button class=copy-to-clipboard title="Sequential Decision Making with Expert Demonstrations under Unobserved Heterogeneity" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Bandit Algorithm, Reinforcement Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07266v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07266v1.pdf filename=2404.07266v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of online sequential decision-making given auxiliary demonstrations from experts who made their decisions based on unobserved contextual information. These demonstrations can be viewed as solving related but slightly different tasks than what the learner faces. This setting arises in many application domains, such as self-driving cars, healthcare, and finance, where expert demonstrations are made using contextual information, which is not recorded in the data available to the learning agent. We model the problem as a <b>zero-shot</b> meta-reinforcement learning setting with an unknown task distribution and a Bayesian regret minimization objective, where the unobserved tasks are encoded as parameters with an unknown prior. We propose the Experts-as-Priors algorithm (ExPerior), a non-parametric empirical Bayes approach that utilizes the principle of maximum entropy to establish an informative prior over the learner&rsquo;s decision-making problem. This prior enables the application of any Bayesian approach for online decision-making, such as posterior sampling. We demonstrate that our strategy surpasses existing behaviour cloning and online algorithms for multi-armed <b>bandits</b> and <b>reinforcement</b> <b>learning,</b> showcasing the utility of our approach in leveraging expert demonstrations across different decision-making setups.</p></p class="citation"></blockquote><h3 id=1030--116218-toward-a-better-understanding-of-fourier-neural-operators-analysis-and-improvement-from-a-spectral-perspective-shaoxiang-qin-et-al-2024>(10/30 | 116/218) Toward a Better Understanding of Fourier Neural Operators: Analysis and Improvement from a Spectral Perspective (Shaoxiang Qin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaoxiang Qin, Fuyuan Lyu, Wenhui Peng, Dingyang Geng, Ju Wang, Naiping Gao, Xue Liu, Liangzhu Leon Wang. (2024)<br><strong>Toward a Better Understanding of Fourier Neural Operators: Analysis and Improvement from a Spectral Perspective</strong><br><button class=copy-to-clipboard title="Toward a Better Understanding of Fourier Neural Operators: Analysis and Improvement from a Spectral Perspective" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07200v1.pdf filename=2404.07200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In solving partial differential equations (PDEs), Fourier Neural Operators (FNOs) have exhibited notable effectiveness compared to <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs).</b> This paper presents clear empirical evidence through spectral analysis to elucidate the superiority of FNO over <b>CNNs:</b> FNO is significantly more capable of learning low-frequencies. This empirical evidence also unveils FNO&rsquo;s distinct low-frequency bias, which limits FNO&rsquo;s effectiveness in learning high-frequency information from PDE data. To tackle this challenge, we introduce SpecBoost, an ensemble learning framework that employs multiple FNOs to better capture high-frequency information. Specifically, a secondary FNO is utilized to learn the overlooked high-frequency information from the prediction residual of the initial FNO. Experiments demonstrate that SpecBoost noticeably enhances FNO&rsquo;s prediction accuracy on diverse PDE applications, achieving an up to 71% improvement.</p></p class="citation"></blockquote><h3 id=1130--117218-a-gauss-newton-approach-for-min-max-optimization-in-generative-adversarial-networks-neel-mishra-et-al-2024>(11/30 | 117/218) A Gauss-Newton Approach for Min-Max Optimization in Generative Adversarial Networks (Neel Mishra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Neel Mishra, Bamdev Mishra, Pratik Jawanpuria, Pawan Kumar. (2024)<br><strong>A Gauss-Newton Approach for Min-Max Optimization in Generative Adversarial Networks</strong><br><button class=copy-to-clipboard title="A Gauss-Newton Approach for Min-Max Optimization in Generative Adversarial Networks" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NA, cs.LG, math-NA, math-OC<br>Keyword Score: 30<br>Keywords: MNIST, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07172v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07172v1.pdf filename=2404.07172v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A novel first-order method is proposed for training <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs).</b> It modifies the Gauss-Newton method to approximate the min-max Hessian and uses the Sherman-Morrison inversion formula to calculate the inverse. The method corresponds to a fixed-point method that ensures necessary contraction. To evaluate its effectiveness, numerical experiments are conducted on various datasets commonly used in image generation tasks, such as <b>MNIST,</b> Fashion <b>MNIST,</b> CIFAR10, FFHQ, and LSUN. Our method is capable of generating high-fidelity images with greater diversity across multiple datasets. It also achieves the highest inception score for CIFAR10 among all compared methods, including state-of-the-art second-order methods. Additionally, its execution time is comparable to that of first-order min-max methods.</p></p class="citation"></blockquote><h3 id=1230--118218-what-needs-to-go-right-for-an-induction-head-a-mechanistic-study-of-in-context-learning-circuits-and-their-formation-aaditya-k-singh-et-al-2024>(12/30 | 118/218) What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation (Aaditya K. Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aaditya K. Singh, Ted Moskovitz, Felix Hill, Stephanie C. Y. Chan, Andrew M. Saxe. (2024)<br><strong>What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation</strong><br><button class=copy-to-clipboard title="What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Transformer, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07129v1.pdf filename=2404.07129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>In-context</b> <b>learning</b> is a powerful emergent ability in <b>transformer</b> models. Prior work in mechanistic interpretability has identified a circuit element that may be critical for <b>in-context</b> <b>learning</b> &ndash; the induction head (IH), which performs a match-and-copy operation. During training of large <b>transformers</b> on natural language data, IHs emerge around the same time as a notable phase change in the loss. Despite the robust evidence for IHs and this interesting coincidence with the phase change, relatively little is known about the diversity and emergence dynamics of IHs. Why is there more than one IH, and how are they dependent on each other? Why do IHs appear all of a sudden, and what are the subcircuits that enable them to emerge? We answer these questions by studying IH emergence dynamics in a controlled setting by training on synthetic data. In doing so, we develop and share a novel optogenetics-inspired causal framework for modifying activations throughout training. Using this framework, we delineate the diverse and additive nature of IHs. By clamping subsets of activations throughout training, we then identify three underlying subcircuits that interact to drive IH formation, yielding the phase change. Furthermore, these subcircuits shed light on data-dependent properties of formation, such as phase change timing, already showing the promise of this more in-depth understanding of subcircuits that need to &ldquo;go right&rdquo; for an induction head.</p></p class="citation"></blockquote><h3 id=1330--119218-towards-learning-stochastic-population-models-by-gradient-descent-justin-n-kreikemeyer-et-al-2024>(13/30 | 119/218) Towards Learning Stochastic Population Models by Gradient Descent (Justin N. Kreikemeyer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Justin N. Kreikemeyer, Philipp Andelfinger, Adelinde M. Uhrmacher. (2024)<br><strong>Towards Learning Stochastic Population Models by Gradient Descent</strong><br><button class=copy-to-clipboard title="Towards Learning Stochastic Population Models by Gradient Descent" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07049v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07049v1.pdf filename=2404.07049v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Increasing effort is put into the development of methods for learning mechanistic models from data. This task entails not only the accurate estimation of parameters, but also a suitable model structure. Recent work on the discovery of dynamical systems formulates this problem as a linear equation system. Here, we explore several <b>simulation-based</b> optimization approaches, which allow much greater freedom in the objective formulation and weaker conditions on the available data. We show that even for relatively small <b>stochastic</b> <b>population</b> <b>models,</b> simultaneous estimation of parameters and structure poses major challenges for optimization procedures. Particularly, we investigate the application of the local <b>stochastic</b> <b>gradient</b> <b>descent</b> method, commonly used for training machine learning models. We demonstrate accurate estimation of models but find that enforcing the inference of parsimonious, interpretable models drastically increases the difficulty. We give an outlook on how this challenge can be overcome.</p></p class="citation"></blockquote><h3 id=1430--120218-logit-calibration-and-feature-contrast-for-robust-federated-learning-on-non-iid-data-yu-qiao-et-al-2024>(14/30 | 120/218) Logit Calibration and Feature Contrast for Robust Federated Learning on Non-IID Data (Yu Qiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Qiao, Chaoning Zhang, Apurba Adhikary, Choong Seon Hong. (2024)<br><strong>Logit Calibration and Feature Contrast for Robust Federated Learning on Non-IID Data</strong><br><button class=copy-to-clipboard title="Logit Calibration and Feature Contrast for Robust Federated Learning on Non-IID Data" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Adversarial Learning, Federated Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06776v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06776v1.pdf filename=2404.06776v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) is a privacy-preserving distributed framework for collaborative model training on devices in edge networks. However, challenges arise due to vulnerability to <b>adversarial</b> <b>examples</b> (AEs) and the non-independent and identically distributed (non-IID) nature of data distribution among devices, hindering the deployment of adversarially robust and accurate learning models at the edge. While <b>adversarial</b> <b>training</b> (AT) is commonly acknowledged as an effective defense strategy against <b>adversarial</b> <b>attacks</b> in centralized training, we shed light on the adverse effects of directly applying AT in FL that can severely compromise accuracy, especially in non-IID challenges. Given this limitation, this paper proposes FatCC, which incorporates local logit \underline{C}alibration and global feature \underline{C}ontrast into the vanilla <b>federated</b> <b>adversarial</b> <b>training</b> (\underline{FAT}) process from both logit and feature perspectives. This approach can effectively enhance the <b>federated</b> <b>system&rsquo;s</b> robust accuracy (RA) and clean accuracy (CA). First, we propose logit calibration, where the logits are calibrated during local <b>adversarial</b> <b>updates,</b> thereby improving <b>adversarial</b> <b>robustness.</b> Second, FatCC introduces feature contrast, which involves a global alignment term that aligns each local representation with unbiased global features, thus further enhancing robustness and accuracy in <b>federated</b> <b>adversarial</b> <b>environments.</b> Extensive experiments across multiple datasets demonstrate that FatCC achieves comparable or superior performance gains in both CA and RA compared to other baselines.</p></p class="citation"></blockquote><h3 id=1530--121218-toward-cross-layer-energy-optimizations-in-machine-learning-systems-jae-won-chung-et-al-2024>(15/30 | 121/218) Toward Cross-Layer Energy Optimizations in Machine Learning Systems (Jae-Won Chung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jae-Won Chung, Mosharaf Chowdhury. (2024)<br><strong>Toward Cross-Layer Energy Optimizations in Machine Learning Systems</strong><br><button class=copy-to-clipboard title="Toward Cross-Layer Energy Optimizations in Machine Learning Systems" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AR, cs-DC, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Generative AI, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06675v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06675v1.pdf filename=2404.06675v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The enormous energy consumption of machine learning (ML) and <b>generative</b> <b>AI</b> workloads shows no sign of waning, taking a toll on operating costs, power delivery, and environmental sustainability. Despite a long line of research on energy-efficient hardware, we found that software plays a critical role in ML energy optimization through two recent works: Zeus and Perseus. This is especially true for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> because their model sizes and, therefore, energy demands are growing faster than hardware efficiency improvements. Therefore, we advocate for a cross-layer approach for energy optimizations in ML systems, where hardware provides architectural support that pushes energy-efficient software further, while software leverages and abstracts the hardware to develop techniques that bring hardware-agnostic energy-efficiency gains.</p></p class="citation"></blockquote><h3 id=1630--122218-rethinking-out-of-distribution-detection-for-reinforcement-learning-advancing-methods-for-evaluation-and-detection-linas-nasvytis-et-al-2024>(16/30 | 122/218) Rethinking Out-of-Distribution Detection for Reinforcement Learning: Advancing Methods for Evaluation and Detection (Linas Nasvytis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linas Nasvytis, Kai Sandbrink, Jakob Foerster, Tim Franzmeyer, Christian Schroeder de Witt. (2024)<br><strong>Rethinking Out-of-Distribution Detection for Reinforcement Learning: Advancing Methods for Evaluation and Detection</strong><br><button class=copy-to-clipboard title="Rethinking Out-of-Distribution Detection for Reinforcement Learning: Advancing Methods for Evaluation and Detection" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Out-of-distribution, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07099v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07099v1.pdf filename=2404.07099v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>reinforcement</b> <b>learning</b> (RL) algorithms have been successfully applied across numerous sequential decision-making problems, their generalization to unforeseen testing environments remains a significant concern. In this paper, we study the problem of <b>out-of-distribution</b> (OOD) detection in RL, which focuses on identifying situations at test time that RL agents have not encountered in their training environments. We first propose a clarification of terminology for OOD detection in RL, which aligns it with the literature from other machine learning domains. We then present new <b>benchmark</b> scenarios for OOD detection, which introduce anomalies with temporal autocorrelation into different components of the agent-environment loop. We argue that such scenarios have been understudied in the current literature, despite their relevance to real-world situations. Confirming our theoretical predictions, our experimental results suggest that state-of-the-art OOD detectors are not able to identify such anomalies. To address this problem, we propose a novel method for OOD detection, which we call DEXTER (Detection via Extraction of Time Series Representations). By treating environment observations as time series data, DEXTER extracts salient time series features, and then leverages an ensemble of isolation forest algorithms to detect anomalies. We find that DEXTER can reliably identify anomalies across <b>benchmark</b> scenarios, exhibiting superior performance compared to both state-of-the-art OOD detectors and high-dimensional changepoint detectors adopted from statistics.</p></p class="citation"></blockquote><h3 id=1730--123218-transfer-learning-via-latent-dependency-factor-for-estimating-pm-25-shrey-gupta-et-al-2024>(17/30 | 123/218) Transfer Learning via Latent Dependency Factor for Estimating PM 2.5 (Shrey Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shrey Gupta, Yongbee Park, Jianzhao Bi, Suyash Gupta, Andreas Züfle, Avani Wildani, Yang Liu. (2024)<br><strong>Transfer Learning via Latent Dependency Factor for Estimating PM 2.5</strong><br><button class=copy-to-clipboard title="Transfer Learning via Latent Dependency Factor for Estimating PM 2.5" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Autoencoder, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07308v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07308v1.pdf filename=2404.07308v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Air pollution, especially particulate matter 2.5 (PM 2.5), is a pressing concern for public health and is difficult to estimate in developing countries (data-poor regions) due to a lack of ground sensors. <b>Transfer</b> <b>learning</b> models can be leveraged to solve this problem, as they use alternate data sources to gain knowledge (i.e., data from data-rich regions). However, current <b>transfer</b> <b>learning</b> methodologies do not account for dependencies between the source and the target domains. We recognize this <b>transfer</b> <b>problem</b> as spatial <b>transfer</b> <b>learning</b> and propose a new feature named Latent Dependency Factor (LDF) that captures spatial and semantic dependencies of both domains and is subsequently added to the datasets. We generate LDF using a novel two-stage <b>autoencoder</b> model that learns from clusters of similar source and target domain data. Our experiments show that <b>transfer</b> <b>models</b> using LDF have a $19.34%$ improvement over the best-performing baselines. We additionally support our experiments with qualitative results.</p></p class="citation"></blockquote><h3 id=1830--124218-scaling-laws-for-data-filtering----data-curation-cannot-be-compute-agnostic-sachin-goyal-et-al-2024>(18/30 | 124/218) Scaling Laws for Data Filtering &ndash; Data Curation cannot be Compute Agnostic (Sachin Goyal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sachin Goyal, Pratyush Maini, Zachary C. Lipton, Aditi Raghunathan, J. Zico Kolter. (2024)<br><strong>Scaling Laws for Data Filtering &ndash; Data Curation cannot be Compute Agnostic</strong><br><button class=copy-to-clipboard title="Scaling Laws for Data Filtering -- Data Curation cannot be Compute Agnostic" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Scaling Law, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07177v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07177v1.pdf filename=2404.07177v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-language</b> models (VLMs) are trained for thousands of GPU hours on carefully curated web datasets. In recent times, data curation has gained prominence with several works developing strategies to retain &lsquo;high-quality&rsquo; subsets of &lsquo;raw&rsquo; scraped data. For instance, the LAION public dataset retained only 10% of the total crawled data. However, these strategies are typically developed agnostic of the available compute for training. In this paper, we first demonstrate that making filtering decisions independent of training compute is often suboptimal: the limited high-quality data rapidly loses its utility when repeated, eventually requiring the inclusion of &lsquo;unseen&rsquo; but &rsquo;lower-quality&rsquo; data. To address this quality-quantity tradeoff ($\texttt{QQT}$), we introduce neural <b>scaling</b> <b>laws</b> that account for the non-homogeneous nature of web data, an angle ignored in existing literature. Our <b>scaling</b> <b>laws</b> (i) characterize the $\textit{differing}$ &lsquo;utility&rsquo; of various quality subsets of web data; (ii) account for how utility diminishes for a data point at its &rsquo;nth&rsquo; repetition; and (iii) formulate the mutual interaction of various data pools when combined, enabling the estimation of model performance on a combination of multiple data pools without ever jointly training on them. Our key message is that data curation $\textit{cannot}$ be agnostic of the total compute that a model will be trained for. Our <b>scaling</b> <b>laws</b> allow us to curate the best possible pool for achieving top performance on Datacomp at various compute budgets, carving out a pareto-frontier for data curation. Code is available at <a href=https://github.com/locuslab/scaling_laws_data_filtering>https://github.com/locuslab/scaling_laws_data_filtering</a>.</p></p class="citation"></blockquote><h3 id=1930--125218-how-consistent-are-clinicians-evaluating-the-predictability-of-sepsis-disease-progression-with-dynamics-models-unnseo-park-et-al-2024>(19/30 | 125/218) How Consistent are Clinicians? Evaluating the Predictability of Sepsis Disease Progression with Dynamics Models (Unnseo Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Unnseo Park, Venkatesh Sivaraman, Adam Perer. (2024)<br><strong>How Consistent are Clinicians? Evaluating the Predictability of Sepsis Disease Progression with Dynamics Models</strong><br><button class=copy-to-clipboard title="How Consistent are Clinicians? Evaluating the Predictability of Sepsis Disease Progression with Dynamics Models" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-HC, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Recommendation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07148v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07148v1.pdf filename=2404.07148v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL) is a promising approach to generate treatment policies for sepsis patients in intensive care. While retrospective evaluation metrics show decreased mortality when these policies are followed, studies with clinicians suggest their <b>recommendations</b> are often spurious. We propose that these shortcomings may be due to lack of diversity in observed actions and outcomes in the training data, and we construct experiments to investigate the feasibility of predicting sepsis disease severity changes due to clinician actions. Preliminary results suggest incorporating action information does not significantly improve model performance, indicating that clinician actions may not be sufficiently variable to yield measurable effects on disease progression. We discuss the implications of these findings for optimizing sepsis treatment.</p></p class="citation"></blockquote><h3 id=2030--126218-crimealarm-towards-intensive-intent-dynamics-in-fine-grained-crime-prediction-kaixi-hu-et-al-2024>(20/30 | 126/218) CrimeAlarm: Towards Intensive Intent Dynamics in Fine-grained Crime Prediction (Kaixi Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaixi Hu, Lin Li, Qing Xie, Xiaohui Tao, Guandong Xu. (2024)<br><strong>CrimeAlarm: Towards Intensive Intent Dynamics in Fine-grained Crime Prediction</strong><br><button class=copy-to-clipboard title="CrimeAlarm: Towards Intensive Intent Dynamics in Fine-grained Crime Prediction" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Curriculum Learning, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06756v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06756v1.pdf filename=2404.06756v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Granularity and accuracy are two crucial factors for crime event prediction. Within fine-grained event classification, multiple criminal intents may alternately exhibit in preceding sequential events, and progress differently in next. Such intensive intent dynamics makes training models hard to capture unobserved intents, and thus leads to sub-optimal generalization performance, especially in the intertwining of numerous potential events. To capture comprehensive criminal intents, this paper proposes a fine-grained sequential crime prediction framework, CrimeAlarm, that equips with a novel mutual <b>distillation</b> strategy inspired by <b>curriculum</b> <b>learning.</b> During the early training phase, spot-shared criminal intents are captured through high-confidence sequence samples. In the later phase, spot-specific intents are gradually learned by increasing the contribution of low-confidence sequences. Meanwhile, the output probability distributions are reciprocally learned between prediction networks to model unobserved criminal intents. Extensive experiments show that CrimeAlarm outperforms state-of-the-art methods in terms of NDCG@5, with improvements of 4.51% for the NYC16 and 7.73% for the CHI18 in accuracy measures.</p></p class="citation"></blockquote><h3 id=2130--127218-knowledge-graphs-for-empirical-concept-retrieval-lenka-tětková-et-al-2024>(21/30 | 127/218) Knowledge graphs for empirical concept retrieval (Lenka Tětková et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lenka Tětková, Teresa Karen Scheidt, Maria Mandrup Fogh, Ellen Marie Gaunby Jørgensen, Finn Årup Nielsen, Lars Kai Hansen. (2024)<br><strong>Knowledge graphs for empirical concept retrieval</strong><br><button class=copy-to-clipboard title="Knowledge graphs for empirical concept retrieval" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 18<br>Keywords: Graph, Explainable AI, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07008v1.pdf filename=2404.07008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Concept-based <b>explainable</b> <b>AI</b> is promising as a tool to improve the understanding of complex models at the premises of a given user, viz.\ as a tool for personalized explainability. An important class of concept-based explainability methods is constructed with empirically defined concepts, indirectly defined through a set of positive and negative examples, as in the TCAV approach (Kim et al., 2018). While it is appealing to the user to avoid formal definitions of concepts and their operationalization, it can be challenging to establish relevant concept datasets. Here, we address this challenge using general <b>knowledge</b> <b>graphs</b> (such as, e.g., Wikidata or WordNet) for comprehensive concept definition and present a workflow for user-driven data collection in both text and image domains. The concepts derived from <b>knowledge</b> <b>graphs</b> are defined interactively, providing an opportunity for personalization and ensuring that the concepts reflect the user&rsquo;s intentions. We test the retrieved concept datasets on two concept-based explainability methods, namely concept activation vectors (CAVs) and concept activation regions (CARs) (Crabbe and van der Schaar, 2022). We show that CAVs and CARs based on these empirical concept datasets provide robust and accurate explanations. Importantly, we also find good alignment between the models&rsquo; representations of concepts and the structure of <b>knowledge</b> <b>graphs,</b> i.e., human representations. This supports our conclusion that <b>knowledge</b> <b>graph-based</b> concepts are relevant for XAI.</p></p class="citation"></blockquote><h3 id=2230--128218-deep-generative-sampling-in-the-dual-divergence-space-a-data-efficient--interpretative-approach-for-generative-ai-sahil-garg-et-al-2024>(22/30 | 128/218) Deep Generative Sampling in the Dual Divergence Space: A Data-efficient & Interpretative Approach for Generative AI (Sahil Garg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sahil Garg, Anderson Schneider, Anant Raj, Kashif Rasul, Yuriy Nevmyvaka, Sneihil Gopal, Amit Dhurandhar, Guillermo Cecchi, Irina Rish. (2024)<br><strong>Deep Generative Sampling in the Dual Divergence Space: A Data-efficient & Interpretative Approach for Generative AI</strong><br><button class=copy-to-clipboard title="Deep Generative Sampling in the Dual Divergence Space: A Data-efficient & Interpretative Approach for Generative AI" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CV, cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 13<br>Keywords: Generative AI, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07377v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07377v1.pdf filename=2404.07377v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Building on the remarkable achievements in <b>generative</b> <b>sampling</b> of natural images, we propose an innovative challenge, potentially overly ambitious, which involves generating <b>samples</b> <b>of</b> entire multivariate time series that resemble images. However, the statistical challenge lies in the small <b>sample</b> <b>size,</b> sometimes consisting of a few hundred subjects. This issue is especially problematic for deep <b>generative</b> <b>models</b> that follow the conventional approach of generating <b>samples</b> <b>from</b> a canonical distribution and then decoding or denoising them to match the true data distribution. In contrast, our method is grounded in information theory and aims to implicitly characterize the distribution of images, particularly the (global and local) dependency structure between pixels. We achieve this by empirically estimating its KL-divergence in the dual form with respect to the respective marginal distribution. This enables us to perform <b>generative</b> <b>sampling</b> directly in the optimized 1-D dual divergence space. Specifically, in the dual space, training <b>samples</b> <b>representing</b> the data distribution are embedded in the form of various clusters between two end points. In theory, any <b>sample</b> <b>embedded</b> between those two end points is in-distribution w.r.t. the data distribution. Our key idea for generating novel <b>samples</b> <b>of</b> images is to interpolate between the clusters via a walk as per gradients of the dual function w.r.t. the data dimensions. In addition to the data efficiency gained from direct sampling, we propose an algorithm that offers a significant reduction in <b>sample</b> <b>complexity</b> for estimating the divergence of the data distribution with respect to the marginal distribution. We provide strong theoretical guarantees along with an extensive empirical evaluation using many real-world datasets from diverse domains, establishing the superiority of our approach w.r.t. state-of-the-art deep learning methods.</p></p class="citation"></blockquote><h3 id=2330--129218-addressing-the-abstraction-and-reasoning-corpus-via-procedural-example-generation-michael-hodel-2024>(23/30 | 129/218) Addressing the Abstraction and Reasoning Corpus via Procedural Example Generation (Michael Hodel, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Hodel. (2024)<br><strong>Addressing the Abstraction and Reasoning Corpus via Procedural Example Generation</strong><br><button class=copy-to-clipboard title="Addressing the Abstraction and Reasoning Corpus via Procedural Example Generation" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07353v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07353v1.pdf filename=2404.07353v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work presents code to procedurally generate examples for the ARC training tasks. For each of the 400 tasks, an example generator following the transformation logic of the original examples was created. In effect, the assumed underlying distribution of examples for any given task was reverse engineered by implementing a means to sample from it. An attempt was made to cover an as large as reasonable space of possible examples for each task. That is, whenever the original examples of a given task may be limited in their diversity e.g. by having the dimensions of the grids, the set of symbols or number of objects constant or within tight bounds, even though the transformation does not require it, such constraints were lifted. Having access to not just a few examples per task, as the case for ARC, but instead very many, should enable a wide range of experiments that may be important stepping stones towards making leaps on the <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=2430--130218-error-mitigation-for-tdoa-uwb-indoor-localization-using-unsupervised-machine-learning-phuong-bich-duong-et-al-2024>(24/30 | 130/218) Error Mitigation for TDoA UWB Indoor Localization using Unsupervised Machine Learning (Phuong Bich Duong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Phuong Bich Duong, Ben Van Herbruggen, Arne Broering, Adnan Shahid, Eli De Poorter. (2024)<br><strong>Error Mitigation for TDoA UWB Indoor Localization using Unsupervised Machine Learning</strong><br><button class=copy-to-clipboard title="Error Mitigation for TDoA UWB Indoor Localization using Unsupervised Machine Learning" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-1, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Clustering, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06824v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06824v1.pdf filename=2404.06824v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Indoor positioning systems based on Ultra-wideband (UWB) technology are gaining recognition for their ability to provide cm-level localization accuracy. However, these systems often encounter challenges caused by dense multi-path fading, leading to positioning errors. To address this issue, in this letter, we propose a novel methodology for <b>unsupervised</b> anchor node selection using deep embedded <b>clustering</b> (DEC). Our approach uses an Auto Encoder (AE) before <b>clustering,</b> thereby better separating UWB features into separable clusters of UWB input signals. We furthermore investigate how to rank these clusters based on their cluster quality, allowing us to remove untrustworthy signals. Experimental results show the efficiency of our proposed method, demonstrating a significant 23.1% reduction in mean absolute error (MAE) compared to without anchor exclusion. Especially in the dense multi-path area, our algorithm achieves even more significant enhancements, reducing the MAE by 26.6% and the 95th percentile error by 49.3% compared to without anchor exclusion.</p></p class="citation"></blockquote><h3 id=2530--131218-toward-industrial-use-of-continual-learning--new-metrics-proposal-for-class-incremental-learning-konaté-mohamed-abbas-et-al-2024>(25/30 | 131/218) Toward industrial use of continual learning : new metrics proposal for class incremental learning (Konaté Mohamed Abbas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Konaté Mohamed Abbas, Anne-Françoise Yao, Thierry Chateau, Pierre Bouges. (2024)<br><strong>Toward industrial use of continual learning : new metrics proposal for class incremental learning</strong><br><button class=copy-to-clipboard title="Toward industrial use of continual learning : new metrics proposal for class incremental learning" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06972v1.pdf filename=2404.06972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate <b>continual</b> <b>learning</b> performance metrics used in class incremental learning strategies for <b>continual</b> <b>learning</b> (CL) using some high performing methods. We investigate especially mean task accuracy. First, we show that it lacks of expressiveness through some simple experiments to capture performance. We show that monitoring average tasks performance is over optimistic and can lead to misleading conclusions for future real life industrial uses. Then, we propose first a simple metric, Minimal Incremental Class Accuracy (MICA) which gives a fair and more useful evaluation of different <b>continual</b> <b>learning</b> methods. Moreover, in order to provide a simple way to easily compare different methods performance in <b>continual</b> <b>learning,</b> we derive another single scalar metric that take into account the learning performance variation as well as our newly introduced metric.</p></p class="citation"></blockquote><h3 id=2630--132218-register-your-forests-decision-tree-ensemble-optimization-by-explicit-cpu-register-allocation-daniel-biebert-et-al-2024>(26/30 | 132/218) Register Your Forests: Decision Tree Ensemble Optimization by Explicit CPU Register Allocation (Daniel Biebert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Biebert, Christian Hakert, Kuan-Hsun Chen, Jian-Jia Chen. (2024)<br><strong>Register Your Forests: Decision Tree Ensemble Optimization by Explicit CPU Register Allocation</strong><br><button class=copy-to-clipboard title="Register Your Forests: Decision Tree Ensemble Optimization by Explicit CPU Register Allocation" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06846v1.pdf filename=2404.06846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bringing high-level machine learning models to efficient and well-suited machine implementations often invokes a bunch of tools, e.g.~code generators, compilers, and optimizers. Along such tool chains, abstractions have to be applied. This leads to not optimally used CPU registers. This is a shortcoming, especially in resource constrained embedded setups. In this work, we present a <b>code</b> <b>generation</b> approach for decision tree ensembles, which produces machine assembly <b>code</b> <b>within</b> a single conversion step directly from the high-level model representation. Specifically, we develop various approaches to effectively allocate registers for the inference of decision tree ensembles. Extensive evaluations of the proposed method are conducted in comparison to the basic realization of C <b>code</b> <b>from</b> the high-level machine learning model and succeeding compilation. The results show that the performance of decision tree ensemble inference can be significantly improved (by up to $\approx1.6\times$), if the methods are applied carefully to the appropriate scenario.</p></p class="citation"></blockquote><h3 id=2730--133218-optimal-regret-with-limited-adaptivity-for-generalized-linear-contextual-bandits-ayush-sawarni-et-al-2024>(27/30 | 133/218) Optimal Regret with Limited Adaptivity for Generalized Linear Contextual Bandits (Ayush Sawarni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayush Sawarni, Nirjhar Das, Siddharth Barman, Gaurav Sinha. (2024)<br><strong>Optimal Regret with Limited Adaptivity for Generalized Linear Contextual Bandits</strong><br><button class=copy-to-clipboard title="Optimal Regret with Limited Adaptivity for Generalized Linear Contextual Bandits" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06831v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06831v2.pdf filename=2404.06831v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the generalized linear contextual <b>bandit</b> problem within the requirements of limited adaptivity. In this paper, we present two algorithms, B-GLinCB and RS-GLinCB, that address, respectively, two prevalent limited adaptivity models: batch learning with stochastic contexts and rare policy switches with adversarial contexts. For both these models, we establish essentially tight regret bounds. Notably, in the obtained bounds, we manage to eliminate a dependence on a key parameter $\kappa$, which captures the non-linearity of the underlying reward model. For our batch learning algorithm B-GLinCB, with $\Omega\left( \log{\log T} \right)$ batches, the regret scales as $\tilde{O}(\sqrt{T})$. Further, we establish that our rarely switching algorithm RS-GLinCB updates its policy at most $\tilde{O}(\log^2 T)$ times and achieves a regret of $\tilde{O}(\sqrt{T})$. Our approach for removing the dependence on $\kappa$ for generalized linear contextual <b>bandits</b> might be of independent interest.</p></p class="citation"></blockquote><h3 id=2830--134218-private-wasserstein-distance-with-random-noises-wenqian-li-et-al-2024>(28/30 | 134/218) Private Wasserstein Distance with Random Noises (Wenqian Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenqian Li, Haozhi Wang, Zhe Huang, Yan Pang. (2024)<br><strong>Private Wasserstein Distance with Random Noises</strong><br><button class=copy-to-clipboard title="Private Wasserstein Distance with Random Noises" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06787v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06787v1.pdf filename=2404.06787v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wasserstein distance is a principle measure of data divergence from a distributional standpoint. However, its application becomes challenging in the context of data privacy, where sharing raw data is restricted. Prior attempts have employed techniques like <b>Differential</b> <b>Privacy</b> or Federated optimization to approximate Wasserstein distance. Nevertheless, these approaches often lack accuracy and robustness against potential attack. In this study, we investigate the underlying triangular properties within the Wasserstein space, leading to a straightforward solution named TriangleWad. This approach enables the computation of Wasserstein distance between datasets stored across different entities. Notably, TriangleWad is 20 times faster, making raw data information truly invisible, enhancing resilience against attacks, and without sacrificing estimation accuracy. Through comprehensive experimentation across various tasks involving both image and text data, we demonstrate its superior performance and generalizations.</p></p class="citation"></blockquote><h3 id=2930--135218-disguised-copyright-infringement-of-latent-diffusion-models-yiwei-lu-et-al-2024>(29/30 | 135/218) Disguised Copyright Infringement of Latent Diffusion Models (Yiwei Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiwei Lu, Matthew Y. R. Yang, Zuoqiu Liu, Gautam Kamath, Yaoliang Yu. (2024)<br><strong>Disguised Copyright Infringement of Latent Diffusion Models</strong><br><button class=copy-to-clipboard title="Disguised Copyright Infringement of Latent Diffusion Models" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06737v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06737v2.pdf filename=2404.06737v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent <b>Diffusion</b> <b>Models</b> on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools. In this paper, we provide a better understanding of such disguised copyright infringement by uncovering the disguises generation algorithm, the revelation of the disguises, and importantly, how to detect them to augment the existing toolbox. Additionally, we introduce a broader notion of acknowledgment for comprehending such indirect access.</p></p class="citation"></blockquote><h3 id=3030--136218-sleepppg-net2-deep-learning-generalization-for-sleep-staging-from-photoplethysmography-shirel-attia-et-al-2024>(30/30 | 136/218) SleepPPG-Net2: Deep learning generalization for sleep staging from photoplethysmography (Shirel Attia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shirel Attia, Revital Shani Hershkovich, Alissa Tabakhov, Angeleene Ang, Sharon Haimov, Riva Tauman, Joachim A. Behar. (2024)<br><strong>SleepPPG-Net2: Deep learning generalization for sleep staging from photoplethysmography</strong><br><button class=copy-to-clipboard title="SleepPPG-Net2: Deep learning generalization for sleep staging from photoplethysmography" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, eess-SP<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06869v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06869v1.pdf filename=2404.06869v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background: Sleep staging is a fundamental component in the diagnosis of sleep disorders and the management of sleep health. Traditionally, this analysis is conducted in clinical settings and involves a time-consuming scoring procedure. Recent data-driven algorithms for sleep staging, using the photoplethysmogram (PPG) time series, have shown high performance on local test sets but lower performance on external datasets due to data drift. Methods: This study aimed to develop a generalizable deep learning model for the task of four class (wake, light, deep, and rapid eye movement (REM)) sleep staging from raw PPG physiological time-series. Six sleep datasets, totaling 2,574 patients recordings, were used. In order to create a more generalizable representation, we developed and evaluated a deep learning model called SleepPPG-Net2, which employs a multi-source domain training approach.SleepPPG-Net2 was <b>benchmarked</b> against two state-of-the-art models. Results: SleepPPG-Net2 showed consistently higher performance over <b>benchmark</b> approaches, with generalization performance (Cohen&rsquo;s kappa) improving by up to 19%. Performance disparities were observed in relation to age, sex, and sleep apnea severity. Conclusion: SleepPPG-Net2 sets a new standard for staging sleep from raw PPG time-series.</p></p class="citation"></blockquote><h2 id=cond-matmtrl-sci-2>cond-mat.mtrl-sci (2)</h2><h3 id=12--137218-bamboo-a-predictive-and-transferable-machine-learning-force-field-framework-for-liquid-electrolyte-development-sheng-gong-et-al-2024>(1/2 | 137/218) BAMBOO: a predictive and transferable machine learning force field framework for liquid electrolyte development (Sheng Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sheng Gong, Yumin Zhang, Zhenliang Mu, Zhichen Pu, Hongyi Wang, Zhiao Yu, Mengyi Chen, Tianze Zheng, Zhi Wang, Lifei Chen, Xiaojie Wu, Shaochen Shi, Weihao Gao, Wen Yan, Liang Xiang. (2024)<br><strong>BAMBOO: a predictive and transferable machine learning force field framework for liquid electrolyte development</strong><br><button class=copy-to-clipboard title="BAMBOO: a predictive and transferable machine learning force field framework for liquid electrolyte development" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-LG, physics-comp-ph<br>Keyword Score: 53<br>Keywords: Graph, Knowledge Distillation, Knowledge Distillation, Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07181v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07181v2.pdf filename=2404.07181v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the widespread applications of machine learning force field (MLFF) on solids and small molecules, there is a notable gap in applying MLFF to complex liquid electrolytes. In this work, we introduce BAMBOO (ByteDance AI Molecular <b>Simulation</b> Booster), a novel framework for molecular dynamics (MD) <b>simulations,</b> with a demonstration of its capabilities in the context of liquid electrolytes for lithium batteries. We design a physics-inspired <b>graph</b> equivariant <b>transformer</b> architecture as the backbone of BAMBOO to learn from quantum mechanical <b>simulations.</b> Additionally, we pioneer an ensemble <b>knowledge</b> <b>distillation</b> approach and apply it on MLFFs to improve the stability of MD <b>simulations.</b> Finally, we propose the density alignment algorithm to align BAMBOO with experimental measurements. BAMBOO demonstrates state-of-the-art accuracy in predicting key electrolyte properties such as density, viscosity, and ionic conductivity across various solvents and salt combinations. Our current model, trained on more than 15 chemical species, achieves the average density error of 0.01 g/cm$^3$ on various compositions compared with experimental data. Moreover, our model demonstrates transferability to molecules not included in the quantum mechanical dataset. We envision this work as paving the way to a &ldquo;universal MLFF&rdquo; capable of simulating properties of common organic liquids.</p></p class="citation"></blockquote><h3 id=22--138218-building-workflows-for-interactive-human-in-the-loop-automated-experiment-hae-in-stem-eels-utkarsh-pratiush-et-al-2024>(2/2 | 138/218) Building Workflows for Interactive Human in the Loop Automated Experiment (hAE) in STEM-EELS (Utkarsh Pratiush et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Utkarsh Pratiush, Kevin M. Roccapriore, Yongtao Liu, Gerd Duscher, Maxim Ziatdinov, Sergei V. Kalinin. (2024)<br><strong>Building Workflows for Interactive Human in the Loop Automated Experiment (hAE) in STEM-EELS</strong><br><button class=copy-to-clipboard title="Building Workflows for Interactive Human in the Loop Automated Experiment (hAE) in STEM-EELS" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-HC<br>Keyword Score: 10<br>Keywords: human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07381v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07381v1.pdf filename=2404.07381v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exploring the structural, chemical, and physical properties of matter on the nano- and atomic scales has become possible with the recent advances in aberration-corrected electron energy-loss spectroscopy (EELS) in scanning transmission electron microscopy (STEM). However, the current paradigm of STEM-EELS relies on the classical rectangular grid sampling, in which all surface regions are assumed to be of equal a priori interest. This is typically not the case for real-world scenarios, where phenomena of interest are concentrated in a small number of spatial locations. One of foundational problems is the discovery of nanometer- or atomic scale structures having specific signatures in EELS spectra. Here we systematically explore the hyperparameters controlling deep kernel learning (DKL) discovery workflows for STEM-EELS and identify the role of the local structural descriptors and acquisition functions on the experiment progression. In agreement with actual experiment, we observe that for certain parameter combinations the experiment path can be trapped in the local minima. We demonstrate the approaches for monitoring automated experiment in the real and feature space of the system and monitor knowledge acquisition of the DKL model. Based on these, we construct intervention strategies, thus defining human-in the loop automated experiment (hAE). This approach can be further extended to other techniques including 4D STEM and other forms of spectroscopic imaging.</p></p class="citation"></blockquote><h2 id=csse-2>cs.SE (2)</h2><h3 id=12--139218-beyond-random-inputs-a-novel-ml-based-hardware-fuzzing-mohamadreza-rostami-et-al-2024>(1/2 | 139/218) Beyond Random Inputs: A Novel ML-Based Hardware Fuzzing (Mohamadreza Rostami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamadreza Rostami, Marco Chilese, Shaza Zeitouni, Rahul Kande, Jeyavijayan Rajendran, Ahmad-Reza Sadeghi. (2024)<br><strong>Beyond Random Inputs: A Novel ML-Based Hardware Fuzzing</strong><br><button class=copy-to-clipboard title="Beyond Random Inputs: A Novel ML-Based Hardware Fuzzing" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AR, cs-CR, cs-LG, cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: Simulation, Simulator, ChatGPT, Large Language Model, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06856v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06856v1.pdf filename=2404.06856v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern computing systems heavily rely on hardware as the root of trust. However, their increasing complexity has given rise to <b>security-critical</b> vulnerabilities that cross-layer at-tacks can exploit. Traditional hardware vulnerability detection methods, such as random regression and formal verification, have limitations. Random regression, while scalable, is slow in exploring hardware, and formal verification techniques are often concerned with manual effort and state explosions. Hardware fuzzing has emerged as an effective approach to exploring and detecting <b>security</b> vulnerabilities in large-scale designs like modern processors. They outperform traditional methods regarding coverage, scalability, and efficiency. However, state-of-the-art fuzzers struggle to achieve comprehensive coverage of intricate hardware designs within a practical timeframe, often falling short of a 70% coverage threshold. We propose a novel ML-based hardware fuzzer, ChatFuzz, to address this challenge. Ourapproach leverages <b>LLMs</b> like <b>ChatGPT</b> to understand processor language, focusing on machine codes and generating assembly code sequences. RL is integrated to guide the input generation process by rewarding the inputs using code coverage metrics. We use the open-source RISCV-based RocketCore processor as our testbed. ChatFuzz achieves condition coverage rate of 75% in just 52 minutes compared to a state-of-the-art fuzzer, which requires a lengthy 30-hour window to reach a similar condition coverage. Furthermore, our fuzzer can attain 80% coverage when provided with a limited pool of 10 <b>simulation</b> instances/licenses within a 130-hour window. During this time, it conducted a total of 199K test cases, of which 6K produced discrepancies with the processor&rsquo;s golden model. Our analysis identified more than 10 unique mismatches, including two new bugs in the RocketCore and discrepancies from the RISC-V ISA Simulator.</p></p class="citation"></blockquote><h3 id=22--140218-research-artifacts-in-software-engineering-publications-status-and-trends-mugeng-liu-et-al-2024>(2/2 | 140/218) Research Artifacts in Software Engineering Publications: Status and Trends (Mugeng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mugeng Liu, Xiaolong Huang, Wei He, Yibing Xie, Jie M. Zhang, Xiang Jing, Zhenpeng Chen, Yun Ma. (2024)<br><strong>Research Artifacts in Software Engineering Publications: Status and Trends</strong><br><button class=copy-to-clipboard title="Research Artifacts in Software Engineering Publications: Status and Trends" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06852v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06852v1.pdf filename=2404.06852v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Software Engineering (SE) community has been embracing the open science policy and encouraging researchers to disclose artifacts in their publications. However, the status and trends of artifact practice and quality remain unclear, lacking insights on further improvement. In this paper, we present an empirical study to characterize the research artifacts in SE publications. Specifically, we manually collect 1,487 artifacts from all 2,196 papers published in top-tier SE conferences (ASE, FSE, ICSE, and ISSTA) from 2017 to 2022. We investigate the common practices (e.g., URL location and format, storage websites), maintenance activities (e.g., last update time and URL validity), popularity (e.g., the number of stars on GitHub and characteristics), and quality (e.g., documentation and code smell) of these artifacts. Based on our analysis, we reveal a rise in publications providing artifacts. The usage of Zenodo for sharing artifacts has significantly increased. However, artifacts stored in GitHub tend to receive few stars, indicating a limited influence on real-world SE applications. We <b>summarize</b> the results and provide suggestions to different stakeholders in conjunction with current guidelines.</p></p class="citation"></blockquote><h2 id=cscy-4>cs.CY (4)</h2><h3 id=14--141218-frontier-ai-ethics-anticipating-and-evaluating-the-societal-impacts-of-generative-agents-seth-lazar-2024>(1/4 | 141/218) Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Generative Agents (Seth Lazar, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seth Lazar. (2024)<br><strong>Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Generative Agents</strong><br><button class=copy-to-clipboard title="Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Generative Agents" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs.CY<br>Keyword Score: 46<br>Keywords: Generative AI, Multi-modal, Multi-modal, Unsupervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06750v1.pdf filename=2404.06750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Some have criticised <b>Generative</b> <b>AI</b> Systems for replicating the familiar pathologies of already widely-deployed AI systems. Other critics highlight how they foreshadow vastly more powerful future systems, which might threaten humanity&rsquo;s survival. The first group says there is nothing new here; the other looks through the present to a perhaps distant horizon. In this paper, I instead pay attention to what makes these particular systems distinctive: both their remarkable scientific achievement, and the most likely and consequential ways in which they will change society over the next five to ten years. In particular, I explore the potential societal impacts and normative questions raised by the looming prospect of <b>&lsquo;Generative</b> <b>Agents&rsquo;,</b> in which <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> form the executive centre of complex, tool-using AI systems that can take <b>unsupervised</b> sequences of actions towards some goal.</p></p class="citation"></blockquote><h3 id=24--142218-accuracy-of-a-large-language-model-in-distinguishing-anti--and-pro-vaccination-messages-on-social-media-the-case-of-human-papillomavirus-vaccination-soojong-kim-et-al-2024>(2/4 | 142/218) Accuracy of a Large Language Model in Distinguishing Anti- And Pro-vaccination Messages on Social Media: The Case of Human Papillomavirus Vaccination (Soojong Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soojong Kim, Kwanho Kim, Claire Wonjeong Jo. (2024)<br><strong>Accuracy of a Large Language Model in Distinguishing Anti- And Pro-vaccination Messages on Social Media: The Case of Human Papillomavirus Vaccination</strong><br><button class=copy-to-clipboard title="Accuracy of a Large Language Model in Distinguishing Anti- And Pro-vaccination Messages on Social Media: The Case of Human Papillomavirus Vaccination" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs.CY<br>Keyword Score: 40<br>Keywords: ChatGPT, Sentiment Analysis, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06731v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06731v1.pdf filename=2404.06731v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Objective. Vaccination has engendered a spectrum of public opinions, with social media acting as a crucial platform for health-related discussions. The emergence of artificial intelligence technologies, such as <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> offers a novel opportunity to efficiently investigate public discourses. This research assesses the accuracy of <b>ChatGPT,</b> a widely used and freely available service built upon an <b>LLM,</b> for <b>sentiment</b> <b>analysis</b> to discern different stances toward Human Papillomavirus (HPV) vaccination. Methods. Messages related to HPV vaccination were collected from social media supporting different message formats: Facebook (long format) and Twitter (short format). A selection of 1,000 human-evaluated messages was input into the <b>LLM,</b> which generated multiple response instances containing its classification results. Accuracy was measured for each message as the level of concurrence between human and machine decisions, ranging between 0 and 1. Results. Average accuracy was notably high when 20 response instances were used to determine the machine decision of each message: .882 (SE = .021) and .750 (SE = .029) for anti- and pro-vaccination long-form; .773 (SE = .027) and .723 (SE = .029) for anti- and pro-vaccination short-form, respectively. Using only three or even one instance did not lead to a severe decrease in accuracy. However, for long-form messages, the language model exhibited significantly lower accuracy in categorizing pro-vaccination messages than anti-vaccination ones. Conclusions. <b>ChatGPT</b> shows potential in analyzing public opinions on HPV vaccination using social media content. However, understanding the characteristics and limitations of a language model within specific public health contexts remains imperative.</p></p class="citation"></blockquote><h3 id=34--143218-leveraging-open-source-models-for-legal-language-modeling-and-analysis-a-case-study-on-the-indian-constitution-vikhyath-gupta-et-al-2024>(3/4 | 143/218) Leveraging open-source models for legal language modeling and analysis: a case study on the Indian constitution (Vikhyath Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vikhyath Gupta, Srinivasa Rao P. (2024)<br><strong>Leveraging open-source models for legal language modeling and analysis: a case study on the Indian constitution</strong><br><button class=copy-to-clipboard title="Leveraging open-source models for legal language modeling and analysis: a case study on the Indian constitution" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06751v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06751v1.pdf filename=2404.06751v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the use of open-source models has gained immense popularity in various fields, including legal language modelling and analysis. These models have proven to be highly effective in tasks such as summarizing legal documents, extracting key information, and even predicting case outcomes. This has revolutionized the legal industry, enabling lawyers, researchers, and policymakers to quickly access and analyse vast amounts of legal text, saving time and resources. This paper presents a novel approach to legal language modeling <b>(LLM)</b> and analysis using open-source models from Hugging Face. We leverage Hugging Face embeddings via LangChain and Sentence <b>Transformers</b> to develop an <b>LLM</b> tailored for legal texts. We then demonstrate the application of this model by extracting insights from the official Constitution of India. Our methodology involves preprocessing the data, splitting it into chunks, using ChromaDB and LangChainVectorStores, and employing the Google/Flan-T5-XXL model for analysis. The trained model is tested on the Indian Constitution, which is available in PDF format. Our findings suggest that our approach holds promise for efficient legal language processing and analysis.</p></p class="citation"></blockquote><h3 id=44--144218-racialethnic-categories-in-ai-and-algorithmic-fairness-why-they-matter-and-what-they-represent-jennifer-mickel-2024>(4/4 | 144/218) Racial/Ethnic Categories in AI and Algorithmic Fairness: Why They Matter and What They Represent (Jennifer Mickel, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jennifer Mickel. (2024)<br><strong>Racial/Ethnic Categories in AI and Algorithmic Fairness: Why They Matter and What They Represent</strong><br><button class=copy-to-clipboard title="Racial/Ethnic Categories in AI and Algorithmic Fairness: Why They Matter and What They Represent" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06717v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06717v1.pdf filename=2404.06717v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Racial diversity has become increasingly discussed within the AI and algorithmic <b>fairness</b> literature, yet little attention is focused on justifying the choices of racial categories and understanding how people are racialized into these chosen racial categories. Even less attention is given to how racial categories shift and how the racialization process changes depending on the context of a dataset or model. An unclear understanding of \textit{who} comprises the racial categories chosen and \textit{how} people are racialized into these categories can lead to varying interpretations of these categories. These varying interpretations can lead to harm when the understanding of racial categories and the racialization process is misaligned from the actual racialization process and racial categories used. Harm can also arise if the racialization process and racial categories used are irrelevant or do not exist in the context they are applied. In this paper, we make two contributions. First, we demonstrate how racial categories with unclear assumptions and little justification can lead to varying datasets that poorly represent groups obfuscated or unrepresented by the given racial categories and models that perform poorly on these groups. Second, we develop a framework, CIRCSheets, for documenting the choices and assumptions in choosing racial categories and the process of racialization into these categories to facilitate transparency in understanding the processes and assumptions made by dataset or model developers when selecting or using these racial categories.</p></p class="citation"></blockquote><h2 id=csai-5>cs.AI (5)</h2><h3 id=15--145218-zero-shot-logical-query-reasoning-on-any-knowledge-graph-mikhail-galkin-et-al-2024>(1/5 | 145/218) Zero-shot Logical Query Reasoning on any Knowledge Graph (Mikhail Galkin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mikhail Galkin, Jincheng Zhou, Bruno Ribeiro, Jian Tang, Zhaocheng Zhu. (2024)<br><strong>Zero-shot Logical Query Reasoning on any Knowledge Graph</strong><br><button class=copy-to-clipboard title="Zero-shot Logical Query Reasoning on any Knowledge Graph" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 43<br>Keywords: Graph, Fine-tuning, Knowledge Graph, Knowledge Graph, Zero-shot, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07198v1.pdf filename=2404.07198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Complex logical query answering (CLQA) in <b>knowledge</b> <b>graphs</b> <b>(KGs)</b> goes beyond simple <b>KG</b> completion and aims at answering compositional queries comprised of multiple projections and logical operations. Existing CLQA methods that learn parameters bound to certain entity or relation vocabularies can only be applied to the <b>graph</b> they are trained on which requires substantial training time before being deployed on a new <b>graph.</b> Here we present UltraQuery, an inductive <b>reasoning</b> model that can <b>zero-shot</b> answer logical queries on any <b>KG.</b> The core idea of UltraQuery is to derive both projections and logical operations as vocabulary-independent functions which generalize to new entities and relations in any <b>KG.</b> With the projection operation initialized from a pre-trained inductive <b>KG</b> <b>reasoning</b> model, UltraQuery can solve CLQA on any <b>KG</b> even if it is only <b>finetuned</b> on a single dataset. Experimenting on 23 datasets, UltraQuery in the <b>zero-shot</b> inference mode shows competitive or better query answering performance than best available baselines and sets a new state of the art on 14 of them.</p></p class="citation"></blockquote><h3 id=25--146218-towards-a-game-theoretic-understanding-of-explanation-based-membership-inference-attacks-kavita-kumari-et-al-2024>(2/5 | 146/218) Towards a Game-theoretic Understanding of Explanation-based Membership Inference Attacks (Kavita Kumari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kavita Kumari, Murtuza Jadliwala, Sumit Kumar Jha, Anindya Maiti. (2024)<br><strong>Towards a Game-theoretic Understanding of Explanation-based Membership Inference Attacks</strong><br><button class=copy-to-clipboard title="Towards a Game-theoretic Understanding of Explanation-based Membership Inference Attacks" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-GT, cs.AI<br>Keyword Score: 35<br>Keywords: Black Box, Continuous Time, Continuous Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07139v1.pdf filename=2404.07139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model explanations improve the transparency of <b>black-box</b> <b>machine</b> learning (ML) models and their decisions; however, they can also be exploited to carry out privacy threats such as membership inference attacks (MIA). Existing works have only analyzed MIA in a single &ldquo;what if&rdquo; interaction scenario between an adversary and the target ML model; thus, it does not discern the factors impacting the capabilities of an adversary in launching MIA in repeated interaction settings. Additionally, these works rely on assumptions about the adversary&rsquo;s knowledge of the target model&rsquo;s structure and, thus, do not guarantee the optimality of the predefined threshold required to distinguish the members from non-members. In this paper, we delve into the domain of explanation-based threshold attacks, where the adversary endeavors to carry out MIA attacks by leveraging the variance of explanations through iterative interactions with the system comprising of the target ML model and its corresponding explanation method. We model such interactions by employing a <b>continuous-time</b> <b>stochastic</b> signaling game framework. In our framework, an adversary plays a stopping game, interacting with the system (having imperfect information about the type of an adversary, i.e., honest or malicious) to obtain explanation variance information and computing an optimal threshold to determine the membership of a datapoint accurately. First, we propose a sound mathematical formulation to prove that such an optimal threshold exists, which can be used to launch MIA. Then, we characterize the conditions under which a unique Markov perfect equilibrium (or steady state) exists in this dynamic system. By means of a comprehensive set of <b>simulations</b> of the proposed game model, we assess different factors that can impact the capability of an adversary to launch MIA in such repeated interaction settings.</p></p class="citation"></blockquote><h3 id=35--147218-learn-from-failure-fine-tuning-llms-with-trial-and-error-data-for-intuitionistic-propositional-logic-proving-chenyang-an-et-al-2024>(3/5 | 147/218) Learn from Failure: Fine-Tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving (Chenyang An et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenyang An, Zhibo Chen, Qihao Ye, Emily First, Letian Peng, Jiayun Zhang, Zihan Wang, Sorin Lerner, Jingbo Shang. (2024)<br><strong>Learn from Failure: Fine-Tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving</strong><br><button class=copy-to-clipboard title="Learn from Failure: Fine-Tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LO, cs.AI<br>Keyword Score: 20<br>Keywords: Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07382v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07382v1.pdf filename=2404.07382v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in Automated Theorem Proving have shown the effectiveness of leveraging a (large) language model that generates tactics (i.e. proof steps) to search through proof states. The current model, while trained solely on successful proof paths, faces a discrepancy at the inference stage, as it must sample and try various tactics at each proof state until finding success, unlike its training which does not incorporate learning from failed attempts. Intuitively, a tactic that leads to a failed search path would indicate that similar tactics should receive less attention during the following trials. In this paper, we demonstrate the benefit of training models that additionally learn from failed search paths. Facing the lack of such trial-and-error data in existing open-source theorem-proving datasets, we curate a dataset on intuitionistic propositional logic theorems and formalize it in Lean, such that we can reliably check the correctness of proofs. We compare our model trained on relatively short trial-and-error information (TrialMaster) with models trained only on the correct paths and discover that the former solves more unseen theorems with lower trial searches.</p></p class="citation"></blockquote><h3 id=45--148218-a-survey-on-the-integration-of-generative-ai-for-critical-thinking-in-mobile-networks-athanasios-karapantelakis-et-al-2024>(4/5 | 148/218) A Survey on the Integration of Generative AI for Critical Thinking in Mobile Networks (Athanasios Karapantelakis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Athanasios Karapantelakis, Alexandros Nikou, Ajay Kattepur, Jean Martins, Leonid Mokrushin, Swarup Kumar Mohalik, Marin Orlic, Aneta Vulgarakis Feljan. (2024)<br><strong>A Survey on the Integration of Generative AI for Critical Thinking in Mobile Networks</strong><br><button class=copy-to-clipboard title="A Survey on the Integration of Generative AI for Critical Thinking in Mobile Networks" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Generative AI, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06946v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06946v1.pdf filename=2404.06946v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the near future, mobile networks are expected to broaden their services and coverage to accommodate a larger user base and diverse user needs. Thus, they will increasingly rely on artificial intelligence (AI) to manage network operation and control costs, undertaking complex decision-making roles. This shift will necessitate the application of techniques that incorporate critical thinking abilities, including <b>reasoning</b> and planning. Symbolic AI techniques already facilitate critical thinking based on existing knowledge. Yet, their use in telecommunications is hindered by the high cost of mostly manual curation of this knowledge and high computational complexity of <b>reasoning</b> tasks. At the same time, there is a spurt of innovations in industries such as telecommunications due to <b>Generative</b> <b>AI</b> (GenAI) technologies, operating independently of human-curated knowledge. However, their capacity for critical thinking remains uncertain. This paper aims to address this gap by examining the current status of GenAI algorithms with critical thinking capabilities and investigating their potential applications in telecom networks. Specifically, the aim of this study is to offer an introduction to the potential utilization of GenAI for critical thinking techniques in mobile networks, while also establishing a foundation for future research.</p></p class="citation"></blockquote><h3 id=55--149218-causal-unit-selection-using-tractable-arithmetic-circuits-haiying-huang-et-al-2024>(5/5 | 149/218) Causal Unit Selection using Tractable Arithmetic Circuits (Haiying Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haiying Huang, Adnan Darwiche. (2024)<br><strong>Causal Unit Selection using Tractable Arithmetic Circuits</strong><br><button class=copy-to-clipboard title="Causal Unit Selection using Tractable Arithmetic Circuits" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI, stat-ME<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06681v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06681v1.pdf filename=2404.06681v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The unit selection problem aims to find objects, called units, that optimize a causal objective function which describes the objects&rsquo; behavior in a causal context (e.g., selecting customers who are about to churn but would most likely change their mind if encouraged). While early studies focused mainly on bounding a specific class of <b>counterfactual</b> objective functions using data, more recent work allows one to find optimal units exactly by reducing the causal objective to a classical objective on a meta-model, and then applying a variant of the classical Variable Elimination (VE) algorithm to the meta-model &ndash; assuming a fully specified causal model is available. In practice, however, finding optimal units using this approach can be very expensive because the used VE algorithm must be exponential in the constrained treewidth of the meta-model, which is larger and denser than the original model. We address this computational challenge by introducing a new approach for unit selection that is not necessarily limited by the constrained treewidth. This is done through compiling the meta-model into a special class of tractable arithmetic circuits that allows the computation of optimal units in time linear in the circuit size. We finally present empirical results on random causal models that show order-of-magnitude speedups based on the proposed method for solving unit selection.</p></p class="citation"></blockquote><h2 id=cset-1>cs.ET (1)</h2><h3 id=11--150218-fast-system-technology-co-optimization-framework-for-emerging-technology-based-on-graph-neural-networks-tianliang-ma-et-al-2024>(1/1 | 150/218) Fast System Technology Co-Optimization Framework for Emerging Technology Based on Graph Neural Networks (Tianliang Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianliang Ma, Guangxi Fan, Xuguang Sun, Zhihui Deng, Kainlu Low, Leilai Shao. (2024)<br><strong>Fast System Technology Co-Optimization Framework for Emerging Technology Based on Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Fast System Technology Co-Optimization Framework for Emerging Technology Based on Graph Neural Networks" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.ET<br>Categories: cs-AI, cs-ET, cs.ET<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06939v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06939v1.pdf filename=2404.06939v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a fast system technology co-optimization (STCO) framework that optimizes power, performance, and area (PPA) for next-generation IC design, addressing the challenges and opportunities presented by novel materials and device architectures. We focus on accelerating the technology level of STCO using AI techniques, by employing <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)-based</b> approaches for both TCAD <b>simulation</b> and cell library characterization, which are interconnected through a unified compact model, collectively achieving over a 100X speedup over traditional methods. These advancements enable comprehensive STCO iterations with runtime speedups ranging from 1.9X to 14.1X and supports both emerging and traditional technologies.</p></p class="citation"></blockquote><h2 id=cshc-9>cs.HC (9)</h2><h3 id=19--151218-biscuit-scaffolding-llm-generated-code-with-ephemeral-uis-in-computational-notebooks-ruijia-cheng-et-al-2024>(1/9 | 151/218) BISCUIT: Scaffolding LLM-Generated Code with Ephemeral UIs in Computational Notebooks (Ruijia Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruijia Cheng, Titus Barik, Alan Leung, Fred Hohman, Jeffrey Nichols. (2024)<br><strong>BISCUIT: Scaffolding LLM-Generated Code with Ephemeral UIs in Computational Notebooks</strong><br><button class=copy-to-clipboard title="BISCUIT: Scaffolding LLM-Generated Code with Ephemeral UIs in Computational Notebooks" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: Code Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07387v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07387v1.pdf filename=2404.07387v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Novices frequently engage with machine learning tutorials in computational notebooks and have been adopting <b>code</b> <b>generation</b> technologies based on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> However, they encounter difficulties in understanding and working with <b>code</b> <b>produced</b> by <b>LLMs.</b> To mitigate these challenges, we introduce a novel workflow into computational notebooks that augments <b>LLM-based</b> <b>code</b> <b>generation</b> with an additional ephemeral UI step, offering users UI-based scaffolds as an intermediate stage between user <b>prompts</b> and <b>code</b> <b>generation.</b> We present this workflow in BISCUIT, an extension for JupyterLab that provides users with ephemeral UIs generated by <b>LLMs</b> based on the context of their <b>code</b> <b>and</b> intentions, scaffolding users to understand, guide, and explore with <b>LLM-generated</b> <b>code.</b> <b>Through</b> 10 user studies where novices used BISCUIT for machine learning tutorials, we discover that BISCUIT offers user semantic representation of <b>code</b> <b>to</b> aid their understanding, reduces the complexity of <b>prompt</b> engineering, and creates a playground for users to explore different variables and iterate on their ideas. We discuss the implications of our findings for UI-centric interactive paradigm in <b>code</b> <b>generation</b> <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=29--152218-worddecipher-enhancing-digital-workspace-communication-with-explainable-ai-for-non-native-english-speakers-yuexi-chen-et-al-2024>(2/9 | 152/218) WordDecipher: Enhancing Digital Workspace Communication with Explainable AI for Non-native English Speakers (Yuexi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuexi Chen, Zhicheng Liu. (2024)<br><strong>WordDecipher: Enhancing Digital Workspace Communication with Explainable AI for Non-native English Speakers</strong><br><button class=copy-to-clipboard title="WordDecipher: Enhancing Digital Workspace Communication with Explainable AI for Non-native English Speakers" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CL, cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: Explainable AI, Large Language Model, Large Language Model, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07005v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07005v1.pdf filename=2404.07005v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Non-native English speakers (NNES) face challenges in digital workspace communication (e.g., emails, Slack messages), often inadvertently translating expressions from their native languages, which can lead to awkward or incorrect usage. Current AI-assisted writing tools are equipped with fluency enhancement and rewriting suggestions; however, NNES may struggle to grasp the subtleties among various expressions, making it challenging to choose the one that accurately reflects their intent. Such challenges are exacerbated in high-stake text-based communications, where the absence of non-verbal cues heightens the risk of misinterpretation. By leveraging the latest advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> and <b>word</b> <b>embeddings,</b> we propose WordDecipher, an <b>explainable</b> <b>AI-assisted</b> writing tool to enhance digital workspace communication for NNES. WordDecipher not only identifies the perceived social intentions detected in users&rsquo; writing, but also generates rewriting suggestions aligned with users&rsquo; intended messages, either numerically or by inferring from users&rsquo; writing in their native language. Then, WordDecipher provides an overview of nuances to help NNES make selections. Through a usage scenario, we demonstrate how WordDecipher can significantly enhance an NNES&rsquo;s ability to communicate her request, showcasing its potential to transform workspace communication for NNES.</p></p class="citation"></blockquote><h3 id=39--153218-we-need-structured-output-towards-user-centered-constraints-on-large-language-model-output-michael-xieyang-liu-et-al-2024>(3/9 | 153/218) &lsquo;We Need Structured Output&rsquo;: Towards User-centered Constraints on Large Language Model Output (Michael Xieyang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Xieyang Liu, Frederick Liu, Alexander J. Fiannaca, Terry Koo, Lucas Dixon, Michael Terry, Carrie J. Cai. (2024)<br><strong>&lsquo;We Need Structured Output&rsquo;: Towards User-centered Constraints on Large Language Model Output</strong><br><button class=copy-to-clipboard title="'We Need Structured Output': Towards User-centered Constraints on Large Language Model Output" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07362v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07362v1.pdf filename=2404.07362v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> can produce creative and diverse responses. However, to integrate them into current developer workflows, it is essential to constrain their outputs to follow specific formats or standards. In this work, we surveyed 51 experienced industry professionals to understand the range of scenarios and motivations driving the need for output constraints from a user-centered perspective. We identified 134 concrete use cases for constraints at two levels: low-level, which ensures the output adhere to a structured format and an appropriate length, and high-level, which requires the output to follow semantic and stylistic guidelines without hallucination. Critically, applying output constraints could not only streamline the currently repetitive process of developing, testing, and integrating <b>LLM</b> <b>prompts</b> for developers, but also enhance the user experience of <b>LLM-powered</b> features and applications. We conclude with a discussion on user preferences and needs towards articulating intended constraints for <b>LLMs,</b> alongside an initial design for a constraint prototyping tool.</p></p class="citation"></blockquote><h3 id=49--154218-mixed-reality-heritage-performance-as-a-decolonising-tool-for-heritage-sites-mariza-dima-et-al-2024>(4/9 | 154/218) Mixed Reality Heritage Performance As a Decolonising Tool for Heritage Sites (Mariza Dima et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mariza Dima, Damon Daylamani-Zad, Vangelis Lympouridis. (2024)<br><strong>Mixed Reality Heritage Performance As a Decolonising Tool for Heritage Sites</strong><br><button class=copy-to-clipboard title="Mixed Reality Heritage Performance As a Decolonising Tool for Heritage Sites" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Augmented Reality (AR), Mixed Reality (MR), Mixed Reality (MR)<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07348v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07348v1.pdf filename=2404.07348v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we introduce two world-first <b>Mixed</b> <b>Reality</b> <b>(MR)</b> experiences that fuse smart <b>AR</b> glasses and live theatre and take place in a heritage site with the purpose to reveal the site&rsquo;s hidden and difficult histories about slavery. We term these unique general audience experiences <b>Mixed</b> <b>Reality</b> Heritage Performances (MRHP). Along with the development of our initial two performances we designed and developed a tool and guidelines that can help heritage organisations with their decolonising process by critically engaging the public with under-represented voices and viewpoints of troubled European and colonial narratives. The evaluations showed the embodied and affective potential of MRHP to attract and educate heritage audiences visitors. Insights of the design process are being formulated into an extensive design toolkit that aims to support experience design, theatre and heritage professionals to collaboratively carry out similar projects.</p></p class="citation"></blockquote><h3 id=59--155218-evaluating-navigation-and-comparison-performance-of-computational-notebooks-on-desktop-and-in-virtual-reality-sungwon-in-et-al-2024>(5/9 | 155/218) Evaluating Navigation and Comparison Performance of Computational Notebooks on Desktop and in Virtual Reality (Sungwon In et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sungwon In, Erick Krokos, Kirsten Whitley, Chris North, Yalong Yang. (2024)<br><strong>Evaluating Navigation and Comparison Performance of Computational Notebooks on Desktop and in Virtual Reality</strong><br><button class=copy-to-clipboard title="Evaluating Navigation and Comparison Performance of Computational Notebooks on Desktop and in Virtual Reality" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Virtual Reality (VR), Virtual Reality (VR)<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07161v1.pdf filename=2404.07161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The computational notebook serves as a versatile tool for data analysis. However, its conventional user interface falls short of keeping pace with the ever-growing data-related tasks, signaling the need for novel approaches. With the rapid development of interaction techniques and computing environments, there is a growing interest in integrating emerging technologies in data-driven workflows. <b>Virtual</b> <b>reality,</b> in particular, has demonstrated its potential in interactive data visualizations. In this work, we aimed to experiment with adapting computational notebooks into <b>VR</b> and verify the potential benefits <b>VR</b> can bring. We focus on the navigation and comparison aspects as they are primitive components in analysts&rsquo; workflow. To further improve comparison, we have designed and implemented a Branching&amp;Merging functionality. We tested computational notebooks on the desktop and in <b>VR,</b> both with and without the added Branching&amp;Merging capability. We found <b>VR</b> significantly facilitated navigation compared to desktop, and the ability to create branches enhanced comparison.</p></p class="citation"></blockquote><h3 id=69--156218-exploring-physiological-responses-in-virtual-reality-based-interventions-for-autism-spectrum-disorder-a-data-driven-investigation-gianpaolo-alvari-et-al-2024>(6/9 | 156/218) Exploring Physiological Responses in Virtual Reality-based Interventions for Autism Spectrum Disorder: A Data-Driven Investigation (Gianpaolo Alvari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gianpaolo Alvari, Ersilia Vallefuoco, Melanie Cristofolini, Elio Salvadori, Marco Dianti, Alessia Moltani, Davide Dal Castello, Paola Venuti, Cesare Furlanello. (2024)<br><strong>Exploring Physiological Responses in Virtual Reality-based Interventions for Autism Spectrum Disorder: A Data-Driven Investigation</strong><br><button class=copy-to-clipboard title="Exploring Physiological Responses in Virtual Reality-based Interventions for Autism Spectrum Disorder: A Data-Driven Investigation" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: 92C30 (Primary) 92C55, 68T99 (Secondary), cs-HC, cs-LG, cs.HC<br>Keyword Score: 20<br>Keywords: Virtual Reality (VR), Virtual Reality (VR)<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07159v1.pdf filename=2404.07159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Virtual</b> <b>Reality</b> <b>(VR)</b> has emerged as a promising tool for enhancing social skills and emotional well-being in individuals with Autism Spectrum Disorder (ASD). Through a technical exploration, this study employs a multiplayer serious gaming environment within <b>VR,</b> engaging 34 individuals diagnosed with ASD and employing high-precision biosensors for a comprehensive view of the participants&rsquo; arousal and responses during the <b>VR</b> sessions. Participants were subjected to a series of 3 <b>virtual</b> <b>scenarios</b> designed in collaboration with stakeholders and clinical experts to promote socio-cognitive skills and emotional regulation in a controlled and structured <b>virtual</b> <b>environment.</b> We combined the framework with wearable non-invasive sensors for bio-signal acquisition, focusing on the collection of heart rate variability, and respiratory patterns to monitor participants behaviors. Further, behavioral assessments were conducted using observation and semi-structured interviews, with the data analyzed in conjunction with physiological measures to identify correlations and explore digital-intervention efficacy. Preliminary analysis revealed significant correlations between physiological responses and behavioral outcomes, indicating the potential of physiological feedback to enhance <b>VR-based</b> interventions for ASD. The study demonstrated the feasibility of using real-time data to adapt <b>virtual</b> <b>scenarios,</b> suggesting a promising avenue to support personalized therapy. The integration of quantitative physiological feedback into digital platforms represents a forward step in the personalized intervention for ASD. By leveraging real-time data to adjust therapeutic content, this approach promises to enhance the efficacy and engagement of digital-based therapies.</p></p class="citation"></blockquote><h3 id=79--157218-sara-smart-ai-reading-assistant-for-reading-comprehension-enkeleda-thaqi-et-al-2024>(7/9 | 157/218) SARA: Smart AI Reading Assistant for Reading Comprehension (Enkeleda Thaqi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enkeleda Thaqi, Mohamed Mantawy, Enkelejda Kasneci. (2024)<br><strong>SARA: Smart AI Reading Assistant for Reading Comprehension</strong><br><button class=copy-to-clipboard title="SARA: Smart AI Reading Assistant for Reading Comprehension" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Mixed Reality (MR), Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06906v1.pdf filename=2404.06906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>SARA integrates Eye Tracking and state-of-the-art <b>large</b> <b>language</b> <b>models</b> in a <b>mixed</b> <b>reality</b> framework to enhance the reading experience by providing personalized assistance in real-time. By tracking eye movements, SARA identifies the text segments that attract the user&rsquo;s attention the most and potentially indicate uncertain areas and comprehension issues. The process involves these key steps: text detection and extraction, gaze tracking and alignment, and assessment of detected reading difficulty. The results are customized solutions presented directly within the user&rsquo;s field of view as virtual overlays on identified difficult text areas. This support enables users to overcome challenges like unfamiliar vocabulary and complex sentences by offering additional context, rephrased solutions, and multilingual help. SARA&rsquo;s innovative approach demonstrates it has the potential to transform the reading experience and improve reading proficiency.</p></p class="citation"></blockquote><h3 id=89--158218-untangling-critical-interaction-with-ai-in-students-written-assessment-antonette-shibani-et-al-2024>(8/9 | 158/218) Untangling Critical Interaction with AI in Students Written Assessment (Antonette Shibani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonette Shibani, Simon Knight, Kirsty Kitto, Ajanie Karunanayake, Simon Buckingham Shum. (2024)<br><strong>Untangling Critical Interaction with AI in Students Written Assessment</strong><br><button class=copy-to-clipboard title="Untangling Critical Interaction with AI in Students Written Assessment" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: I-2; K-3-1, cs-AI, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06955v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06955v1.pdf filename=2404.06955v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial Intelligence (AI) has become a ubiquitous part of society, but a key challenge exists in ensuring that humans are equipped with the required critical thinking and AI literacy skills to interact with machines effectively by understanding their capabilities and limitations. These skills are particularly important for learners to develop in the age of <b>generative</b> <b>AI</b> where AI tools can demonstrate complex knowledge and ability previously thought to be uniquely human. To activate effective human-AI partnerships in writing, this paper provides a first step toward conceptualizing the notion of critical learner interaction with AI. Using both theoretical models and empirical data, our preliminary findings suggest a general lack of Deep interaction with AI during the writing process. We believe that the outcomes can lead to better task and tool design in the future for learners to develop deep, critical thinking when interacting with AI.</p></p class="citation"></blockquote><h3 id=99--159218-incremental-xai-memorable-understanding-of-ai-with-incremental-explanations-jessica-y-bo-et-al-2024>(9/9 | 159/218) Incremental XAI: Memorable Understanding of AI with Incremental Explanations (Jessica Y. Bo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jessica Y. Bo, Pan Hao, Brian Y. Lim. (2024)<br><strong>Incremental XAI: Memorable Understanding of AI with Incremental Explanations</strong><br><button class=copy-to-clipboard title="Incremental XAI: Memorable Understanding of AI with Incremental Explanations" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06733v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06733v1.pdf filename=2404.06733v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many <b>explainable</b> <b>AI</b> (XAI) techniques strive for interpretability by providing concise salient information, such as sparse linear factors. However, users either only see inaccurate global explanations, or highly-varying local explanations. We propose to provide more detailed explanations by leveraging the human cognitive capacity to accumulate knowledge by incrementally receiving more details. Focusing on linear factor explanations (factors $\times$ values = outcome), we introduce Incremental XAI to automatically partition explanations for general and atypical instances by providing Base + Incremental factors to help users read and remember more faithful explanations. Memorability is improved by reusing base factors and reducing the number of factors shown in atypical cases. In modeling, formative, and summative user studies, we evaluated the faithfulness, memorability and understandability of Incremental XAI against baseline explanation methods. This work contributes towards more usable explanation that users can better ingrain to facilitate intuitive engagement with AI.</p></p class="citation"></blockquote><h2 id=cscr-5>cs.CR (5)</h2><h3 id=15--160218-differentially-private-gans-for-generating-synthetic-indoor-location-data-vahideh-moghtadaiee-et-al-2024>(1/5 | 160/218) Differentially Private GANs for Generating Synthetic Indoor Location Data (Vahideh Moghtadaiee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vahideh Moghtadaiee, Mina Alishahi, Milad Rabiei. (2024)<br><strong>Differentially Private GANs for Generating Synthetic Indoor Location Data</strong><br><button class=copy-to-clipboard title="Differentially Private GANs for Generating Synthetic Indoor Location Data" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR, eess-SP<br>Keyword Score: 40<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Differential Privacy, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07366v1.pdf filename=2404.07366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of location-based services has led to the widespread adoption of indoor localization systems, which enable location tracking of individuals within enclosed spaces such as buildings. While these systems provide numerous benefits such as improved <b>security</b> and personalized services, they also raise concerns regarding privacy violations. As such, there is a growing need for privacy-preserving solutions that can protect users&rsquo; sensitive location information while still enabling the functionality of indoor localization systems. In recent years, Differentially Private <b>Generative</b> <b>Adversarial</b> <b>Networks</b> (DPGANs) have emerged as a powerful methodology that aims to protect the privacy of individual data points while generating realistic synthetic data similar to original data. DPGANs combine the power of <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs)</b> with the privacy-preserving technique of <b>differential</b> <b>privacy</b> (DP). In this paper, we introduce an indoor localization framework employing DPGANs in order to generate privacy-preserving indoor location data. We evaluate the performance of our framework on a real-world indoor localization dataset and demonstrate its effectiveness in preserving privacy while maintaining the accuracy of the localization system.</p></p class="citation"></blockquote><h3 id=25--161218-poisoning-prevention-in-federated-learning-and-differential-privacy-via-stateful-proofs-of-execution-norrathep-rattanavipanon-et-al-2024>(2/5 | 161/218) Poisoning Prevention in Federated Learning and Differential Privacy via Stateful Proofs of Execution (Norrathep Rattanavipanon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Norrathep Rattanavipanon, Ivan De Oliveira Nunes. (2024)<br><strong>Poisoning Prevention in Federated Learning and Differential Privacy via Stateful Proofs of Execution</strong><br><button class=copy-to-clipboard title="Poisoning Prevention in Federated Learning and Differential Privacy via Stateful Proofs of Execution" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 30<br>Keywords: Federated Learning, Differential Privacy, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06721v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06721v2.pdf filename=2404.06721v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise in IoT-driven distributed data analytics, coupled with increasing privacy concerns, has led to a demand for effective privacy-preserving and <b>federated</b> <b>data</b> collection/model training mechanisms. In response, approaches such as <b>Federated</b> <b>Learning</b> (FL) and Local <b>Differential</b> <b>Privacy</b> (LDP) have been proposed and attracted much attention over the past few years. However, they still share the common limitation of being vulnerable to poisoning attacks wherein adversaries compromising edge devices feed forged (a.k.a. poisoned) data to aggregation back-ends, undermining the integrity of FL/LDP results. In this work, we propose a system-level approach to remedy this issue based on a novel <b>security</b> notion of Proofs of Stateful Execution (PoSX) for IoT/embedded devices&rsquo; software. To realize the PoSX concept, we design SLAPP: a System-Level Approach for Poisoning Prevention. SLAPP leverages commodity <b>security</b> features of embedded devices - in particular ARM TrustZoneM <b>security</b> extensions - to verifiably bind raw sensed data to their correct usage as part of FL/LDP edge device routines. As a consequence, it offers robust <b>security</b> guarantees against poisoning. Our evaluation, based on real-world prototypes featuring multiple cryptographic primitives and data collection schemes, showcases SLAPP&rsquo;s <b>security</b> and low overhead.</p></p class="citation"></blockquote><h3 id=35--162218-indoor-location-fingerprinting-privacy-a-comprehensive-survey-amir-fathalizadeh-et-al-2024>(3/5 | 162/218) Indoor Location Fingerprinting Privacy: A Comprehensive Survey (Amir Fathalizadeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Fathalizadeh, Vahideh Moghtadaiee, Mina Alishahi. (2024)<br><strong>Indoor Location Fingerprinting Privacy: A Comprehensive Survey</strong><br><button class=copy-to-clipboard title="Indoor Location Fingerprinting Privacy: A Comprehensive Survey" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR, eess-SP<br>Keyword Score: 20<br>Keywords: Federated Learning, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07345v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07345v1.pdf filename=2404.07345v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pervasive integration of Indoor Positioning Systems (IPS) arises from the limitations of Global Navigation Satellite Systems (GNSS) in indoor environments, leading to the widespread adoption of Location-Based Services (LBS). Specifically, indoor location fingerprinting employs diverse signal fingerprints from user devices, enabling precise location identification by Location Service Providers (LSP). Despite its broad applications across various domains, indoor location fingerprinting introduces a notable privacy risk, as both LSP and potential adversaries inherently have access to this sensitive information, compromising users&rsquo; privacy. Consequently, concerns regarding privacy vulnerabilities in this context necessitate a focused exploration of privacy-preserving mechanisms. In response to these concerns, this survey presents a comprehensive review of Privacy-Preserving Mechanisms in Indoor Location Fingerprinting (ILFPPM) based on cryptographic, anonymization, <b>differential</b> <b>privacy</b> (DP), and <b>federated</b> <b>learning</b> (FL) techniques. We also propose a distinctive and novel grouping of privacy vulnerabilities, adversary and attack models, and available evaluation metrics specific to indoor location fingerprinting systems. Given the identified limitations and research gaps in this survey, we highlight numerous prospective opportunities for future investigation, aiming to motivate researchers interested in advancing this field. This survey serves as a valuable reference for researchers and provides a clear overview for those beyond this specific research domain.</p></p class="citation"></blockquote><h3 id=45--163218-atlas-x-equity-financing-unlocking-new-methods-to-securely-obfuscate-axe-inventory-data-based-on-differential-privacy-antigoni-polychroniadou-et-al-2024>(4/5 | 163/218) Atlas-X Equity Financing: Unlocking New Methods to Securely Obfuscate Axe Inventory Data Based on Differential Privacy (Antigoni Polychroniadou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antigoni Polychroniadou, Gabriele Cipriani, Richard Hua, Tucker Balch. (2024)<br><strong>Atlas-X Equity Financing: Unlocking New Methods to Securely Obfuscate Axe Inventory Data Based on Differential Privacy</strong><br><button class=copy-to-clipboard title="Atlas-X Equity Financing: Unlocking New Methods to Securely Obfuscate Axe Inventory Data Based on Differential Privacy" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 13<br>Keywords: Benchmarking, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06686v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06686v1.pdf filename=2404.06686v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Banks publish daily a list of available securities/assets (axe list) to selected clients to help them effectively locate Long (buy) or Short (sell) trades at reduced financing rates. This reduces costs for the bank, as the list aggregates the bank&rsquo;s internal firm inventory per asset for all clients of long as well as short trades. However, this is somewhat problematic: (1) the bank&rsquo;s inventory is revealed; (2) trades of clients who contribute to the aggregated list, particularly those deemed large, are revealed to other clients. Clients conducting sizable trades with the bank and possessing a portion of the aggregated asset exceeding $50%$ are considered to be concentrated clients. This could potentially reveal a trading concentrated client&rsquo;s activity to their competitors, thus providing an unfair advantage over the market. Atlas-X Axe Obfuscation, powered by new <b>differential</b> <b>private</b> methods, enables a bank to obfuscate its published axe list on a daily basis while under continual observation, thus maintaining an acceptable inventory Profit and Loss (P&amp;L) cost pertaining to the noisy obfuscated axe list while reducing the clients&rsquo; trading activity leakage. Our main <b>differential</b> <b>private</b> innovation is a <b>differential</b> <b>private</b> aggregator for streams (time series data) of both positive and negative integers under continual observation. For the last two years, Atlas-X system has been live in production across three major regions-USA, Europe, and Asia-at J.P. Morgan, a major financial institution, facilitating significant profitability. To our knowledge, it is the first <b>differential</b> <b>privacy</b> solution to be deployed in the financial sector. We also report <b>benchmarks</b> of our algorithm based on (anonymous) real and synthetic data to showcase the quality of our obfuscation and its success in production.</p></p class="citation"></blockquote><h3 id=55--164218-security-assessment-of-the-lg-cryptosystem-étienne-burle-et-al-2024>(5/5 | 164/218) Security Assessment of the LG Cryptosystem (Étienne Burle et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Étienne Burle, Hervé Talé Kalachi, Freddy Lende Metouke, Ayoub Otmani. (2024)<br><strong>Security Assessment of the LG Cryptosystem</strong><br><button class=copy-to-clipboard title="Security Assessment of the LG Cryptosystem" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06815v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06815v1.pdf filename=2404.06815v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The LG cryptosystem is a public-key encryption scheme in the rank metric using the recent family of $\lambdav-$Gabidulin codes and introduced in 2019 by Lau and Tan. In this paper, we present a cryptanalysis showing that the <b>security</b> of several parameters of the scheme have been overestimated. We also show the existence of some weak keys allowing an attacker to find in polynomial time an alternative private key.</p></p class="citation"></blockquote><h2 id=eessas-5>eess.AS (5)</h2><h3 id=15--165218-conformer-1-robust-asr-via-large-scale-semisupervised-bootstrapping-kevin-zhang-et-al-2024>(1/5 | 165/218) Conformer-1: Robust ASR via Large-Scale Semisupervised Bootstrapping (Kevin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kevin Zhang, Luka Chkhetiani, Francis McCann Ramirez, Yash Khare, Andrea Vanzo, Michael Liang, Sergio Ramirez Martin, Gabriel Oexle, Ruben Bousbib, Taufiquzzaman Peyash, Michael Nguyen, Dillon Pulliam, Domenic Donato. (2024)<br><strong>Conformer-1: Robust ASR via Large-Scale Semisupervised Bootstrapping</strong><br><button class=copy-to-clipboard title="Conformer-1: Robust ASR via Large-Scale Semisupervised Bootstrapping" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-CL, cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 40<br>Keywords: Recurrent Neural Network, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07341v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07341v1.pdf filename=2404.07341v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents Conformer-1, an end-to-end <b>Automatic</b> <b>Speech</b> <b>Recognition</b> <b>(ASR)</b> model trained on an extensive dataset of 570k hours of <b>speech</b> <b>audio</b> data, 91% of which was acquired from publicly available sources. To achieve this, we perform Noisy Student Training after generating pseudo-labels for the unlabeled public data using a strong Conformer <b>RNN-T</b> baseline model. The addition of these pseudo-labeled data results in remarkable improvements in relative Word Error Rate (WER) by 11.5% and 24.3% for our asynchronous and realtime models, respectively. Additionally, the model is more robust to background noise owing to the addition of these data. The results obtained in this study demonstrate that the incorporation of pseudo-labeled publicly available data is a highly effective strategy for improving <b>ASR</b> accuracy and noise robustness.</p></p class="citation"></blockquote><h3 id=25--166218-towards-efficient-and-real-time-piano-transcription-using-neural-autoregressive-models-taegyun-kwon-et-al-2024>(2/5 | 166/218) Towards Efficient and Real-Time Piano Transcription Using Neural Autoregressive Models (Taegyun Kwon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taegyun Kwon, Dasaem Jeong, Juhan Nam. (2024)<br><strong>Towards Efficient and Real-Time Piano Transcription Using Neural Autoregressive Models</strong><br><button class=copy-to-clipboard title="Towards Efficient and Real-Time Piano Transcription Using Neural Autoregressive Models" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, LSTM, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06818v1.pdf filename=2404.06818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, advancements in neural network designs and the availability of large-scale labeled datasets have led to significant improvements in the accuracy of piano transcription models. However, most previous work focused on high-performance offline transcription, neglecting deliberate consideration of model size. The goal of this work is to implement real-time inference for piano transcription while ensuring both high performance and lightweight. To this end, we propose novel architectures for <b>convolutional</b> <b>recurrent</b> <b>neural</b> <b>networks,</b> redesigning an existing autoregressive piano transcription model. First, we extend the acoustic module by adding a frequency-conditioned FiLM layer to the <b>CNN</b> module to adapt the <b>convolutional</b> filters on the frequency axis. Second, we improve note-state sequence modeling by using a pitchwise <b>LSTM</b> that focuses on note-state transitions within a note. In addition, we augment the autoregressive connection with an enhanced recursive context. Using these components, we propose two types of models; one for high performance and the other for high compactness. Through extensive experiments, we show that the proposed models are comparable to state-of-the-art models in terms of note accuracy on the MAESTRO dataset. We also investigate the effective model size and real-time inference latency by gradually streamlining the architecture. Finally, we conduct cross-data evaluation on unseen piano datasets and in-depth analysis to elucidate the effect of the proposed components in the view of note length and pitch range.</p></p class="citation"></blockquote><h3 id=35--167218-covomix-advancing-zero-shot-speech-generation-for-human-like-multi-talker-conversations-leying-zhang-et-al-2024>(3/5 | 167/218) CoVoMix: Advancing Zero-Shot Speech Generation for Human-like Multi-talker Conversations (Leying Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leying Zhang, Yao Qian, Long Zhou, Shujie Liu, Dongmei Wang, Xiaofei Wang, Midia Yousefi, Yanmin Qian, Jinyu Li, Lei He, Sheng Zhao, Michael Zeng. (2024)<br><strong>CoVoMix: Advancing Zero-Shot Speech Generation for Human-like Multi-talker Conversations</strong><br><button class=copy-to-clipboard title="CoVoMix: Advancing Zero-Shot Speech Generation for Human-like Multi-talker Conversations" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-AI, cs-CL, cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 30<br>Keywords: Zero-shot, Text-to-speech, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06690v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06690v1.pdf filename=2404.06690v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>zero-shot</b> <b>text-to-speech</b> <b>(TTS)</b> modeling have led to significant strides in generating high-fidelity and diverse speech. However, dialogue generation, along with achieving human-like naturalness in speech, continues to be a challenge in the field. In this paper, we introduce CoVoMix: Conversational Voice Mixture Generation, a novel model for <b>zero-shot,</b> human-like, multi-speaker, multi-round dialogue speech generation. CoVoMix is capable of first converting dialogue text into multiple streams of discrete tokens, with each token stream representing semantic information for individual talkers. These token streams are then fed into a flow-matching based acoustic model to generate mixed mel-spectrograms. Finally, the speech waveforms are produced using a HiFi-GAN model. Furthermore, we devise a comprehensive set of metrics for measuring the effectiveness of dialogue modeling and generation. Our experimental results show that CoVoMix can generate dialogues that are not only human-like in their naturalness and coherence but also involve multiple talkers engaging in multiple rounds of conversation. These dialogues, generated within a single channel, are characterized by seamless speech transitions, including overlapping speech, and appropriate paralinguistic behaviors such as laughter. Audio samples are available at <a href=https://aka.ms/covomix>https://aka.ms/covomix</a>.</p></p class="citation"></blockquote><h3 id=45--168218-efficient-sound-field-reconstruction-with-conditional-invertible-neural-networks-xenofon-karakonstantis-et-al-2024>(4/5 | 168/218) Efficient Sound Field Reconstruction with Conditional Invertible Neural Networks (Xenofon Karakonstantis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xenofon Karakonstantis, Efren Fernandez-Grande, Peter Gerstoft. (2024)<br><strong>Efficient Sound Field Reconstruction with Conditional Invertible Neural Networks</strong><br><button class=copy-to-clipboard title="Efficient Sound Field Reconstruction with Conditional Invertible Neural Networks" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06928v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06928v1.pdf filename=2404.06928v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we introduce a method for estimating sound fields in reverberant environments using a conditional invertible neural network (CINN). Sound field reconstruction can be hindered by experimental errors, limited spatial data, model mismatches, and long inference times, leading to potentially flawed and prolonged characterizations. Further, the complexity of managing inherent uncertainties often escalates computational demands or is neglected in models. Our approach seeks to balance accuracy and computational efficiency, while incorporating uncertainty estimates to tailor reconstructions to specific needs. By training a CINN with Monte Carlo <b>simulations</b> of random wave fields, our method reduces the dependency on extensive datasets and enables inference from sparse experimental data. The CINN proves versatile at reconstructing Room Impulse Responses (RIRs), by acting either as a likelihood model for maximum a posteriori estimation or as an approximate posterior distribution through amortized Bayesian inference. Compared to traditional Bayesian methods, the CINN achieves similar accuracy with greater efficiency and without requiring its adaptation to distinct sound field conditions.</p></p class="citation"></blockquote><h3 id=55--169218-what-is-learnt-by-the-learnable-front-end-leaf-adapting-per-channel-energy-normalisation-pcen-to-noisy-conditions-hanyu-meng-et-al-2024>(5/5 | 169/218) What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions (Hanyu Meng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanyu Meng, Vidhyasaharan Sethu, Eliathamby Ambikairajah. (2024)<br><strong>What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions</strong><br><button class=copy-to-clipboard title="What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess-SP, eess.AS<br>Keyword Score: 10<br>Keywords: Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06702v1.pdf filename=2404.06702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is increasing interest in the use of the LEArnable Front-end (LEAF) in a variety of speech processing systems. However, there is a dearth of analyses of what is actually learnt and the relative importance of training the different components of the front-end. In this paper, we investigate this question on keyword spotting, speech-based <b>emotion</b> <b>recognition</b> and language identification tasks and find that the filters for spectral decomposition and the low pass filter used to estimate spectral energy variations exhibit no learning and the per-channel energy normalisation (PCEN) is the key component that is learnt. Following this, we explore the potential of adapting only the PCEN layer with a small amount of noisy data to enable it to learn appropriate dynamic range compression that better suits the noise conditions. This in turn enables a system trained on clean speech to work more accurately on noisy test data as demonstrated by the experimental results reported in this paper.</p></p class="citation"></blockquote><h2 id=eesssy-7>eess.SY (7)</h2><h3 id=17--170218-dual-ensemble-kalman-filter-for-stochastic-optimal-control-anant-a-joshi-et-al-2024>(1/7 | 170/218) Dual Ensemble Kalman Filter for Stochastic Optimal Control (Anant A. Joshi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anant A. Joshi, Amirhossein Taghvaei, Prashant G. Mehta, Sean P. Meyn. (2024)<br><strong>Dual Ensemble Kalman Filter for Stochastic Optimal Control</strong><br><button class=copy-to-clipboard title="Dual Ensemble Kalman Filter for Stochastic Optimal Control" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 40<br>Keywords: Continuous Time, Continuous Time, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06696v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06696v1.pdf filename=2404.06696v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, stochastic optimal control problems in <b>continuous</b> <b>time</b> and space are considered. In recent years, such problems have received renewed attention from the lens of <b>reinforcement</b> <b>learning</b> (RL) which is also one of our motivation. The main contribution is a <b>simulation-based</b> algorithm &ndash; dual ensemble Kalman filter (EnKF) &ndash; to numerically approximate the solution of these problems. The paper extends our previous work where the dual EnKF was applied in deterministic settings of the problem. The theoretical results and algorithms are illustrated with numerical experiments.</p></p class="citation"></blockquote><h3 id=27--171218-structured-reinforcement-learning-for-media-streaming-at-the-wireless-edge-archana-bura-et-al-2024>(2/7 | 171/218) Structured Reinforcement Learning for Media Streaming at the Wireless Edge (Archana Bura et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Archana Bura, Sarat Chandra Bobbili, Shreyas Rameshkumar, Desik Rengarajan, Dileep Kalathil, Srinivas Shakkottai. (2024)<br><strong>Structured Reinforcement Learning for Media Streaming at the Wireless Edge</strong><br><button class=copy-to-clipboard title="Structured Reinforcement Learning for Media Streaming at the Wireless Edge" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AI, cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07315v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07315v1.pdf filename=2404.07315v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Media streaming is the dominant application over wireless edge (access) networks. The increasing softwarization of such networks has led to efforts at intelligent control, wherein application-specific actions may be dynamically taken to enhance the user experience. The goal of this work is to develop and demonstrate learning-based policies for optimal decision making to determine which clients to dynamically prioritize in a video streaming setting. We formulate the policy design question as a constrained Markov decision problem (CMDP), and observe that by using a Lagrangian relaxation we can decompose it into single-client problems. Further, the optimal policy takes a threshold form in the video buffer length, which enables us to design an efficient constrained <b>reinforcement</b> <b>learning</b> (CRL) algorithm to learn it. Specifically, we show that a natural policy gradient (NPG) based algorithm that is derived using the structure of our problem converges to the globally optimal policy. We then develop a <b>simulation</b> environment for training, and a real-world intelligent controller attached to a WiFi access point for evaluation. We empirically show that the structured learning approach enables fast learning. Furthermore, such a structured policy can be easily deployed due to low computational complexity, leading to policy execution taking only about 15$\mu$s. Using YouTube streaming experiments in a resource constrained scenario, we demonstrate that the CRL approach can increase QoE by over 30%.</p></p class="citation"></blockquote><h3 id=37--172218-synchronization-conditions-for-nonlinear-oscillator-networks-sanjeev-kumar-pandey-et-al-2024>(3/7 | 172/218) Synchronization Conditions for Nonlinear Oscillator Networks (Sanjeev Kumar Pandey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanjeev Kumar Pandey, Shaunak Sen, Indra Narayan Kar. (2024)<br><strong>Synchronization Conditions for Nonlinear Oscillator Networks</strong><br><button class=copy-to-clipboard title="Synchronization Conditions for Nonlinear Oscillator Networks" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06752v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06752v1.pdf filename=2404.06752v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding conditions for the synchronization of a network of interconnected oscillators is a challenging problem. Typically, only sufficient conditions are reported for the synchronization problem. Here, we adopted the Lyapunov-Floquet theory and the Master Stability Function approach in order to derive the synchronization conditions for a set of coupled nonlinear oscillators. We found that the positivity of the coupling constant is a necessary and sufficient condition for synchronizing linearly full-state coupled identical oscillators. Moreover, in the case of partial state coupling, the asymptotic convergence of volume in state space is ensured by a positive coupling constant. The numerical calculation of the Master Stability Function for a <b>benchmark</b> two-dimensional oscillator validates the synchronization corresponding to the positive coupling. The results are illustrated using numerical <b>simulations</b> and experimentation on <b>benchmark</b> oscillators.</p></p class="citation"></blockquote><h3 id=47--173218-lyapunov-based-deep-residual-neural-network-resnet-adaptive-control-omkar-sudhir-patil-et-al-2024>(4/7 | 173/218) Lyapunov-Based Deep Residual Neural Network (ResNet) Adaptive Control (Omkar Sudhir Patil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omkar Sudhir Patil, Duc M. Le, Emily J. Griffis, Warren E. Dixon. (2024)<br><strong>Lyapunov-Based Deep Residual Neural Network (ResNet) Adaptive Control</strong><br><button class=copy-to-clipboard title="Lyapunov-Based Deep Residual Neural Network (ResNet) Adaptive Control" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07385v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07385v1.pdf filename=2404.07385v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Neural Network (DNN)-based controllers have emerged as a tool to compensate for unstructured uncertainties in nonlinear dynamical systems. A recent breakthrough in the adaptive control literature provides a Lyapunov-based approach to derive weight adaptation laws for each layer of a fully-connected feedforward DNN-based adaptive controller. However, deriving weight adaptation laws from a Lyapunov-based analysis remains an open problem for deep residual neural networks (ResNets). This paper provides the first result on Lyapunov-derived weight adaptation for a ResNet-based adaptive controller. A nonsmooth Lyapunov-based analysis is provided to guarantee asymptotic tracking error convergence. Comparative Monte Carlo <b>simulations</b> are provided to demonstrate the performance of the developed ResNet-based adaptive controller. The ResNet-based adaptive controller shows a 64% improvement in the tracking and function approximation performance, in comparison to a fully-connected DNN-based adaptive controller.</p></p class="citation"></blockquote><h3 id=57--174218-analytical-formula-for-calculations-of-armour-losses-in-three-core-power-cables-marius-hatlo-et-al-2024>(5/7 | 174/218) Analytical Formula for Calculations of Armour Losses in Three-Core Power Cables (Marius Hatlo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marius Hatlo, Martin Hovde. (2024)<br><strong>Analytical Formula for Calculations of Armour Losses in Three-Core Power Cables</strong><br><button class=copy-to-clipboard title="Analytical Formula for Calculations of Armour Losses in Three-Core Power Cables" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06998v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06998v1.pdf filename=2404.06998v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the past decade, significant progress has been made in the field of loss and rating calculations for armoured three-core cables. This development was <b>prompted</b> by an industry realization that the applicable international standards often overestimate losses, leading to unnecessarily bulky and more expensive cables. Starting with first-principles, this paper presents an accurate analytical formula for armour losses in three-core cables. The formula has undergone rigorous validation against 3D Finite Element Analysis (FEA) and demonstrate excellent accuracy. In the specific cases examined, the largest deviation from FEA results in terms of armour loss is approximately 2.4 percent for fully armoured cables. Although this study specifically focuses on armour losses, it establishes the groundwork for precise loss calculations in armoured three-core cables, including the conductor and screen losses. And the work presented here formed the basis for the complete loss calculations presented in the CIGRE Technical Brochure 908.</p></p class="citation"></blockquote><h3 id=67--175218-multi-agent-soft-actor-critic-with-global-loss-for-autonomous-mobility-on-demand-fleet-control-zeno-woywood-et-al-2024>(6/7 | 175/218) Multi-Agent Soft Actor-Critic with Global Loss for Autonomous Mobility-on-Demand Fleet Control (Zeno Woywood et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeno Woywood, Jasper I. Wiltfang, Julius Luy, Tobias Enders, Maximilian Schiffer. (2024)<br><strong>Multi-Agent Soft Actor-Critic with Global Loss for Autonomous Mobility-on-Demand Fleet Control</strong><br><button class=copy-to-clipboard title="Multi-Agent Soft Actor-Critic with Global Loss for Autonomous Mobility-on-Demand Fleet Control" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-MA, cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06975v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06975v1.pdf filename=2404.06975v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a sequential decision-making problem for a profit-maximizing operator of an Autonomous Mobility-on-Demand system. Optimizing a central operator&rsquo;s vehicle-to-request dispatching policy requires efficient and effective fleet control strategies. To this end, we employ a multi-agent Soft Actor-Critic algorithm combined with weighted bipartite matching. We propose a novel vehicle-based algorithm architecture and adapt the critic&rsquo;s loss function to appropriately consider global actions. Furthermore, we extend our algorithm to incorporate rebalancing capabilities. Through numerical experiments, we show that our approach outperforms state-of-the-art <b>benchmarks</b> by up to 12.9% for dispatching and up to 38.9% with integrated rebalancing.</p></p class="citation"></blockquote><h3 id=77--176218-iterative-distributed-moving-horizon-estimation-of-linear-systems-with-penalties-on-both-system-disturbances-and-noise-xiaojie-li-et-al-2024>(7/7 | 176/218) Iterative distributed moving horizon estimation of linear systems with penalties on both system disturbances and noise (Xiaojie Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaojie Li, Song Bo, Yan Qin, Xunyuan Yin. (2024)<br><strong>Iterative distributed moving horizon estimation of linear systems with penalties on both system disturbances and noise</strong><br><button class=copy-to-clipboard title="Iterative distributed moving horizon estimation of linear systems with penalties on both system disturbances and noise" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06706v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06706v1.pdf filename=2404.06706v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, partition-based distributed state estimation of general linear systems is considered. A distributed moving horizon state estimation scheme is developed via decomposing the entire system model into subsystem models and partitioning the global objective function of centralized moving horizon estimation (MHE) into local objective functions. The subsystem estimators of the distributed scheme that are required to be executed iteratively within each sampling period are designed based on MHE. Two distributed MHE algorithms are proposed to handle the unconstrained case and the case when hard constraints on states and disturbances, respectively. Sufficient conditions on the convergence of the estimates and the stability of the estimation error dynamics for the entire system are derived for both cases. A <b>benchmark</b> reactor-separator process example is introduced to illustrate the proposed distributed state estimation approach.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--177218-voiceshop-a-unified-speech-to-speech-framework-for-identity-preserving-zero-shot-voice-editing-philip-anastassiou-et-al-2024>(1/2 | 177/218) VoiceShop: A Unified Speech-to-Speech Framework for Identity-Preserving Zero-Shot Voice Editing (Philip Anastassiou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philip Anastassiou, Zhenyu Tang, Kainan Peng, Dongya Jia, Jiaxin Li, Ming Tu, Yuping Wang, Yuxuan Wang, Mingbo Ma. (2024)<br><strong>VoiceShop: A Unified Speech-to-Speech Framework for Identity-Preserving Zero-Shot Voice Editing</strong><br><button class=copy-to-clipboard title="VoiceShop: A Unified Speech-to-Speech Framework for Identity-Preserving Zero-Shot Voice Editing" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 40<br>Keywords: Fine-tuning, Out-of-distribution, Zero-shot, Speech-to-Speech Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06674v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06674v2.pdf filename=2404.06674v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present VoiceShop, a novel <b>speech-to-speech</b> framework that can modify multiple attributes of speech, such as age, gender, accent, and speech style, in a single forward pass while preserving the input speaker&rsquo;s timbre. Previous works have been constrained to specialized models that can only edit these attributes individually and suffer from the following pitfalls: the magnitude of the conversion effect is weak, there is no <b>zero-shot</b> capability for <b>out-of-distribution</b> speakers, or the synthesized outputs exhibit undesirable timbre leakage. Our work proposes solutions for each of these issues in a simple modular framework based on a conditional diffusion backbone model with optional normalizing flow-based and sequence-to-sequence speaker attribute-editing modules, whose components can be combined or removed during inference to meet a wide array of tasks without additional model <b>finetuning.</b> Audio samples are available at \url{https://voiceshopai.github.io}.</p></p class="citation"></blockquote><h3 id=22--178218-learning-multidimensional-disentangled-representations-of-instrumental-sounds-for-musical-similarity-assessment-yuka-hashizume-et-al-2024>(2/2 | 178/218) Learning Multidimensional Disentangled Representations of Instrumental Sounds for Musical Similarity Assessment (Yuka Hashizume et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuka Hashizume, Li Li, Atsushi Miyashita, Tomoki Toda. (2024)<br><strong>Learning Multidimensional Disentangled Representations of Instrumental Sounds for Musical Similarity Assessment</strong><br><button class=copy-to-clipboard title="Learning Multidimensional Disentangled Representations of Instrumental Sounds for Musical Similarity Assessment" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06682v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06682v1.pdf filename=2404.06682v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To achieve a flexible <b>recommendation</b> and retrieval system, it is desirable to calculate music similarity by focusing on multiple partial elements of musical pieces and allowing the users to select the element they want to focus on. A previous study proposed using multiple individual networks for calculating music similarity based on each instrumental sound, but it is impractical to use each signal as a query in search systems. Using separated instrumental sounds alternatively resulted in less accuracy due to artifacts. In this paper, we propose a method to compute similarities focusing on each instrumental sound with a single network that takes mixed sounds as input instead of individual instrumental sounds. Specifically, we design a single similarity embedding space with disentangled dimensions for each instrument, extracted by Conditional Similarity Networks, which is trained by the triplet loss using masks. Experimental results have shown that (1) the proposed method can obtain more accurate feature representation than using individual networks using separated sounds as input, (2) each sub-embedding space can hold the characteristics of the corresponding instrument, and (3) the selection of similar musical pieces focusing on each instrumental sound by the proposed method can obtain human consent, especially in drums and guitar.</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--179218-gcv-turbo-end-to-end-acceleration-of-gnn-based-computer-vision-tasks-on-fpga-bingyi-zhang-et-al-2024>(1/1 | 179/218) GCV-Turbo: End-to-end Acceleration of GNN-based Computer Vision Tasks on FPGA (Bingyi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingyi Zhang, Rajgopal Kannan, Carl Busart, Viktor Prasanna. (2024)<br><strong>GCV-Turbo: End-to-end Acceleration of GNN-based Computer Vision Tasks on FPGA</strong><br><button class=copy-to-clipboard title="GCV-Turbo: End-to-end Acceleration of GNN-based Computer Vision Tasks on FPGA" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-CV, cs-DC, cs.DC, eess-IV<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07188v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07188v1.pdf filename=2404.07188v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have recently empowered various novel computer vision (CV) tasks. In <b>GNN-based</b> CV tasks, a combination of <b>CNN</b> layers and <b>GNN</b> layers or only <b>GNN</b> layers are employed. This paper introduces GCV-Turbo, a domain-specific accelerator on FPGA for end-to-end acceleration of <b>GNN-based</b> CV tasks. GCV-Turbo consists of two key components: (1) a \emph{novel} hardware architecture optimized for the computation kernels in both <b>CNNs</b> and <b>GNNs</b> using the same set of computation resources. (2) a PyTorch-compatible compiler that takes a user-defined model as input, performs end-to-end optimization for the computation <b>graph</b> <b>of</b> <b>a</b> given <b>GNN-based</b> CV task, and produces optimized code for hardware execution. The hardware architecture and the compiler work synergistically to support a variety of <b>GNN-based</b> CV tasks. We implement GCV-Turbo on a state-of-the-art FPGA and evaluate its performance across six representative <b>GNN-based</b> CV tasks with diverse input data modalities (e.g., image, human skeleton, point cloud). Compared with state-of-the-art CPU (GPU) implementations, GCV-Turbo achieves an average latency reduction of $68.4\times$ ($4.1\times$) on these six <b>GNN-based</b> CV tasks. Moreover, GCV-Turbo supports the execution of the standalone <b>CNNs</b> or <b>GNNs,</b> achieving performance comparable to that of state-of-the-art <b>CNN</b> <b>(GNN)</b> accelerators for widely used <b>CNN-only</b> <b>(GNN-only)</b> models.</p></p class="citation"></blockquote><h2 id=eessiv-3>eess.IV (3)</h2><h3 id=13--180218-improving-multi-center-generalizability-of-gan-based-fat-suppression-using-federated-learning-pranav-kulkarni-et-al-2024>(1/3 | 180/218) Improving Multi-Center Generalizability of GAN-Based Fat Suppression using Federated Learning (Pranav Kulkarni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pranav Kulkarni, Adway Kanhere, Harshita Kukreja, Vivian Zhang, Paul H. Yi, Vishwa S. Parekh. (2024)<br><strong>Improving Multi-Center Generalizability of GAN-Based Fat Suppression using Federated Learning</strong><br><button class=copy-to-clipboard title="Improving Multi-Center Generalizability of GAN-Based Fat Suppression using Federated Learning" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Federated Learning, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07374v1.pdf filename=2404.07374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>Adversarial</b> <b>Network</b> <b>(GAN)-based</b> synthesis of fat suppressed (FS) MRIs from non-FS proton density sequences has the potential to accelerate acquisition of knee MRIs. However, <b>GANs</b> trained on single-site data have poor generalizability to external data. We show that <b>federated</b> <b>learning</b> can improve multi-center generalizability of <b>GANs</b> for synthesizing FS MRIs, while facilitating privacy-preserving multi-institutional collaborations.</p></p class="citation"></blockquote><h3 id=23--181218-accelerating-cardiac-mri-reconstruction-with-cmratt-an-attention-driven-approach-anam-hashmi-et-al-2024>(2/3 | 181/218) Accelerating Cardiac MRI Reconstruction with CMRatt: An Attention-Driven Approach (Anam Hashmi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anam Hashmi, Julia Dietlmeier, Kathleen M. Curran, Noel E. O&rsquo;Connor. (2024)<br><strong>Accelerating Cardiac MRI Reconstruction with CMRatt: An Attention-Driven Approach</strong><br><button class=copy-to-clipboard title="Accelerating Cardiac MRI Reconstruction with CMRatt: An Attention-Driven Approach" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 13<br>Keywords: Benchmarking, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06941v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06941v1.pdf filename=2404.06941v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cine cardiac magnetic resonance (CMR) imaging is recognised as the <b>benchmark</b> modality for the comprehensive assessment of cardiac function. Nevertheless, the acquisition process of cine CMR is considered as an impediment due to its prolonged scanning time. One commonly used strategy to expedite the acquisition process is through k-space undersampling, though it comes with a drawback of introducing aliasing effects in the reconstructed image. Lately, deep learning-based methods have shown remarkable results over traditional approaches in rapidly achieving precise CMR reconstructed images. This study aims to explore the untapped potential of attention mechanisms incorporated with a deep learning model within the context of the CMR reconstruction problem. We are motivated by the fact that attention has proven beneficial in downstream tasks such as image classification and segmentation, but has not been systematically analysed in the context of CMR reconstruction. Our primary goal is to identify the strengths and potential limitations of attention algorithms when integrated with a <b>convolutional</b> backbone model such as a U-Net. To achieve this, we <b>benchmark</b> different state-of-the-art spatial and channel attention mechanisms on the CMRxRecon dataset and quantitatively evaluate the quality of reconstruction using objective metrics. Furthermore, inspired by the best performing attention mechanism, we propose a new, simple yet effective, attention pipeline specifically optimised for the task of cardiac image reconstruction that outperforms other state-of-the-art attention methods. The layer and model code will be made publicly available.</p></p class="citation"></blockquote><h3 id=33--182218-rethinking-perceptual-metrics-for-medical-image-translation-nicholas-konz-et-al-2024>(3/3 | 182/218) Rethinking Perceptual Metrics for Medical Image Translation (Nicholas Konz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicholas Konz, Yuwen Chen, Hanxue Gu, Haoyu Dong, Maciej A. Mazurowski. (2024)<br><strong>Rethinking Perceptual Metrics for Medical Image Translation</strong><br><button class=copy-to-clipboard title="Rethinking Perceptual Metrics for Medical Image Translation" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07318v1.pdf filename=2404.07318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern medical <b>image</b> <b>translation</b> methods use generative models for tasks such as the conversion of CT <b>images</b> <b>to</b> MRI. Evaluating these methods typically relies on some chosen downstream task in the target domain, such as segmentation. On the other hand, task-agnostic metrics are attractive, such as the network feature-based perceptual metrics (e.g., FID) that are common to <b>image</b> <b>translation</b> in general computer vision. In this paper, we investigate evaluation metrics for medical <b>image</b> <b>translation</b> on two medical <b>image</b> <b>translation</b> tasks (GE breast MRI to Siemens breast MRI and lumbar spine MRI to CT), tested on various state-of-the-art translation methods. We show that perceptual metrics do not generally correlate with segmentation metrics due to them extending poorly to the anatomical constraints of this sub-field, with FID being especially inconsistent. However, we find that the lesser-used pixel-level SWD metric may be useful for subtle intra-modality translation. Our results demonstrate the need for further research into helpful metrics for medical <b>image</b> <b>translation.</b></p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=12--183218-iterative-solvers-in-adaptive-fem-philipp-bringmann-et-al-2024>(1/2 | 183/218) Iterative solvers in adaptive FEM (Philipp Bringmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Bringmann, Ani Miraçi, Dirk Praetorius. (2024)<br><strong>Iterative solvers in adaptive FEM</strong><br><button class=copy-to-clipboard title="Iterative solvers in adaptive FEM" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 41A25, 65N15, 65N30, 65N50, 65Y20, cs-NA, math-NA, math.NA<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07126v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07126v1.pdf filename=2404.07126v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This chapter provides an overview of state-of-the-art adaptive finite element methods (AFEMs) for the numerical solution of second-order elliptic partial differential equations (PDEs), where the primary focus is on the optimal interplay of local mesh refinement and iterative solution of the arising discrete systems. Particular emphasis is placed on the thorough description of the essential ingredients necessary to design adaptive algorithms of optimal complexity, i.e., algorithms that mathematically guarantee the optimal rate of convergence with respect to the overall computational cost and, hence, time. Crucially, adaptivity induces reliability of the computed numerical approximations by means of a-posteriori error control. This ensures that the error committed by the numerical scheme is bounded from above by computable quantities. The analysis of the adaptive algorithms is based on the study of appropriate quasi-error quantities that include and balance different components of the overall error. Importantly, the quasi-errors <b>stemming</b> from an adaptive algorithm with contractive iterative solver satisfy a centerpiece concept, namely, full R-linear convergence. This guarantees that the adaptive algorithm is essentially contracting this quasi-error at each step and it turns out to be the cornerstone for the optimal complexity of AFEM. The unified analysis of the adaptive algorithms is presented in the context of symmetric linear PDEs. Extensions to goal-oriented, non-symmetric, as well as non-linear PDEs are presented with suitable nested iterative solvers fitting into the general analytical framework of the linear symmetric case. Numerical experiments highlight the theoretical results and emphasize the practical relevance and gain of adaptivity with iterative solvers for numerical <b>simulations</b> with optimal complexity.</p></p class="citation"></blockquote><h3 id=22--184218-a-conservative-eulerian-finite-element-method-for-transport-and-diffusion-in-moving-domains-maxim-olshanskii-et-al-2024>(2/2 | 184/218) A conservative Eulerian finite element method for transport and diffusion in moving domains (Maxim Olshanskii et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maxim Olshanskii, Henry von Wahl. (2024)<br><strong>A conservative Eulerian finite element method for transport and diffusion in moving domains</strong><br><button class=copy-to-clipboard title="A conservative Eulerian finite element method for transport and diffusion in moving domains" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65M12, 65M20, 65M60, 65M85, cs-NA, math-NA, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07130v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07130v1.pdf filename=2404.07130v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper introduces a finite element method for an Eulerian formulation of partial differential equations governing the transport and diffusion of a scalar quantity in a time-dependent domain. The method follows the idea from Lehrenfeld & Olshanskii [ESAIM: M2AN, 53(2): 585-614, 2019] of a solution extension to realise the Eulearian time-stepping scheme. However, a reformulation of the partial differential equation is suggested to derive a scheme which conserves the quantity under consideration exactly on the discrete level. For the spatial discretisation, the paper considers an unfitted finite element method. Ghost-penalty stabilisation is used to release the discrete solution extension and gives a scheme robust against arbitrary intersections between the mesh and <b>geometry</b> interface. The stability is analysed for both first- and second-order backward differentiation formula versions of the scheme. Several numerical examples in two and three spatial dimensions are included to illustrate the potential of this method.</p></p class="citation"></blockquote><h2 id=csni-4>cs.NI (4)</h2><h3 id=14--185218-pacp-priority-aware-collaborative-perception-for-connected-and-autonomous-vehicles-zhengru-fang-et-al-2024>(1/4 | 185/218) PACP: Priority-Aware Collaborative Perception for Connected and Autonomous Vehicles (Zhengru Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengru Fang, Senkang Hu, Haonan An, Yuang Zhang, Jingjing Wang, Hangcheng Cao, Xianhao Chen, Yuguang Fang. (2024)<br><strong>PACP: Priority-Aware Collaborative Perception for Connected and Autonomous Vehicles</strong><br><button class=copy-to-clipboard title="PACP: Priority-Aware Collaborative Perception for Connected and Autonomous Vehicles" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 23<br>Keywords: Graph, Autoencoder, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06891v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06891v1.pdf filename=2404.06891v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Surrounding perceptions are quintessential for safe driving for connected and autonomous vehicles (CAVs), where the Bird&rsquo;s Eye View has been employed to accurately capture spatial relationships among vehicles. However, severe inherent limitations of BEV, like blind spots, have been identified. Collaborative perception has emerged as an effective solution to overcoming these limitations through data fusion from multiple views of surrounding vehicles. While most existing collaborative perception strategies adopt a fully connected <b>graph</b> predicated on <b>fairness</b> in transmissions, they often neglect the varying importance of individual vehicles due to channel variations and perception redundancy. To address these challenges, we propose a novel Priority-Aware Collaborative Perception (PACP) framework to employ a BEV-match mechanism to determine the priority levels based on the correlation between nearby CAVs and the ego vehicle for perception. By leveraging submodular optimization, we find near-optimal transmission rates, link connectivity, and compression metrics. Moreover, we deploy a deep learning-based adaptive <b>autoencoder</b> to modulate the image reconstruction quality under dynamic channel conditions. Finally, we conduct extensive studies and demonstrate that our scheme significantly outperforms the state-of-the-art schemes by 8.27% and 13.60%, respectively, in terms of utility and precision of the Intersection over Union.</p></p class="citation"></blockquote><h3 id=24--186218-emf-mitigation-via-5g-and-6g-mac-scheduling-silvio-mandelli-et-al-2024>(2/4 | 186/218) EMF Mitigation via 5G and 6G MAC Scheduling (Silvio Mandelli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Silvio Mandelli, Lorenzo Maggi, Bill Zheng, Christophe Grangeat, Azra Zejnilagic. (2024)<br><strong>EMF Mitigation via 5G and 6G MAC Scheduling</strong><br><button class=copy-to-clipboard title="EMF Mitigation via 5G and 6G MAC Scheduling" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06830v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06830v1.pdf filename=2404.06830v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High antenna directivity allows for high throughput transmission but also increases the exposure to electromagnetic field (EMF) of the end-users. Health regulations impose limitations on the incident power density, that generate a negative impact on network performance. In this work we focus at the slot-by-slot operations of a cellular Medium Access Control (MAC) scheduler to constrain the short-term EMF exposure upon real-time resource allocation, minimizing the impacts on network performance. We assume that the long-term EMF exposure is controlled by a proper outer-loop technique, that is not the object of this paper. Due to the minimal computational complexity allowed in MAC scheduling, existing solutions allowing practical implementation are few and focused at sub-optimal approaches curbing radio resource allocation. Our contribution is the derivation of a computationally efficient water-filling solution to allocate power and - then - resources, with a feasible integration of the necessary algorithms in the operations of a 5G MAC scheduler. We finally evaluate our proposal versus the prior art approaches with system level <b>simulations</b> with realistic modeling of physical and MAC level cellular procedures. We conclude that our proposal can control EMF with considerable less impact on network performance, making it a standout candidate for 5G and future 6G MAC scheduler implementations.</p></p class="citation"></blockquote><h3 id=34--187218-agent-driven-generative-semantic-communication-for-remote-surveillance-wanting-yang-et-al-2024>(3/4 | 187/218) Agent-driven Generative Semantic Communication for Remote Surveillance (Wanting Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanting Yang, Zehui Xiong, Yanli Yuan, Wenchao Jiang, Tony Q. S. Quek, Merouane Debbah. (2024)<br><strong>Agent-driven Generative Semantic Communication for Remote Surveillance</strong><br><button class=copy-to-clipboard title="Agent-driven Generative Semantic Communication for Remote Surveillance" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-LG, cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06997v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06997v1.pdf filename=2404.06997v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the era of 6G, featuring compelling visions of intelligent transportation system, digital twins, remote surveillance is poised to become a ubiquitous practice. The substantial data volume and frequent updates present challenges in wireless networks. To address this, we propose a novel agent-driven generative semantic communication (A-GSC) framework based on <b>reinforcement</b> <b>learning.</b> In contrast to the existing research on semantic communication (SemCom), which mainly focuses on semantic compression or semantic sampling, we seamlessly cascade both together by jointly considering the intrinsic attributes of source information and the contextual information regarding the task. Notably, the introduction of the generative artificial intelligence (GAI) enables the independent design of semantic encoders and decoders. In this work, we develop an agent-assisted semantic encoder leveraging the knowledge based soft actor-critic algorithm, which can track the semantic changes, channel condition, and sampling intervals, so as to perform adaptive semantic sampling. Accordingly, we design a semantic decoder with both predictive and generative capabilities, which consists of two tailored modules. Moreover, the effectiveness of the designed models has been verified based on the dataset generated from CDNet2014, and the performance gain of the overall A-GSC framework in both energy saving and reconstruction accuracy have been demonstrated.</p></p class="citation"></blockquote><h3 id=44--188218-responsible-federated-learning-in-smart-transportation-outlooks-and-challenges-xiaowen-huang-et-al-2024>(4/4 | 188/218) Responsible Federated Learning in Smart Transportation: Outlooks and Challenges (Xiaowen Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaowen Huang, Tao Huang, Shushi Gu, Shuguang Zhao, Guanglin Zhang. (2024)<br><strong>Responsible Federated Learning in Smart Transportation: Outlooks and Challenges</strong><br><button class=copy-to-clipboard title="Responsible Federated Learning in Smart Transportation: Outlooks and Challenges" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06777v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06777v1.pdf filename=2404.06777v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integrating artificial intelligence (AI) and <b>federated</b> <b>learning</b> (FL) in smart transportation has raised critical issues regarding their responsible use. Ensuring responsible AI is paramount for the stability and sustainability of intelligent transportation systems. Despite its importance, research on the responsible application of AI and FL in this domain remains nascent, with a paucity of in-depth investigations into their confluence. Our study analyzes the roles of FL in smart transportation, as well as the promoting effect of responsible AI on distributed smart transportation. Lastly, we discuss the challenges of developing and implementing responsible FL in smart transportation and propose potential solutions. By integrating responsible AI and <b>federated</b> <b>learning,</b> intelligent transportation systems are expected to achieve a higher degree of intelligence, personalization, safety, and transparency.</p></p class="citation"></blockquote><h2 id=csit-8>cs.IT (8)</h2><h3 id=18--189218-joint-active-and-passive-irs-aided-wireless-communication-elements-allocation-and-achievable-rate-chaoying-huang-et-al-2024>(1/8 | 189/218) Joint Active And Passive IRS Aided Wireless Communication: Elements Allocation and Achievable Rate (Chaoying Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaoying Huang, Wen Chen, Qingqing Wu. (2024)<br><strong>Joint Active And Passive IRS Aided Wireless Communication: Elements Allocation and Achievable Rate</strong><br><button class=copy-to-clipboard title="Joint Active And Passive IRS Aided Wireless Communication: Elements Allocation and Achievable Rate" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06880v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06880v2.pdf filename=2404.06880v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Equipping reflecting elements at the active intelligent reflecting surface (AIRS) enhances signal amplification capability but meanwhile incurs non-negligible amplification noise, which thus challenges the determination of elements allocation for maximizing achievable rate in multi-cooperative AIRS and passive IRS (PIRS) jointly aided wireless communication system. To tackle this issue, we consider the downlink communication from a single-antenna transmitter (Tx) to a single-antenna receiver (Rx), which aided by a pair of AIRS and PIRS with two different deployment orders. Specifically, we target to determine the number of AIRS/PIRS elements over both transmission orders under given deployment budget for the achievable rate maximization. Our analysis illustrates that the PIRS should be allocated more elements than the AIRS for achieving optimized rate and linear signal-to-noise ratio (SNR) scaling orders are attained in both schemes. <b>Simulation</b> results are provided to evaluate the proposed algorithm and compare the rate performance of the AIRS and PIRS jointly aided wireless system with various <b>benchmark</b> systems.</p></p class="citation"></blockquote><h3 id=28--190218-digital-over-the-air-computation-achieving-high-reliability-via-bit-slicing-jiawei-liu-et-al-2024>(2/8 | 190/218) Digital Over-the-Air Computation: Achieving High Reliability via Bit-Slicing (Jiawei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Liu, Yi Gong, Kaibin Huang. (2024)<br><strong>Digital Over-the-Air Computation: Achieving High Reliability via Bit-Slicing</strong><br><button class=copy-to-clipboard title="Digital Over-the-Air Computation: Achieving High Reliability via Bit-Slicing" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07121v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07121v1.pdf filename=2404.07121v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>6G mobile networks aim to realize ubiquitous intelligence at the network edge via distributed learning, sensing, and data analytics. Their common operation is to aggregate high-dimensional data, which causes a communication bottleneck that cannot be resolved using traditional orthogonal multi-access schemes. A promising solution, called over-the-air computation (AirComp), exploits channels&rsquo; waveform superposition property to enable simultaneous access, thereby overcoming the bottleneck. Nevertheless, its reliance on uncoded linear analog modulation exposes data to perturbation by noise and interference. Hence, the traditional analog AirComp falls short of meeting the high-reliability requirement for 6G. Overcoming the limitation of analog AirComp motivates this work, which focuses on developing a framework for digital AirComp. The proposed framework features digital modulation of each data value, integrated with the bit-slicing technique to allocate its bits to multiple symbols, thereby increasing the AirComp reliability. To optimally detect the aggregated digital symbols, we derive the optimal maximum a posteriori detector that is shown to outperform the traditional maximum likelihood detector. Furthermore, a comparative performance analysis of digital AirComp with respect to its analog counterpart with repetition coding is conducted to quantify the practical signal-to-noise ratio (SNR) regime favoring the proposed scheme. On the other hand, digital AirComp is enhanced by further development to feature awareness of heterogeneous bit importance levels and its exploitation in channel adaptation. Lastly, <b>simulation</b> results demonstrate the achivability of substantial reliability improvement of digital AirComp over its analog counterpart given the same channel uses.</p></p class="citation"></blockquote><h3 id=38--191218-on-the-performance-of-irs-assisted-ssk-and-rpm-over-rician-fading-channels-harsh-raj-et-al-2024>(3/8 | 191/218) On the Performance of IRS-Assisted SSK and RPM over Rician Fading Channels (Harsh Raj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harsh Raj, Ugrasen Singh, B. R. Manoj. (2024)<br><strong>On the Performance of IRS-Assisted SSK and RPM over Rician Fading Channels</strong><br><button class=copy-to-clipboard title="On the Performance of IRS-Assisted SSK and RPM over Rician Fading Channels" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07044v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07044v1.pdf filename=2404.07044v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents the index modulation, that is, the space-shift keying (SSK) and reflection phase modulation (RPM) schemes for intelligent reflecting surface (IRS)-assisted wireless network. IRS simultaneously reflects the incoming information signal from the base station and explicitly encodes the local information bits in the reflection phase shift of IRS elements. The phase shift of the IRS elements is employed according to local data from the RPM constellation. A joint detection using a maximum-likelihood (ML) decoder is performed for the SSK and RPM symbols over a realistic fading scenario modeled as the Rician fading channel. The pairwise error probability over Rician fading channels is derived and utilized to determine the average bit error rate. In addition, the ergodic capacity of the presented system is derived. The derived analytical results are verified and are in exact agreement with Monte-Carlo <b>simulations.</b></p></p class="citation"></blockquote><h3 id=48--192218-near-optimal-channel-estimation-for-dense-array-systems-mingyao-cui-et-al-2024>(4/8 | 192/218) Near-Optimal Channel Estimation for Dense Array Systems (Mingyao Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyao Cui, Zijian Zhang, Linglong Dai, Kaibin Huang. (2024)<br><strong>Near-Optimal Channel Estimation for Dense Array Systems</strong><br><button class=copy-to-clipboard title="Near-Optimal Channel Estimation for Dense Array Systems" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06806v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06806v1.pdf filename=2404.06806v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>By deploying a large number of antennas with sub-half-wavelength spacing in a compact space, dense array systems(DASs) can fully unleash the multiplexing-and-diversity gains of limited apertures. To acquire these gains, accurate channel state information acquisition is necessary but challenging due to the large antenna numbers. To overcome this obstacle, this paper reveals that exploiting the high spatial correlation of DAS channels is crucial while designing the observation matrix for optimal/near-optimal channel estimation. Firstly, we prove that the observation matrix design is equivalent to a time-domain duality of multiple-input multiple-output precoding, which can be ideally addressed by the water-filling principle. For practical realizations, a novel ice-filling algorithm is proposed to design amplitude-and-phase controllable observation matrices, and a majorization-minimization algorithm is proposed to address the phase-only controllable case. Particularly, we prove that the ice-filling algorithm can be viewed as a ``quantized" water-filling algorithm. To support the sub-optimality of the proposed designs, we provide comprehensive analyses on the achievable mean square errors and their asymptotic expressions. Finally, numerical <b>simulations</b> verify that our proposed channel estimation designs can achieve the near-optimal performance and outperform existing approaches significantly.</p></p class="citation"></blockquote><h3 id=58--193218-perfectly-secure-key-agreement-over-a-full-duplex-wireless-channel-gerhard-wunder-et-al-2024>(5/8 | 193/218) Perfectly Secure Key Agreement Over a Full Duplex Wireless Channel (Gerhard Wunder et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gerhard Wunder, Axel Flinth, Daniel Becker, Benedikt Groß. (2024)<br><strong>Perfectly Secure Key Agreement Over a Full Duplex Wireless Channel</strong><br><button class=copy-to-clipboard title="Perfectly Secure Key Agreement Over a Full Duplex Wireless Channel" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06952v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06952v1.pdf filename=2404.06952v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Secret key generation (SKG) between authenticated devices is a pivotal task for secure communications. Diffie-Hellman (DH) is de-facto standard but not post-quantum secure. In this paper, we shall invent and analyze a new <b>security</b> primitive that is specifically designed for WPAN. For WPAN, wireless channel-based SKG has been proposed but was not widely deployed due to its critical dependence on the channel&rsquo;s entropy which is uncontrollable. We formulate a different approach: We still exploit channel properties but mainly hinge on the reciprocity of the wireless channel and not on the channel&rsquo;s entropy. The radio advantage comes from the use of full duplex communication. We show that in this situation both legitimate parties can agree on a common secret key even without ever probing the channel at all. At the core is a new bisparse blind deconvolution scheme for which we prove correctness and information-theoretic, i.e. perfect, <b>security.</b> We show that, ultimately, a secret key can be extracted and give a lower bound for the number of secret key bits which is then verified by experiments.</p></p class="citation"></blockquote><h3 id=68--194218-new-partial-orders-of-polar-codes-for-bmsc-liuquan-yao-et-al-2024>(6/8 | 194/218) New Partial Orders of Polar Codes for BMSC (Liuquan Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liuquan Yao, Zhichao Liu, Yuan Li, Huazi Zhang, Jun Wang, Guiying Yan, Zhiming Ma. (2024)<br><strong>New Partial Orders of Polar Codes for BMSC</strong><br><button class=copy-to-clipboard title="New Partial Orders of Polar Codes for BMSC" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Business Email Compromise<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06736v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06736v1.pdf filename=2404.06736v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we define partial orders (POs) of polar codes based on the Bhattacharyya parameter and the bit-error probability, respectively. These POs are applicable to arbitrary binary memoryless symmetric channel (BMSC). Leveraging the extremal inequalities of polarization transformation, we derive new POs for BMSC based on the corresponding POs observed in the Binary Erasure Channel <b>(BEC).</b> %Additionally, we discover more special POs in the Binary Symmetric Channel (BSC). We provide examples that demonstrate the inability of existing POs to deduce these novel POs. Furthermore, we establish upper bounds for the expansion parameter $\beta$ if the polar codes constructed by $\beta$-expansion method obey these POs.</p></p class="citation"></blockquote><h3 id=78--195218-fractional-decoding-of-algebraic-geometry-codes-over-extension-fields-eduardo-camps-moreno-et-al-2024>(7/8 | 195/218) Fractional decoding of algebraic geometry codes over extension fields (Eduardo Camps-Moreno et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eduardo Camps-Moreno, Gretchen L. Matthews, Welington Santos. (2024)<br><strong>Fractional decoding of algebraic geometry codes over extension fields</strong><br><button class=copy-to-clipboard title="Fractional decoding of algebraic geometry codes over extension fields" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-AG, math-IT<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07201v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07201v1.pdf filename=2404.07201v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study algebraic <b>geometry</b> codes from curves over $\mathbb{F}_{q^\ell}$ through their virtual projections which are algebraic geometric codes over $\mathbb{F}<em>q$. We use the virtual projections to provide fractional decoding algorithms for the codes over $\mathbb{F}</em>{q^\ell}$. Fractional decoding seeks to perform error correction using a smaller fraction of $\mathbb{F}_q$-symbols than a typical decoding algorithm. In one instance, the bound on the number of correctable errors differs from the usual lower bound by the degree of a pole divisor of an annihilator function. In another, we view the virtual projections as interleaved codes to, with high probability, correct more errors than anticipated.</p></p class="citation"></blockquote><h3 id=88--196218-characterising-directed-and-undirected-metrics-of-high-order-interdependence-fernando-e-rosas-et-al-2024>(8/8 | 196/218) Characterising directed and undirected metrics of high-order interdependence (Fernando E. Rosas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fernando E. Rosas, Pedro A. M. Mediano, Michael Gastpar. (2024)<br><strong>Characterising directed and undirected metrics of high-order interdependence</strong><br><button class=copy-to-clipboard title="Characterising directed and undirected metrics of high-order interdependence" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07140v1.pdf filename=2404.07140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Systems of interest for theoretical or experimental work often exhibit high-order interactions, corresponding to statistical interdependencies in groups of variables that cannot be reduced to dependencies in subsets of them. While still under active development, the framework of partial information decomposition (PID) has emerged as the dominant approach to conceptualise and calculate high-order interdependencies. PID approaches can be grouped in two types: directed approaches that divide variables into sources and targets, and undirected approaches that treat all variables equally. Directed and undirected approaches are usually employed to investigate different scenarios, and hence little is known about how these two types of approaches may relate to each other, or if their corresponding quantities are linked in some way. In this paper we investigate the relationship between the redundancy-synergy index (RSI) and the O-information, which are practical metrics of directed and undirected high-order interdependencies, respectively. Our results reveal tight links between these two quantities, and provide interpretations of them in terms of likelihood ratios in a hypothesis testing setting, as well as in terms of projections in information <b>geometry.</b></p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--197218-fairem360-a-suite-for-responsible-entity-matching-nima-shahbazi-et-al-2024>(1/1 | 197/218) FairEM360: A Suite for Responsible Entity Matching (Nima Shahbazi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nima Shahbazi, Mahdi Erfanian, Abolfazl Asudeh, Fatemeh Nargesian, Divesh Srivastava. (2024)<br><strong>FairEM360: A Suite for Responsible Entity Matching</strong><br><button class=copy-to-clipboard title="FairEM360: A Suite for Responsible Entity Matching" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-CY, cs-DB, cs-LG, cs.DB<br>Keyword Score: 20<br>Keywords: Fairness, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07354v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07354v1.pdf filename=2404.07354v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Entity matching is one the earliest tasks that occur in the big data pipeline and is alarmingly exposed to unintentional biases that affect the quality of data. Identifying and mitigating the biases that exist in the data or are introduced by the matcher at this stage can contribute to promoting <b>fairness</b> in downstream tasks. This demonstration showcases FairEM360, a framework for 1) auditing the output of entity matchers across a wide range of <b>fairness</b> measures and paradigms, 2) providing potential explanations for the underlying reasons for unfairness, and 3) providing resolutions for the unfairness issues through an exploratory process with <b>human-in-the-loop</b> feedback, utilizing an ensemble of matchers. We aspire for FairEM360 to contribute to the prioritization of <b>fairness</b> as a key consideration in the evaluation of EM pipelines.</p></p class="citation"></blockquote><h2 id=quant-ph-5>quant-ph (5)</h2><h3 id=15--198218-a-modified-depolarization-approach-for-efficient-quantum-machine-learning-bikram-khanal-et-al-2024>(1/5 | 198/218) A Modified Depolarization Approach for Efficient Quantum Machine Learning (Bikram Khanal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bikram Khanal, Pablo Rivas. (2024)<br><strong>A Modified Depolarization Approach for Efficient Quantum Machine Learning</strong><br><button class=copy-to-clipboard title="A Modified Depolarization Approach for Efficient Quantum Machine Learning" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07330v1.pdf filename=2404.07330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum Computing in the Noisy Intermediate-Scale Quantum (NISQ) era has shown promising applications in machine learning, optimization, and cryptography. Despite the progress, challenges persist due to system noise, errors, and decoherence that complicate the <b>simulation</b> of quantum systems. The depolarization channel is a standard tool for simulating a quantum system&rsquo;s noise. However, modeling such noise for practical applications is computationally expensive when we have limited hardware resources, as is the case in the NISQ era. We propose a modified representation for a single-qubit depolarization channel with two Kraus operators based only on X and Z Pauli matrices. Our approach reduces the computational complexity from six to four matrix multiplications per execution of a channel. Experiments on a Quantum Machine Learning (QML) model on the Iris dataset across various circuit depths and depolarization rates validate that our approach maintains the model&rsquo;s accuracy while improving efficiency. This simplified noise model enables more scalable <b>simulations</b> of quantum circuits under depolarization, advancing capabilities in the NISQ era.</p></p class="citation"></blockquote><h3 id=25--199218-quantum-algorithms-to-simulate-quadratic-classical-hamiltonians-and-optimal-control-hari-krovi-2024>(2/5 | 199/218) Quantum algorithms to simulate quadratic classical Hamiltonians and optimal control (Hari Krovi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hari Krovi. (2024)<br><strong>Quantum algorithms to simulate quadratic classical Hamiltonians and optimal control</strong><br><button class=copy-to-clipboard title="Quantum algorithms to simulate quadratic classical Hamiltonians and optimal control" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-CC, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07303v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07303v1.pdf filename=2404.07303v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Simulation</b> of realistic classical mechanical systems is of great importance to many areas of engineering such as robotics, dynamics of rotating machinery and control theory. In this work, we develop quantum algorithms to estimate quantities of interest such as the kinetic energy in a given classical mechanical system in the presence of friction or damping as well as forcing or source terms, which makes the algorithm of practical interest. We show that for such systems, the quantum algorithm scales polynomially with the logarithm of the dimension of the system. We cast this problem in terms of Hamilton&rsquo;s equations of motion (equivalent to the first variation of the Lagrangian) and solve them using quantum algorithms for differential equations. We then consider the hardness of estimating the kinetic energy of a damped coupled oscillator system. We show that estimating the kinetic energy at a given time of this system to within additive precision is BQP hard when the strength of the damping term is bounded by an inverse polynomial in the number of qubits. We then consider the problem of designing optimal control of classical systems, which can be cast as the second variation of the Lagrangian. In this direction, we first consider the Riccati equation, which is a nonlinear differential equation ubiquitous in control theory. We give an efficient quantum algorithm to solve the Riccati differential equation well into the nonlinear regime. To our knowledge, this is the first example of any nonlinear differential equation that can be solved when the strength of the nonlinearity is asymptotically greater than the amount of dissipation. We then show how to use this algorithm to solve the linear quadratic regulator problem, which is an example of the Hamilton-Jacobi-Bellman equation.</p></p class="citation"></blockquote><h3 id=35--200218-quantum-tunneling-from-theory-to-error-mitigated-quantum-simulation-sorana-catrina-et-al-2024>(3/5 | 200/218) Quantum Tunneling: From Theory to Error-Mitigated Quantum Simulation (Sorana Catrina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sorana Catrina, Alexandra Băicoianu. (2024)<br><strong>Quantum Tunneling: From Theory to Error-Mitigated Quantum Simulation</strong><br><button class=copy-to-clipboard title="Quantum Tunneling: From Theory to Error-Mitigated Quantum Simulation" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-ET, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07034v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07034v1.pdf filename=2404.07034v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ever since the discussions about a possible quantum computer arised, quantum <b>simulations</b> have been at the forefront of possible utilities and the task of quantum <b>simulations</b> is one that promises quantum advantage. In recent years, <b>simulations</b> of large molecules through VQE or dynamics of many-body spin Hamiltonians may be possible, and even able to achieve useful results with the use of error mitigation techniques. Simulating smaller models is also important, and currently, in the NISQ (Noisy intermediate-scale quantum) era, it is easier and less prone to errors. This current study encompasses the theoretical background and the hardware aware circuit implementation of a quantum tunneling <b>simulation.</b> Specifically, this study presents the theoretical background needed for such implementation and highlights the main steps of development. Building on classic approaches of quantum tunneling <b>simulations,</b> this study improves the result of such <b>simulations</b> by employing error mitigation techniques (ZNE and REM) and uses them in conjunction with multiprogramming of the quantum chip for solving the hardware under-utilization problem that arises in such contexts. Moreover, we highlight the need for hardware-aware circuit implementations and discuss these considerations in detail to give an end-to-end workflow overview of quantum <b>simulations.</b></p></p class="citation"></blockquote><h3 id=45--201218-statistical-evaluation-of-571-gaas-quantum-point-contact-transistors-showing-the-07-anomaly-in-quantized-conductance-using-millikelvin-cryogenic-on-chip-multiplexing-pengcheng-ma-et-al-2024>(4/5 | 201/218) Statistical evaluation of 571 GaAs quantum point contact transistors showing the 0.7 anomaly in quantized conductance using millikelvin cryogenic on-chip multiplexing (Pengcheng Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengcheng Ma, Kaveh Delfanazari, Reuben K. Puddy, Jiahui Li, Moda Cao, Teng Yi, Jonathan P. Griffiths, Harvey E. Beere, David A. Ritchie, Michael J. Kelly, Charles G. Smith. (2024)<br><strong>Statistical evaluation of 571 GaAs quantum point contact transistors showing the 0.7 anomaly in quantized conductance using millikelvin cryogenic on-chip multiplexing</strong><br><button class=copy-to-clipboard title="Statistical evaluation of 571 GaAs quantum point contact transistors showing the 0.7 anomaly in quantized conductance using millikelvin cryogenic on-chip multiplexing" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cond-mat-mes-hall, cs-AR, cs-SY, eess-SY, quant-ph, quant-ph<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06784v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06784v1.pdf filename=2404.06784v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The mass production and the practical number of cryogenic quantum devices producible in a single chip are limited to the number of electrical contact pads and wiring of the cryostat or dilution refrigerator. It is, therefore, beneficial to contrast the measurements of hundreds of devices fabricated in a single chip in one cooldown process to promote the scalability, integrability, reliability, and reproducibility of quantum devices and to save evaluation time, cost and energy. Here, we use a cryogenic on-chip multiplexer architecture and investigate the statistics of the 0.7 anomaly observed on the first three plateaus of the <b>quantized</b> conductance of semiconductor quantum point contact (QPC) transistors. Our single chips contain 256 split gate field effect QPC transistors (QFET) each, with two 16-branch multiplexed source-drain and gate pads, allowing individual transistors to be selected, addressed and controlled through an electrostatic gate voltage process. A total of 1280 quantum transistors with nano-scale dimensions are patterned in 5 different chips of GaAs heterostructures. From the measurements of 571 functioning QPCs taken at temperatures T= 1.4 K and T= 40 mK, it is found that the spontaneous polarisation model and Kondo effect do not fit our results. Furthermore, some of the features in our data largely agreed with van Hove model with short-range interactions. Our approach provides further insight into the quantum mechanical properties and microscopic origin of the 0.7 anomaly in QPCs, paving the way for the development of semiconducting quantum circuits and integrated cryogenic electronics, for scalable quantum logic control, readout, synthesis, and processing applications.</p></p class="citation"></blockquote><h3 id=55--202218-certifying-almost-all-quantum-states-with-few-single-qubit-measurements-hsin-yuan-huang-et-al-2024>(5/5 | 202/218) Certifying almost all quantum states with few single-qubit measurements (Hsin-Yuan Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hsin-Yuan Huang, John Preskill, Mehdi Soleimanifar. (2024)<br><strong>Certifying almost all quantum states with few single-qubit measurements</strong><br><button class=copy-to-clipboard title="Certifying almost all quantum states with few single-qubit measurements" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-IT, cs-LG, math-IT, quant-ph, quant-ph<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07281v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07281v1.pdf filename=2404.07281v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Certifying that an n-qubit state synthesized in the lab is close to the target state is a fundamental task in quantum information science. However, existing rigorous protocols either require deep quantum circuits or exponentially many single-qubit measurements. In this work, we prove that almost all n-qubit target states, including those with exponential circuit complexity, can be certified from only O(n^2) single-qubit measurements. This result is established by a new technique that relates certification to the mixing time of a random walk. Our protocol has applications for <b>benchmarking</b> quantum systems, for optimizing quantum circuits to generate a desired target state, and for learning and verifying neural networks, tensor networks, and various other representations of quantum states using only single-qubit measurements. We show that such verified representations can be used to efficiently predict highly non-local properties that would otherwise require an exponential number of measurements. We demonstrate these applications in numerical experiments with up to 120 qubits, and observe advantage over existing methods such as cross-entropy <b>benchmarking</b> (XEB).</p></p class="citation"></blockquote><h2 id=physicsoptics-1>physics.optics (1)</h2><h3 id=11--203218-scwatter-open-source-coupled-wave-scattering-simulation-for-spectroscopy-and-microscopy-ruijiao-sun-et-al-2024>(1/1 | 203/218) sCWatter: Open source coupled wave scattering simulation for spectroscopy and microscopy (Ruijiao Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruijiao Sun, Rohith Reddy, David Mayerich. (2024)<br><strong>sCWatter: Open source coupled wave scattering simulation for spectroscopy and microscopy</strong><br><button class=copy-to-clipboard title="sCWatter: Open source coupled wave scattering simulation for spectroscopy and microscopy" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.optics<br>Categories: cs-MS, physics-optics, physics.optics<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07293v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07293v1.pdf filename=2404.07293v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several emerging microscopy imaging methods rely on complex interactions between the incident light and the sample. These include interferometry, spectroscopy, and nonlinear optics. Reconstructing a sample from the measured scattered field relies on fast and accurate optical models. Fast approaches like ray tracing and the Born approximation have limitations that are limited when working with high numerical apertures. This paper presents sCWatter, an open-source tool that utilizes coupled wave theory (CWT) to simulate and visualize the 3D electric field scattered by complex samples. The sample refractive index is specified on a volumetric grid, while the incident field is provided as a 2D image orthogonal to the optical path. We introduce connection equations between layers that significantly reduce the dimensionality of the CW linear system, enabling efficient parallel processing on consumer hardware. Further optimizations using Intel MKL and CUDA significantly accelerate both field <b>simulation</b> and visualization.</p></p class="citation"></blockquote><h2 id=csds-3>cs.DS (3)</h2><h3 id=13--204218-an-asymptotically-optimal-algorithm-for-generating-bin-cardinalities-luc-devroye-et-al-2024>(1/3 | 204/218) An asymptotically optimal algorithm for generating bin cardinalities (Luc Devroye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luc Devroye, Dimitrios Los. (2024)<br><strong>An asymptotically optimal algorithm for generating bin cardinalities</strong><br><button class=copy-to-clipboard title="An asymptotically optimal algorithm for generating bin cardinalities" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: 65C10, 60C05, 11K45, 68W20, 68U20, 68W27, 68W40, G-3; G-2-m; F-2-2, cs-DM, cs-DS, cs.DS<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07011v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07011v1.pdf filename=2404.07011v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the balls-into-bins setting, $n$ balls are thrown uniformly at random into $n$ bins. The na"{i}ve way to generate the final load vector takes $\Theta(n)$ time. However, it is well-known that this load vector has with high probability bin cardinalities of size $\Theta(\frac{\log n}{\log \log n})$. Here, we present an algorithm in the RAM model that generates the bin cardinalities of the final load vector in the optimal $\Theta(\frac{\log n}{\log \log n})$ time in expectation and with high probability. Further, the algorithm that we present is still optimal for any $m \in [n, n \log n]$ balls and can also be used as a building block to efficiently simulate more involved load balancing algorithms. In particular, for the Two-Choice algorithm, which samples two bins in each step and allocates to the least-loaded of the two, we obtain roughly a quadratic speed-up over the na"{i}ve <b>simulation.</b></p></p class="citation"></blockquote><h3 id=23--205218-language-generation-in-the-limit-jon-kleinberg-et-al-2024>(2/3 | 205/218) Language Generation in the Limit (Jon Kleinberg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jon Kleinberg, Sendhil Mullainathan. (2024)<br><strong>Language Generation in the Limit</strong><br><button class=copy-to-clipboard title="Language Generation in the Limit" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-AI, cs-CL, cs-DS, cs-LG, cs.DS<br>Keyword Score: 20<br>Keywords: Language Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06757v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06757v1.pdf filename=2404.06757v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although current <b>large</b> <b>language</b> <b>models</b> are complex, the most basic specifications of the underlying <b>language</b> <b>generation</b> problem itself are simple to state: given a finite set of training samples from an unknown <b>language,</b> <b>produce</b> valid new strings from the <b>language</b> <b>that</b> don&rsquo;t already appear in the training data. Here we ask what we can conclude about <b>language</b> <b>generation</b> using only this specification, without further assumptions. In particular, suppose that an adversary enumerates the strings of an unknown target <b>language</b> <b>L</b> that is known only to come from one of a possibly infinite list of candidates. A computational agent is trying to learn to generate from this <b>language;</b> <b>we</b> say that the agent generates from L in the limit if after some finite point in the enumeration of L, the agent is able to produce new elements that come exclusively from L and that have not yet been presented by the adversary. Our main result is that there is an agent that is able to generate in the limit for every countable list of candidate <b>languages.</b> <b>This</b> contrasts dramatically with negative results due to Gold and Angluin in a well-studied model of <b>language</b> <b>learning</b> where the goal is to identify an unknown <b>language</b> <b>from</b> samples; the difference between these results suggests that identifying a <b>language</b> <b>is</b> a fundamentally different problem than generating from it.</p></p class="citation"></blockquote><h3 id=33--206218-fully-dynamic-correlation-clustering-breaking-3-approximation-soheil-behnezhad-et-al-2024>(3/3 | 206/218) Fully Dynamic Correlation Clustering: Breaking 3-Approximation (Soheil Behnezhad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soheil Behnezhad, Moses Charikar, Vincent Cohen-Addad, Alma Ghafari, Weiyun Ma. (2024)<br><strong>Fully Dynamic Correlation Clustering: Breaking 3-Approximation</strong><br><button class=copy-to-clipboard title="Fully Dynamic Correlation Clustering: Breaking 3-Approximation" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06797v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06797v1.pdf filename=2404.06797v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the classic correlation <b>clustering</b> in the dynamic setting. Given $n$ objects and a complete labeling of the object-pairs as either similar or dissimilar, the goal is to partition the objects into arbitrarily many clusters while minimizing disagreements with the labels. In the dynamic setting, an update consists of a flip of a label of an edge. In a breakthrough result, [BDHSS, FOCS'19] showed how to maintain a 3-approximation with polylogarithmic update time by providing a dynamic implementation of the \pivot{} algorithm of [ACN, STOC'05]. Since then, it has been a major open problem to determine whether the 3-approximation barrier can be broken in the fully dynamic setting. In this paper, we resolve this problem. Our algorithm, \modifiedpivot{}, locally improves the output of \pivot{} by moving some vertices to other existing clusters or new singleton clusters. We present an analysis showing that this modification does indeed improve the approximation to below 3. We also show that its output can be maintained in polylogarithmic time per update.</p></p class="citation"></blockquote><h2 id=q-finst-1>q-fin.ST (1)</h2><h3 id=11--207218-predicting-mergers-and-acquisitions-in-competitive-industries-a-model-based-on-temporal-dynamics-and-industry-networks-dayu-yang-2024>(1/1 | 207/218) Predicting Mergers and Acquisitions in Competitive Industries: A Model Based on Temporal Dynamics and Industry Networks (Dayu Yang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dayu Yang. (2024)<br><strong>Predicting Mergers and Acquisitions in Competitive Industries: A Model Based on Temporal Dynamics and Industry Networks</strong><br><button class=copy-to-clipboard title="Predicting Mergers and Acquisitions in Competitive Industries: A Model Based on Temporal Dynamics and Industry Networks" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.ST<br>Categories: cs-LG, cs-SI, q-fin-GN, q-fin-ST, q-fin.ST<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07298v1.pdf filename=2404.07298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>M&amp;A activities are pivotal for market consolidation, enabling firms to augment market power through strategic complementarities. Existing research often overlooks the peer effect, the mutual influence of M&amp;A behaviors among firms, and fails to capture complex interdependencies within industry networks. Common approaches suffer from reliance on ad-hoc feature engineering, data truncation leading to significant information loss, reduced predictive accuracy, and challenges in real-world application. Additionally, the rarity of M&amp;A events necessitates data rebalancing in conventional models, introducing bias and undermining prediction reliability. We propose an innovative M&amp;A predictive model utilizing the Temporal Dynamic Industry Network (TDIN), leveraging temporal point processes and deep learning to adeptly capture industry-wide M&amp;A dynamics. This model facilitates accurate, detailed deal-level predictions without arbitrary data manipulation or rebalancing, demonstrated through superior evaluation results from M&amp;A cases between January 1997 and December 2020. Our approach marks a significant improvement over traditional models by providing detailed insights into M&amp;A activities and strategic <b>recommendations</b> for specific firms.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--208218-temperature-prediction-for-stored-grain-a-multi-model-fusion-strategy-based-on-machine-learning-donghao-chen-et-al-2024>(1/1 | 208/218) Temperature Prediction for Stored Grain: A Multi-model Fusion Strategy Based on Machine Learning (Donghao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Donghao Chen, Binkun Liu. (2024)<br><strong>Temperature Prediction for Stored Grain: A Multi-model Fusion Strategy Based on Machine Learning</strong><br><button class=copy-to-clipboard title="Temperature Prediction for Stored Grain: A Multi-model Fusion Strategy Based on Machine Learning" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07175v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07175v1.pdf filename=2404.07175v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Temperature fluctuations significantly affect microorganism growth and pest activity in grain stacks. Thus, precise monitoring and forecasting of grain stack temperature are essential for maintaining the quality and safety of grain storage. This paper proposes a multi-model fusion approach to predict grain temperature using historical temperature data of stored grains and meteorological data from the region. Based on the proposed approaches, four distinct machine learning models, namely Adaboost, decision tree, extra trees, and random forest, are first developed. These models are then <b>fine-tuned</b> through parameter optimization to enhance their predictive capabilities. Subsequently, the optimized models are combined to form different ensemble models. In essence, the fusion process integrates the predictions of each individual model as new feature inputs into the prediction model. Furthermore, the study utilizes the random forest to identify the key factors influencing grain temperature, providing insights into the importance of different influencing factors. The experimental results demonstrate that the fusion models proposed in this paper have achieved higher prediction accuracy and robustness compared with the traditional prediction methods (i.e., single-model prediction). Additionally, the analysis of feature importance also offers empirical evidence for understanding the factors influencing grain temperature.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--209218-data-driven-quasiconformal-morphodynamic-flows-salem-mosleh-et-al-2024>(1/1 | 209/218) Data-driven quasiconformal morphodynamic flows (Salem Mosleh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Salem Mosleh, Gary P. T. Choi, L. Mahadevan. (2024)<br><strong>Data-driven quasiconformal morphodynamic flows</strong><br><button class=copy-to-clipboard title="Data-driven quasiconformal morphodynamic flows" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cond-mat-soft, cs-CG, cs.CG, physics-bio-ph, q-bio-QM<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07073v1.pdf filename=2404.07073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Temporal imaging of biological epithelial structures yields shape data at <b>discrete</b> <b>time</b> points, leading to a natural question: how can we reconstruct the most likely path of growth patterns consistent with these <b>discrete</b> <b>observations?</b> We present a physically plausible framework to solve this inverse problem by creating a framework that generalizes quasiconformal maps to quasiconformal flows. By allowing for the spatio-temporal variation of the shear and dilatation fields during the growth process, subject to regulatory mechanisms, we are led to a type of generalized Ricci flow. When guided by observational data associated with surface shape as a function of time, this leads to a constrained optimization problem. Deploying our data-driven algorithmic approach to the shape of insect wings, leaves and even sculpted faces, we show how optimal quasiconformal flows allow us to characterize the morphogenesis of a range of surfaces.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--210218-pod-suboptimal-control-of-evolution-problems-theory-and-applications-stefan-banholzer-et-al-2024>(1/1 | 210/218) POD Suboptimal Control of Evolution Problems: Theory and Applications (Stefan Banholzer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefan Banholzer, Dennis Beermann, Luca Mechelli, Stefan Volkwein. (2024)<br><strong>POD Suboptimal Control of Evolution Problems: Theory and Applications</strong><br><button class=copy-to-clipboard title="POD Suboptimal Control of Evolution Problems: Theory and Applications" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-NA, math-AP, math-NA, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07015v1.pdf filename=2404.07015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The work is organized as follows. First an introduction is given in Chapter 1. In Chapter 2 we introduce the POD method in finite and infinite-dimensional Hilbert spaces and discuss various applications. Chapter 3 is devoted to to POD-based Galerkin schemes for evolution problems. Mainly, we study linear problems taking different discretization methods into account. We provide a certified a-priori and a-posteriori error analysis. Furthermore, the numerical realizations are explained and illustrated by test examples. Quadratic programming problems governed by liner evolution problems are investigated in Chapter 4. As in Chapter 3 we present a certified a-priori and a-posteriori error analysis. Moreover, we discuss basis update strategies. In Chapter 5 we give an outlook to further directions in reduced-order modeling in optimal control and optimization. More precisely, a nonlinear optimal control problem is studied. Moreover, state-constrained optimization problems are solved by a tailored combination of primal-dual active set methods and POD-aesed reduced-order modeling. Furthermore, POD Galerkin methods for multiobjective optimal control problems are investigated. Finally, some required theoretical foundations are <b>summarized</b> in the appendix.</p></p class="citation"></blockquote><h2 id=cspf-1>cs.PF (1)</h2><h3 id=11--211218-impact-of-extensions-on-browser-performance-an-empirical-study-on-google-chrome-bihui-jin-et-al-2024>(1/1 | 211/218) Impact of Extensions on Browser Performance: An Empirical Study on Google Chrome (Bihui Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bihui Jin, Heng Li, Ying Zou. (2024)<br><strong>Impact of Extensions on Browser Performance: An Empirical Study on Google Chrome</strong><br><button class=copy-to-clipboard title="Impact of Extensions on Browser Performance: An Empirical Study on Google Chrome" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PF<br>Categories: cs-HC, cs-PF, cs-SE, cs.PF<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06827v1.pdf filename=2404.06827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Web browsers have been used widely by users to conduct various online activities, such as information seeking or online shopping. To improve user experience and extend the functionality of browsers, practitioners provide mechanisms to allow users to install third-party-provided plugins (i.e., extensions) on their browsers. However, little is known about the performance implications caused by such extensions. In this paper, we conduct an empirical study to understand the impact of extensions on the user-perceived performance (i.e., energy consumption and page load time) of Google Chrome, the most popular browser. We study a total of 72 representative extensions from 11 categories (e.g., Developer Tools and Sports). We observe that browser performance can be negatively impacted by the use of extensions, even when the extensions are used in unintended circumstances (e.g., when logging into an extension is not granted but required, or when an extension is not used for designated websites). We also identify a set of factors that significantly influence the performance impact of extensions, such as code complexity and privacy practices (i.e., collection of user data) adopted by the extensions. Based on our empirical observations, we provide <b>recommendations</b> for developers and users to mitigate the performance impact of browser extensions, such as conducting performance testing and optimization for unintended usage scenarios of extensions, or adhering to proper usage practices of extensions (e.g., logging into an extension when required).</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--212218-algorithms-and-analysis-for-optimizing-robust-objectives-in-fair-machine-learning-cyrus-cousins-2024>(1/1 | 212/218) Algorithms and Analysis for Optimizing Robust Objectives in Fair Machine Learning (Cyrus Cousins, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cyrus Cousins. (2024)<br><strong>Algorithms and Analysis for Optimizing Robust Objectives in Fair Machine Learning</strong><br><button class=copy-to-clipboard title="Algorithms and Analysis for Optimizing Robust Objectives in Fair Machine Learning" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06703v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06703v1.pdf filename=2404.06703v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The original position or veil of ignorance argument of John Rawls, perhaps the most famous argument for egalitarianism, states that our concept of <b>fairness,</b> justice, or welfare should be decided from behind a veil of ignorance, and thus must consider everyone impartially (invariant to our identity). This can be posed as a zero-sum game, where a Daemon constructs a world, and an adversarial Angel then places the Daemon into the world. This game incentivizes the Daemon to maximize the minimum utility over all people (i.e., to maximize egalitarian welfare). In some sense, this is the most extreme form of risk aversion or robustness, and we show that by weakening the Angel, milder robust objectives arise, which we argue are effective robust proxies for fair learning or allocation tasks. In particular, the utilitarian, Gini, and power-mean welfare concepts arise from special cases of the adversarial game, which has philosophical implications for the understanding of each of these concepts. We also motivate a new <b>fairness</b> concept that essentially fuses the nonlinearity of the power-mean with the piecewise nature of the Gini class. Then, exploiting the relationship between <b>fairness</b> and robustness, we show that these robust <b>fairness</b> concepts can all be efficiently optimized under mild conditions via standard maximin optimization techniques. Finally, we show that such methods apply in machine learning contexts, and moreover we show generalization bounds for robust fair machine learning tasks.</p></p class="citation"></blockquote><h2 id=csne-3>cs.NE (3)</h2><h3 id=13--213218-neural-optimizer-equation-decay-function-and-learning-rate-schedule-joint-evolution-brandon-morgan-et-al-2024>(1/3 | 213/218) Neural Optimizer Equation, Decay Function, and Learning Rate Schedule Joint Evolution (Brandon Morgan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brandon Morgan, Dean Hougen. (2024)<br><strong>Neural Optimizer Equation, Decay Function, and Learning Rate Schedule Joint Evolution</strong><br><button class=copy-to-clipboard title="Neural Optimizer Equation, Decay Function, and Learning Rate Schedule Joint Evolution" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06679v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06679v1.pdf filename=2404.06679v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A major contributor to the quality of a deep learning model is the selection of the optimizer. We propose a new dual-joint search space in the realm of neural optimizer search (NOS), along with an integrity check, to automate the process of finding deep learning optimizers. Our dual-joint search space simultaneously allows for the optimization of not only the update equation, but also internal decay functions and learning rate schedules for optimizers. We search the space using our proposed mutation-only, particle-based genetic algorithm able to be massively parallelized for our domain-specific problem. We evaluate our candidate optimizers on the CIFAR-10 dataset using a small ConvNet. To assess generalization, the final optimizers were then transferred to large-scale image classification on CIFAR- 100 and TinyImageNet, while also being <b>fine-tuned</b> on Flowers102, Cars196, and Caltech101 using EfficientNetV2Small. We found multiple optimizers, learning rate schedules, and Adam variants that outperformed Adam, as well as other standard deep learning optimizers, across the image classification tasks.</p></p class="citation"></blockquote><h3 id=23--214218-semantically-correlated-memories-in-a-dense-associative-model-thomas-f-burns-2024>(2/3 | 214/218) Semantically-correlated memories in a dense associative model (Thomas F Burns, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas F Burns. (2024)<br><strong>Semantically-correlated memories in a dense associative model</strong><br><button class=copy-to-clipboard title="Semantically-correlated memories in a dense associative model" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: 68T07, 92B20, 68T01, 00A69, I-2; I-5; I-4; J-2; J-3, cs-AI, cs-LG, cs-NE, cs.NE, q-bio-NC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07123v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07123v2.pdf filename=2404.07123v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>I introduce a novel associative memory model named Correlated Dense Associative Memory (CDAM), which integrates both auto- and hetero-association in a unified framework for continuous-valued memory patterns. Employing an arbitrary <b>graph</b> structure to semantically link memory patterns, CDAM is theoretically and numerically analysed, revealing four distinct dynamical modes: auto-association, narrow hetero-association, wide hetero-association, and neutral quiescence. Drawing inspiration from inhibitory modulation studies, I employ anti-Hebbian learning rules to control the range of hetero-association, extract multi-scale representations of community structures in <b>graphs,</b> and stabilise the recall of temporal sequences. Experimental demonstrations showcase CDAM&rsquo;s efficacy in handling real-world data, replicating a classical neuroscience experiment, performing image retrieval, and simulating arbitrary finite automata.</p></p class="citation"></blockquote><h3 id=33--215218-a-tight-o4kp_c-runtime-bound-for-a-μ1-ga-on-jump_k-for-realistic-crossover-probabilities-andre-opris-et-al-2024>(3/3 | 215/218) A Tight $O(4^k/p_c)$ Runtime Bound for a ($μ$+1) GA on Jump$_k$ for Realistic Crossover Probabilities (Andre Opris et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andre Opris, Johannes Lengler, Dirk Sudholt. (2024)<br><strong>A Tight $O(4^k/p_c)$ Runtime Bound for a ($μ$+1) GA on Jump$_k$ for Realistic Crossover Probabilities</strong><br><button class=copy-to-clipboard title="A Tight $O(4^k/p_c)$ Runtime Bound for a ($μ$+1) GA on Jump$_k$ for Realistic Crossover Probabilities" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: 68Q25, 68Q87, 68W20, 68W40, 68W50, F-2-2; G-3, cs-NE, cs.NE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07061v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07061v1.pdf filename=2404.07061v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Jump$_k$ <b>benchmark</b> was the first problem for which crossover was proven to give a speedup over mutation-only evolutionary algorithms. Jansen and Wegener (2002) proved an upper bound of $O({\rm poly}(n) + 4^k/p_c)$ for the ($\mu$+1)~Genetic Algorithm ($(\mu+1)$ GA), but only for unrealistically small crossover probabilities $p_c$. To this date, it remains an open problem to prove similar upper bounds for realistic~$p_c$; the best known runtime bound for $p_c = \Omega(1)$ is $O((n/\chi)^{k-1})$, $\chi$ a positive constant. Using recently developed techniques, we analyse the evolution of the population diversity, measured as sum of pairwise Hamming distances, for a variant of the \muga on Jump$_k$. We show that population diversity converges to an equilibrium of near-perfect diversity. This yields an improved and tight time bound of $O(\mu n \log(k) + 4^k/p_c)$ for a range of~$k$ under the mild assumptions $p_c = O(1/k)$ and $\mu \in \Omega(kn)$. For all constant~$k$ the restriction is satisfied for some $p_c = \Omega(1)$. Our work partially solves a problem that has been open for more than 20 years.</p></p class="citation"></blockquote><h2 id=csdm-1>cs.DM (1)</h2><h3 id=11--216218-probabilistic-estimates-of-the-diameters-of-the-rubiks-cube-groups-so-hirata-2024>(1/1 | 216/218) Probabilistic estimates of the diameters of the Rubik&rsquo;s Cube groups (So Hirata, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>So Hirata. (2024)<br><strong>Probabilistic estimates of the diameters of the Rubik&rsquo;s Cube groups</strong><br><button class=copy-to-clipboard title="Probabilistic estimates of the diameters of the Rubik's Cube groups" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs-DS, cs.DM, math-CO, math-PR<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07337v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07337v1.pdf filename=2404.07337v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The diameter of the Cayley <b>graph</b> of the Rubik&rsquo;s Cube group is the fewest number of turns needed to solve the Cube from any initial configurations. For the 2$\times$2$\times$2 Cube, the diameter is 11 in the half-turn metric, 14 in the quarter-turn metric, 19 in the semi-quarter-turn metric, and 10 in the bi-quarter-turn metric. For the 3$\times$3$\times$3 Cube, the diameter was determined by Rikicki et al. to be 20 in the half-turn metric and 26 in the quarter-turn metric. This study shows that a modified version of the coupon collector&rsquo;s problem in probabilistic theory can predict the diameters correctly for both 2$\times$2$\times$2 and 3$\times$3$\times$3 Cubes insofar as the quarter-turn metric is adopted. In the half-turn metric, the diameters are overestimated by one and two, respectively, for the 2$\times$2$\times$2 and 3$\times$3$\times$3 Cubes, whereas for the 2$\times$2$\times$2 Cube in the semi-quarter-turn and bi-quarter-turn metrics, they are overestimated by two and underestimated by one, respectively. Invoking the same probabilistic logic, the diameters of the 4$\times$4$\times$4 and 5$\times$5$\times$5 Cubes are predicted to be 48 (41) and 68 (58) in the quarter-turn (half-turn) metric, whose precise determinations are far beyond reach of classical supercomputing. It is shown that the probabilistically estimated diameter is accurately approximated by $\ln N / \ln r + \ln N / r$, where $N$ is the number of configurations and $r$ is the branching ratio.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--217218-on-the-existence-of-δ-temporal-cliques-in-random-simple-temporal-graphs-george-b-mertzios-et-al-2024>(1/1 | 217/218) On the existence of $δ$-temporal cliques in random simple temporal graphs (George B. Mertzios et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>George B. Mertzios, Sotiris Nikoletseas, Christoforos Raptopoulos, Paul G. Spirakis. (2024)<br><strong>On the existence of $δ$-temporal cliques in random simple temporal graphs</strong><br><button class=copy-to-clipboard title="On the existence of $δ$-temporal cliques in random simple temporal graphs" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07147v1.pdf filename=2404.07147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider random simple temporal <b>graphs</b> in which every edge of the complete <b>graph</b> $K_n$ appears once within the time interval [0,1] independently and uniformly at random. Our main result is a sharp threshold on the size of any maximum $\delta$-clique (namely a clique with edges appearing at most $\delta$ apart within [0,1]) in random instances of this model, for any constant~$\delta$. In particular, using the probabilistic method, we prove that the size of a maximum $\delta$-clique is approximately $\frac{2\log{n}}{\log{\frac{1}{\delta}}}$ with high probability (whp). What seems surprising is that, even though the random simple temporal <b>graph</b> contains $\Theta(n^2)$ overlapping $\delta$-windows, which (when viewed separately) correspond to different random instances of the Erdos-Renyi random <b>graphs</b> model, the size of the maximum $\delta$-clique in the former model and the maximum clique size of the latter are approximately the same. Furthermore, we show that the minimum interval containing a $\delta$-clique is $\delta-o(\delta)$ whp. We use this result to show that any polynomial time algorithm for $\delta$-TEMPORAL CLIQUE is unlikely to have very large probability of success.</p></p class="citation"></blockquote><h2 id=csdl-1>cs.DL (1)</h2><h3 id=11--218218-linked-open-data-per-la-valorizzazione-di-collezioni-culturali-il-dataset-mythlod-valentina-pasqual-et-al-2024>(1/1 | 218/218) Linked open data per la valorizzazione di collezioni culturali: il dataset mythLOD (Valentina Pasqual et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Valentina Pasqual, Francesca Tomasi. (2024)<br><strong>Linked open data per la valorizzazione di collezioni culturali: il dataset mythLOD</strong><br><button class=copy-to-clipboard title="Linked open data per la valorizzazione di collezioni culturali: il dataset mythLOD" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-DL, cs.DL<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07006v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07006v1.pdf filename=2404.07006v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The formal representation of cultural metadata has always been a challenge, considering both the heterogeneity of cultural objects and the need to document the interpretive act exercised by experts. This article provides an overview of the revalorization of the digital collection Mythologiae in Linked Open Data format. The research aims to explore the data of a collection of artworks (Mythologiae) by promoting the potential of the Semantic Web, focusing particularly on the formal representation of the association of cultural objects with literary sources, as realized by experts, also documenting their interpretations. The workflow consisted of defining the data model, cleaning and disambiguating the data, converting it (from tabular structure to <b>graph),</b> and conducting testing activities (particularly expert domain review of the dataset through competency questions and data visualizations). The result is the mythLOD platform, which presents the dataset and detailed research documentation. Additionally, the platform hosts two data visualization spaces (the online catalogue and a data storytelling experiment on the case study of the Aeneid) that enrich the project documentation as user-friendly test units for the dataset and constitute an additional project documentation tool and exploration of the collection.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/ title="arXiv @ 2024.04.11" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.04.11</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Bandit Algorithm Basic</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-37>cs.CL (37)</a><ul><li><a href=#137--1218-llms-in-biomedicine-a-study-on-clinical-named-entity-recognition-masoud-monajatipoor-et-al-2024>(1/37 | 1/218) LLMs in Biomedicine: A study on clinical Named Entity Recognition (Masoud Monajatipoor et al., 2024)</a></li><li><a href=#237--2218-superposition-prompting-improving-and-accelerating-retrieval-augmented-generation-thomas-merth-et-al-2024>(2/37 | 2/218) Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation (Thomas Merth et al., 2024)</a></li><li><a href=#337--3218-onco-retriever-generative-classifier-for-retrieval-of-ehr-records-in-oncology-shashi-kant-gupta-et-al-2024>(3/37 | 3/218) Onco-Retriever: Generative Classifier for Retrieval of EHR Records in Oncology (Shashi Kant Gupta et al., 2024)</a></li><li><a href=#437--4218-xnlieu-a-dataset-for-cross-lingual-nli-in-basque-maite-heredia-et-al-2024>(4/37 | 4/218) XNLIeu: a dataset for cross-lingual NLI in Basque (Maite Heredia et al., 2024)</a></li><li><a href=#537--5218-dynamic-generation-of-personalities-with-large-language-models-jianzhi-liu-et-al-2024>(5/37 | 5/218) Dynamic Generation of Personalities with Large Language Models (Jianzhi Liu et al., 2024)</a></li><li><a href=#637--6218-hybrid-multi-stage-decoding-for-few-shot-ner-with-entity-aware-contrastive-learning-peipei-liu-et-al-2024>(6/37 | 6/218) Hybrid Multi-stage Decoding for Few-shot NER with Entity-aware Contrastive Learning (Peipei Liu et al., 2024)</a></li><li><a href=#737--7218-control-dag-constrained-decoding-for-non-autoregressive-directed-acyclic-t5-using-weighted-finite-state-automata-jinghong-chen-et-al-2024>(7/37 | 7/218) Control-DAG: Constrained Decoding for Non-Autoregressive Directed Acyclic T5 using Weighted Finite State Automata (Jinghong Chen et al., 2024)</a></li><li><a href=#837--8218-not-all-contexts-are-equal-teaching-llms-credibility-aware-generation-ruotong-pan-et-al-2024>(8/37 | 8/218) Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation (Ruotong Pan et al., 2024)</a></li><li><a href=#937--9218-simpler-becomes-harder-do-llms-exhibit-a-coherent-behavior-on-simplified-corpora-miriam-anschütz-et-al-2024>(9/37 | 9/218) Simpler becomes Harder: Do LLMs Exhibit a Coherent Behavior on Simplified Corpora? (Miriam Anschütz et al., 2024)</a></li><li><a href=#1037--10218-grasame-injecting-token-level-structural-information-to-pretrained-language-models-via-graph-guided-self-attention-mechanism-shuzhou-yuan-et-al-2024>(10/37 | 10/218) GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism (Shuzhou Yuan et al., 2024)</a></li><li><a href=#1137--11218-metacheckgpt----a-multi-task-hallucination-detector-using-llm-uncertainty-and-meta-models-rahul-mehta-et-al-2024>(11/37 | 11/218) MetaCheckGPT &ndash; A Multi-task Hallucination Detector Using LLM Uncertainty and Meta-models (Rahul Mehta et al., 2024)</a></li><li><a href=#1237--12218-transferable-and-efficient-non-factual-content-detection-via-probe-training-with-offline-consistency-checking-xiaokang-zhang-et-al-2024>(12/37 | 12/218) Transferable and Efficient Non-Factual Content Detection via Probe Training with Offline Consistency Checking (Xiaokang Zhang et al., 2024)</a></li><li><a href=#1337--13218-from-model-centered-to-human-centered-revision-distance-as-a-metric-for-text-evaluation-in-llms-based-applications-yongqiang-ma-et-al-2024>(13/37 | 13/218) From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications (Yongqiang Ma et al., 2024)</a></li><li><a href=#1437--14218-accelerating-inference-in-large-language-models-with-a-unified-layer-skipping-strategy-yijin-liu-et-al-2024>(14/37 | 14/218) Accelerating Inference in Large Language Models with a Unified Layer Skipping Strategy (Yijin Liu et al., 2024)</a></li><li><a href=#1537--15218-llama-vits-enhancing-tts-synthesis-with-semantic-awareness-xincan-feng-et-al-2024>(15/37 | 15/218) Llama-VITS: Enhancing TTS Synthesis with Semantic Awareness (Xincan Feng et al., 2024)</a></li><li><a href=#1637--16218-leave-no-context-behind-efficient-infinite-context-transformers-with-infini-attention-tsendsuren-munkhdalai-et-al-2024>(16/37 | 16/218) Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (Tsendsuren Munkhdalai et al., 2024)</a></li><li><a href=#1737--17218-continuous-language-model-interpolation-for-dynamic-and-controllable-text-generation-sara-kangaslahti-et-al-2024>(17/37 | 17/218) Continuous Language Model Interpolation for Dynamic and Controllable Text Generation (Sara Kangaslahti et al., 2024)</a></li><li><a href=#1837--18218-groundedness-in-retrieval-augmented-long-form-generation-an-empirical-study-alessandro-stolfo-2024>(18/37 | 18/218) Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study (Alessandro Stolfo, 2024)</a></li><li><a href=#1937--19218-personality-aware-student-simulation-for-conversational-intelligent-tutoring-systems-zhengyuan-liu-et-al-2024>(19/37 | 19/218) Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems (Zhengyuan Liu et al., 2024)</a></li><li><a href=#2037--20218-mathvc-an-llm-simulated-multi-character-virtual-classroom-for-mathematics-education-murong-yue-et-al-2024>(20/37 | 20/218) MathVC: An LLM-Simulated Multi-Character Virtual Classroom for Mathematics Education (Murong Yue et al., 2024)</a></li><li><a href=#2137--21218-whats-mine-becomes-yours-defining-annotating-and-detecting-context-dependent-paraphrases-in-news-interview-dialogs-anna-wegmann-et-al-2024>(21/37 | 21/218) What&rsquo;s Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs (Anna Wegmann et al., 2024)</a></li><li><a href=#2237--22218-graph-chain-of-thought-augmenting-large-language-models-by-reasoning-on-graphs-bowen-jin-et-al-2024>(22/37 | 22/218) Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs (Bowen Jin et al., 2024)</a></li><li><a href=#2337--23218-towards-robustness-of-text-to-visualization-translation-against-lexical-and-phrasal-variability-jinwei-lu-et-al-2024>(23/37 | 23/218) Towards Robustness of Text-to-Visualization Translation against Lexical and Phrasal Variability (Jinwei Lu et al., 2024)</a></li><li><a href=#2437--24218-a-mathematical-theory-for-learning-semantic-languages-by-abstract-learners-kuo-yu-liao-et-al-2024>(24/37 | 24/218) A Mathematical Theory for Learning Semantic Languages by Abstract Learners (Kuo-Yu Liao et al., 2024)</a></li><li><a href=#2537--25218-emotion-cause-pair-extraction-method-based-on-multi-granularity-information-and-multi-module-interaction-mingrui-fu-et-al-2024>(25/37 | 25/218) Emotion-cause pair extraction method based on multi-granularity information and multi-module interaction (Mingrui Fu et al., 2024)</a></li><li><a href=#2637--26218-goex-perspectives-and-designs-towards-a-runtime-for-autonomous-llm-applications-shishir-g-patil-et-al-2024>(26/37 | 26/218) GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications (Shishir G. Patil et al., 2024)</a></li><li><a href=#2737--27218-cqil-inference-latency-optimization-with-concurrent-computation-of-quasi-independent-layers-longwei-zou-et-al-2024>(27/37 | 27/218) CQIL: Inference Latency Optimization with Concurrent Computation of Quasi-Independent Layers (Longwei Zou et al., 2024)</a></li><li><a href=#2837--28218-culturalteaming-ai-assisted-interactive-red-teaming-for-challenging-llms-lack-of-multicultural-knowledge-yu-ying-chiu-et-al-2024>(28/37 | 28/218) CulturalTeaming: AI-Assisted Interactive Red-Teaming for Challenging LLMs&rsquo; (Lack of) Multicultural Knowledge (Yu Ying Chiu et al., 2024)</a></li><li><a href=#2937--29218-improving-language-model-reasoning-with-self-motivated-learning-yunlong-feng-et-al-2024>(29/37 | 29/218) Improving Language Model Reasoning with Self-motivated Learning (Yunlong Feng et al., 2024)</a></li><li><a href=#3037--30218-event-grounded-criminal-court-view-generation-withcooperative-large-language-models-linan-yue-et-al-2024>(30/37 | 30/218) Event Grounded Criminal Court View Generation withCooperative (Large) Language Models (Linan Yue et al., 2024)</a></li><li><a href=#3137--31218-does-mapo-tofu-contain-coffee-probing-llms-for-food-related-cultural-knowledge-li-zhou-et-al-2024>(31/37 | 31/218) Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural Knowledge (Li Zhou et al., 2024)</a></li><li><a href=#3237--32218-exploring-concept-depth-how-large-language-models-acquire-knowledge-at-different-layers-mingyu-jin-et-al-2024>(32/37 | 32/218) Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers? (Mingyu Jin et al., 2024)</a></li><li><a href=#3337--33218-meta4xnli-a-crosslingual-parallel-corpus-for-metaphor-detection-and-interpretation-elisa-sanchez-bayona-et-al-2024>(33/37 | 33/218) Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation (Elisa Sanchez-Bayona et al., 2024)</a></li><li><a href=#3437--34218-a-computational-analysis-of-the-dehumanisation-of-migrants-from-syria-and-ukraine-in-slovene-news-media-jaya-caporusso-et-al-2024>(34/37 | 34/218) A Computational Analysis of the Dehumanisation of Migrants from Syria and Ukraine in Slovene News Media (Jaya Caporusso et al., 2024)</a></li><li><a href=#3537--35218-lm-transparency-tool-interactive-tool-for-analyzing-transformer-language-models-igor-tufanov-et-al-2024>(35/37 | 35/218) LM Transparency Tool: Interactive Tool for Analyzing Transformer Language Models (Igor Tufanov et al., 2024)</a></li><li><a href=#3637--36218-charles-translator-a-machine-translation-system-between-ukrainian-and-czech-martin-popel-et-al-2024>(36/37 | 36/218) Charles Translator: A Machine Translation System between Ukrainian and Czech (Martin Popel et al., 2024)</a></li><li><a href=#3737--37218-diffusiondialog-a-diffusion-model-for-diverse-dialog-generation-with-latent-space-jianxiang-xiang-et-al-2024>(37/37 | 37/218) DiffusionDialog: A Diffusion Model for Diverse Dialog Generation with Latent Space (Jianxiang Xiang et al., 2024)</a></li></ul></li><li><a href=#cscv-53>cs.CV (53)</a><ul><li><a href=#153--38218-adapting-llama-decoder-to-vision-transformer-jiahao-wang-et-al-2024>(1/53 | 38/218) Adapting LLaMA Decoder to Vision Transformer (Jiahao Wang et al., 2024)</a></li><li><a href=#253--39218-vllms-provide-better-context-for-emotion-understanding-through-common-sense-reasoning-alexandros-xenos-et-al-2024>(2/53 | 39/218) VLLMs Provide Better Context for Emotion Understanding Through Common Sense Reasoning (Alexandros Xenos et al., 2024)</a></li><li><a href=#353--40218-implicit-multi-spectral-transformer-an-lightweight-and-effective-visible-to-infrared-image-translation-model-yijia-chen-et-al-2024>(3/53 | 40/218) Implicit Multi-Spectral Transformer: An Lightweight and Effective Visible to Infrared Image Translation Model (Yijia Chen et al., 2024)</a></li><li><a href=#453--41218-realmdreamer-text-driven-3d-scene-generation-with-inpainting-and-depth-diffusion-jaidev-shriram-et-al-2024>(4/53 | 41/218) RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion (Jaidev Shriram et al., 2024)</a></li><li><a href=#553--42218-tuning-free-adaptive-style-incorporation-for-structure-consistent-text-driven-style-transfer-yanqi-ge-et-al-2024>(5/53 | 42/218) Tuning-Free Adaptive Style Incorporation for Structure-Consistent Text-Driven Style Transfer (Yanqi Ge et al., 2024)</a></li><li><a href=#653--43218-ai-guided-defect-detection-techniques-to-model-single-crystal-diamond-growth-rohan-reddy-mekala-et-al-2024>(6/53 | 43/218) AI-Guided Defect Detection Techniques to Model Single Crystal Diamond Growth (Rohan Reddy Mekala et al., 2024)</a></li><li><a href=#753--44218-dreamscene360-unconstrained-text-to-3d-scene-generation-with-panoramic-gaussian-splatting-shijie-zhou-et-al-2024>(7/53 | 44/218) DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting (Shijie Zhou et al., 2024)</a></li><li><a href=#853--45218-object-conditioned-energy-based-attention-map-alignment-in-text-to-image-diffusion-models-yasi-zhang-et-al-2024>(8/53 | 45/218) Object-Conditioned Energy-Based Attention Map Alignment in Text-to-Image Diffusion Models (Yasi Zhang et al., 2024)</a></li><li><a href=#953--46218-scaling-multi-camera-3d-object-detection-through-weak-to-strong-eliciting-hao-lu-et-al-2024>(9/53 | 46/218) Scaling Multi-Camera 3D Object Detection through Weak-to-Strong Eliciting (Hao Lu et al., 2024)</a></li><li><a href=#1053--47218-safegen-mitigating-unsafe-content-generation-in-text-to-image-models-xinfeng-li-et-al-2024>(10/53 | 47/218) SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models (Xinfeng Li et al., 2024)</a></li><li><a href=#1153--48218-gooddrag-towards-good-practices-for-drag-editing-with-diffusion-models-zewei-zhang-et-al-2024>(11/53 | 48/218) GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models (Zewei Zhang et al., 2024)</a></li><li><a href=#1253--49218-solving-masked-jigsaw-puzzles-with-diffusion-vision-transformers-jinyang-liu-et-al-2024>(12/53 | 49/218) Solving Masked Jigsaw Puzzles with Diffusion Vision Transformers (Jinyang Liu et al., 2024)</a></li><li><a href=#1353--50218-trajpred-trajectory-prediction-with-region-based-relation-learning-chen-zhou-et-al-2024>(13/53 | 50/218) TrajPRed: Trajectory Prediction with Region-based Relation Learning (Chen Zhou et al., 2024)</a></li><li><a href=#1453--51218-urban-architect-steerable-3d-urban-scene-generation-with-layout-prior-fan-lu-et-al-2024>(14/53 | 51/218) Urban Architect: Steerable 3D Urban Scene Generation with Layout Prior (Fan Lu et al., 2024)</a></li><li><a href=#1553--52218-monoselfrecon-purely-self-supervised-explicit-generalizable-3d-reconstruction-of-indoor-scenes-from-monocular-rgb-views-runfa-li-et-al-2024>(15/53 | 52/218) MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D Reconstruction of Indoor Scenes from Monocular RGB Views (Runfa Li et al., 2024)</a></li><li><a href=#1653--53218-umbrae-unified-multimodal-decoding-of-brain-signals-weihao-xia-et-al-2024>(16/53 | 53/218) UMBRAE: Unified Multimodal Decoding of Brain Signals (Weihao Xia et al., 2024)</a></li><li><a href=#1753--54218-hrvda-high-resolution-visual-document-assistant-chaohu-liu-et-al-2024>(17/53 | 54/218) HRVDA: High-Resolution Visual Document Assistant (Chaohu Liu et al., 2024)</a></li><li><a href=#1853--55218-unsupervised-visible-infrared-reid-via-pseudo-label-correction-and-modality-level-alignment-yexin-liu-et-al-2024>(18/53 | 55/218) Unsupervised Visible-Infrared ReID via Pseudo-label Correction and Modality-level Alignment (Yexin Liu et al., 2024)</a></li><li><a href=#1953--56218-deep-generative-data-assimilation-in-multimodal-setting-yongquan-qu-et-al-2024>(19/53 | 56/218) Deep Generative Data Assimilation in Multimodal Setting (Yongquan Qu et al., 2024)</a></li><li><a href=#2053--57218-multi-modal-document-presentation-attack-detection-with-forensics-trace-disentanglement-changsheng-chen-et-al-2024>(20/53 | 57/218) Multi-modal Document Presentation Attack Detection With Forensics Trace Disentanglement (Changsheng Chen et al., 2024)</a></li><li><a href=#2153--58218-unified-language-driven-zero-shot-domain-adaptation-senqiao-yang-et-al-2024>(21/53 | 58/218) Unified Language-driven Zero-shot Domain Adaptation (Senqiao Yang et al., 2024)</a></li><li><a href=#2253--59218-brave-broadening-the-visual-encoding-of-vision-language-models-oğuzhan-fatih-kar-et-al-2024>(22/53 | 59/218) BRAVE: Broadening the visual encoding of vision-language models (Oğuzhan Fatih Kar et al., 2024)</a></li><li><a href=#2353--60218-oracle-large-vision-language-models-for-knowledge-guided-holistic-or-domain-modeling-ege-özsoy-et-al-2024>(23/53 | 60/218) ORacle: Large Vision-Language Models for Knowledge-Guided Holistic OR Domain Modeling (Ege Özsoy et al., 2024)</a></li><li><a href=#2453--61218-diffusion-based-inpainting-of-incomplete-euclidean-distance-matrices-of-trajectories-generated-by-a-fractional-brownian-motion-alexander-lobashev-et-al-2024>(24/53 | 61/218) Diffusion-based inpainting of incomplete Euclidean distance matrices of trajectories generated by a fractional Brownian motion (Alexander Lobashev et al., 2024)</a></li><li><a href=#2553--62218-medrg-medical-report-grounding-with-multi-modal-large-language-model-ke-zou-et-al-2024>(25/53 | 62/218) MedRG: Medical Report Grounding with Multi-modal Large Language Model (Ke Zou et al., 2024)</a></li><li><a href=#2653--63218-sparse-points-to-dense-clouds-enhancing-3d-detection-with-limited-lidar-data-aakash-kumar-et-al-2024>(26/53 | 63/218) Sparse Points to Dense Clouds: Enhancing 3D Detection with Limited LiDAR Data (Aakash Kumar et al., 2024)</a></li><li><a href=#2753--64218-a-transformer-based-model-for-the-prediction-of-human-gaze-behavior-on-videos-suleyman-ozdel-et-al-2024>(27/53 | 64/218) A Transformer-Based Model for the Prediction of Human Gaze Behavior on Videos (Suleyman Ozdel et al., 2024)</a></li><li><a href=#2853--65218-instantmesh-efficient-3d-mesh-generation-from-a-single-image-with-sparse-view-large-reconstruction-models-jiale-xu-et-al-2024>(28/53 | 65/218) InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models (Jiale Xu et al., 2024)</a></li><li><a href=#2953--66218-move-anything-with-layered-scene-diffusion-jiawei-ren-et-al-2024>(29/53 | 66/218) Move Anything with Layered Scene Diffusion (Jiawei Ren et al., 2024)</a></li><li><a href=#3053--67218-lost-in-translation-modern-neural-networks-still-struggle-with-small-realistic-image-transformations-ofir-shifman-et-al-2024>(30/53 | 67/218) Lost in Translation: Modern Neural Networks Still Struggle With Small Realistic Image Transformations (Ofir Shifman et al., 2024)</a></li><li><a href=#3153--68218-driver-attention-tracking-and-analysis-dat-viet-thanh-nguyen-et-al-2024>(31/53 | 68/218) Driver Attention Tracking and Analysis (Dat Viet Thanh Nguyen et al., 2024)</a></li><li><a href=#3253--69218-adversarial-purification-for-no-reference-image-quality-metrics-applicability-study-and-new-methods-aleksandr-gushchin-et-al-2024>(32/53 | 69/218) Adversarial purification for no-reference image-quality metrics: applicability study and new methods (Aleksandr Gushchin et al., 2024)</a></li><li><a href=#3353--70218-fine-color-guidance-in-diffusion-models-and-its-application-to-image-compression-at-extremely-low-bitrates-tom-bordin-et-al-2024>(33/53 | 70/218) Fine color guidance in diffusion models and its application to image compression at extremely low bitrates (Tom Bordin et al., 2024)</a></li><li><a href=#3453--71218-zero-shot-point-cloud-completion-via-2d-priors-tianxin-huang-et-al-2024>(34/53 | 71/218) Zero-shot Point Cloud Completion Via 2D Priors (Tianxin Huang et al., 2024)</a></li><li><a href=#3553--72218-efficient-and-scalable-chinese-vector-font-generation-via-component-composition-jinyu-song-et-al-2024>(35/53 | 72/218) Efficient and Scalable Chinese Vector Font Generation via Component Composition (Jinyu Song et al., 2024)</a></li><li><a href=#3653--73218-gaze-guided-graph-neural-network-for-action-anticipation-conditioned-on-intention-suleyman-ozdel-et-al-2024>(36/53 | 73/218) Gaze-Guided Graph Neural Network for Action Anticipation Conditioned on Intention (Suleyman Ozdel et al., 2024)</a></li><li><a href=#3753--74218-3dmambacomplete-exploring-structured-state-space-model-for-point-cloud-completion-yixuan-li-et-al-2024>(37/53 | 74/218) 3DMambaComplete: Exploring Structured State Space Model for Point Cloud Completion (Yixuan Li et al., 2024)</a></li><li><a href=#3853--75218-identification-of-fine-grained-systematic-errors-via-controlled-scene-generation-valentyn-boreiko-et-al-2024>(38/53 | 75/218) Identification of Fine-grained Systematic Errors via Controlled Scene Generation (Valentyn Boreiko et al., 2024)</a></li><li><a href=#3953--76218-multi-label-continual-learning-for-the-medical-domain-a-novel-benchmark-marina-ceccon-et-al-2024>(39/53 | 76/218) Multi-Label Continual Learning for the Medical Domain: A Novel Benchmark (Marina Ceccon et al., 2024)</a></li><li><a href=#4053--77218-udiff-generating-conditional-unsigned-distance-fields-with-optimal-wavelet-diffusion-junsheng-zhou-et-al-2024>(40/53 | 77/218) UDiFF: Generating Conditional Unsigned Distance Fields with Optimal Wavelet Diffusion (Junsheng Zhou et al., 2024)</a></li><li><a href=#4153--78218-splatpose--detect-pose-agnostic-3d-anomaly-detection-mathis-kruse-et-al-2024>(41/53 | 78/218) SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection (Mathis Kruse et al., 2024)</a></li><li><a href=#4253--79218-self-supervised-monocular-depth-estimation-on-water-scenes-via-specular-reflection-prior-zhengyang-lu-et-al-2024>(42/53 | 79/218) Self-supervised Monocular Depth Estimation on Water Scenes via Specular Reflection Prior (Zhengyang Lu et al., 2024)</a></li><li><a href=#4353--80218-mocap-to-visual-domain-adaptation-for-efficient-human-mesh-estimation-from-2d-keypoints-bedirhan-uguz-et-al-2024>(43/53 | 80/218) MoCap-to-Visual Domain Adaptation for Efficient Human Mesh Estimation from 2D Keypoints (Bedirhan Uguz et al., 2024)</a></li><li><a href=#4453--81218-an-evidential-enhanced-tri-branch-consistency-learning-method-for-semi-supervised-medical-image-segmentation-zhenxi-zhang-et-al-2024>(44/53 | 81/218) An Evidential-enhanced Tri-Branch Consistency Learning Method for Semi-supervised Medical Image Segmentation (Zhenxi Zhang et al., 2024)</a></li><li><a href=#4553--82218-accurate-tennis-court-line-detection-on-amateur-recorded-matches-sameer-agrawal-et-al-2024>(45/53 | 82/218) Accurate Tennis Court Line Detection on Amateur Recorded Matches (Sameer Agrawal et al., 2024)</a></li><li><a href=#4653--83218-yolo-based-ocean-eddy-localization-with-aws-sagemaker-seraj-al-mahmud-mostafa-et-al-2024>(46/53 | 83/218) YOLO based Ocean Eddy Localization with AWS SageMaker (Seraj Al Mahmud Mostafa et al., 2024)</a></li><li><a href=#4753--84218-an-animation-based-augmentation-approach-for-action-recognition-from-discontinuous-video-xingyu-song-et-al-2024>(47/53 | 84/218) An Animation-based Augmentation Approach for Action Recognition from Discontinuous Video (Xingyu Song et al., 2024)</a></li><li><a href=#4853--85218-convolution-based-probability-gradient-loss-for-semantic-segmentation-guohang-shan-et-al-2024>(48/53 | 85/218) Convolution-based Probability Gradient Loss for Semantic Segmentation (Guohang Shan et al., 2024)</a></li><li><a href=#4953--86218-perception-oriented-video-frame-interpolation-via-asymmetric-blending-guangyang-wu-et-al-2024>(49/53 | 86/218) Perception-Oriented Video Frame Interpolation via Asymmetric Blending (Guangyang Wu et al., 2024)</a></li><li><a href=#5053--87218-efficient-denoising-using-score-embedding-in-score-based-diffusion-models-andrew-s-na-et-al-2024>(50/53 | 87/218) Efficient Denoising using Score Embedding in Score-based Diffusion Models (Andrew S. Na et al., 2024)</a></li><li><a href=#5153--88218-peavs-perceptual-evaluation-of-audio-visual-synchrony-grounded-in-viewers-opinion-scores-lucas-goncalves-et-al-2024>(51/53 | 88/218) PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers&rsquo; Opinion Scores (Lucas Goncalves et al., 2024)</a></li><li><a href=#5253--89218-unfolding-admm-for-enhanced-subspace-clustering-of-hyperspectral-images-xianlu-li-et-al-2024>(52/53 | 89/218) Unfolding ADMM for Enhanced Subspace Clustering of Hyperspectral Images (Xianlu Li et al., 2024)</a></li><li><a href=#5353--90218-sparse-global-matching-for-video-frame-interpolation-with-large-motion-chunxu-liu-et-al-2024>(53/53 | 90/218) Sparse Global Matching for Video Frame Interpolation with Large Motion (Chunxu Liu et al., 2024)</a></li></ul></li><li><a href=#csro-12>cs.RO (12)</a><ul><li><a href=#112--91218-vision-language-model-based-physical-reasoning-for-robot-liquid-perception-wenqiang-lai-et-al-2024>(1/12 | 91/218) Vision-Language Model-based Physical Reasoning for Robot Liquid Perception (Wenqiang Lai et al., 2024)</a></li><li><a href=#212--92218-beyond-gait-learning-knee-angle-for-seamless-prosthesis-control-in-multiple-scenarios-pengwei-wang-et-al-2024>(2/12 | 92/218) Beyond Gait: Learning Knee Angle for Seamless Prosthesis Control in Multiple Scenarios (Pengwei Wang et al., 2024)</a></li><li><a href=#312--93218-using-neural-networks-to-model-hysteretic-kinematics-in-tendon-actuated-continuum-robots-yuan-wang-et-al-2024>(3/12 | 93/218) Using Neural Networks to Model Hysteretic Kinematics in Tendon-Actuated Continuum Robots (Yuan Wang et al., 2024)</a></li><li><a href=#412--94218-enhancing-safety-in-mixed-traffic-learning-based-modeling-and-efficient-control-of-autonomous-and-human-driven-vehicles-jie-wang-et-al-2024>(4/12 | 94/218) Enhancing Safety in Mixed Traffic: Learning-Based Modeling and Efficient Control of Autonomous and Human-Driven Vehicles (Jie Wang et al., 2024)</a></li><li><a href=#512--95218-cbfkit-a-control-barrier-function-toolbox-for-robotics-applications-mitchell-black-et-al-2024>(5/12 | 95/218) CBFKIT: A Control Barrier Function Toolbox for Robotics Applications (Mitchell Black et al., 2024)</a></li><li><a href=#612--96218-wild-visual-navigation-fast-traversability-learning-via-pre-trained-models-and-online-self-supervision-matías-mattamala-et-al-2024>(6/12 | 96/218) Wild Visual Navigation: Fast Traversability Learning via Pre-Trained Models and Online Self-Supervision (Matías Mattamala et al., 2024)</a></li><li><a href=#712--97218-laplass-latent-space-planning-for-stochastic-systems-marlyse-reeves-et-al-2024>(7/12 | 97/218) LaPlaSS: Latent Space Planning for Stochastic Systems (Marlyse Reeves et al., 2024)</a></li><li><a href=#812--98218-a-data-efficient-framework-for-learning-local-heuristics-rishi-veerapaneni-et-al-2024>(8/12 | 98/218) A Data Efficient Framework for Learning Local Heuristics (Rishi Veerapaneni et al., 2024)</a></li><li><a href=#912--99218-fast-and-accurate-relative-motion-tracking-for-two-industrial-robots-honglu-he-et-al-2024>(9/12 | 99/218) Fast and Accurate Relative Motion Tracking for Two Industrial Robots (Honglu He et al., 2024)</a></li><li><a href=#1012--100218-incorporating-explanations-into-human-machine-interfaces-for-trust-and-situation-awareness-in-autonomous-vehicles-shahin-atakishiyev-et-al-2024>(10/12 | 100/218) Incorporating Explanations into Human-Machine Interfaces for Trust and Situation Awareness in Autonomous Vehicles (Shahin Atakishiyev et al., 2024)</a></li><li><a href=#1112--101218-reward-learning-from-suboptimal-demonstrations-with-applications-in-surgical-electrocautery-zohre-karimi-et-al-2024>(11/12 | 101/218) Reward Learning from Suboptimal Demonstrations with Applications in Surgical Electrocautery (Zohre Karimi et al., 2024)</a></li><li><a href=#1212--102218-deep-reinforcement-learning-for-mobile-robot-path-planning-hao-liu-et-al-2024>(12/12 | 102/218) Deep Reinforcement Learning for Mobile Robot Path Planning (Hao Liu et al., 2024)</a></li></ul></li><li><a href=#csir-4>cs.IR (4)</a><ul><li><a href=#14--103218-nfarec-a-negative-feedback-aware-recommender-model-xinfeng-wang-et-al-2024>(1/4 | 103/218) NFARec: A Negative Feedback-Aware Recommender Model (Xinfeng Wang et al., 2024)</a></li><li><a href=#24--104218-cadrec-contextualized-and-debiased-recommender-model-xinfeng-wang-et-al-2024>(2/4 | 104/218) CaDRec: Contextualized and Debiased Recommender Model (Xinfeng Wang et al., 2024)</a></li><li><a href=#34--105218-quati-a-brazilian-portuguese-information-retrieval-dataset-from-native-speakers-mirelle-bueno-et-al-2024>(3/4 | 105/218) Quati: A Brazilian Portuguese Information Retrieval Dataset from Native Speakers (Mirelle Bueno et al., 2024)</a></li><li><a href=#44--106218-transtarec-time-adaptive-translating-embedding-model-for-next-poi-recommendation-yiping-sun-2024>(4/4 | 106/218) TransTARec: Time-Adaptive Translating Embedding Model for Next POI Recommendation (Yiping Sun, 2024)</a></li></ul></li><li><a href=#cslg-30>cs.LG (30)</a><ul><li><a href=#130--107218-forecasting-the-future-with-future-technologies-advancements-in-large-meteorological-models-hailong-shu-et-al-2024>(1/30 | 107/218) Forecasting the Future with Future Technologies: Advancements in Large Meteorological Models (Hailong Shu et al., 2024)</a></li><li><a href=#230--108218-how-to-craft-backdoors-with-unlabeled-data-alone-yifei-wang-et-al-2024>(2/30 | 108/218) How to Craft Backdoors with Unlabeled Data Alone? (Yifei Wang et al., 2024)</a></li><li><a href=#330--109218-advancing-real-time-pandemic-forecasting-using-large-language-models-a-covid-19-case-study-hongru-du-et-al-2024>(3/30 | 109/218) Advancing Real-time Pandemic Forecasting Using Large Language Models: A COVID-19 Case Study (Hongru Du et al., 2024)</a></li><li><a href=#430--110218-latim-longitudinal-representation-learning-in-continuous-time-models-to-predict-disease-progression-rachid-zeghlache-et-al-2024>(4/30 | 110/218) LaTiM: Longitudinal representation learning in continuous-time models to predict disease progression (Rachid Zeghlache et al., 2024)</a></li><li><a href=#530--111218-global-contrastive-training-for-multimodal-electronic-health-records-with-language-supervision-yingbo-ma-et-al-2024>(5/30 | 111/218) Global Contrastive Training for Multimodal Electronic Health Records with Language Supervision (Yingbo Ma et al., 2024)</a></li><li><a href=#630--112218-vn-egnn-e3-equivariant-graph-neural-networks-with-virtual-nodes-enhance-protein-binding-site-identification-florian-sestak-et-al-2024>(6/30 | 112/218) VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification (Florian Sestak et al., 2024)</a></li><li><a href=#730--113218-fip-a-fixed-point-approach-for-causal-generative-modeling-meyer-scetbon-et-al-2024>(7/30 | 113/218) FiP: a Fixed-Point Approach for Causal Generative Modeling (Meyer Scetbon et al., 2024)</a></li><li><a href=#830--114218-gansemble-for-small-and-imbalanced-data-sets-a-baseline-for-synthetic-microplastics-data-daniel-platnick-et-al-2024>(8/30 | 114/218) GANsemble for Small and Imbalanced Data Sets: A Baseline for Synthetic Microplastics Data (Daniel Platnick et al., 2024)</a></li><li><a href=#930--115218-sequential-decision-making-with-expert-demonstrations-under-unobserved-heterogeneity-vahid-balazadeh-et-al-2024>(9/30 | 115/218) Sequential Decision Making with Expert Demonstrations under Unobserved Heterogeneity (Vahid Balazadeh et al., 2024)</a></li><li><a href=#1030--116218-toward-a-better-understanding-of-fourier-neural-operators-analysis-and-improvement-from-a-spectral-perspective-shaoxiang-qin-et-al-2024>(10/30 | 116/218) Toward a Better Understanding of Fourier Neural Operators: Analysis and Improvement from a Spectral Perspective (Shaoxiang Qin et al., 2024)</a></li><li><a href=#1130--117218-a-gauss-newton-approach-for-min-max-optimization-in-generative-adversarial-networks-neel-mishra-et-al-2024>(11/30 | 117/218) A Gauss-Newton Approach for Min-Max Optimization in Generative Adversarial Networks (Neel Mishra et al., 2024)</a></li><li><a href=#1230--118218-what-needs-to-go-right-for-an-induction-head-a-mechanistic-study-of-in-context-learning-circuits-and-their-formation-aaditya-k-singh-et-al-2024>(12/30 | 118/218) What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation (Aaditya K. Singh et al., 2024)</a></li><li><a href=#1330--119218-towards-learning-stochastic-population-models-by-gradient-descent-justin-n-kreikemeyer-et-al-2024>(13/30 | 119/218) Towards Learning Stochastic Population Models by Gradient Descent (Justin N. Kreikemeyer et al., 2024)</a></li><li><a href=#1430--120218-logit-calibration-and-feature-contrast-for-robust-federated-learning-on-non-iid-data-yu-qiao-et-al-2024>(14/30 | 120/218) Logit Calibration and Feature Contrast for Robust Federated Learning on Non-IID Data (Yu Qiao et al., 2024)</a></li><li><a href=#1530--121218-toward-cross-layer-energy-optimizations-in-machine-learning-systems-jae-won-chung-et-al-2024>(15/30 | 121/218) Toward Cross-Layer Energy Optimizations in Machine Learning Systems (Jae-Won Chung et al., 2024)</a></li><li><a href=#1630--122218-rethinking-out-of-distribution-detection-for-reinforcement-learning-advancing-methods-for-evaluation-and-detection-linas-nasvytis-et-al-2024>(16/30 | 122/218) Rethinking Out-of-Distribution Detection for Reinforcement Learning: Advancing Methods for Evaluation and Detection (Linas Nasvytis et al., 2024)</a></li><li><a href=#1730--123218-transfer-learning-via-latent-dependency-factor-for-estimating-pm-25-shrey-gupta-et-al-2024>(17/30 | 123/218) Transfer Learning via Latent Dependency Factor for Estimating PM 2.5 (Shrey Gupta et al., 2024)</a></li><li><a href=#1830--124218-scaling-laws-for-data-filtering----data-curation-cannot-be-compute-agnostic-sachin-goyal-et-al-2024>(18/30 | 124/218) Scaling Laws for Data Filtering &ndash; Data Curation cannot be Compute Agnostic (Sachin Goyal et al., 2024)</a></li><li><a href=#1930--125218-how-consistent-are-clinicians-evaluating-the-predictability-of-sepsis-disease-progression-with-dynamics-models-unnseo-park-et-al-2024>(19/30 | 125/218) How Consistent are Clinicians? Evaluating the Predictability of Sepsis Disease Progression with Dynamics Models (Unnseo Park et al., 2024)</a></li><li><a href=#2030--126218-crimealarm-towards-intensive-intent-dynamics-in-fine-grained-crime-prediction-kaixi-hu-et-al-2024>(20/30 | 126/218) CrimeAlarm: Towards Intensive Intent Dynamics in Fine-grained Crime Prediction (Kaixi Hu et al., 2024)</a></li><li><a href=#2130--127218-knowledge-graphs-for-empirical-concept-retrieval-lenka-tětková-et-al-2024>(21/30 | 127/218) Knowledge graphs for empirical concept retrieval (Lenka Tětková et al., 2024)</a></li><li><a href=#2230--128218-deep-generative-sampling-in-the-dual-divergence-space-a-data-efficient--interpretative-approach-for-generative-ai-sahil-garg-et-al-2024>(22/30 | 128/218) Deep Generative Sampling in the Dual Divergence Space: A Data-efficient & Interpretative Approach for Generative AI (Sahil Garg et al., 2024)</a></li><li><a href=#2330--129218-addressing-the-abstraction-and-reasoning-corpus-via-procedural-example-generation-michael-hodel-2024>(23/30 | 129/218) Addressing the Abstraction and Reasoning Corpus via Procedural Example Generation (Michael Hodel, 2024)</a></li><li><a href=#2430--130218-error-mitigation-for-tdoa-uwb-indoor-localization-using-unsupervised-machine-learning-phuong-bich-duong-et-al-2024>(24/30 | 130/218) Error Mitigation for TDoA UWB Indoor Localization using Unsupervised Machine Learning (Phuong Bich Duong et al., 2024)</a></li><li><a href=#2530--131218-toward-industrial-use-of-continual-learning--new-metrics-proposal-for-class-incremental-learning-konaté-mohamed-abbas-et-al-2024>(25/30 | 131/218) Toward industrial use of continual learning : new metrics proposal for class incremental learning (Konaté Mohamed Abbas et al., 2024)</a></li><li><a href=#2630--132218-register-your-forests-decision-tree-ensemble-optimization-by-explicit-cpu-register-allocation-daniel-biebert-et-al-2024>(26/30 | 132/218) Register Your Forests: Decision Tree Ensemble Optimization by Explicit CPU Register Allocation (Daniel Biebert et al., 2024)</a></li><li><a href=#2730--133218-optimal-regret-with-limited-adaptivity-for-generalized-linear-contextual-bandits-ayush-sawarni-et-al-2024>(27/30 | 133/218) Optimal Regret with Limited Adaptivity for Generalized Linear Contextual Bandits (Ayush Sawarni et al., 2024)</a></li><li><a href=#2830--134218-private-wasserstein-distance-with-random-noises-wenqian-li-et-al-2024>(28/30 | 134/218) Private Wasserstein Distance with Random Noises (Wenqian Li et al., 2024)</a></li><li><a href=#2930--135218-disguised-copyright-infringement-of-latent-diffusion-models-yiwei-lu-et-al-2024>(29/30 | 135/218) Disguised Copyright Infringement of Latent Diffusion Models (Yiwei Lu et al., 2024)</a></li><li><a href=#3030--136218-sleepppg-net2-deep-learning-generalization-for-sleep-staging-from-photoplethysmography-shirel-attia-et-al-2024>(30/30 | 136/218) SleepPPG-Net2: Deep learning generalization for sleep staging from photoplethysmography (Shirel Attia et al., 2024)</a></li></ul></li><li><a href=#cond-matmtrl-sci-2>cond-mat.mtrl-sci (2)</a><ul><li><a href=#12--137218-bamboo-a-predictive-and-transferable-machine-learning-force-field-framework-for-liquid-electrolyte-development-sheng-gong-et-al-2024>(1/2 | 137/218) BAMBOO: a predictive and transferable machine learning force field framework for liquid electrolyte development (Sheng Gong et al., 2024)</a></li><li><a href=#22--138218-building-workflows-for-interactive-human-in-the-loop-automated-experiment-hae-in-stem-eels-utkarsh-pratiush-et-al-2024>(2/2 | 138/218) Building Workflows for Interactive Human in the Loop Automated Experiment (hAE) in STEM-EELS (Utkarsh Pratiush et al., 2024)</a></li></ul></li><li><a href=#csse-2>cs.SE (2)</a><ul><li><a href=#12--139218-beyond-random-inputs-a-novel-ml-based-hardware-fuzzing-mohamadreza-rostami-et-al-2024>(1/2 | 139/218) Beyond Random Inputs: A Novel ML-Based Hardware Fuzzing (Mohamadreza Rostami et al., 2024)</a></li><li><a href=#22--140218-research-artifacts-in-software-engineering-publications-status-and-trends-mugeng-liu-et-al-2024>(2/2 | 140/218) Research Artifacts in Software Engineering Publications: Status and Trends (Mugeng Liu et al., 2024)</a></li></ul></li><li><a href=#cscy-4>cs.CY (4)</a><ul><li><a href=#14--141218-frontier-ai-ethics-anticipating-and-evaluating-the-societal-impacts-of-generative-agents-seth-lazar-2024>(1/4 | 141/218) Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Generative Agents (Seth Lazar, 2024)</a></li><li><a href=#24--142218-accuracy-of-a-large-language-model-in-distinguishing-anti--and-pro-vaccination-messages-on-social-media-the-case-of-human-papillomavirus-vaccination-soojong-kim-et-al-2024>(2/4 | 142/218) Accuracy of a Large Language Model in Distinguishing Anti- And Pro-vaccination Messages on Social Media: The Case of Human Papillomavirus Vaccination (Soojong Kim et al., 2024)</a></li><li><a href=#34--143218-leveraging-open-source-models-for-legal-language-modeling-and-analysis-a-case-study-on-the-indian-constitution-vikhyath-gupta-et-al-2024>(3/4 | 143/218) Leveraging open-source models for legal language modeling and analysis: a case study on the Indian constitution (Vikhyath Gupta et al., 2024)</a></li><li><a href=#44--144218-racialethnic-categories-in-ai-and-algorithmic-fairness-why-they-matter-and-what-they-represent-jennifer-mickel-2024>(4/4 | 144/218) Racial/Ethnic Categories in AI and Algorithmic Fairness: Why They Matter and What They Represent (Jennifer Mickel, 2024)</a></li></ul></li><li><a href=#csai-5>cs.AI (5)</a><ul><li><a href=#15--145218-zero-shot-logical-query-reasoning-on-any-knowledge-graph-mikhail-galkin-et-al-2024>(1/5 | 145/218) Zero-shot Logical Query Reasoning on any Knowledge Graph (Mikhail Galkin et al., 2024)</a></li><li><a href=#25--146218-towards-a-game-theoretic-understanding-of-explanation-based-membership-inference-attacks-kavita-kumari-et-al-2024>(2/5 | 146/218) Towards a Game-theoretic Understanding of Explanation-based Membership Inference Attacks (Kavita Kumari et al., 2024)</a></li><li><a href=#35--147218-learn-from-failure-fine-tuning-llms-with-trial-and-error-data-for-intuitionistic-propositional-logic-proving-chenyang-an-et-al-2024>(3/5 | 147/218) Learn from Failure: Fine-Tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving (Chenyang An et al., 2024)</a></li><li><a href=#45--148218-a-survey-on-the-integration-of-generative-ai-for-critical-thinking-in-mobile-networks-athanasios-karapantelakis-et-al-2024>(4/5 | 148/218) A Survey on the Integration of Generative AI for Critical Thinking in Mobile Networks (Athanasios Karapantelakis et al., 2024)</a></li><li><a href=#55--149218-causal-unit-selection-using-tractable-arithmetic-circuits-haiying-huang-et-al-2024>(5/5 | 149/218) Causal Unit Selection using Tractable Arithmetic Circuits (Haiying Huang et al., 2024)</a></li></ul></li><li><a href=#cset-1>cs.ET (1)</a><ul><li><a href=#11--150218-fast-system-technology-co-optimization-framework-for-emerging-technology-based-on-graph-neural-networks-tianliang-ma-et-al-2024>(1/1 | 150/218) Fast System Technology Co-Optimization Framework for Emerging Technology Based on Graph Neural Networks (Tianliang Ma et al., 2024)</a></li></ul></li><li><a href=#cshc-9>cs.HC (9)</a><ul><li><a href=#19--151218-biscuit-scaffolding-llm-generated-code-with-ephemeral-uis-in-computational-notebooks-ruijia-cheng-et-al-2024>(1/9 | 151/218) BISCUIT: Scaffolding LLM-Generated Code with Ephemeral UIs in Computational Notebooks (Ruijia Cheng et al., 2024)</a></li><li><a href=#29--152218-worddecipher-enhancing-digital-workspace-communication-with-explainable-ai-for-non-native-english-speakers-yuexi-chen-et-al-2024>(2/9 | 152/218) WordDecipher: Enhancing Digital Workspace Communication with Explainable AI for Non-native English Speakers (Yuexi Chen et al., 2024)</a></li><li><a href=#39--153218-we-need-structured-output-towards-user-centered-constraints-on-large-language-model-output-michael-xieyang-liu-et-al-2024>(3/9 | 153/218) &lsquo;We Need Structured Output&rsquo;: Towards User-centered Constraints on Large Language Model Output (Michael Xieyang Liu et al., 2024)</a></li><li><a href=#49--154218-mixed-reality-heritage-performance-as-a-decolonising-tool-for-heritage-sites-mariza-dima-et-al-2024>(4/9 | 154/218) Mixed Reality Heritage Performance As a Decolonising Tool for Heritage Sites (Mariza Dima et al., 2024)</a></li><li><a href=#59--155218-evaluating-navigation-and-comparison-performance-of-computational-notebooks-on-desktop-and-in-virtual-reality-sungwon-in-et-al-2024>(5/9 | 155/218) Evaluating Navigation and Comparison Performance of Computational Notebooks on Desktop and in Virtual Reality (Sungwon In et al., 2024)</a></li><li><a href=#69--156218-exploring-physiological-responses-in-virtual-reality-based-interventions-for-autism-spectrum-disorder-a-data-driven-investigation-gianpaolo-alvari-et-al-2024>(6/9 | 156/218) Exploring Physiological Responses in Virtual Reality-based Interventions for Autism Spectrum Disorder: A Data-Driven Investigation (Gianpaolo Alvari et al., 2024)</a></li><li><a href=#79--157218-sara-smart-ai-reading-assistant-for-reading-comprehension-enkeleda-thaqi-et-al-2024>(7/9 | 157/218) SARA: Smart AI Reading Assistant for Reading Comprehension (Enkeleda Thaqi et al., 2024)</a></li><li><a href=#89--158218-untangling-critical-interaction-with-ai-in-students-written-assessment-antonette-shibani-et-al-2024>(8/9 | 158/218) Untangling Critical Interaction with AI in Students Written Assessment (Antonette Shibani et al., 2024)</a></li><li><a href=#99--159218-incremental-xai-memorable-understanding-of-ai-with-incremental-explanations-jessica-y-bo-et-al-2024>(9/9 | 159/218) Incremental XAI: Memorable Understanding of AI with Incremental Explanations (Jessica Y. Bo et al., 2024)</a></li></ul></li><li><a href=#cscr-5>cs.CR (5)</a><ul><li><a href=#15--160218-differentially-private-gans-for-generating-synthetic-indoor-location-data-vahideh-moghtadaiee-et-al-2024>(1/5 | 160/218) Differentially Private GANs for Generating Synthetic Indoor Location Data (Vahideh Moghtadaiee et al., 2024)</a></li><li><a href=#25--161218-poisoning-prevention-in-federated-learning-and-differential-privacy-via-stateful-proofs-of-execution-norrathep-rattanavipanon-et-al-2024>(2/5 | 161/218) Poisoning Prevention in Federated Learning and Differential Privacy via Stateful Proofs of Execution (Norrathep Rattanavipanon et al., 2024)</a></li><li><a href=#35--162218-indoor-location-fingerprinting-privacy-a-comprehensive-survey-amir-fathalizadeh-et-al-2024>(3/5 | 162/218) Indoor Location Fingerprinting Privacy: A Comprehensive Survey (Amir Fathalizadeh et al., 2024)</a></li><li><a href=#45--163218-atlas-x-equity-financing-unlocking-new-methods-to-securely-obfuscate-axe-inventory-data-based-on-differential-privacy-antigoni-polychroniadou-et-al-2024>(4/5 | 163/218) Atlas-X Equity Financing: Unlocking New Methods to Securely Obfuscate Axe Inventory Data Based on Differential Privacy (Antigoni Polychroniadou et al., 2024)</a></li><li><a href=#55--164218-security-assessment-of-the-lg-cryptosystem-étienne-burle-et-al-2024>(5/5 | 164/218) Security Assessment of the LG Cryptosystem (Étienne Burle et al., 2024)</a></li></ul></li><li><a href=#eessas-5>eess.AS (5)</a><ul><li><a href=#15--165218-conformer-1-robust-asr-via-large-scale-semisupervised-bootstrapping-kevin-zhang-et-al-2024>(1/5 | 165/218) Conformer-1: Robust ASR via Large-Scale Semisupervised Bootstrapping (Kevin Zhang et al., 2024)</a></li><li><a href=#25--166218-towards-efficient-and-real-time-piano-transcription-using-neural-autoregressive-models-taegyun-kwon-et-al-2024>(2/5 | 166/218) Towards Efficient and Real-Time Piano Transcription Using Neural Autoregressive Models (Taegyun Kwon et al., 2024)</a></li><li><a href=#35--167218-covomix-advancing-zero-shot-speech-generation-for-human-like-multi-talker-conversations-leying-zhang-et-al-2024>(3/5 | 167/218) CoVoMix: Advancing Zero-Shot Speech Generation for Human-like Multi-talker Conversations (Leying Zhang et al., 2024)</a></li><li><a href=#45--168218-efficient-sound-field-reconstruction-with-conditional-invertible-neural-networks-xenofon-karakonstantis-et-al-2024>(4/5 | 168/218) Efficient Sound Field Reconstruction with Conditional Invertible Neural Networks (Xenofon Karakonstantis et al., 2024)</a></li><li><a href=#55--169218-what-is-learnt-by-the-learnable-front-end-leaf-adapting-per-channel-energy-normalisation-pcen-to-noisy-conditions-hanyu-meng-et-al-2024>(5/5 | 169/218) What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions (Hanyu Meng et al., 2024)</a></li></ul></li><li><a href=#eesssy-7>eess.SY (7)</a><ul><li><a href=#17--170218-dual-ensemble-kalman-filter-for-stochastic-optimal-control-anant-a-joshi-et-al-2024>(1/7 | 170/218) Dual Ensemble Kalman Filter for Stochastic Optimal Control (Anant A. Joshi et al., 2024)</a></li><li><a href=#27--171218-structured-reinforcement-learning-for-media-streaming-at-the-wireless-edge-archana-bura-et-al-2024>(2/7 | 171/218) Structured Reinforcement Learning for Media Streaming at the Wireless Edge (Archana Bura et al., 2024)</a></li><li><a href=#37--172218-synchronization-conditions-for-nonlinear-oscillator-networks-sanjeev-kumar-pandey-et-al-2024>(3/7 | 172/218) Synchronization Conditions for Nonlinear Oscillator Networks (Sanjeev Kumar Pandey et al., 2024)</a></li><li><a href=#47--173218-lyapunov-based-deep-residual-neural-network-resnet-adaptive-control-omkar-sudhir-patil-et-al-2024>(4/7 | 173/218) Lyapunov-Based Deep Residual Neural Network (ResNet) Adaptive Control (Omkar Sudhir Patil et al., 2024)</a></li><li><a href=#57--174218-analytical-formula-for-calculations-of-armour-losses-in-three-core-power-cables-marius-hatlo-et-al-2024>(5/7 | 174/218) Analytical Formula for Calculations of Armour Losses in Three-Core Power Cables (Marius Hatlo et al., 2024)</a></li><li><a href=#67--175218-multi-agent-soft-actor-critic-with-global-loss-for-autonomous-mobility-on-demand-fleet-control-zeno-woywood-et-al-2024>(6/7 | 175/218) Multi-Agent Soft Actor-Critic with Global Loss for Autonomous Mobility-on-Demand Fleet Control (Zeno Woywood et al., 2024)</a></li><li><a href=#77--176218-iterative-distributed-moving-horizon-estimation-of-linear-systems-with-penalties-on-both-system-disturbances-and-noise-xiaojie-li-et-al-2024>(7/7 | 176/218) Iterative distributed moving horizon estimation of linear systems with penalties on both system disturbances and noise (Xiaojie Li et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--177218-voiceshop-a-unified-speech-to-speech-framework-for-identity-preserving-zero-shot-voice-editing-philip-anastassiou-et-al-2024>(1/2 | 177/218) VoiceShop: A Unified Speech-to-Speech Framework for Identity-Preserving Zero-Shot Voice Editing (Philip Anastassiou et al., 2024)</a></li><li><a href=#22--178218-learning-multidimensional-disentangled-representations-of-instrumental-sounds-for-musical-similarity-assessment-yuka-hashizume-et-al-2024>(2/2 | 178/218) Learning Multidimensional Disentangled Representations of Instrumental Sounds for Musical Similarity Assessment (Yuka Hashizume et al., 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--179218-gcv-turbo-end-to-end-acceleration-of-gnn-based-computer-vision-tasks-on-fpga-bingyi-zhang-et-al-2024>(1/1 | 179/218) GCV-Turbo: End-to-end Acceleration of GNN-based Computer Vision Tasks on FPGA (Bingyi Zhang et al., 2024)</a></li></ul></li><li><a href=#eessiv-3>eess.IV (3)</a><ul><li><a href=#13--180218-improving-multi-center-generalizability-of-gan-based-fat-suppression-using-federated-learning-pranav-kulkarni-et-al-2024>(1/3 | 180/218) Improving Multi-Center Generalizability of GAN-Based Fat Suppression using Federated Learning (Pranav Kulkarni et al., 2024)</a></li><li><a href=#23--181218-accelerating-cardiac-mri-reconstruction-with-cmratt-an-attention-driven-approach-anam-hashmi-et-al-2024>(2/3 | 181/218) Accelerating Cardiac MRI Reconstruction with CMRatt: An Attention-Driven Approach (Anam Hashmi et al., 2024)</a></li><li><a href=#33--182218-rethinking-perceptual-metrics-for-medical-image-translation-nicholas-konz-et-al-2024>(3/3 | 182/218) Rethinking Perceptual Metrics for Medical Image Translation (Nicholas Konz et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#12--183218-iterative-solvers-in-adaptive-fem-philipp-bringmann-et-al-2024>(1/2 | 183/218) Iterative solvers in adaptive FEM (Philipp Bringmann et al., 2024)</a></li><li><a href=#22--184218-a-conservative-eulerian-finite-element-method-for-transport-and-diffusion-in-moving-domains-maxim-olshanskii-et-al-2024>(2/2 | 184/218) A conservative Eulerian finite element method for transport and diffusion in moving domains (Maxim Olshanskii et al., 2024)</a></li></ul></li><li><a href=#csni-4>cs.NI (4)</a><ul><li><a href=#14--185218-pacp-priority-aware-collaborative-perception-for-connected-and-autonomous-vehicles-zhengru-fang-et-al-2024>(1/4 | 185/218) PACP: Priority-Aware Collaborative Perception for Connected and Autonomous Vehicles (Zhengru Fang et al., 2024)</a></li><li><a href=#24--186218-emf-mitigation-via-5g-and-6g-mac-scheduling-silvio-mandelli-et-al-2024>(2/4 | 186/218) EMF Mitigation via 5G and 6G MAC Scheduling (Silvio Mandelli et al., 2024)</a></li><li><a href=#34--187218-agent-driven-generative-semantic-communication-for-remote-surveillance-wanting-yang-et-al-2024>(3/4 | 187/218) Agent-driven Generative Semantic Communication for Remote Surveillance (Wanting Yang et al., 2024)</a></li><li><a href=#44--188218-responsible-federated-learning-in-smart-transportation-outlooks-and-challenges-xiaowen-huang-et-al-2024>(4/4 | 188/218) Responsible Federated Learning in Smart Transportation: Outlooks and Challenges (Xiaowen Huang et al., 2024)</a></li></ul></li><li><a href=#csit-8>cs.IT (8)</a><ul><li><a href=#18--189218-joint-active-and-passive-irs-aided-wireless-communication-elements-allocation-and-achievable-rate-chaoying-huang-et-al-2024>(1/8 | 189/218) Joint Active And Passive IRS Aided Wireless Communication: Elements Allocation and Achievable Rate (Chaoying Huang et al., 2024)</a></li><li><a href=#28--190218-digital-over-the-air-computation-achieving-high-reliability-via-bit-slicing-jiawei-liu-et-al-2024>(2/8 | 190/218) Digital Over-the-Air Computation: Achieving High Reliability via Bit-Slicing (Jiawei Liu et al., 2024)</a></li><li><a href=#38--191218-on-the-performance-of-irs-assisted-ssk-and-rpm-over-rician-fading-channels-harsh-raj-et-al-2024>(3/8 | 191/218) On the Performance of IRS-Assisted SSK and RPM over Rician Fading Channels (Harsh Raj et al., 2024)</a></li><li><a href=#48--192218-near-optimal-channel-estimation-for-dense-array-systems-mingyao-cui-et-al-2024>(4/8 | 192/218) Near-Optimal Channel Estimation for Dense Array Systems (Mingyao Cui et al., 2024)</a></li><li><a href=#58--193218-perfectly-secure-key-agreement-over-a-full-duplex-wireless-channel-gerhard-wunder-et-al-2024>(5/8 | 193/218) Perfectly Secure Key Agreement Over a Full Duplex Wireless Channel (Gerhard Wunder et al., 2024)</a></li><li><a href=#68--194218-new-partial-orders-of-polar-codes-for-bmsc-liuquan-yao-et-al-2024>(6/8 | 194/218) New Partial Orders of Polar Codes for BMSC (Liuquan Yao et al., 2024)</a></li><li><a href=#78--195218-fractional-decoding-of-algebraic-geometry-codes-over-extension-fields-eduardo-camps-moreno-et-al-2024>(7/8 | 195/218) Fractional decoding of algebraic geometry codes over extension fields (Eduardo Camps-Moreno et al., 2024)</a></li><li><a href=#88--196218-characterising-directed-and-undirected-metrics-of-high-order-interdependence-fernando-e-rosas-et-al-2024>(8/8 | 196/218) Characterising directed and undirected metrics of high-order interdependence (Fernando E. Rosas et al., 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--197218-fairem360-a-suite-for-responsible-entity-matching-nima-shahbazi-et-al-2024>(1/1 | 197/218) FairEM360: A Suite for Responsible Entity Matching (Nima Shahbazi et al., 2024)</a></li></ul></li><li><a href=#quant-ph-5>quant-ph (5)</a><ul><li><a href=#15--198218-a-modified-depolarization-approach-for-efficient-quantum-machine-learning-bikram-khanal-et-al-2024>(1/5 | 198/218) A Modified Depolarization Approach for Efficient Quantum Machine Learning (Bikram Khanal et al., 2024)</a></li><li><a href=#25--199218-quantum-algorithms-to-simulate-quadratic-classical-hamiltonians-and-optimal-control-hari-krovi-2024>(2/5 | 199/218) Quantum algorithms to simulate quadratic classical Hamiltonians and optimal control (Hari Krovi, 2024)</a></li><li><a href=#35--200218-quantum-tunneling-from-theory-to-error-mitigated-quantum-simulation-sorana-catrina-et-al-2024>(3/5 | 200/218) Quantum Tunneling: From Theory to Error-Mitigated Quantum Simulation (Sorana Catrina et al., 2024)</a></li><li><a href=#45--201218-statistical-evaluation-of-571-gaas-quantum-point-contact-transistors-showing-the-07-anomaly-in-quantized-conductance-using-millikelvin-cryogenic-on-chip-multiplexing-pengcheng-ma-et-al-2024>(4/5 | 201/218) Statistical evaluation of 571 GaAs quantum point contact transistors showing the 0.7 anomaly in quantized conductance using millikelvin cryogenic on-chip multiplexing (Pengcheng Ma et al., 2024)</a></li><li><a href=#55--202218-certifying-almost-all-quantum-states-with-few-single-qubit-measurements-hsin-yuan-huang-et-al-2024>(5/5 | 202/218) Certifying almost all quantum states with few single-qubit measurements (Hsin-Yuan Huang et al., 2024)</a></li></ul></li><li><a href=#physicsoptics-1>physics.optics (1)</a><ul><li><a href=#11--203218-scwatter-open-source-coupled-wave-scattering-simulation-for-spectroscopy-and-microscopy-ruijiao-sun-et-al-2024>(1/1 | 203/218) sCWatter: Open source coupled wave scattering simulation for spectroscopy and microscopy (Ruijiao Sun et al., 2024)</a></li></ul></li><li><a href=#csds-3>cs.DS (3)</a><ul><li><a href=#13--204218-an-asymptotically-optimal-algorithm-for-generating-bin-cardinalities-luc-devroye-et-al-2024>(1/3 | 204/218) An asymptotically optimal algorithm for generating bin cardinalities (Luc Devroye et al., 2024)</a></li><li><a href=#23--205218-language-generation-in-the-limit-jon-kleinberg-et-al-2024>(2/3 | 205/218) Language Generation in the Limit (Jon Kleinberg et al., 2024)</a></li><li><a href=#33--206218-fully-dynamic-correlation-clustering-breaking-3-approximation-soheil-behnezhad-et-al-2024>(3/3 | 206/218) Fully Dynamic Correlation Clustering: Breaking 3-Approximation (Soheil Behnezhad et al., 2024)</a></li></ul></li><li><a href=#q-finst-1>q-fin.ST (1)</a><ul><li><a href=#11--207218-predicting-mergers-and-acquisitions-in-competitive-industries-a-model-based-on-temporal-dynamics-and-industry-networks-dayu-yang-2024>(1/1 | 207/218) Predicting Mergers and Acquisitions in Competitive Industries: A Model Based on Temporal Dynamics and Industry Networks (Dayu Yang, 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--208218-temperature-prediction-for-stored-grain-a-multi-model-fusion-strategy-based-on-machine-learning-donghao-chen-et-al-2024>(1/1 | 208/218) Temperature Prediction for Stored Grain: A Multi-model Fusion Strategy Based on Machine Learning (Donghao Chen et al., 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--209218-data-driven-quasiconformal-morphodynamic-flows-salem-mosleh-et-al-2024>(1/1 | 209/218) Data-driven quasiconformal morphodynamic flows (Salem Mosleh et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--210218-pod-suboptimal-control-of-evolution-problems-theory-and-applications-stefan-banholzer-et-al-2024>(1/1 | 210/218) POD Suboptimal Control of Evolution Problems: Theory and Applications (Stefan Banholzer et al., 2024)</a></li></ul></li><li><a href=#cspf-1>cs.PF (1)</a><ul><li><a href=#11--211218-impact-of-extensions-on-browser-performance-an-empirical-study-on-google-chrome-bihui-jin-et-al-2024>(1/1 | 211/218) Impact of Extensions on Browser Performance: An Empirical Study on Google Chrome (Bihui Jin et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--212218-algorithms-and-analysis-for-optimizing-robust-objectives-in-fair-machine-learning-cyrus-cousins-2024>(1/1 | 212/218) Algorithms and Analysis for Optimizing Robust Objectives in Fair Machine Learning (Cyrus Cousins, 2024)</a></li></ul></li><li><a href=#csne-3>cs.NE (3)</a><ul><li><a href=#13--213218-neural-optimizer-equation-decay-function-and-learning-rate-schedule-joint-evolution-brandon-morgan-et-al-2024>(1/3 | 213/218) Neural Optimizer Equation, Decay Function, and Learning Rate Schedule Joint Evolution (Brandon Morgan et al., 2024)</a></li><li><a href=#23--214218-semantically-correlated-memories-in-a-dense-associative-model-thomas-f-burns-2024>(2/3 | 214/218) Semantically-correlated memories in a dense associative model (Thomas F Burns, 2024)</a></li><li><a href=#33--215218-a-tight-o4kp_c-runtime-bound-for-a-μ1-ga-on-jump_k-for-realistic-crossover-probabilities-andre-opris-et-al-2024>(3/3 | 215/218) A Tight $O(4^k/p_c)$ Runtime Bound for a ($μ$+1) GA on Jump$_k$ for Realistic Crossover Probabilities (Andre Opris et al., 2024)</a></li></ul></li><li><a href=#csdm-1>cs.DM (1)</a><ul><li><a href=#11--216218-probabilistic-estimates-of-the-diameters-of-the-rubiks-cube-groups-so-hirata-2024>(1/1 | 216/218) Probabilistic estimates of the diameters of the Rubik&rsquo;s Cube groups (So Hirata, 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--217218-on-the-existence-of-δ-temporal-cliques-in-random-simple-temporal-graphs-george-b-mertzios-et-al-2024>(1/1 | 217/218) On the existence of $δ$-temporal cliques in random simple temporal graphs (George B. Mertzios et al., 2024)</a></li></ul></li><li><a href=#csdl-1>cs.DL (1)</a><ul><li><a href=#11--218218-linked-open-data-per-la-valorizzazione-di-collezioni-culturali-il-dataset-mythlod-valentina-pasqual-et-al-2024>(1/1 | 218/218) Linked open data per la valorizzazione di collezioni culturali: il dataset mythLOD (Valentina Pasqual et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>