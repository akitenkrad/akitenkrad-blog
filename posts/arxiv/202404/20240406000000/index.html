<!doctype html><html><head><title>arXiv @ 2024.04.06</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.04.06"><meta property="og:description" content="Primary Categories cs.AI (3) cs.CE (1) cs.CG (2) cs.CL (45) cs.CR (7) cs.CV (65) cs.CY (1) cs.DB (1) cs.DC (3) cs.DS (2) cs.ET (1) cs.HC (4) cs.IR (8) cs.IT (4) cs.LG (42) cs.LO (3) cs.NE (2) cs.PL (1) cs.RO (16) cs.SE (3) econ.EM (1) eess.AS (2) eess.IV (4) eess.SP (2) eess.SY (2) math.CO (1) math.NA (1) math.OC (2) physics.ao-ph (1) quant-ph (4) stat.ME (1) stat.ML (2) Keywords keyword cs.CL cs."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202404/20240406000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-06T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-06T00:00:00+00:00"><meta name=description content="arXiv @ 2024.04.06"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202404/20240406000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Saturday, Apr 6, 2024</p></div><div class=title><h1>arXiv @ 2024.04.06</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#csai-3>cs.AI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#cscg-2>cs.CG (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#cscl-45>cs.CL (45)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#cscr-7>cs.CR (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#cscv-65>cs.CV (65)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#csdc-3>cs.DC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#cset-1>cs.ET (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#cshc-4>cs.HC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#csir-8>cs.IR (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#csit-4>cs.IT (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#cslg-42>cs.LG (42)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#cslo-3>cs.LO (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#csne-2>cs.NE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#csro-16>cs.RO (16)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#csse-3>cs.SE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#econem-1>econ.EM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#eessas-2>eess.AS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#eessiv-4>eess.IV (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#eesssp-2>eess.SP (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#eesssy-2>eess.SY (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#mathna-1>math.NA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#mathoc-2>math.OC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#physicsao-ph-1>physics.ao-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#quant-ph-4>quant-ph (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#statme-1>stat.ME (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/#statml-2>stat.ML (2)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Adversarial Attack</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Alpaca</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Aspect-based Sentiment Analysis</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td>1</td><td></td></tr><tr><td>BERT</td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>BERTScore</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td>14</td><td>22</td><td>9</td><td>1</td></tr><tr><td>Black Box</td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>ChatGPT</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Chatbot</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Claude</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td>5</td><td>3</td><td></td></tr><tr><td>Convolution</td><td>1</td><td>4</td><td>6</td><td></td></tr><tr><td>Convolutional Neural Network</td><td>1</td><td>8</td><td>3</td><td></td></tr><tr><td>Coreference Resolution</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Counter-factual</td><td>2</td><td></td><td>2</td><td></td></tr><tr><td>Counterfactual Reasoning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Curriculum Learning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Dialogue System</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Differential Privacy</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>9</td><td>2</td><td></td></tr><tr><td>Distribution Shift</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Event Detection</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Explainable AI</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Face Recognition</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>3</td><td></td></tr><tr><td>Few-shot</td><td>4</td><td></td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>11</td><td>12</td><td>1</td><td></td></tr><tr><td>Foundation Model</td><td></td><td>2</td><td></td><td></td></tr><tr><td>GLUE</td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT</td><td>6</td><td></td><td>2</td><td></td></tr><tr><td>GPT-4</td><td>4</td><td></td><td>2</td><td></td></tr><tr><td>GPT-4 turbo</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Generative AI</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>4</td><td>3</td><td></td></tr><tr><td>Geometry</td><td></td><td>7</td><td></td><td></td></tr><tr><td>Graph</td><td>4</td><td>1</td><td>5</td><td>2</td></tr><tr><td>Graph Convolutional Network</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Graph Neural Network</td><td>1</td><td></td><td>7</td><td></td></tr><tr><td>Grounding</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Hallucination Detection</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Human Intervention</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Image2text</td><td></td><td>2</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>5</td><td></td><td></td><td></td></tr><tr><td>Information Retrieval</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Intent Detection</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>4</td><td>5</td><td>9</td><td></td></tr><tr><td>Knowledge Graph</td><td>5</td><td></td><td></td><td></td></tr><tr><td>LLaMA</td><td>1</td><td></td><td></td><td></td></tr><tr><td>LSTM</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Language Generation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>48</td><td>10</td><td>5</td><td>6</td></tr><tr><td>MNIST</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Machine Unlearning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Message-Passing</td><td>1</td><td></td><td>3</td><td></td></tr><tr><td>Mistral</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Model Extraction</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Model Quantization</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Multi-modal</td><td>4</td><td>17</td><td>4</td><td>2</td></tr><tr><td>Named Entity Recognition</td><td>4</td><td></td><td></td><td></td></tr><tr><td>Natural Language Explanation</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Noise-tolerant</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Optical Character Recognition</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Outlier Detection</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Perplexity</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Prompt</td><td>12</td><td>9</td><td>1</td><td></td></tr><tr><td>Prompt Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td>3</td><td>2</td><td></td></tr><tr><td>Question Answering</td><td>9</td><td>2</td><td></td><td></td></tr><tr><td>Reasoning</td><td>14</td><td>2</td><td></td><td>1</td></tr><tr><td>Recommendation</td><td>2</td><td></td><td>2</td><td></td></tr><tr><td>Recommender System</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td>2</td><td>1</td><td>7</td><td>1</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Relation Extraction</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Rouge</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td>1</td><td>3</td><td>2</td><td></td></tr><tr><td>Self-Distillation</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Sentence Embedding</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td>1</td><td>6</td><td>10</td></tr><tr><td>Simulator</td><td></td><td>1</td><td>6</td><td>10</td></tr><tr><td>Summarization</td><td>4</td><td>1</td><td>1</td><td></td></tr><tr><td>Supervised Learning</td><td>2</td><td>6</td><td>6</td><td></td></tr><tr><td>Temporal Knowledge Graph</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>5</td><td>1</td><td></td></tr><tr><td>Tokenization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Transformer</td><td>4</td><td>10</td><td>4</td><td>1</td></tr><tr><td>Unsupervised Learning</td><td></td><td>4</td><td></td><td></td></tr><tr><td>Video-and-Language</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>4</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>5</td><td></td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Word Embedding</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td>7</td><td></td><td>1</td></tr><tr><td>Zero-shot Learning</td><td></td><td></td><td></td><td>1</td></tr><tr><td>human-in-the-loop</td><td></td><td></td><td></td><td>1</td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-45>cs.CL (45)</h2><h3 id=145--1237-enhancing-the-performance-of-aspect-based-sentiment-analysis-systems-chen-li-et-al-2024>(1/45 | 1/237) Enhancing the Performance of Aspect-Based Sentiment Analysis Systems (Chen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Li, Jinli Zhang, Huidong Tang, Peng Ju, Debo Cheng, Yasuhiko Morimoto. (2024)<br><strong>Enhancing the Performance of Aspect-Based Sentiment Analysis Systems</strong><br><button class=copy-to-clipboard title="Enhancing the Performance of Aspect-Based Sentiment Analysis Systems" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 116<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Message-Passing, Graph, Benchmarking, Convolution, Convolutional Neural Network, LSTM, LSTM, Transformer, Aspect-based Sentiment Analysis, Sentiment Analysis, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03259v1.pdf filename=2404.03259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Aspect-based</b> <b>sentiment</b> <b>analysis</b> aims to predict <b>sentiment</b> <b>polarity</b> with fine granularity. While <b>Graph</b> <b>Convolutional</b> <b>Networks</b> <b>(GCNs)</b> are widely utilized for <b>sentimental</b> <b>feature</b> extraction, their naive application for syntactic feature extraction can compromise information preservation. This study introduces an innovative edge-enhanced <b>GCN,</b> named SentiSys, to navigate the syntactic <b>graph</b> <b>while</b> <b>preserving</b> intact feature information, leading to enhanced performance. Specifically,we first integrate a bidirectional <b>long</b> <b>short-term</b> <b>memory</b> <b>(Bi-LSTM)</b> network and a <b>self-attention-based</b> <b>transformer.</b> This combination facilitates effective text encoding, preventing the loss of information and predicting <b>long</b> <b>dependency</b> <b>text.</b> <b>A</b> bidirectional <b>GCN</b> (Bi-GCN) with message passing is then employed to encode relationships between entities. Additionally, unnecessary information is filtered out using an aspect-specific masking technique. To validate the effectiveness of our proposed model, we conduct extensive evaluation experiments and ablation studies on four <b>benchmark</b> datasets. The results consistently demonstrate improved performance in <b>aspect-based</b> <b>sentiment</b> <b>analysis</b> when employing SentiSys. This approach successfully addresses the challenges associated with syntactic feature extraction, highlighting its potential for advancing <b>sentiment</b> <b>analysis</b> methodologies.</p></p class="citation"></blockquote><h3 id=245--2237-evaluating-llms-at-detecting-errors-in-llm-responses-ryo-kamoi-et-al-2024>(2/45 | 2/237) Evaluating LLMs at Detecting Errors in LLM Responses (Ryo Kamoi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryo Kamoi, Sarkar Snigdha Sarathi Das, Renze Lou, Jihyun Janice Ahn, Yilun Zhao, Xiaoxin Lu, Nan Zhang, Yusen Zhang, Ranran Haoran Zhang, Sujeeth Reddy Vummanthala, Salika Dave, Shaobo Qin, Arman Cohan, Wenpeng Yin, Rui Zhang. (2024)<br><strong>Evaluating LLMs at Detecting Errors in LLM Responses</strong><br><button class=copy-to-clipboard title="Evaluating LLMs at Detecting Errors in LLM Responses" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 103<br>Keywords: Benchmarking, Claude, GPT, GPT-4, LLaMA, Instruction Following, Reasoning, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03602v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03602v1.pdf filename=2404.03602v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> being widely used across various tasks, detecting errors in their responses is increasingly crucial. However, little research has been conducted on error detection of <b>LLM</b> responses. Collecting error annotations on <b>LLM</b> responses is challenging due to the subjective nature of many NLP tasks, and thus previous research focuses on tasks of little practical value (e.g., word sorting) or limited error types (e.g., faithfulness in <b>summarization).</b> This work introduces ReaLMistake, the first error detection <b>benchmark</b> consisting of objective, realistic, and diverse errors made by <b>LLMs.</b> ReaLMistake contains three challenging and meaningful tasks that introduce objectively assessable errors in four categories <b>(reasoning</b> correctness, <b>instruction-following,</b> <b>context-faithfulness,</b> and parameterized knowledge), eliciting naturally observed and diverse errors in responses of <b>GPT-4</b> and <b>Llama</b> 2 70B annotated by experts. We use ReaLMistake to evaluate error detectors based on 12 <b>LLMs.</b> Our findings show: 1) Top <b>LLMs</b> like <b>GPT-4</b> and <b>Claude</b> 3 detect errors made by <b>LLMs</b> at very low recall, and all <b>LLM-based</b> error detectors perform much worse than humans. 2) Explanations by <b>LLM-based</b> error detectors lack reliability. 3) <b>LLMs-based</b> error detection is sensitive to small changes in <b>prompts</b> but remains challenging to improve. 4) Popular approaches to improving <b>LLMs,</b> including self-consistency and majority vote, do not improve the error detection performance. Our <b>benchmark</b> and code are provided at <a href=https://github.com/psunlpgroup/ReaLMistake>https://github.com/psunlpgroup/ReaLMistake</a>.</p></p class="citation"></blockquote><h3 id=345--3237-intent-detection-and-entity-extraction-from-biomedical-literature-ankan-mullick-et-al-2024>(3/45 | 3/237) Intent Detection and Entity Extraction from BioMedical Literature (Ankan Mullick et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ankan Mullick, Mukur Gupta, Pawan Goyal. (2024)<br><strong>Intent Detection and Entity Extraction from BioMedical Literature</strong><br><button class=copy-to-clipboard title="Intent Detection and Entity Extraction from BioMedical Literature" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Fine-tuning, Supervised Learning, ChatGPT, Transformer, Intent Detection, Named Entity Recognition, Named Entity Recognition, Natural Language Understanding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03598v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03598v1.pdf filename=2404.03598v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Biomedical queries have become increasingly prevalent in web searches, reflecting the growing interest in accessing biomedical literature. Despite recent research on <b>large-language</b> <b>models</b> <b>(LLMs)</b> motivated by endeavours to attain generalized intelligence, their efficacy in replacing task and domain-specific <b>natural</b> <b>language</b> <b>understanding</b> approaches remains questionable. In this paper, we address this question by conducting a comprehensive empirical evaluation of <b>intent</b> <b>detection</b> and <b>named</b> <b>entity</b> <b>recognition</b> <b>(NER)</b> tasks from biomedical text. We show that <b>Supervised</b> Fine Tuned approaches are still relevant and more effective than general-purpose <b>LLMs.</b> Biomedical <b>transformer</b> models such as PubMedBERT can surpass <b>ChatGPT</b> on <b>NER</b> task with only 5 <b>supervised</b> examples.</p></p class="citation"></blockquote><h3 id=445--4237-how-does-multi-task-training-affect-transformer-in-context-capabilities-investigations-with-function-classes-harmon-bhasin-et-al-2024>(4/45 | 4/237) How does Multi-Task Training Affect Transformer In-Context Capabilities? Investigations with Function Classes (Harmon Bhasin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harmon Bhasin, Timothy Ossowski, Yiqiao Zhong, Junjie Hu. (2024)<br><strong>How does Multi-Task Training Affect Transformer In-Context Capabilities? Investigations with Function Classes</strong><br><button class=copy-to-clipboard title="How does Multi-Task Training Affect Transformer In-Context Capabilities? Investigations with Function Classes" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 100<br>Keywords: Curriculum Learning, Few-shot, Out-of-distribution, Transfer Learning, Transformer, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03558v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03558v1.pdf filename=2404.03558v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLM)</b> have recently shown the extraordinary ability to perform unseen tasks based on <b>few-shot</b> examples provided as text, also known as <b>in-context</b> <b>learning</b> <b>(ICL).</b> While recent works have attempted to understand the mechanisms driving <b>ICL,</b> few have explored training strategies that incentivize these models to generalize to multiple tasks. Multi-task learning (MTL) for generalist models is a promising direction that offers <b>transfer</b> <b>learning</b> potential, enabling <b>large</b> <b>parameterized</b> <b>models</b> to be trained from simpler, related tasks. In this work, we investigate the combination of MTL with <b>ICL</b> to build models that efficiently learn tasks while being robust to <b>out-of-distribution</b> examples. We propose several effective <b>curriculum</b> <b>learning</b> strategies that allow <b>ICL</b> models to achieve higher data efficiency and more stable convergence. Our experiments reveal that <b>ICL</b> models can effectively learn difficult tasks by training on progressively harder tasks while mixing in prior tasks, denoted as mixed <b>curriculum</b> <b>in</b> this work. Our code and models are available at <a href=https://github.com/harmonbhasin/curriculum_learning_icl>https://github.com/harmonbhasin/curriculum_learning_icl</a> .</p></p class="citation"></blockquote><h3 id=545--5237-banglaautokg-automatic-bangla-knowledge-graph-construction-with-semantic-neural-graph-filtering-azmine-toushik-wasi-et-al-2024>(5/45 | 5/237) BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with Semantic Neural Graph Filtering (Azmine Toushik Wasi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Dong-Kyu Chae. (2024)<br><strong>BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with Semantic Neural Graph Filtering</strong><br><button class=copy-to-clipboard title="BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with Semantic Neural Graph Filtering" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs-LG, cs-NE, cs-SI, cs.CL<br>Keyword Score: 93<br>Keywords: Graph, Graph Neural Network, Knowledge Graph, Knowledge Graph, BERT, Information Retrieval, Named Entity Recognition, Named Entity Recognition, Reasoning, Large Language Model, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03528v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03528v2.pdf filename=2404.03528v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>Graphs</b> <b>(KGs)</b> have proven essential in <b>information</b> <b>processing</b> and <b>reasoning</b> applications because they link related entities and give context-rich <b>information,</b> <b>supporting</b> efficient <b>information</b> <b>retrieval</b> and <b>knowledge</b> <b>discovery;</b> presenting <b>information</b> <b>flow</b> in a very effective manner. Despite being widely used globally, Bangla is relatively underrepresented in <b>KGs</b> due to a lack of comprehensive datasets, encoders, <b>NER</b> <b>(named</b> <b>entity</b> <b>recognition)</b> models, POS (part-of-speech) taggers, and lemmatizers, hindering efficient <b>information</b> <b>processing</b> and <b>reasoning</b> applications in the language. Addressing the <b>KG</b> scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework that is able to automatically construct Bengali <b>KGs</b> from any Bangla text. We utilize multilingual <b>LLMs</b> to understand various languages and correlate entities and relations universally. By employing a translation dictionary to identify English equivalents and extracting <b>word</b> <b>features</b> from pre-trained <b>BERT</b> models, we construct the foundational <b>KG.</b> To reduce noise and align <b>word</b> <b>embeddings</b> with our goal, we employ <b>graph-based</b> polynomial filters. Lastly, we implement a <b>GNN-based</b> semantic filter, which elevates contextual understanding and trims unnecessary edges, culminating in the formation of the definitive <b>KG.</b> Empirical findings and case studies demonstrate the universal effectiveness of our model, capable of autonomously constructing semantically enriched <b>KGs</b> from any text.</p></p class="citation"></blockquote><h3 id=645--6237-canttalkaboutthis-aligning-language-models-to-stay-on-topic-in-dialogues-makesh-narsimhan-sreedhar-et-al-2024>(6/45 | 6/237) CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues (Makesh Narsimhan Sreedhar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Makesh Narsimhan Sreedhar, Traian Rebedea, Shaona Ghosh, Christopher Parisien. (2024)<br><strong>CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues</strong><br><button class=copy-to-clipboard title="CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, GPT, GPT-4, GPT-4 turbo, Chatbot, Instruction Following, Reasoning, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03820v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03820v1.pdf filename=2404.03820v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>instruction-tuning</b> <b>datasets</b> have predominantly focused on specific tasks like mathematical or logical <b>reasoning.</b> There has been a notable gap in data designed for aligning language models to maintain topic relevance in conversations - a critical aspect for deploying <b>chatbots</b> to production. We introduce the CantTalkAboutThis dataset to help language models remain focused on the subject at hand during task-oriented interactions. It consists of synthetic dialogues on a wide range of conversation topics from different domains. These dialogues are interspersed with distractor turns that intentionally divert the <b>chatbot</b> from the predefined topic. <b>Fine-tuning</b> language models on this dataset helps make them resilient to deviating from the role assigned and improves their ability to maintain topical coherence compared to general-purpose <b>instruction-tuned</b> <b>LLMs</b> like <b>GPT-4-turbo</b> <b>and</b> Mixtral-Instruct. Additionally, preliminary observations suggest that training models on this dataset also enhance their performance on fine-grained <b>instruction</b> <b>following</b> tasks.</p></p class="citation"></blockquote><h3 id=745--7237-can-small-language-models-help-large-language-models-reason-better-lm-guided-chain-of-thought-jooyoung-lee-et-al-2024>(7/45 | 7/237) Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought (Jooyoung Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jooyoung Lee, Fan Yang, Thanh Tran, Qian Hu, Emre Barut, Kai-Wei Chang, Chengwei Su. (2024)<br><strong>Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought</strong><br><button class=copy-to-clipboard title="Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 88<br>Keywords: Benchmarking, Black Box, Knowledge Distillation, Knowledge Distillation, Reinforcement Learning, Question Answering, Question Answering, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03414v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03414v1.pdf filename=2404.03414v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel framework, LM-Guided CoT, that leverages a lightweight (i.e., &lt;1B) language model (LM) for guiding a <b>black-box</b> <b>large</b> <b>(i.e.,</b> <b>>10B)</b> LM in <b>reasoning</b> tasks. Specifically, the lightweight LM first generates a rationale for each input instance. The Frozen <b>large</b> <b>LM</b> <b>is</b> then <b>prompted</b> to predict a task output based on the rationale generated by the lightweight LM. Our approach is resource-efficient in the sense that it only requires training the lightweight LM. We optimize the model through 1) <b>knowledge</b> <b>distillation</b> and 2) <b>reinforcement</b> <b>learning</b> from rationale-oriented and task-oriented reward signals. We assess our method with multi-hop extractive <b>question</b> <b>answering</b> <b>(QA)</b> <b>benchmarks,</b> HotpotQA, and 2WikiMultiHopQA. Experimental results show that our approach outperforms all baselines regarding answer prediction accuracy. We also find that <b>reinforcement</b> <b>learning</b> helps the model to produce higher-quality rationales with improved <b>QA</b> performance.</p></p class="citation"></blockquote><h3 id=845--8237-shroom-indelab-at-semeval-2024-task-6-zero--and-few-shot-llm-based-classification-for-hallucination-detection-bradley-p-allen-et-al-2024>(8/45 | 8/237) SHROOM-INDElab at SemEval-2024 Task 6: Zero- and Few-Shot LLM-Based Classification for Hallucination Detection (Bradley P. Allen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bradley P. Allen, Fina Polat, Paul Groth. (2024)<br><strong>SHROOM-INDElab at SemEval-2024 Task 6: Zero- and Few-Shot LLM-Based Classification for Hallucination Detection</strong><br><button class=copy-to-clipboard title="SHROOM-INDElab at SemEval-2024 Task 6: Zero- and Few-Shot LLM-Based Classification for Hallucination Detection" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, Zero-shot, Hallucination Detection, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03732v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03732v1.pdf filename=2404.03732v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We describe the University of Amsterdam Intelligent Data Engineering Lab team&rsquo;s entry for the SemEval-2024 Task 6 competition. The SHROOM-INDElab system builds on previous work on using <b>prompt</b> programming and <b>in-context</b> <b>learning</b> with <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to build classifiers for <b>hallucination</b> <b>detection,</b> and extends that work through the incorporation of context-specific definition of task, role, and target concept, and automated generation of examples for use in a <b>few-shot</b> <b>prompting</b> approach. The resulting system achieved fourth-best and sixth-best performance in the model-agnostic track and model-aware tracks for Task 6, respectively, and evaluation using the validation sets showed that the system&rsquo;s classification decisions were consistent with those of the crowd-sourced human labellers. We further found that a <b>zero-shot</b> approach provided better accuracy than a <b>few-shot</b> approach using automatically generated examples. Code for the system described in this paper is available on Github.</p></p class="citation"></blockquote><h3 id=945--9237-evaluating-generative-language-models-in-information-extraction-as-subjective-question-correction-yuchen-fan-et-al-2024>(9/45 | 9/237) Evaluating Generative Language Models in Information Extraction as Subjective Question Correction (Yuchen Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuchen Fan, Yantao Liu, Zijun Yao, Jifan Yu, Lei Hou, Juanzi Li. (2024)<br><strong>Evaluating Generative Language Models in Information Extraction as Subjective Question Correction</strong><br><button class=copy-to-clipboard title="Evaluating Generative Language Models in Information Extraction as Subjective Question Correction" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Information Retrieval, Natural Language Inference, Natural Language Inference, Relation Extraction, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03532v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03532v1.pdf filename=2404.03532v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have showcased remarkable prowess in various tasks necessitating sophisticated cognitive behaviors. Nevertheless, a paradoxical performance discrepancy is observed, where these models underperform in seemingly elementary tasks like <b>relation</b> <b>extraction</b> and event extraction due to two issues in conventional evaluation. (1) The imprecision of existing evaluation metrics that struggle to effectively gauge semantic consistency between model outputs and ground truth, and (2) The inherent incompleteness of evaluation <b>benchmarks,</b> primarily due to restrictive human annotation schemas, resulting in underestimated <b>LLM</b> performances. Inspired by the principles in subjective question correction, we propose a new evaluation method, SQC-Score. This method innovatively utilizes <b>LLMs,</b> <b>fine-tuned</b> through subjective question correction data, to refine matching between model outputs and golden labels. Additionally, by incorporating a <b>Natural</b> <b>Language</b> <b>Inference</b> <b>(NLI)</b> model, SQC-Score enriches golden labels, addressing <b>benchmark</b> incompleteness by acknowledging correct yet previously omitted answers. Results on three <b>information</b> <b>extraction</b> tasks show that SQC-Score is more preferred by human annotators than the baseline metrics. Utilizing SQC-Score, we conduct a comprehensive evaluation of the state-of-the-art <b>LLMs</b> and provide insights for future research for <b>information</b> <b>extraction.</b> Dataset and associated codes can be accessed at <a href=https://github.com/THU-KEG/SQC-Score>https://github.com/THU-KEG/SQC-Score</a>.</p></p class="citation"></blockquote><h3 id=1045--10237-untangle-the-knot-interweaving-conflicting-knowledge-and-reasoning-skills-in-large-language-models-yantao-liu-et-al-2024>(10/45 | 10/237) Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models (Yantao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yantao Liu, Zijun Yao, Xin Lv, Yuchen Fan, Shulin Cao, Jifan Yu, Lei Hou, Juanzi Li. (2024)<br><strong>Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models</strong><br><button class=copy-to-clipboard title="Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Supervised Learning, Question Answering, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03577v1.pdf filename=2404.03577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Providing knowledge documents for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has emerged as a promising solution to update the static knowledge inherent in their parameters. However, knowledge in the document may conflict with the memory of <b>LLMs</b> due to outdated or incorrect knowledge in the <b>LLMs&rsquo;</b> parameters. This leads to the necessity of examining the capability of <b>LLMs</b> to assimilate supplemental external knowledge that conflicts with their memory. While previous studies have explained to what extent <b>LLMs</b> extract conflicting knowledge from the provided text, they neglect the necessity to reason with conflicting knowledge. Furthermore, there lack a detailed analysis on strategies to enable <b>LLMs</b> to resolve conflicting knowledge via <b>prompting,</b> decoding strategy, and <b>supervised</b> <b>fine-tuning.</b> To address these limitations, we construct a new dataset, dubbed KNOT, for knowledge conflict resolution examination in the form of <b>question</b> <b>answering.</b> KNOT facilitates in-depth analysis by dividing <b>reasoning</b> with conflicting knowledge into three levels: (1) Direct Extraction, which directly extracts conflicting knowledge to answer <b>questions.</b> <b>(2)</b> Explicit <b>Reasoning,</b> which reasons with conflicting knowledge when the <b>reasoning</b> path is explicitly provided in the <b>question.</b> <b>(3)</b> Implicit <b>Reasoning,</b> where <b>reasoning</b> with conflicting knowledge requires <b>LLMs</b> to infer the <b>reasoning</b> path independently to answer <b>questions.</b> <b>We</b> also conduct extensive experiments on KNOT to establish empirical guidelines for <b>LLMs</b> to utilize conflicting knowledge in complex circumstances. Dataset and associated codes can be accessed at <a href=https://github.com/THU-KEG/KNOT>https://github.com/THU-KEG/KNOT</a> .</p></p class="citation"></blockquote><h3 id=1145--11237-the-probabilities-also-matter-a-more-faithful-metric-for-faithfulness-of-free-text-explanations-in-large-language-models-noah-y-siegel-et-al-2024>(11/45 | 11/237) The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models (Noah Y. Siegel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Noah Y. Siegel, Oana-Maria Camburu, Nicolas Heess, Maria Perez-Ortiz. (2024)<br><strong>The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models</strong><br><button class=copy-to-clipboard title="The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Counter-factual, Few-shot, Natural Language Explanation, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03189v1.pdf filename=2404.03189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In order to oversee advanced AI systems, it is important to understand their underlying decision-making process. When <b>prompted,</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can provide <b>natural</b> <b>language</b> <b>explanations</b> or <b>reasoning</b> traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are faithful, i.e., truly capture the factors responsible for the model&rsquo;s predictions. In this work, we introduce Correlational Explanatory Faithfulness (CEF), a metric that can be used in faithfulness tests based on input interventions. Previous metrics used in such tests take into account only binary changes in the predictions. Our metric accounts for the total shift in the model&rsquo;s predicted label distribution, more accurately reflecting the explanations&rsquo; faithfulness. We then introduce the Correlational <b>Counterfactual</b> Test (CCT) by instantiating CEF on the <b>Counterfactual</b> Test (CT) from Atanasova et al. (2023). We evaluate the faithfulness of free-text explanations generated by <b>few-shot-prompted</b> <b>LLMs</b> from the Llama2 family on three NLP tasks. We find that our metric measures aspects of faithfulness which the CT misses.</p></p class="citation"></blockquote><h3 id=1245--12237-nlp-at-uc-santa-cruz-at-semeval-2024-task-5-legal-answer-validation-using-few-shot-multi-choice-qa-anish-pahilajani-et-al-2024>(12/45 | 12/237) NLP at UC Santa Cruz at SemEval-2024 Task 5: Legal Answer Validation using Few-Shot Multi-Choice QA (Anish Pahilajani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anish Pahilajani, Samyak Rajesh Jain, Devasha Trivedi. (2024)<br><strong>NLP at UC Santa Cruz at SemEval-2024 Task 5: Legal Answer Validation using Few-Shot Multi-Choice QA</strong><br><button class=copy-to-clipboard title="NLP at UC Santa Cruz at SemEval-2024 Task 5: Legal Answer Validation using Few-Shot Multi-Choice QA" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, Fine-tuning, BERT, GPT, Question Answering, Reasoning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03150v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03150v1.pdf filename=2404.03150v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents our submission to the SemEval 2024 Task 5: The Legal Argument <b>Reasoning</b> Task in Civil Procedure. We present two approaches to solving the task of legal answer validation, given an introduction to the case, a question and an answer candidate. Firstly, we <b>fine-tuned</b> pre-trained <b>BERT-based</b> models and found that models trained on domain knowledge perform better. Secondly, we performed <b>few-shot</b> <b>prompting</b> on <b>GPT</b> models and found that reformulating the answer validation task to be a multiple-choice <b>QA</b> task remarkably improves the performance of the model. Our best submission is a <b>BERT-based</b> model that achieved the 7th place out of 20.</p></p class="citation"></blockquote><h3 id=1345--13237-a-cause-effect-look-at-alleviating-hallucination-of-knowledge-grounded-dialogue-generation-jifan-yu-et-al-2024>(13/45 | 13/237) A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation (Jifan Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jifan Yu, Xiaohan Zhang, Yifan Xu, Xuanyu Lei, Zijun Yao, Jing Zhang, Lei Hou, Juanzi Li. (2024)<br><strong>A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation</strong><br><button class=copy-to-clipboard title="A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 65<br>Keywords: Counter-factual, Knowledge Graph, Noise-tolerant, Counterfactual Reasoning, Dialogue System, Reasoning, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03491v1.pdf filename=2404.03491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Empowered by the large-scale <b>pretrained</b> <b>language</b> <b>models,</b> existing <b>dialogue</b> <b>systems</b> have demonstrated impressive performance conducting fluent and natural-sounding conversations. However, they are still plagued by the hallucination problem, causing unpredictable factual errors in the generated responses. Recently, knowledge-grounded <b>dialogue</b> <b>generation</b> models, that intentionally invoke external knowledge resources to more informative responses, are also proven to be effective in reducing hallucination. Following the idea of getting high-quality knowledge, a few efforts have achieved pretty good performance on this issue. As some inevitable knowledge noises may also lead to hallucinations, it is emergent to investigate the reason and future directions for building <b>noise-tolerant</b> methods in <b>KGD</b> tasks. In this paper, we analyze the causal story behind this problem with <b>counterfactual</b> <b>reasoning</b> methods. Based on the causal effect analysis, we propose a possible solution for alleviating the hallucination in <b>KGD</b> by exploiting the <b>dialogue-knowledge</b> <b>interaction.</b> Experimental results of our example implementation show that this method can reduce hallucination without disrupting other <b>dialogue</b> <b>performance,</b> while keeping adaptive to different generation models. We hope our efforts can support and call for more attention to developing lightweight techniques towards robust and trusty <b>dialogue</b> <b>systems.</b></p></p class="citation"></blockquote><h3 id=1445--14237-unveiling-llms-the-evolution-of-latent-representations-in-a-temporal-knowledge-graph-marco-bronzini-et-al-2024>(14/45 | 14/237) Unveiling LLMs: The Evolution of Latent Representations in a Temporal Knowledge Graph (Marco Bronzini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea Passerini. (2024)<br><strong>Unveiling LLMs: The Evolution of Latent Representations in a Temporal Knowledge Graph</strong><br><button class=copy-to-clipboard title="Unveiling LLMs: The Evolution of Latent Representations in a Temporal Knowledge Graph" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs.CL<br>Keyword Score: 58<br>Keywords: Graph, Knowledge Graph, Reasoning, Large Language Model, Large Language Model, Prompt, Temporal Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03623v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03623v1.pdf filename=2404.03623v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> demonstrate an impressive capacity to recall a vast range of common factual <b>knowledge</b> <b>information.</b> However, unravelling the underlying <b>reasoning</b> of <b>LLMs</b> and explaining their internal mechanisms of exploiting this factual <b>knowledge</b> <b>remain</b> active areas of investigation. Our work analyzes the factual <b>knowledge</b> <b>encoded</b> in the latent representation of <b>LLMs</b> when <b>prompted</b> to assess the truthfulness of factual claims. We propose an end-to-end framework that jointly decodes the factual <b>knowledge</b> <b>embedded</b> in the latent space of <b>LLMs</b> from a vector space to a set of ground predicates and represents its evolution across the layers using a <b>temporal</b> <b>knowledge</b> <b>graph.</b> Our framework relies on the technique of activation patching which intervenes in the inference computation of a model by dynamically altering its latent representations. Consequently, we neither rely on external models nor training processes. We showcase our framework with local and global interpretability analyses using two claim verification datasets: FEVER and CLIMATE-FEVER. The local interpretability analysis exposes different latent errors from representation to multi-hop <b>reasoning</b> errors. On the other hand, the global analysis uncovered patterns in the underlying evolution of the model&rsquo;s factual <b>knowledge</b> <b>(e.g.,</b> store-and-seek factual information). By enabling <b>graph-based</b> analyses of the latent representations, this work represents a step towards the mechanistic interpretability of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1545--15237-autowebglm-bootstrap-and-reinforce-a-large-language-model-based-web-navigating-agent-hanyu-lai-et-al-2024>(15/45 | 15/237) AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent (Hanyu Lai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, Jie Tang. (2024)<br><strong>AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent</strong><br><button class=copy-to-clipboard title="AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Reinforcement Learning, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03648v1.pdf filename=2404.03648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have fueled many intelligent agent tasks, such as web navigation &ndash; but most existing agents perform far from satisfying in real-world webpages due to three factors: (1) the versatility of actions on webpages, (2) HTML text exceeding model processing capacity, and (3) the complexity of decision-making due to the open-domain nature of web. In light of the challenge, we develop AutoWebGLM, a <b>GPT-4-outperforming</b> automated web navigation agent built upon ChatGLM3-6B. Inspired by human browsing patterns, we design an HTML simplification algorithm to represent webpages, preserving vital information succinctly. We employ a hybrid human-AI method to build web browsing data for curriculum training. Then, we bootstrap the model by <b>reinforcement</b> <b>learning</b> and rejection sampling to further facilitate webpage comprehension, browser operations, and efficient task decomposition by itself. For testing, we establish a bilingual <b>benchmark</b> &ndash; AutoWebBench &ndash; for real-world web browsing tasks. We evaluate AutoWebGLM across diverse web navigation <b>benchmarks,</b> revealing its improvements but also underlying challenges to tackle real environments. Related code, model, and data will be released at \url{https://github.com/THUDM/AutoWebGLM}.</p></p class="citation"></blockquote><h3 id=1645--16237-reft-representation-finetuning-for-language-models-zhengxuan-wu-et-al-2024>(16/45 | 16/237) ReFT: Representation Finetuning for Language Models (Zhengxuan Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky, Christopher D. Manning, Christopher Potts. (2024)<br><strong>ReFT: Representation Finetuning for Language Models</strong><br><button class=copy-to-clipboard title="ReFT: Representation Finetuning for Language Models" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Alpaca, Common-sense Reasoning, Reasoning, GLUE<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03592v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03592v1.pdf filename=2404.03592v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parameter-efficient <b>fine-tuning</b> (PEFT) methods seek to adapt large models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. Here, we pursue this hypothesis by developing a family of $\textbf{Representation <b>Finetuning</b> (ReFT)}$ methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT). LoReFT is a drop-in replacement for existing PEFTs and learns interventions that are 10x-50x more parameter-efficient than prior state-of-the-art PEFTs. We showcase LoReFT on eight <b>commonsense</b> <b>reasoning</b> tasks, four arithmetic <b>reasoning</b> tasks, <b>Alpaca-Eval</b> v1.0, and <b>GLUE.</b> In all these evaluations, LoReFT delivers the best balance of efficiency and performance, and almost always outperforms state-of-the-art PEFTs. We release a generic ReFT training library publicly at <a href=https://github.com/stanfordnlp/pyreft>https://github.com/stanfordnlp/pyreft</a>.</p></p class="citation"></blockquote><h3 id=1745--17237-nicolay-r-at-semeval-2024-task-3-using-flan-t5-for-reasoning-emotion-cause-in-conversations-with-chain-of-thought-on-emotion-states-nicolay-rusnachenko-et-al-2024>(17/45 | 17/237) nicolay-r at SemEval-2024 Task 3: Using Flan-T5 for Reasoning Emotion Cause in Conversations with Chain-of-Thought on Emotion States (Nicolay Rusnachenko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolay Rusnachenko, Huizhi Liang. (2024)<br><strong>nicolay-r at SemEval-2024 Task 3: Using Flan-T5 for Reasoning Emotion Cause in Conversations with Chain-of-Thought on Emotion States</strong><br><button class=copy-to-clipboard title="nicolay-r at SemEval-2024 Task 3: Using Flan-T5 for Reasoning Emotion Cause in Conversations with Chain-of-Thought on Emotion States" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, Reasoning, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03361v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03361v1.pdf filename=2404.03361v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emotion expression is one of the essential traits of conversations. It may be self-related or caused by another speaker. The variety of reasons may serve as a source of the further emotion causes: conversation history, speaker&rsquo;s emotional state, etc. Inspired by the most recent advances in Chain-of-Thought, in this work, we exploit the existing three-hop <b>reasoning</b> approach (THOR) to perform <b>large</b> <b>language</b> <b>model</b> <b>instruction-tuning</b> <b>for</b> answering: emotion states (THOR-state), and emotion caused by one speaker to the other (THOR-cause). We equip THOR-cause with the <b>reasoning</b> revision (rr) for devising a <b>reasoning</b> path in <b>fine-tuning.</b> In particular, we rely on the annotated speaker emotion states to revise <b>reasoning</b> path. Our final submission, based on Flan-T5-base (250M) and the rule-based span correction technique, preliminary tuned with THOR-state and <b>fine-tuned</b> with THOR-cause-rr on competition training data, results in 3rd and 4th places (F1-proportional) and 5th place (F1-strict) among 15 participating teams. Our THOR implementation fork is publicly available: <a href=https://github.com/nicolay-r/THOR-ECAC>https://github.com/nicolay-r/THOR-ECAC</a></p></p class="citation"></blockquote><h3 id=1845--18237-visualization-of-thought-elicits-spatial-reasoning-in-large-language-models-wenshan-wu-et-al-2024>(18/45 | 18/237) Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models (Wenshan Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, Furu Wei. (2024)<br><strong>Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models</strong><br><button class=copy-to-clipboard title="Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03622v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03622v1.pdf filename=2404.03622v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have exhibited impressive performance in language comprehension and various <b>reasoning</b> tasks. However, their abilities in spatial <b>reasoning,</b> a crucial aspect of human cognition, remain relatively unexplored. Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as \textbf{the Mind&rsquo;s Eye}, enabling the imagination of the unseen world. Inspired by this cognitive capacity, we propose Visualization-of-Thought (\textbf{VoT}) <b>prompting.</b> VoT aims to elicit spatial <b>reasoning</b> of <b>LLMs</b> by visualizing their <b>reasoning</b> traces, thereby guiding subsequent <b>reasoning</b> steps. We employed VoT for multi-hop spatial <b>reasoning</b> tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds. Experimental results demonstrated that VoT significantly enhances the spatial <b>reasoning</b> abilities of <b>LLMs.</b> Notably, VoT outperformed existing <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) in these tasks. While VoT works surprisingly well on <b>LLMs,</b> the ability to generate \textit{mental images} to facilitate spatial <b>reasoning</b> resembles the mind&rsquo;s eye process, suggesting its potential viability in MLLMs.</p></p class="citation"></blockquote><h3 id=1945--19237-sailor-open-language-models-for-south-east-asia-longxu-dou-et-al-2024>(19/45 | 19/237) Sailor: Open Language Models for South-East Asia (Longxu Dou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Longxu Dou, Qian Liu, Guangtao Zeng, Jia Guo, Jiahui Zhou, Wei Lu, Min Lin. (2024)<br><strong>Sailor: Open Language Models for South-East Asia</strong><br><button class=copy-to-clipboard title="Sailor: Open Language Models for South-East Asia" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Common-sense Reasoning, Question Answering, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03608v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03608v1.pdf filename=2404.03608v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Sailor, a family of open language models ranging from 0.5B to 7B parameters, tailored for South-East Asian (SEA) languages. These models are continually pre-trained from Qwen1.5, a great language model for multilingual use cases. From Qwen1.5, Sailor models accept 200B to 400B tokens, primarily covering the languages of English, Chinese, Vietnamese, Thai, Indonesian, Malay, and Lao. The training leverages several techniques, including BPE dropout for improving the model robustness, aggressive data cleaning and deduplication, and small proxy models to optimize data mixture. Experimental results on four typical tasks indicate that Sailor models demonstrate strong performance across different <b>benchmarks,</b> including <b>commonsense</b> <b>reasoning,</b> <b>question</b> <b>answering,</b> reading comprehension and examination. Embracing the open-source spirit, we share our insights through this report to spark a wider interest in developing <b>large</b> <b>language</b> <b>models</b> for multilingual use cases.</p></p class="citation"></blockquote><h3 id=2045--20237-okay-lets-do-this-modeling-event-coreference-with-generated-rationales-and-knowledge-distillation-abhijnan-nath-et-al-2024>(20/45 | 20/237) Okay, Let&rsquo;s Do This! Modeling Event Coreference with Generated Rationales and Knowledge Distillation (Abhijnan Nath et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhijnan Nath, Shadi Manafi, Avyakta Chelle, Nikhil Krishnaswamy. (2024)<br><strong>Okay, Let&rsquo;s Do This! Modeling Event Coreference with Generated Rationales and Knowledge Distillation</strong><br><button class=copy-to-clipboard title="Okay, Let's Do This! Modeling Event Coreference with Generated Rationales and Knowledge Distillation" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Clustering, Knowledge Distillation, Knowledge Distillation, Coreference Resolution, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03196v1.pdf filename=2404.03196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In NLP, Event <b>Coreference</b> <b>Resolution</b> (ECR) is the task of connecting event clusters that refer to the same underlying real-life event, usually via neural systems. In this work, we investigate using abductive free-text rationales (FTRs) generated by modern autoregressive <b>LLMs</b> as distant supervision of smaller student models for cross-document <b>coreference</b> <b>(CDCR)</b> of events. We implement novel rationale-oriented event <b>clustering</b> and <b>knowledge</b> <b>distillation</b> methods for event <b>coreference</b> <b>scoring</b> that leverage enriched information from the FTRs for improved CDCR without additional annotation or expensive document <b>clustering.</b> Our model using <b>coreference</b> <b>specific</b> <b>knowledge</b> <b>distillation</b> achieves SOTA B3 F1 on the ECB+ and GVC corpora and we establish a new baseline on the AIDA Phase 1 corpus. Our code can be found at <a href=https://github.com/csu-signal/llama_cdcr>https://github.com/csu-signal/llama_cdcr</a></p></p class="citation"></blockquote><h3 id=2145--21237-personalized-llm-response-generation-with-parameterized-memory-injection-kai-zhang-et-al-2024>(21/45 | 21/237) Personalized LLM Response Generation with Parameterized Memory Injection (Kai Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Zhang, Lizhi Qing, Yangyang Kang, Xiaozhong Liu. (2024)<br><strong>Personalized LLM Response Generation with Parameterized Memory Injection</strong><br><button class=copy-to-clipboard title="Personalized LLM Response Generation with Parameterized Memory Injection" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03565v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03565v1.pdf filename=2404.03565v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have exhibited remarkable proficiency in comprehending and generating natural language. On the other hand, personalized <b>LLM</b> response generation holds the potential to offer substantial benefits for individuals in critical areas such as medical. Existing research has explored memory-augmented methods to <b>prompt</b> the <b>LLM</b> with pre-stored user-specific knowledge for personalized response generation in terms of new queries. We contend that such paradigm is unable to perceive fine-granularity information. In this study, we propose a novel \textbf{M}emory-\textbf{i}njected approach using parameter-efficient <b>fine-tuning</b> (PEFT) and along with a Bayesian Optimisation searching strategy to achieve \textbf{L}LM \textbf{P}ersonalization(\textbf{MiLP}).</p></p class="citation"></blockquote><h3 id=2245--22237-learn-when-not-to-trust-language-models-a-privacy-centric-adaptive-model-aware-approach-chengkai-huang-et-al-2024>(22/45 | 22/237) Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive Model-Aware Approach (Chengkai Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengkai Huang, Rui Wang, Kaige Xie, Tong Yu, Lina Yao. (2024)<br><strong>Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive Model-Aware Approach</strong><br><button class=copy-to-clipboard title="Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive Model-Aware Approach" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Continual Learning, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03514v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03514v1.pdf filename=2404.03514v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Retrieval-augmented <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been remarkably competent in various NLP tasks. Despite their great success, the knowledge provided by the retrieval process is not always useful for improving the model prediction, since in some samples <b>LLMs</b> may already be quite knowledgeable and thus be able to answer the question correctly without retrieval. Aiming to save the cost of retrieval, previous work has proposed to determine when to do/skip the retrieval in a data-aware manner by analyzing the <b>LLMs&rsquo;</b> pretraining data. However, these data-aware methods pose privacy risks and memory limitations, especially when requiring access to sensitive or extensive pretraining data. Moreover, these methods offer limited adaptability under <b>fine-tuning</b> or <b>continual</b> <b>learning</b> settings. We hypothesize that token embeddings are able to capture the model&rsquo;s intrinsic knowledge, which offers a safer and more straightforward way to judge the need for retrieval without the privacy risks associated with accessing pre-training data. Moreover, it alleviates the need to retain all the data utilized during model pre-training, necessitating only the upkeep of the token embeddings. Extensive experiments and in-depth analyses demonstrate the superiority of our model-aware approach.</p></p class="citation"></blockquote><h3 id=2345--23237-generative-ai-and-teachers----for-us-or-against-us-a-case-study-jenny-pettersson-et-al-2024>(23/45 | 23/237) Generative AI and Teachers &ndash; For Us or Against Us? A Case Study (Jenny Pettersson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jenny Pettersson, Elias Hult, Tim Eriksson, Tosin Adewumi. (2024)<br><strong>Generative AI and Teachers &ndash; For Us or Against Us? A Case Study</strong><br><button class=copy-to-clipboard title="Generative AI and Teachers -- For Us or Against Us? A Case Study" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Generative AI, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03486v1.pdf filename=2404.03486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present insightful results of a survey on the adoption of <b>generative</b> <b>artificial</b> intelligence (GenAI) by university teachers in their teaching activities. The transformation of education by GenAI, particularly <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> has been presenting both opportunities and challenges, including cheating by students. We prepared the online survey according to best practices and the questions were created by the authors, who have pedagogy experience. The survey contained 12 questions and a pilot study was first conducted. The survey was then sent to all teachers in multiple departments across different campuses of the university of interest in Sweden: Lule{\aa} University of Technology. The survey was available in both Swedish and English. The results show that 35 teachers (more than half) use GenAI out of 67 respondents. Preparation is the teaching activity with the most frequency that GenAI is used for and <b>ChatGPT</b> is the most commonly used GenAI. 59% say it has impacted their teaching, however, 55% say there should be legislation around the use of GenAI, especially as inaccuracies and cheating are the biggest concerns.</p></p class="citation"></blockquote><h3 id=2445--24237-concept----an-evaluation-protocol-on-conversation-recommender-systems-with-system--and-user-centric-factors-chen-huang-et-al-2024>(24/45 | 24/237) Concept &ndash; An Evaluation Protocol on Conversation Recommender Systems with System- and User-centric Factors (Chen Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Huang, Peixin Qin, Yang Deng, Wenqiang Lei, Jiancheng Lv, Tat-Seng Chua. (2024)<br><strong>Concept &ndash; An Evaluation Protocol on Conversation Recommender Systems with System- and User-centric Factors</strong><br><button class=copy-to-clipboard title="Concept -- An Evaluation Protocol on Conversation Recommender Systems with System- and User-centric Factors" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Recommendation, Recommender System, ChatGPT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03304v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03304v1.pdf filename=2404.03304v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The conversational <b>recommendation</b> system (CRS) has been criticized regarding its user experience in real-world scenarios, despite recent significant progress achieved in academia. Existing evaluation protocols for CRS may prioritize system-centric factors such as effectiveness and fluency in conversation while neglecting user-centric aspects. Thus, we propose a new and inclusive evaluation protocol, Concept, which integrates both system- and user-centric factors. We conceptualise three key characteristics in representing such factors and further divide them into six primary abilities. To implement Concept, we adopt a <b>LLM-based</b> user simulator and evaluator with scoring rubrics that are tailored for each primary ability. Our protocol, Concept, serves a dual purpose. First, it provides an overview of the pros and cons in current CRS models. Second, it pinpoints the problem of low usability in the &ldquo;omnipotent&rdquo; <b>ChatGPT</b> and offers a comprehensive reference guide for evaluating CRS, thereby setting the foundation for CRS improvement.</p></p class="citation"></blockquote><h3 id=2545--25237-uncertainty-in-language-models-assessment-through-rank-calibration-xinmeng-huang-et-al-2024>(25/45 | 25/237) Uncertainty in Language Models: Assessment through Rank-Calibration (Xinmeng Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinmeng Huang, Shuo Li, Mengxin Yu, Matteo Sesia, Hamed Hassani, Insup Lee, Osbert Bastani, Edgar Dobriban. (2024)<br><strong>Uncertainty in Language Models: Assessment through Rank-Calibration</strong><br><button class=copy-to-clipboard title="Uncertainty in Language Models: Assessment through Rank-Calibration" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL, stat-ML<br>Keyword Score: 40<br>Keywords: Language Generation, Natural Language Generation, Prompt, Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03163v1.pdf filename=2404.03163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Language</b> <b>Models</b> (LMs) have shown promising performance in <b>natural</b> <b>language</b> <b>generation.</b> However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via <b>prompting,</b> many uncertainty measures ($e.g.$, semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges ($e.g.$, $[0,\infty)$ or $[0,1]$). In this work, we address this issue by developing a novel and practical framework, termed $Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score ($e.g.$, <b>ROUGE</b> or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.</p></p class="citation"></blockquote><h3 id=2645--26237-scaffolding-language-learning-via-multi-modal-tutoring-systems-with-pedagogical-instructions-zhengyuan-liu-et-al-2024>(26/45 | 26/237) Scaffolding Language Learning via Multi-modal Tutoring Systems with Pedagogical Instructions (Zhengyuan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengyuan Liu, Stella Xin Yin, Carolyn Lee, Nancy F. Chen. (2024)<br><strong>Scaffolding Language Learning via Multi-modal Tutoring Systems with Pedagogical Instructions</strong><br><button class=copy-to-clipboard title="Scaffolding Language Learning via Multi-modal Tutoring Systems with Pedagogical Instructions" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 36<br>Keywords: Benchmarking, Multi-modal, GPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03429v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03429v1.pdf filename=2404.03429v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Intelligent tutoring systems (ITSs) that imitate human tutors and aim to provide immediate and customized instructions or feedback to learners have shown their effectiveness in education. With the emergence of generative artificial intelligence, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> further entitle the systems to complex and coherent conversational interactions. These systems would be of great help in language education as it involves developing skills in communication, which, however, drew relatively less attention. Additionally, due to the complicated cognitive development at younger ages, more endeavors are needed for practical uses. Scaffolding refers to a teaching technique where teachers provide support and guidance to students for learning and developing new concepts or skills. It is an effective way to support diverse learning needs, goals, processes, and outcomes. In this work, we investigate how pedagogical instructions facilitate the scaffolding in ITSs, by conducting a case study on guiding children to describe images for language learning. We construct different types of scaffolding tutoring systems grounded in four fundamental learning theories: knowledge construction, inquiry-based learning, dialogic teaching, and zone of proximal development. For qualitative and quantitative analyses, we build and refine a seven-dimension rubric to evaluate the scaffolding process. In our experiment on <b>GPT-4V,</b> we observe that <b>LLMs</b> demonstrate strong potential to follow pedagogical instructions and achieve self-paced learning in different student groups. Moreover, we extend our evaluation framework from a manual to an automated approach, paving the way to <b>benchmark</b> various conversational tutoring systems.</p></p class="citation"></blockquote><h3 id=2745--27237-training-llms-over-neurally-compressed-text-brian-lester-et-al-2024>(27/45 | 27/237) Training LLMs over Neurally Compressed Text (Brian Lester et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah Constant. (2024)<br><strong>Training LLMs over Neurally Compressed Text</strong><br><button class=copy-to-clipboard title="Training LLMs over Neurally Compressed Text" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03626v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03626v1.pdf filename=2404.03626v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we explore the idea of training <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train <b>LLMs</b> directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text na"ively compressed via Arithmetic Coding is not readily learnable by <b>LLMs.</b> To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on <b>perplexity</b> and inference speed <b>benchmarks.</b> While our method delivers worse <b>perplexity</b> than subword tokenizers for models trained with the same parameter count, it has the benefit of shorter sequence lengths. Shorter sequence lengths require fewer autoregressive generation steps, and reduce latency. Finally, we provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers.</p></p class="citation"></blockquote><h3 id=2845--28237-select-and-summarize-scene-saliency-for-movie-script-summarization-rohit-saxena-et-al-2024>(28/45 | 28/237) Select and Summarize: Scene Saliency for Movie Script Summarization (Rohit Saxena et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohit Saxena, Frank Keller. (2024)<br><strong>Select and Summarize: Scene Saliency for Movie Script Summarization</strong><br><button class=copy-to-clipboard title="Select and Summarize: Scene Saliency for Movie Script Summarization" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Question Answering, Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03561v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03561v1.pdf filename=2404.03561v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Abstractive <b>summarization</b> for long-form narrative texts such as movie scripts is challenging due to the computational and memory constraints of current language models. A movie script typically comprises a large number of scenes; however, only a fraction of these scenes are salient, i.e., important for understanding the overall narrative. The salience of a scene can be operationalized by considering it as salient if it is mentioned in the summary. Automatically identifying salient scenes is difficult due to the lack of suitable datasets. In this work, we introduce a scene saliency dataset that consists of human-annotated salient scenes for 100 movies. We propose a two-stage abstractive <b>summarization</b> approach which first identifies the salient scenes in script and then generates a summary using only those scenes. Using <b>QA-based</b> evaluation, we show that our model outperforms previous state-of-the-art <b>summarization</b> methods and reflects the information content of a movie more accurately than a model that takes the whole movie script as input.</p></p class="citation"></blockquote><h3 id=2945--29237-reevaluating-bias-detection-in-language-models-the-role-of-implicit-norm-farnaz-kohankhaki-et-al-2024>(29/45 | 29/237) Reevaluating Bias Detection in Language Models: The Role of Implicit Norm (Farnaz Kohankhaki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Farnaz Kohankhaki, Jacob-Junqi Tian, David Emerson, Laleh Seyyed-Kalantari, Faiza Khan Khattak. (2024)<br><strong>Reevaluating Bias Detection in Language Models: The Role of Implicit Norm</strong><br><button class=copy-to-clipboard title="Reevaluating Bias Detection in Language Models: The Role of Implicit Norm" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Sentiment Analysis, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03471v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03471v1.pdf filename=2404.03471v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> trained on vast datasets, can carry biases that manifest in various forms, from overt discrimination to implicit stereotypes. One facet of bias is performance disparities in <b>LLMs,</b> often harming underprivileged groups, such as racial minorities. A common approach to quantifying bias is to use template-based bias probes, which explicitly state group membership (e.g. White) and evaluate if the outcome of a task, <b>sentiment</b> <b>analysis</b> for instance, is invariant to the change of group membership (e.g. change White race to Black). This approach is widely used in bias quantification. However, in this work, we find evidence of an unexpectedly overlooked consequence of using template-based probes for <b>LLM</b> bias quantification. We find that in doing so, text examples associated with White ethnicities appear to be classified as exhibiting negative <b>sentiment</b> <b>at</b> elevated rates. We hypothesize that the scenario arises artificially through a mismatch between the pre-training text of <b>LLMs</b> and the templates used to measure bias through reporting bias, unstated norms that imply group membership without explicit statement. Our finding highlights the potential misleading impact of varying group membership through explicit mention in bias quantification</p></p class="citation"></blockquote><h3 id=3045--30237-learning-to-plan-and-generate-text-with-citations-constanza-fierro-et-al-2024>(30/45 | 30/237) Learning to Plan and Generate Text with Citations (Constanza Fierro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Constanza Fierro, Reinald Kim Amplayo, Fantine Huot, Nicola De Cao, Joshua Maynez, Shashi Narayan, Mirella Lapata. (2024)<br><strong>Learning to Plan and Generate Text with Citations</strong><br><button class=copy-to-clipboard title="Learning to Plan and Generate Text with Citations" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Grounding, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03381v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03381v1.pdf filename=2404.03381v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing demand for the deployment of <b>LLMs</b> in information-seeking scenarios has spurred efforts in creating verifiable systems, which generate responses to queries along with supporting evidence. In this paper, we explore the attribution capabilities of plan-based models which have been recently shown to improve the faithfulness, <b>grounding,</b> and controllability of generated text. We conceptualize plans as a sequence of <b>questions</b> <b>which</b> serve as blueprints of the generated content and its organization. We propose two attribution models that utilize different variants of blueprints, an abstractive model where <b>questions</b> <b>are</b> generated from scratch, and an extractive model where <b>questions</b> <b>are</b> copied from the input. Experiments on long-form <b>question-answering</b> <b>show</b> that planning consistently improves attribution quality. Moreover, the citations generated by blueprint models are more accurate compared to those obtained from <b>LLM-based</b> pipelines lacking a planning component.</p></p class="citation"></blockquote><h3 id=3145--31237-probing-large-language-models-for-scalar-adjective-lexical-semantics-and-scalar-diversity-pragmatics-fangru-lin-et-al-2024>(31/45 | 31/237) Probing Large Language Models for Scalar Adjective Lexical Semantics and Scalar Diversity Pragmatics (Fangru Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangru Lin, Daniel Altshuler, Janet B. Pierrehumbert. (2024)<br><strong>Probing Large Language Models for Scalar Adjective Lexical Semantics and Scalar Diversity Pragmatics</strong><br><button class=copy-to-clipboard title="Probing Large Language Models for Scalar Adjective Lexical Semantics and Scalar Diversity Pragmatics" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: GPT, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03301v1.pdf filename=2404.03301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scalar adjectives pertain to various domain scales and vary in intensity within each scale (e.g. certain is more intense than likely on the likelihood scale). Scalar implicatures arise from the consideration of alternative statements which could have been made. They can be triggered by scalar adjectives and require listeners to reason pragmatically about them. Some scalar adjectives are more likely to trigger scalar implicatures than others. This phenomenon is referred to as scalar diversity. In this study, we probe different families of <b>Large</b> <b>Language</b> <b>Models</b> such as <b>GPT-4</b> for their knowledge of the lexical semantics of scalar adjectives and one specific aspect of their pragmatics, namely scalar diversity. We find that they encode rich lexical-semantic information about scalar adjectives. However, the rich lexical-semantic knowledge does not entail a good understanding of scalar diversity. We also compare current models of different sizes and complexities and find that larger models are not always better. Finally, we explain our probing results by leveraging linguistic intuitions and model training objectives.</p></p class="citation"></blockquote><h3 id=3245--32237-robust-pronoun-use-fidelity-with-english-llms-are-they-reasoning-repeating-or-just-biased-vagrant-gautam-et-al-2024>(32/45 | 32/237) Robust Pronoun Use Fidelity with English LLMs: Are they Reasoning, Repeating, or Just Biased? (Vagrant Gautam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vagrant Gautam, Eileen Bingert, Dawei Zhu, Anne Lauscher, Dietrich Klakow. (2024)<br><strong>Robust Pronoun Use Fidelity with English LLMs: Are they Reasoning, Repeating, or Just Biased?</strong><br><button class=copy-to-clipboard title="Robust Pronoun Use Fidelity with English LLMs: Are they Reasoning, Repeating, or Just Biased?" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03134v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03134v1.pdf filename=2404.03134v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robust, faithful and harm-free pronoun use for individuals is an important goal for language models as their use increases, but prior work tends to study only one or two of these components at a time. To measure progress towards the combined goal, we introduce the task of pronoun use fidelity: given a context introducing a co-referring entity and pronoun, the task is to reuse the correct pronoun later, independent of potential distractors. We present a carefully-designed dataset of over 5 million instances to evaluate pronoun use fidelity in English, and we use it to evaluate 37 popular <b>large</b> <b>language</b> <b>models</b> across architectures (encoder-only, decoder-only and encoder-decoder) and scales (11M-70B parameters). We find that while models can mostly faithfully reuse previously-specified pronouns in the presence of no distractors, they are significantly worse at processing she/her/her, singular they and neopronouns. Additionally, models are not robustly faithful to pronouns, as they are easily distracted. With even one additional sentence containing a distractor pronoun, accuracy drops on average by 34%. With 5 distractor sentences, accuracy drops by 52% for decoder-only models and 13% for encoder-only models. We show that widely-used <b>large</b> <b>language</b> <b>models</b> are still brittle, with <b>large</b> <b>gaps</b> <b>in</b> <b>reasoning</b> and in processing different pronouns in a setting that is very simple for humans, and we encourage researchers in bias and <b>reasoning</b> to bridge them.</p></p class="citation"></blockquote><h3 id=3345--33237-a-comparative-analysis-of-word-level-metric-differential-privacy-benchmarking-the-privacy-utility-trade-off-stephen-meisenbacher-et-al-2024>(33/45 | 33/237) A Comparative Analysis of Word-Level Metric Differential Privacy: Benchmarking The Privacy-Utility Trade-off (Stephen Meisenbacher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephen Meisenbacher, Nihildev Nandakumar, Alexandra Klymenko, Florian Matthes. (2024)<br><strong>A Comparative Analysis of Word-Level Metric Differential Privacy: Benchmarking The Privacy-Utility Trade-off</strong><br><button class=copy-to-clipboard title="A Comparative Analysis of Word-Level Metric Differential Privacy: Benchmarking The Privacy-Utility Trade-off" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Word Embedding, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03324v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03324v1.pdf filename=2404.03324v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The application of <b>Differential</b> <b>Privacy</b> to Natural Language Processing techniques has emerged in relevance in recent years, with an increasing number of studies published in established NLP outlets. In particular, the adaptation of <b>Differential</b> <b>Privacy</b> for use in NLP tasks has first focused on the $\textit{word-level}$, where calibrated noise is added to <b>word</b> <b>embedding</b> vectors to achieve &ldquo;noisy&rdquo; representations. To this end, several implementations have appeared in the literature, each presenting an alternative method of achieving <b>word-level</b> <b>Differential</b> <b>Privacy.</b> Although each of these includes its own evaluation, no comparative analysis has been performed to investigate the performance of such methods relative to each other. In this work, we conduct such an analysis, comparing seven different algorithms on two NLP tasks with varying hyperparameters, including the $\textit{epsilon ($\varepsilon$)}$ parameter, or privacy budget. In addition, we provide an in-depth analysis of the results with a focus on the privacy-utility trade-off, as well as open-source our implementation code for further reproduction. As a result of our analysis, we give insight into the benefits and challenges of <b>word-level</b> <b>Differential</b> <b>Privacy,</b> and accordingly, we suggest concrete steps forward for the research field.</p></p class="citation"></blockquote><h3 id=3445--34237-probelm-plausibility-ranking-evaluation-for-language-models-zhangdie-yuan-et-al-2024>(34/45 | 34/237) PRobELM: Plausibility Ranking Evaluation for Language Models (Zhangdie Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhangdie Yuan, Chenxi Whitehouse, Eric Chamoun, Rami Aly, Andreas Vlachos. (2024)<br><strong>PRobELM: Plausibility Ranking Evaluation for Language Models</strong><br><button class=copy-to-clipboard title="PRobELM: Plausibility Ranking Evaluation for Language Models" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Question Answering, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03818v1.pdf filename=2404.03818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces PRobELM (Plausibility Ranking Evaluation for Language Models), a <b>benchmark</b> designed to assess language models&rsquo; ability to discern more plausible from less plausible scenarios through their parametric knowledge. While <b>benchmarks</b> such as TruthfulQA emphasise factual accuracy or truthfulness, and others such as COPA explore plausible scenarios without explicitly incorporating world knowledge, PRobELM seeks to bridge this gap by evaluating models&rsquo; capabilities to prioritise plausible scenarios that leverage world knowledge over less plausible alternatives. This design allows us to assess the potential of language models for downstream use cases such as literature-based discovery where the focus is on identifying information that is likely but not yet known. Our <b>benchmark</b> is constructed from a dataset curated from Wikidata edit histories, tailored to align the temporal bounds of the training data for the evaluated models. PRobELM facilitates the evaluation of language models across multiple <b>prompting</b> types, including statement, text completion, and <b>question-answering.</b> <b>Experiments</b> with 10 models of various sizes and architectures on the relationship between model scales, training recency, and plausibility performance, reveal that factual accuracy does not directly correlate with plausibility performance and that up-to-date training data enhances plausibility assessment across different model architectures.</p></p class="citation"></blockquote><h3 id=3545--35237-towards-pareto-optimal-throughput-in-small-language-model-serving-pol-g-recasens-et-al-2024>(35/45 | 35/237) Towards Pareto Optimal Throughput in Small Language Model Serving (Pol G. Recasens et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pol G. Recasens, Yue Zhu, Chen Wang, Eun Kyung Lee, Olivier Tardieu, Alaa Youssef, Jordi Torres, Josep Ll. Berral. (2024)<br><strong>Towards Pareto Optimal Throughput in Small Language Model Serving</strong><br><button class=copy-to-clipboard title="Towards Pareto Optimal Throughput in Small Language Model Serving" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03353v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03353v1.pdf filename=2404.03353v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have revolutionized the state-of-the-art of many different natural language processing tasks. Although serving <b>LLMs</b> is computationally and memory demanding, the rise of Small Language Models (SLMs) offers new opportunities for resource-constrained users, who now are able to serve small models with cutting-edge performance. In this paper, we present a set of experiments designed to <b>benchmark</b> SLM inference at performance and energy levels. Our analysis provides a new perspective in serving, highlighting that the small memory footprint of SLMs allows for reaching the Pareto-optimal throughput within the resource capacity of a single accelerator. In this regard, we present an initial set of findings demonstrating how model replication can effectively improve resource utilization for serving SLMs.</p></p class="citation"></blockquote><h3 id=3645--36237-locating-and-editing-factual-associations-in-mamba-arnab-sen-sharma-et-al-2024>(36/45 | 36/237) Locating and Editing Factual Associations in Mamba (Arnab Sen Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arnab Sen Sharma, David Atkinson, David Bau. (2024)<br><strong>Locating and Editing Factual Associations in Mamba</strong><br><button class=copy-to-clipboard title="Locating and Editing Factual Associations in Mamba" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03646v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03646v1.pdf filename=2404.03646v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the mechanisms of factual recall in the Mamba state space model. Our work is inspired by previous findings in autoregressive <b>transformer</b> language models suggesting that their knowledge recall is localized to particular modules at specific token locations; we therefore ask whether factual recall in Mamba can be similarly localized. To investigate this, we conduct four lines of experiments on Mamba. First, we apply causal tracing or interchange interventions to localize key components inside Mamba that are responsible for recalling facts, revealing that specific components within middle layers show strong causal effects at the last token of the subject, while the causal effect of intervening on later layers is most pronounced at the last token of the <b>prompt,</b> matching previous findings on autoregressive <b>transformers.</b> Second, we show that rank-one model editing methods can successfully insert facts at specific locations, again resembling findings on <b>transformer</b> models. Third, we examine the linearity of Mamba&rsquo;s representations of factual relations. Finally we adapt attention-knockout techniques to Mamba to dissect information flow during factual recall. We compare Mamba directly to a similar-sized <b>transformer</b> and conclude that despite significant differences in architectural approach, when it comes to factual recall, the two architectures share many similarities.</p></p class="citation"></blockquote><h3 id=3745--37237-easse-de-easier-automatic-sentence-simplification-evaluation-for-german-regina-stodden-2024>(37/45 | 37/237) EASSE-DE: Easier Automatic Sentence Simplification Evaluation for German (Regina Stodden, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Regina Stodden. (2024)<br><strong>EASSE-DE: Easier Automatic Sentence Simplification Evaluation for German</strong><br><button class=copy-to-clipboard title="EASSE-DE: Easier Automatic Sentence Simplification Evaluation for German" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Recommendation, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03563v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03563v2.pdf filename=2404.03563v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we propose EASSE-multi, a framework for easier automatic sentence evaluation for languages other than English. Compared to the original EASSE framework, EASSE-multi does not focus only on English. It contains tokenizers and versions of text simplification evaluation metrics which are suitable for multiple languages. In this paper, we exemplify the usage of EASSE-multi for German TS, resulting in EASSE-DE. Further, we compare text simplification results when evaluating with different language or <b>tokenization</b> settings of the metrics. Based on this, we formulate <b>recommendations</b> on how to make the evaluation of (German) TS models more transparent and better comparable. The code of EASSE-multi and its German specialisation (EASSE-DE) can be found at <a href=https://github.com/rstodden/easse-de>https://github.com/rstodden/easse-de</a>.</p></p class="citation"></blockquote><h3 id=3845--38237-edisum-summarizing-and-explaining-wikipedia-edits-at-scale-marija-šakota-et-al-2024>(38/45 | 38/237) Edisum: Summarizing and Explaining Wikipedia Edits at Scale (Marija Šakota et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marija Šakota, Isaac Johnson, Guosheng Feng, Robert West. (2024)<br><strong>Edisum: Summarizing and Explaining Wikipedia Edits at Scale</strong><br><button class=copy-to-clipboard title="Edisum: Summarizing and Explaining Wikipedia Edits at Scale" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03428v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03428v1.pdf filename=2404.03428v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An edit summary is a succinct comment written by a Wikipedia editor explaining the nature of, and reasons for, an edit to a Wikipedia page. Edit summaries are crucial for maintaining the encyclopedia: they are the first thing seen by content moderators and help them decide whether to accept or reject an edit. Additionally, edit summaries constitute a valuable data source for researchers. Unfortunately, as we show, for many edits, summaries are either missing or incomplete. To overcome this problem and help editors write useful edit summaries, we propose a model for recommending edit summaries generated by a language model trained to produce good edit summaries given the representation of an edit diff. This is a challenging task for multiple reasons, including mixed-quality training data, the need to understand not only what was changed in the article but also why it was changed, and efficiency requirements imposed by the scale of Wikipedia. We address these challenges by curating a mix of human and synthetically generated training data and <b>fine-tuning</b> a generative language model sufficiently small to be used on Wikipedia at scale. Our model performs on par with human editors. Commercial <b>large</b> <b>language</b> <b>models</b> are able to solve this task better than human editors, but would be too expensive to run on Wikipedia at scale. More broadly, this paper showcases how language modeling technology can be used to support humans in maintaining one of the largest and most visible projects on the Web.</p></p class="citation"></blockquote><h3 id=3945--39237-how-easily-do-irrelevant-inputs-skew-the-responses-of-large-language-models-siye-wu-et-al-2024>(39/45 | 39/237) How Easily do Irrelevant Inputs Skew the Responses of Large Language Models? (Siye Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, Yanghua Xiao. (2024)<br><strong>How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?</strong><br><button class=copy-to-clipboard title="How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03302v1.pdf filename=2404.03302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>By leveraging the retrieval of information from external knowledge databases, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> exhibit enhanced capabilities for accomplishing many knowledge-intensive tasks. However, due to the inherent flaws of current retrieval systems, there might exist irrelevant information within those retrieving top-ranked passages. In this work, we present a comprehensive investigation into the robustness of <b>LLMs</b> to different types of irrelevant information under various conditions. We initially introduce a framework to construct high-quality irrelevant information that ranges from semantically unrelated, partially related, and related to questions. Furthermore, our analysis demonstrates that the constructed irrelevant information not only scores highly on similarity metrics, being highly retrieved by existing systems, but also bears semantic connections to the context. Our investigation reveals that current <b>LLMs</b> still face challenges in discriminating highly semantically related information and can be easily distracted by these irrelevant yet misleading contents. Besides, we also find that current solutions for handling irrelevant information have limitations in improving the robustness of <b>LLMs</b> to such distractions. Resources are available at <a href=https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information>https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information</a>.</p></p class="citation"></blockquote><h3 id=4045--40237-the-death-of-feature-engineering-bert-with-linguistic-features-on-squad-20-jiawei-li-et-al-2024>(40/45 | 40/237) The Death of Feature Engineering? BERT with Linguistic Features on SQuAD 2.0 (Jiawei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Li, Yue Zhang. (2024)<br><strong>The Death of Feature Engineering? BERT with Linguistic Features on SQuAD 2.0</strong><br><button class=copy-to-clipboard title="The Death of Feature Engineering? BERT with Linguistic Features on SQuAD 2.0" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: BERT, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03184v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03184v1.pdf filename=2404.03184v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine reading comprehension is an essential natural language processing task, which takes into a pair of context and query and predicts the corresponding answer to query. In this project, we developed an end-to-end <b>question</b> <b>answering</b> model incorporating <b>BERT</b> and additional linguistic features. We conclude that the <b>BERT</b> base model will be improved by incorporating the features. The EM score and F1 score are improved 2.17 and 2.14 compared with BERT(base). Our best single model reaches EM score 76.55 and F1 score 79.97 in the hidden test set. Our error analysis also shows that the linguistic architecture can help model understand the context better in that it can locate answers that <b>BERT</b> only model predicted &ldquo;No Answer&rdquo; wrongly.</p></p class="citation"></blockquote><h3 id=4145--41237-from-news-to-summaries-building-a-hungarian-corpus-for-extractive-and-abstractive-summarization-botond-barta-et-al-2024>(41/45 | 41/237) From News to Summaries: Building a Hungarian Corpus for Extractive and Abstractive Summarization (Botond Barta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Botond Barta, Dorina Lakatos, Attila Nagy, Milán Konor Nyist, Judit Ács. (2024)<br><strong>From News to Summaries: Building a Hungarian Corpus for Extractive and Abstractive Summarization</strong><br><button class=copy-to-clipboard title="From News to Summaries: Building a Hungarian Corpus for Extractive and Abstractive Summarization" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03555v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03555v1.pdf filename=2404.03555v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training <b>summarization</b> models requires substantial amounts of training data. However for less resourceful languages like Hungarian, openly available models and datasets are notably scarce. To address this gap our paper introduces HunSum-2 an open-source Hungarian corpus suitable for training abstractive and extractive <b>summarization</b> models. The dataset is assembled from segments of the Common Crawl corpus undergoing thorough cleaning, preprocessing and deduplication. In addition to abstractive <b>summarization</b> we generate sentence-level labels for extractive <b>summarization</b> using sentence similarity. We train baseline models for both extractive and abstractive <b>summarization</b> using the collected dataset. To demonstrate the effectiveness of the trained models, we perform both quantitative and qualitative evaluation. Our dataset, models and code are publicly available, encouraging replication, further research, and real-world applications across various domains.</p></p class="citation"></blockquote><h3 id=4245--42237-evaluating-document-simplification-on-the-importance-of-separately-assessing-simplicity-and-meaning-preservation-liam-cripwell-et-al-2024>(42/45 | 42/237) Evaluating Document Simplification: On the Importance of Separately Assessing Simplicity and Meaning Preservation (Liam Cripwell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liam Cripwell, Joël Legrand, Claire Gardent. (2024)<br><strong>Evaluating Document Simplification: On the Importance of Separately Assessing Simplicity and Meaning Preservation</strong><br><button class=copy-to-clipboard title="Evaluating Document Simplification: On the Importance of Separately Assessing Simplicity and Meaning Preservation" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: BERTScore<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03278v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03278v1.pdf filename=2404.03278v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text simplification intends to make a text easier to read while preserving its core meaning. Intuitively and as shown in previous works, these two dimensions (simplification and meaning preservation) are often-times inversely correlated. An overly conservative text will fail to simplify sufficiently, whereas extreme simplification will degrade meaning preservation. Yet, popular evaluation metrics either aggregate meaning preservation and simplification into a single score (SARI, LENS), or target meaning preservation alone <b>(BERTScore,</b> QuestEval). Moreover, these metrics usually require a set of references and most previous work has only focused on sentence-level simplification. In this paper, we focus on the evaluation of document-level text simplification and compare existing models using distinct metrics for meaning preservation and simplification. We leverage existing metrics from similar tasks and introduce a reference-less metric variant for simplicity, showing that models are mostly biased towards either simplification or meaning preservation, seldom performing well on both dimensions. Making use of the fact that the metrics we use are all reference-less, we also investigate the performance of existing models when applied to unseen data (where reference simplifications are unavailable).</p></p class="citation"></blockquote><h3 id=4345--43237-knowledge-graph-representation-for-political-information-sources-tinatin-osmonova-et-al-2024>(43/45 | 43/237) Knowledge Graph Representation for Political Information Sources (Tinatin Osmonova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tinatin Osmonova, Alexey Tikhonov, Ivan P. Yamshchikov. (2024)<br><strong>Knowledge Graph Representation for Political Information Sources</strong><br><button class=copy-to-clipboard title="Knowledge Graph Representation for Political Information Sources" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SI, cs.CL<br>Keyword Score: 8<br>Keywords: Graph, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03437v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03437v1.pdf filename=2404.03437v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rise of computational social science, many scholars utilize data analysis and natural language processing tools to analyze social media, news articles, and other accessible data sources for examining political and social discourse. Particularly, the study of the emergence of echo-chambers due to the dissemination of specific information has become a topic of interest in mixed methods research areas. In this paper, we analyze data collected from two news portals, Breitbart News (BN) and New York Times (NYT) to prove the hypothesis that the formation of echo-chambers can be partially explained on the level of an individual information consumption rather than a collective topology of individuals&rsquo; social networks. Our research findings are presented through <b>knowledge</b> <b>graphs,</b> utilizing a dataset spanning 11.5 years gathered from BN and NYT media portals. We demonstrate that the application of <b>knowledge</b> <b>representation</b> techniques to the aforementioned news streams highlights, contrary to common assumptions, shows relative &ldquo;internal&rdquo; neutrality of both sources and polarizing attitude towards a small fraction of entities. Additionally, we argue that such characteristics in information sources lead to fundamental disparities in audience worldviews, potentially acting as a catalyst for the formation of echo-chambers.</p></p class="citation"></blockquote><h3 id=4445--44237-schroedingers-threshold-when-the-auc-doesnt-predict-accuracy-juri-opitz-2024>(44/45 | 44/237) Schroedinger&rsquo;s Threshold: When the AUC doesn&rsquo;t predict Accuracy (Juri Opitz, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juri Opitz. (2024)<br><strong>Schroedinger&rsquo;s Threshold: When the AUC doesn&rsquo;t predict Accuracy</strong><br><button class=copy-to-clipboard title="Schroedinger's Threshold: When the AUC doesn't predict Accuracy" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03344v1.pdf filename=2404.03344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Area Under Curve measure (AUC) seems apt to evaluate and compare diverse models, possibly without calibration. An important example of AUC application is the evaluation and <b>benchmarking</b> of models that predict faithfulness of generated text. But we show that the AUC yields an academic and optimistic notion of accuracy that can misalign with the actual accuracy observed in application, yielding significant changes in <b>benchmark</b> rankings. To paint a more realistic picture of downstream model performance (and prepare a model for actual application), we explore different calibration modes, testing calibration data and method.</p></p class="citation"></blockquote><h3 id=4545--45237-m3tcm-multi-modal-multi-task-context-model-for-utterance-classification-in-motivational-interviews-sayed-muddashir-hossain-et-al-2024>(45/45 | 45/237) M3TCM: Multi-modal Multi-task Context Model for Utterance Classification in Motivational Interviews (Sayed Muddashir Hossain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sayed Muddashir Hossain, Jan Alexandersson, Philipp Müller. (2024)<br><strong>M3TCM: Multi-modal Multi-task Context Model for Utterance Classification in Motivational Interviews</strong><br><button class=copy-to-clipboard title="M3TCM: Multi-modal Multi-task Context Model for Utterance Classification in Motivational Interviews" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03312v1.pdf filename=2404.03312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate utterance classification in motivational interviews is crucial to automatically understand the quality and dynamics of client-therapist interaction, and it can serve as a key input for systems mediating such interactions. Motivational interviews exhibit three important characteristics. First, there are two distinct roles, namely client and therapist. Second, they are often highly emotionally charged, which can be expressed both in text and in prosody. Finally, context is of central importance to classify any given utterance. Previous works did not adequately incorporate all of these characteristics into utterance classification approaches for mental health dialogues. In contrast, we present M3TCM, a <b>Multi-modal,</b> Multi-task Context Model for utterance classification. Our approach for the first time employs multi-task learning to effectively model both joint and individual components of therapist and client behaviour. Furthermore, M3TCM integrates information from the text and speech modality as well as the conversation context. With our novel approach, we outperform the state of the art for utterance classification on the recently introduced AnnoMI dataset with a relative improvement of 20% for the client- and by 15% for therapist utterance classification. In extensive ablation studies, we quantify the improvement resulting from each contribution.</p></p class="citation"></blockquote><h2 id=csir-8>cs.IR (8)</h2><h3 id=18--46237-a-comprehensive-survey-on-self-supervised-learning-for-recommendation-xubin-ren-et-al-2024>(1/8 | 46/237) A Comprehensive Survey on Self-Supervised Learning for Recommendation (Xubin Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xubin Ren, Wei Wei, Lianghao Xia, Chao Huang. (2024)<br><strong>A Comprehensive Survey on Self-Supervised Learning for Recommendation</strong><br><button class=copy-to-clipboard title="A Comprehensive Survey on Self-Supervised Learning for Recommendation" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 110<br>Keywords: Graph Neural Network, Adversarial Learning, Contrastive Learning, Recommendation, Recommender System, Self-supervised Learning, Self-supervised Learning, Supervised Learning, Supervised Learning, Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03354v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03354v1.pdf filename=2404.03354v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommender</b> <b>systems</b> play a crucial role in tackling the challenge of information overload by delivering personalized <b>recommendations</b> based on individual user preferences. Deep learning techniques, such as <b>RNNs,</b> <b>GNNs,</b> and <b>Transformer</b> architectures, have significantly propelled the advancement of <b>recommender</b> <b>systems</b> by enhancing their comprehension of user behaviors and preferences. However, <b>supervised</b> <b>learning</b> methods encounter challenges in real-life scenarios due to data sparsity, resulting in limitations in their ability to learn representations effectively. To address this, <b>self-supervised</b> <b>learning</b> (SSL) techniques have emerged as a solution, leveraging inherent data structures to generate supervision signals without relying solely on labeled data. By leveraging unlabeled data and extracting meaningful representations, <b>recommender</b> <b>systems</b> utilizing SSL can make accurate predictions and <b>recommendations</b> even when confronted with data sparsity. In this paper, we provide a comprehensive review of <b>self-supervised</b> <b>learning</b> frameworks designed for <b>recommender</b> <b>systems,</b> encompassing a thorough analysis of over 170 papers. We conduct an exploration of nine distinct scenarios, enabling a comprehensive understanding of SSL-enhanced <b>recommenders</b> <b>in</b> different contexts. For each domain, we elaborate on different <b>self-supervised</b> <b>learning</b> paradigms, namely <b>contrastive</b> <b>learning,</b> generative learning, and <b>adversarial</b> <b>learning,</b> so as to present technical details of how SSL enhances <b>recommender</b> <b>systems</b> in various contexts. We consistently maintain the related open-source materials at <a href=https://github.com/HKUDS/Awesome-SSLRec-Papers>https://github.com/HKUDS/Awesome-SSLRec-Papers</a>.</p></p class="citation"></blockquote><h3 id=28--47237-do-large-language-models-rank-fairly-an-empirical-study-on-the-fairness-of-llms-as-rankers-yuan-wang-et-al-2024>(2/8 | 47/237) Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers (Yuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Wang, Xuyang Wu, Hsin-Tai Wu, Zhiqiang Tao, Yi Fang. (2024)<br><strong>Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers</strong><br><button class=copy-to-clipboard title="Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 63<br>Keywords: Benchmarking, Fairness, GPT, Information Retrieval, Natural Language Understanding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03192v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03192v1.pdf filename=2404.03192v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in <b>information</b> <b>retrieval</b> has raised a critical reevaluation of <b>fairness</b> in the text-ranking models. <b>LLMs,</b> such as <b>GPT</b> models and Llama2, have shown effectiveness in <b>natural</b> <b>language</b> <b>understanding</b> tasks, and prior works (e.g., RankGPT) have also demonstrated that the <b>LLMs</b> exhibit better performance than the traditional ranking models in the ranking task. However, their <b>fairness</b> remains largely unexplored. This paper presents an empirical study evaluating these <b>LLMs</b> using the TREC Fair Ranking dataset, focusing on the representation of binary protected attributes such as gender and geographic location, which are historically underrepresented in search outcomes. Our analysis delves into how these <b>LLMs</b> handle queries and documents related to these attributes, aiming to uncover biases in their ranking algorithms. We assess <b>fairness</b> from both user and content perspectives, contributing an empirical <b>benchmark</b> for evaluating <b>LLMs</b> as the fair ranker.</p></p class="citation"></blockquote><h3 id=38--48237-understanding-language-modeling-paradigm-adaptations-in-recommender-systems-lessons-learned-and-open-challenges-lemei-zhang-et-al-2024>(3/8 | 48/237) Understanding Language Modeling Paradigm Adaptations in Recommender Systems: Lessons Learned and Open Challenges (Lemei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lemei Zhang, Peng Liu, Yashar Deldjoo, Yong Zheng, Jon Atle Gulla. (2024)<br><strong>Understanding Language Modeling Paradigm Adaptations in Recommender Systems: Lessons Learned and Open Challenges</strong><br><button class=copy-to-clipboard title="Understanding Language Modeling Paradigm Adaptations in Recommender Systems: Lessons Learned and Open Challenges" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 60<br>Keywords: Recommendation, Recommender System, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03788v1.pdf filename=2404.03788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emergence of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has achieved tremendous success in the field of Natural Language Processing owing to diverse training paradigms that empower <b>LLMs</b> to effectively capture intricate linguistic patterns and semantic representations. In particular, the recent &ldquo;pre-train, <b>prompt</b> and predict&rdquo; training paradigm has attracted significant attention as an approach for learning generalizable models with limited labeled data. In line with this advancement, these training paradigms have recently been adapted to the <b>recommendation</b> domain and are seen as a promising direction in both academia and industry. This half-day tutorial aims to provide a thorough understanding of extracting and transferring knowledge from pre-trained models learned through different training paradigms to improve <b>recommender</b> <b>systems</b> from various perspectives, such as generality, sparsity, effectiveness and trustworthiness. In this tutorial, we first introduce the basic concepts and a generic architecture of the language modeling paradigm for <b>recommendation</b> purposes. Then, we focus on recent advancements in adapting <b>LLM-related</b> training strategies and optimization objectives for different <b>recommendation</b> tasks. After that, we will systematically introduce ethical issues in <b>LLM-based</b> <b>recommender</b> <b>systems</b> and discuss possible approaches to assessing and mitigating them. We will also <b>summarize</b> the relevant datasets, evaluation metrics, and an empirical study on the <b>recommendation</b> performance of training paradigms. Finally, we will conclude the tutorial with a discussion of open challenges and future directions.</p></p class="citation"></blockquote><h3 id=48--49237-sequential-recommendation-for-optimizing-both-immediate-feedback-and-long-term-retention-ziru-liu-et-al-2024>(4/8 | 49/237) Sequential Recommendation for Optimizing Both Immediate Feedback and Long-term Retention (Ziru Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziru Liu, Shuchang Liu, Zijian Zhang, Qingpeng Cai, Xiangyu Zhao, Kesen Zhao, Lantao Hu, Peng Jiang, Kun Gai. (2024)<br><strong>Sequential Recommendation for Optimizing Both Immediate Feedback and Long-term Retention</strong><br><button class=copy-to-clipboard title="Sequential Recommendation for Optimizing Both Immediate Feedback and Long-term Retention" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 60<br>Keywords: Contrastive Learning, Recommendation, Recommender System, Reinforcement Learning, Transformer, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03637v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03637v1.pdf filename=2404.03637v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the landscape of <b>Recommender</b> <b>System</b> (RS) applications, <b>reinforcement</b> <b>learning</b> (RL) has recently emerged as a powerful tool, primarily due to its proficiency in optimizing long-term rewards. Nevertheless, it suffers from instability in the learning process, <b>stemming</b> from the intricate interactions among bootstrapping, off-policy training, and function approximation. Moreover, in multi-reward <b>recommendation</b> scenarios, designing a proper reward setting that reconciles the inner dynamics of various tasks is quite intricate. In response to these challenges, we introduce DT4IER, an advanced decision <b>transformer-based</b> <b>recommendation</b> model that is engineered to not only elevate the effectiveness of <b>recommendations</b> but also to achieve a harmonious balance between immediate user engagement and long-term retention. The DT4IER applies an innovative multi-reward design that adeptly balances short and long-term rewards with user-specific attributes, which serve to enhance the contextual richness of the reward sequence ensuring a more informed and personalized <b>recommendation</b> process. To enhance its predictive capabilities, DT4IER incorporates a high-dimensional encoder, skillfully designed to identify and leverage the intricate interrelations across diverse tasks. Furthermore, we integrate a <b>contrastive</b> <b>learning</b> approach within the action embedding predictions, a strategy that significantly boosts the model&rsquo;s overall performance. Experiments on three real-world datasets demonstrate the effectiveness of DT4IER against state-of-the-art Sequential <b>Recommender</b> <b>Systems</b> (SRSs) and Multi-Task Learning (MTL) models in terms of both prediction accuracy and effectiveness in specific tasks. The source code is accessible online to facilitate replication</p></p class="citation"></blockquote><h3 id=58--50237-a-directional-diffusion-graph-transformer-for-recommendation-zixuan-yi-et-al-2024>(5/8 | 50/237) A Directional Diffusion Graph Transformer for Recommendation (Zixuan Yi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zixuan Yi, Xi Wang, Iadh Ounis. (2024)<br><strong>A Directional Diffusion Graph Transformer for Recommendation</strong><br><button class=copy-to-clipboard title="A Directional Diffusion Graph Transformer for Recommendation" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 46<br>Keywords: Diffusion Model, Graph, Benchmarking, Recommendation, Recommender System, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03326v1.pdf filename=2404.03326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In real-world <b>recommender</b> <b>systems,</b> implicitly collected user feedback, while abundant, often includes noisy false-positive and false-negative interactions. The possible misinterpretations of the user-item interactions pose a significant challenge for traditional <b>graph</b> neural <b>recommenders.</b> <b>These</b> approaches aggregate the users&rsquo; or items&rsquo; neighbours based on implicit user-item interactions in order to accurately capture the users&rsquo; profiles. To account for and model possible noise in the users&rsquo; interactions in <b>graph</b> neural <b>recommenders,</b> <b>we</b> propose a novel <b>Diffusion</b> <b>Graph</b> <b>Transformer</b> (DiffGT) model for top-k <b>recommendation.</b> Our DiffGT model employs a <b>diffusion</b> <b>process,</b> which includes a forward phase for gradually introducing noise to implicit interactions, followed by a reverse process to iteratively refine the representations of the users&rsquo; hidden preferences (i.e., a denoising process). In our proposed approach, given the inherent anisotropic structure observed in the user-item interaction <b>graph,</b> we specifically use anisotropic and directional Gaussian noises in the forward <b>diffusion</b> <b>process.</b> Our approach differs from the sole use of isotropic Gaussian noises in existing <b>diffusion</b> <b>models.</b> In the reverse <b>diffusion</b> <b>process,</b> to reverse the effect of noise added earlier and recover the true users&rsquo; preferences, we integrate a <b>graph</b> <b>transformer</b> architecture with a linear attention module to denoise the noisy user/item embeddings in an effective and efficient manner. In addition, such a reverse <b>diffusion</b> <b>process</b> is further guided by personalised information (e.g., interacted items) to enable the accurate estimation of the users&rsquo; preferences on items. Our extensive experiments conclusively demonstrate the superiority of our proposed <b>graph</b> <b>diffusion</b> <b>model</b> over ten existing state-of-the-art approaches across three <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=68--51237-genqrensemble-zero-shot-llm-ensemble-prompting-for-generative-query-reformulation-kaustubh-dhole-et-al-2024>(6/8 | 51/237) GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation (Kaustubh Dhole et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaustubh Dhole, Eugene Agichtein. (2024)<br><strong>GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation</strong><br><button class=copy-to-clipboard title="GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-CL, cs-IR, cs.IR<br>Keyword Score: 43<br>Keywords: Benchmarking, Zero-shot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03746v1.pdf filename=2404.03746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Query Reformulation(QR) is a set of techniques used to transform a user&rsquo;s original search query to a text that better aligns with the user&rsquo;s intent and improves their search experience. Recently, <b>zero-shot</b> QR has been shown to be a promising approach due to its ability to exploit knowledge inherent in <b>large</b> <b>language</b> <b>models.</b> By taking inspiration from the success of ensemble <b>prompting</b> strategies which have benefited many tasks, we investigate if they can help improve query reformulation. In this context, we propose an ensemble based <b>prompting</b> technique, GenQREnsemble which leverages paraphrases of a <b>zero-shot</b> instruction to generate multiple sets of keywords ultimately improving retrieval performance. We further introduce its post-retrieval variant, GenQREnsembleRF to incorporate pseudo relevant feedback. On evaluations over four IR <b>benchmarks,</b> we find that GenQREnsemble generates better reformulations with relative nDCG@10 improvements up to 18% and MAP improvements upto 24% over the previous <b>zero-shot</b> state-of-art. On the MSMarco Passage Ranking task, GenQREnsembleRF shows relative gains of 5% MRR using pseudo-relevance feedback, and 9% nDCG@10 using relevant feedback documents.</p></p class="citation"></blockquote><h3 id=78--52237-does-knowledge-graph-really-matter-for-recommender-systems-haonan-zhang-et-al-2024>(7/8 | 52/237) Does Knowledge Graph Really Matter for Recommender Systems? (Haonan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haonan Zhang, Dongxia Wang, Zhu Sun, Yanhui Li, Youcheng Sun, Huizhi Liang, Wenhai Wang. (2024)<br><strong>Does Knowledge Graph Really Matter for Recommender Systems?</strong><br><button class=copy-to-clipboard title="Does Knowledge Graph Really Matter for Recommender Systems?" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs-LG, cs.IR<br>Keyword Score: 33<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03164v1.pdf filename=2404.03164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommender</b> <b>systems</b> (RSs) are designed to provide personalized <b>recommendations</b> to users. Recently, <b>knowledge</b> <b>graphs</b> <b>(KGs)</b> have been widely introduced in RSs to improve <b>recommendation</b> accuracy. In this study, however, we demonstrate that RSs do not necessarily perform worse even if the <b>KG</b> is downgraded to the user-item interaction <b>graph</b> only (or removed). We propose an evaluation framework KG4RecEval to systematically evaluate how much a <b>KG</b> contributes to the <b>recommendation</b> accuracy of a <b>KG-based</b> RS, using our defined metric KGER <b>(KG</b> utilization efficiency in <b>recommendation).</b> We consider the scenarios where <b>knowledge</b> <b>in</b> a <b>KG</b> gets completely removed, randomly distorted and decreased, and also where <b>recommendations</b> are for cold-start users. Our extensive experiments on four commonly used datasets and a number of state-of-the-art <b>KG-based</b> RSs reveal that: to remove, randomly distort or decrease <b>knowledge</b> <b>does</b> not necessarily decrease <b>recommendation</b> accuracy, even for cold-start users. These findings inspire us to rethink how to better utilize <b>knowledge</b> <b>from</b> existing <b>KGs,</b> whereby we discuss and provide insights into what characteristics of datasets and <b>KG-based</b> RSs may help improve <b>KG</b> utilization efficiency.</p></p class="citation"></blockquote><h3 id=88--53237-to-search-or-to-recommend-predicting-open-app-motivation-with-neural-hawkes-process-zhongxiang-sun-et-al-2024>(8/8 | 53/237) To Search or to Recommend: Predicting Open-App Motivation with Neural Hawkes Process (Zhongxiang Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongxiang Sun, Zihua Si, Xiao Zhang, Xiaoxue Zang, Yang Song, Hongteng Xu, Jun Xu. (2024)<br><strong>To Search or to Recommend: Predicting Open-App Motivation with Neural Hawkes Process</strong><br><button class=copy-to-clipboard title="To Search or to Recommend: Predicting Open-App Motivation with Neural Hawkes Process" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 23<br>Keywords: Benchmarking, Recommendation, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03267v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03267v1.pdf filename=2404.03267v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Incorporating Search and <b>Recommendation</b> (S&amp;R) services within a singular application is prevalent in online platforms, leading to a new task termed open-app motivation prediction, which aims to predict whether users initiate the application with the specific intent of information searching, or to explore recommended content for entertainment. Studies have shown that predicting users&rsquo; motivation to open an app can help to improve user engagement and enhance performance in various downstream tasks. However, accurately predicting open-app motivation is not trivial, as it is influenced by user-specific factors, search queries, clicked items, as well as their temporal occurrences. Furthermore, these activities occur sequentially and exhibit intricate temporal dependencies. Inspired by the success of the Neural Hawkes Process (NHP) in modeling temporal dependencies in sequences, this paper proposes a novel neural Hawkes process model to capture the temporal dependencies between historical user browsing and querying actions. The model, referred to as Neural Hawkes Process-based Open-App Motivation prediction model (NHP-OAM), employs a hierarchical <b>transformer</b> and a novel intensity function to encode multiple factors, and open-app motivation prediction layer to integrate time and user-specific information for predicting users&rsquo; open-app motivations. To demonstrate the superiority of our NHP-OAM model and construct a <b>benchmark</b> for the Open-App Motivation Prediction task, we not only extend the public S&amp;R dataset ZhihuRec but also construct a new real-world Open-App Motivation Dataset (OAMD). Experiments on these two datasets validate NHP-OAM&rsquo;s superiority over baseline models. Further downstream application experiments demonstrate NHP-OAM&rsquo;s effectiveness in predicting users&rsquo; Open-App Motivation, highlighting the immense application value of NHP-OAM.</p></p class="citation"></blockquote><h2 id=cslg-42>cs.LG (42)</h2><h3 id=142--54237-direct-nash-optimization-teaching-language-models-to-self-improve-with-general-preferences-corby-rosset-et-al-2024>(1/42 | 54/237) Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences (Corby Rosset et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, Tengyang Xie. (2024)<br><strong>Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences</strong><br><button class=copy-to-clipboard title="Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 100<br>Keywords: Contrastive Learning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, GPT, GPT-4, GPT-4 turbo, Mistral, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03715v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03715v1.pdf filename=2404.03715v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies post-training <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> using preference feedback from a powerful oracle to help a model iteratively improve over itself. The typical approach for post-training <b>LLMs</b> involves <b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF),</b> which traditionally separates reward learning and subsequent policy optimization. However, such a reward maximization approach is limited by the nature of &ldquo;point-wise&rdquo; rewards (such as Bradley-Terry model), which fails to express complex intransitive or cyclic preference relations. While advances on <b>RLHF</b> show reward learning and policy optimization can be merged into a single <b>contrastive</b> <b>objective</b> for stability, they yet still remain tethered to the reward maximization framework. Recently, a new wave of research sidesteps the reward maximization presumptions in favor of directly optimizing over &ldquo;pair-wise&rdquo; or general preferences. In this paper, we introduce Direct Nash Optimization (DNO), a provable and scalable algorithm that marries the simplicity and stability of <b>contrastive</b> <b>learning</b> with theoretical generality from optimizing general preferences. Because DNO is a batched on-policy algorithm using a regression-based objective, its implementation is straightforward and efficient. Moreover, DNO enjoys monotonic improvement across iterations that help it improve even over a strong teacher (such as <b>GPT-4).</b> <b>In</b> our experiments, a resulting 7B parameter Orca-2.5 model aligned by DNO achieves the state-of-the-art win-rate against <b>GPT-4-Turbo</b> <b>of</b> 33% on AlpacaEval 2.0 (even after controlling for response length), an absolute gain of 26% (7% to 33%) over the initializing model. It outperforms models with far more parameters, including <b>Mistral</b> <b>Large,</b> <b>Self-Rewarding</b> <b>LM</b> (70B parameters), and older versions of <b>GPT-4.</b></p></p class="citation"></blockquote><h3 id=242--55237-knowledge-distillation-based-model-extraction-attack-using-private-counterfactual-explanations-fatima-ezzeddine-et-al-2024>(2/42 | 55/237) Knowledge Distillation-Based Model Extraction Attack using Private Counterfactual Explanations (Fatima Ezzeddine et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fatima Ezzeddine, Omran Ayoub, Silvia Giordano. (2024)<br><strong>Knowledge Distillation-Based Model Extraction Attack using Private Counterfactual Explanations</strong><br><button class=copy-to-clipboard title="Knowledge Distillation-Based Model Extraction Attack using Private Counterfactual Explanations" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-CY, cs-LG, cs.LG<br>Keyword Score: 90<br>Keywords: Counter-factual, Explainable AI, Generative Adversarial Network, Generative Adversarial Network, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Differential Privacy, Model Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03348v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03348v1.pdf filename=2404.03348v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, there has been a notable increase in the deployment of machine learning (ML) <b>models</b> <b>as</b> services (MLaaS) across diverse production software applications. In parallel, <b>explainable</b> <b>AI</b> (XAI) continues to evolve, addressing the necessity for transparency and trustworthiness in ML <b>models.</b> <b>XAI</b> techniques aim to enhance the transparency of ML <b>models</b> <b>by</b> providing insights, in terms of the <b>model&rsquo;s</b> <b>explanations,</b> into their decision-making process. Simultaneously, some MLaaS platforms now offer explanations alongside the ML prediction outputs. This setup has elevated concerns regarding vulnerabilities in MLaaS, particularly in relation to privacy leakage attacks such as <b>model</b> <b>extraction</b> attacks (MEA). This is due to the fact that explanations can unveil insights about the inner workings of the <b>model</b> <b>which</b> could be exploited by malicious users. In this work, we focus on investigating how <b>model</b> <b>explanations,</b> particularly <b>Generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs)-based</b> <b>counterfactual</b> explanations (CFs), can be exploited for performing MEA within the MLaaS platform. We also delve into assessing the effectiveness of incorporating <b>differential</b> <b>privacy</b> (DP) as a mitigation strategy. To this end, we first propose a novel MEA methodology based on <b>Knowledge</b> <b>Distillation</b> <b>(KD)</b> to enhance the efficiency of extracting a substitute <b>model</b> <b>of</b> a target <b>model</b> <b>exploiting</b> CFs. Then, we advise an approach for training CF generators incorporating DP to generate private CFs. We conduct thorough experimental evaluations on real-world datasets and demonstrate that our proposed <b>KD-based</b> MEA can yield a high-fidelity substitute <b>model</b> <b>with</b> reduced queries with respect to baseline approaches. Furthermore, our findings reveal that the inclusion of a privacy layer impacts the performance of the explainer, the quality of CFs, and results in a reduction in the MEA performance.</p></p class="citation"></blockquote><h3 id=342--56237-on-the-surprising-efficacy-of-distillation-as-an-alternative-to-pre-training-small-models-sean-farhat-et-al-2024>(3/42 | 56/237) On the Surprising Efficacy of Distillation as an Alternative to Pre-Training Small Models (Sean Farhat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sean Farhat, Deming Chen. (2024)<br><strong>On the Surprising Efficacy of Distillation as an Alternative to Pre-Training Small Models</strong><br><button class=copy-to-clipboard title="On the Surprising Efficacy of Distillation as an Alternative to Pre-Training Small Models" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 80<br>Keywords: Contrastive Learning, Convolution, Fine-tuning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03263v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03263v1.pdf filename=2404.03263v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose that small models may not need to absorb the cost of pre-training to reap its benefits. Instead, they can capitalize on the astonishing results achieved by modern, enormous models to a surprising degree. We observe that, when <b>distilled</b> on a task from a pre-trained teacher model, a small model can achieve or surpass the performance it would achieve if it was pre-trained then <b>finetuned</b> on that task. To allow this phenomenon to be easily leveraged, we establish a connection reducing <b>knowledge</b> <b>distillation</b> to modern <b>contrastive</b> <b>learning,</b> opening two doors: (1) vastly different model architecture pairings can work for the <b>distillation,</b> and (2) most <b>contrastive</b> <b>learning</b> algorithms rooted in the theory of Noise <b>Contrastive</b> <b>Estimation</b> can be easily applied and used. We demonstrate this paradigm using pre-trained teacher models from open-source model hubs, <b>Transformer</b> and <b>convolution</b> based model combinations, and a novel <b>distillation</b> algorithm that massages the Alignment/Uniformity perspective of <b>contrastive</b> <b>learning</b> by Wang & Isola (2020) into a <b>distillation</b> objective. We choose this flavor of <b>contrastive</b> <b>learning</b> due to its low computational cost, an overarching theme of this work. We also observe that this phenomenon tends not to occur if the task is data-limited. However, this can be alleviated by leveraging yet another scale-inspired development: large, pre-trained generative models for dataset augmentation. Again, we use an open-source model, and our rudimentary <b>prompts</b> are sufficient to boost the small model`s performance. Thus, we highlight a training method for small models that is up to 94% faster than the standard pre-training paradigm without sacrificing performance. For practitioners discouraged from fully utilizing modern foundation datasets for their small models due to the prohibitive scale, we believe our work keeps that door open.</p></p class="citation"></blockquote><h3 id=442--57237-red-teaming-gpt-4v-are-gpt-4v-safe-against-unimulti-modal-jailbreak-attacks-shuo-chen-et-al-2024>(4/42 | 57/237) Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks? (Shuo Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuo Chen, Zhen Han, Bailan He, Zifeng Ding, Wenqian Yu, Philip Torr, Volker Tresp, Jindong Gu. (2024)<br><strong>Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?</strong><br><button class=copy-to-clipboard title="Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CR, cs-LG, cs.LG<br>Keyword Score: 49<br>Keywords: Benchmarking, Multi-modal, Multi-modal, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03411v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03411v1.pdf filename=2404.03411v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Various jailbreak attacks have been proposed to red-team <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and revealed the vulnerable safeguards of <b>LLMs.</b> Besides, some methods are not limited to the textual modality and extend the jailbreak attack to <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) by perturbing the visual input. However, the absence of a universal evaluation <b>benchmark</b> complicates the performance reproduction and fair comparison. Besides, there is a lack of comprehensive evaluation of closed-source state-of-the-art (SOTA) models, especially MLLMs, such as <b>GPT-4V.</b> To address these issues, this work first builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 different safety policies. Based on this dataset, extensive red-teaming experiments are conducted on 11 different <b>LLMs</b> and MLLMs, including both SOTA proprietary models and open-source models. We then conduct a deep analysis of the evaluated results and find that (1) <b>GPT4</b> and <b>GPT-4V</b> demonstrate better robustness against jailbreak attacks compared to open-source <b>LLMs</b> and MLLMs. (2) Llama2 and Qwen-VL-Chat are more robust compared to other open-source models. (3) The transferability of visual jailbreak methods is relatively limited compared to textual jailbreak methods. The dataset and code can be found here <a href=https://anonymous.4open.science/r/red_teaming_gpt4-C1CE/README.md>https://anonymous.4open.science/r/red_teaming_gpt4-C1CE/README.md</a> .</p></p class="citation"></blockquote><h3 id=542--58237-theoretical-and-empirical-insights-into-the-origins-of-degree-bias-in-graph-neural-networks-arjun-subramonian-et-al-2024>(5/42 | 58/237) Theoretical and Empirical Insights into the Origins of Degree Bias in Graph Neural Networks (Arjun Subramonian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arjun Subramonian, Jian Kang, Yizhou Sun. (2024)<br><strong>Theoretical and Empirical Insights into the Origins of Degree Bias in Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Theoretical and Empirical Insights into the Origins of Degree Bias in Graph Neural Networks" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 43<br>Keywords: Message-Passing, Node Classification, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03139v1.pdf filename=2404.03139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> often perform better for high-degree <b>nodes</b> <b>than</b> low-degree <b>nodes</b> <b>on</b> <b>node</b> <b>classification</b> tasks. This degree bias can reinforce social marginalization by, e.g., sidelining authors of lowly-cited papers when predicting paper topics in citation networks. While researchers have proposed numerous hypotheses for why <b>GNN</b> degree bias occurs, we find via a survey of 38 degree bias papers that these hypotheses are often not rigorously validated, and can even be contradictory. Thus, we provide an analysis of the origins of degree bias in <b>message-passing</b> <b>GNNs</b> with different <b>graph</b> <b>filters.</b> <b>We</b> prove that high-degree test <b>nodes</b> <b>tend</b> to have a lower probability of misclassification regardless of how <b>GNNs</b> are trained. Moreover, we show that degree bias arises from a variety of factors that are associated with a <b>node&rsquo;s</b> <b>degree</b> (e.g., homophily of neighbors, diversity of neighbors). Furthermore, we show that during training, some <b>GNNs</b> may adjust their loss on low-degree <b>nodes</b> <b>more</b> slowly than on high-degree <b>nodes;</b> <b>however,</b> with sufficiently many epochs of training, <b>message-passing</b> <b>GNNs</b> can achieve their maximum possible training accuracy, which is not significantly limited by their expressive power. Throughout our analysis, we connect our findings to previously-proposed hypotheses for the origins of degree bias, supporting and unifying some while drawing doubt to others. We validate our theoretical findings on 8 common real-world networks, and based on our theoretical and empirical insights, describe a roadmap to alleviate degree bias.</p></p class="citation"></blockquote><h3 id=642--59237-learning-social-fairness-preferences-from-non-expert-stakeholder-opinions-in-kidney-placement-mukund-telukunta-et-al-2024>(6/42 | 59/237) Learning Social Fairness Preferences from Non-Expert Stakeholder Opinions in Kidney Placement (Mukund Telukunta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mukund Telukunta, Sukruth Rao, Gabriella Stickney, Venkata Sriram Siddardh Nadendla, Casey Canfield. (2024)<br><strong>Learning Social Fairness Preferences from Non-Expert Stakeholder Opinions in Kidney Placement</strong><br><button class=copy-to-clipboard title="Learning Social Fairness Preferences from Non-Expert Stakeholder Opinions in Kidney Placement" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-HC, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Fairness, Recommendation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03800v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03800v1.pdf filename=2404.03800v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern kidney placement incorporates several intelligent <b>recommendation</b> systems which exhibit social discrimination due to biases inherited from training data. Although initial attempts were made in the literature to study algorithmic <b>fairness</b> in kidney placement, these methods replace true outcomes with surgeons&rsquo; decisions due to the long delays involved in recording such outcomes reliably. However, the replacement of true outcomes with surgeons&rsquo; decisions disregards expert stakeholders&rsquo; biases as well as social opinions of other stakeholders who do not possess medical expertise. This paper alleviates the latter concern and designs a novel <b>fairness</b> feedback survey to evaluate an acceptance rate predictor (ARP) that predicts a kidney&rsquo;s acceptance rate in a given kidney-match pair. The survey is launched on Prolific, a crowdsourcing platform, and public opinions are collected from 85 anonymous crowd participants. A novel social <b>fairness</b> preference learning algorithm is proposed based on minimizing social feedback regret computed using a novel logit-based <b>fairness</b> feedback model. The proposed model and learning algorithm are both validated using <b>simulation</b> experiments as well as Prolific data. Public preferences towards group <b>fairness</b> notions in the context of kidney placement have been estimated and discussed in detail. The specific ARP tested in the Prolific survey has been deemed fair by the participants.</p></p class="citation"></blockquote><h3 id=742--60237-exploration-is-harder-than-prediction-cryptographically-separating-reinforcement-learning-from-supervised-learning-noah-golowich-et-al-2024>(7/42 | 60/237) Exploration is Harder than Prediction: Cryptographically Separating Reinforcement Learning from Supervised Learning (Noah Golowich et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Noah Golowich, Ankur Moitra, Dhruv Rohatgi. (2024)<br><strong>Exploration is Harder than Prediction: Cryptographically Separating Reinforcement Learning from Supervised Learning</strong><br><button class=copy-to-clipboard title="Exploration is Harder than Prediction: Cryptographically Separating Reinforcement Learning from Supervised Learning" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CC, cs-CR, cs-DS, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Markov Decision Process, Reinforcement Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03774v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03774v1.pdf filename=2404.03774v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Supervised</b> <b>learning</b> is often computationally easy in practice. But to what extent does this mean that other modes of learning, such as <b>reinforcement</b> <b>learning</b> (RL), ought to be computationally easy by extension? In this work we show the first cryptographic separation between RL and <b>supervised</b> <b>learning,</b> by exhibiting a class of block <b>MDPs</b> and associated decoding functions where reward-free exploration is provably computationally harder than the associated regression problem. We also show that there is no computationally efficient algorithm for reward-directed RL in block <b>MDPs,</b> even when given access to an oracle for this regression problem. It is known that being able to perform regression in block <b>MDPs</b> is necessary for finding a good policy; our results suggest that it is not sufficient. Our separation lower bound uses a new robustness property of the Learning Parities with Noise (LPN) hardness assumption, which is crucial in handling the dependent nature of RL data. We argue that separations and oracle lower bounds, such as ours, are a more meaningful way to prove hardness of learning because the constructions better reflect the practical reality that <b>supervised</b> <b>learning</b> by itself is often not the computational bottleneck.</p></p class="citation"></blockquote><h3 id=842--61237-approximate-gradient-coding-for-privacy-flexible-federated-learning-with-non-iid-data-okko-makkonen-et-al-2024>(8/42 | 61/237) Approximate Gradient Coding for Privacy-Flexible Federated Learning with Non-IID Data (Okko Makkonen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Okko Makkonen, Sampo Niemelä, Camilla Hollanti, Serge Kas Hanna. (2024)<br><strong>Approximate Gradient Coding for Privacy-Flexible Federated Learning with Non-IID Data</strong><br><button class=copy-to-clipboard title="Approximate Gradient Coding for Privacy-Flexible Federated Learning with Non-IID Data" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-DC, cs-IT, cs-LG, cs.LG, math-IT, stat-ML<br>Keyword Score: 40<br>Keywords: MNIST, Federated Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03524v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03524v1.pdf filename=2404.03524v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work focuses on the challenges of non-IID data and stragglers/dropouts in <b>federated</b> <b>learning.</b> We introduce and explore a privacy-flexible paradigm that models parts of the clients&rsquo; local data as non-private, offering a more versatile and business-oriented perspective on privacy. Within this framework, we propose a data-driven strategy for mitigating the effects of label heterogeneity and client straggling on <b>federated</b> <b>learning.</b> Our solution combines both offline data sharing and approximate gradient coding techniques. Through numerical <b>simulations</b> using the <b>MNIST</b> dataset, we demonstrate that our approach enables achieving a deliberate trade-off between privacy and utility, leading to improved model convergence and accuracy while using an adaptable portion of non-private data.</p></p class="citation"></blockquote><h3 id=942--62237-knowledge-based-convolutional-neural-network-for-the-simulation-and-prediction-of-two-phase-darcy-flows-zakaria-elabid-et-al-2024>(9/42 | 62/237) Knowledge-Based Convolutional Neural Network for the Simulation and Prediction of Two-Phase Darcy Flows (Zakaria Elabid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zakaria Elabid, Daniel Busby, Abdenour Hadid. (2024)<br><strong>Knowledge-Based Convolutional Neural Network for the Simulation and Prediction of Two-Phase Darcy Flows</strong><br><button class=copy-to-clipboard title="Knowledge-Based Convolutional Neural Network for the Simulation and Prediction of Two-Phase Darcy Flows" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-flu-dyn<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03240v1.pdf filename=2404.03240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Physics-informed neural networks (PINNs) have gained significant prominence as a powerful tool in the field of scientific computing and <b>simulations.</b> Their ability to seamlessly integrate physical principles into deep learning architectures has revolutionized the approaches to solving complex problems in physics and engineering. However, a persistent challenge faced by mainstream PINNs lies in their handling of discontinuous input data, leading to inaccuracies in predictions. This study addresses these challenges by incorporating the discretized forms of the governing equations into the PINN framework. We propose to combine the power of neural networks with the dynamics imposed by the discretized differential equations. By discretizing the governing equations, the PINN learns to account for the discontinuities and accurately capture the underlying relationships between inputs and outputs, improving the accuracy compared to traditional interpolation techniques. Moreover, by leveraging the power of neural networks, the computational cost associated with numerical <b>simulations</b> is substantially reduced. We evaluate our model on a large-scale dataset for the prediction of pressure and saturation fields demonstrating high accuracies compared to non-physically aware models.</p></p class="citation"></blockquote><h3 id=1042--63237-generalization-bounds-for-message-passing-networks-on-mixture-of-graphons-sohir-maskey-et-al-2024>(10/42 | 63/237) Generalization Bounds for Message Passing Networks on Mixture of Graphons (Sohir Maskey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sohir Maskey, Gitta Kutyniok, Ron Levie. (2024)<br><strong>Generalization Bounds for Message Passing Networks on Mixture of Graphons</strong><br><button class=copy-to-clipboard title="Generalization Bounds for Message Passing Networks on Mixture of Graphons" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03473v1.pdf filename=2404.03473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the generalization capabilities of Message Passing Neural Networks (MPNNs), a prevalent class of <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNN).</b> We derive generalization bounds specifically for MPNNs with normalized sum aggregation and mean aggregation. Our analysis is based on a data generation model incorporating a finite set of template graphons. Each <b>graph</b> <b>within</b> <b>this</b> framework is generated by sampling from one of the graphons with a certain degree of perturbation. In particular, we extend previous MPNN generalization results to a more realistic setting, which includes the following modifications: 1) we analyze simple random <b>graphs</b> <b>with</b> <b>Bernoulli-distributed</b> edges instead of weighted <b>graphs;</b> <b>2)</b> <b>we</b> sample both <b>graphs</b> <b>and</b> <b>graph</b> <b>signals</b> <b>from</b> perturbed graphons instead of clean graphons; and 3) we analyze sparse <b>graphs</b> <b>instead</b> <b>of</b> dense <b>graphs.</b> <b>In</b> <b>this</b> more realistic and challenging scenario, we provide a generalization bound that decreases as the average number of nodes in the <b>graphs</b> <b>increases.</b> <b>Our</b> results imply that MPNNs with higher complexity than the size of the training set can still generalize effectively, as long as the <b>graphs</b> <b>are</b> <b>sufficiently</b> large.</p></p class="citation"></blockquote><h3 id=1142--64237-on-the-theoretical-expressive-power-and-the-design-space-of-higher-order-graph-transformers-cai-zhou-et-al-2024>(11/42 | 64/237) On the Theoretical Expressive Power and the Design Space of Higher-Order Graph Transformers (Cai Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cai Zhou, Rose Yu, Yusu Wang. (2024)<br><strong>On the Theoretical Expressive Power and the Design Space of Higher-Order Graph Transformers</strong><br><button class=copy-to-clipboard title="On the Theoretical Expressive Power and the Design Space of Higher-Order Graph Transformers" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CG, cs-LG, cs.LG, math-GN<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03380v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03380v1.pdf filename=2404.03380v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>transformers</b> <b>have</b> recently received significant attention in <b>graph</b> <b>learning,</b> <b>partly</b> due to their ability to capture more global interaction via <b>self-attention.</b> Nevertheless, while higher-order <b>graph</b> <b>neural</b> <b>networks</b> have been reasonably well studied, the exploration of extending <b>graph</b> <b>transformers</b> <b>to</b> higher-order variants is just starting. Both theoretical understanding and empirical results are limited. In this paper, we provide a systematic study of the theoretical expressive power of order-$k$ <b>graph</b> <b>transformers</b> <b>and</b> sparse variants. We first show that, an order-$k$ <b>graph</b> <b>transformer</b> <b>without</b> additional structural information is less expressive than the $k$-Weisfeiler Lehman ($k$-WL) test despite its high computational cost. We then explore strategies to both sparsify and enhance the higher-order <b>graph</b> <b>transformers,</b> <b>aiming</b> to improve both their efficiency and expressiveness. Indeed, sparsification based on neighborhood information can enhance the expressive power, as it provides additional information about input <b>graph</b> <b>structures.</b> <b>In</b> particular, we show that a natural neighborhood-based sparse order-$k$ <b>transformer</b> model is not only computationally efficient, but also expressive &ndash; as expressive as $k$-WL test. We further study several other sparse <b>graph</b> <b>attention</b> <b>models</b> that are computationally efficient and provide their expressiveness analysis. Finally, we provide experimental results to show the effectiveness of the different sparsification strategies.</p></p class="citation"></blockquote><h3 id=1242--65237-graph-neural-networks-for-electric-and-hydraulic-data-fusion-to-enhance-short-term-forecasting-of-pumped-storage-hydroelectricity-raffael-theiler-et-al-2024>(12/42 | 65/237) Graph Neural Networks for Electric and Hydraulic Data Fusion to Enhance Short-term Forecasting of Pumped-storage Hydroelectricity (Raffael Theiler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raffael Theiler, Olga Fink. (2024)<br><strong>Graph Neural Networks for Electric and Hydraulic Data Fusion to Enhance Short-term Forecasting of Pumped-storage Hydroelectricity</strong><br><button class=copy-to-clipboard title="Graph Neural Networks for Electric and Hydraulic Data Fusion to Enhance Short-term Forecasting of Pumped-storage Hydroelectricity" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SP, eess-SY<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03368v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03368v1.pdf filename=2404.03368v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pumped-storage hydropower plants (PSH) actively participate in grid power-frequency control and therefore often operate under dynamic conditions, which results in rapidly varying system states. Predicting these dynamically changing states is essential for comprehending the underlying sensor and machine conditions. This understanding aids in detecting anomalies and faults, ensuring the reliable operation of the connected power grid, and in identifying faulty and miscalibrated sensors. PSH are complex, highly interconnected systems encompassing electrical and hydraulic subsystems, each characterized by their respective underlying networks that can individually be represented as <b>graphs.</b> <b>To</b> <b>take</b> advantage of this relational inductive bias, <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have been separately applied to state forecasting tasks in the individual subsystems, but without considering their interdependencies. In PSH, however, these subsystems depend on the same control input, making their operations highly interdependent and interconnected. Consequently, hydraulic and electrical sensor data should be fused across PSH subsystems to improve state forecasting accuracy. This approach has not been explored in <b>GNN</b> literature yet because many available PSH <b>graphs</b> <b>are</b> <b>limited</b> to their respective subsystem boundaries, which makes the method unsuitable to be applied directly. In this work, we introduce the application of spectral-temporal <b>graph</b> <b>neural</b> <b>networks,</b> which leverage <b>self-attention</b> mechanisms to concurrently capture and learn meaningful subsystem interdependencies and the dynamic patterns observed in electric and hydraulic sensors. Our method effectively fuses data from the PSH&rsquo;s subsystems by operating on a unified, system-wide <b>graph,</b> <b>learned</b> <b>directly</b> from the data, This approach leads to demonstrably improved state forecasting performance and enhanced generalizability.</p></p class="citation"></blockquote><h3 id=1342--66237-silofuse-cross-silo-synthetic-data-generation-with-latent-tabular-diffusion-models-aditya-shankar-et-al-2024>(13/42 | 66/237) SiloFuse: Cross-silo Synthetic Data Generation with Latent Tabular Diffusion Models (Aditya Shankar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aditya Shankar, Hans Brouwer, Rihan Hai, Lydia Chen. (2024)<br><strong>SiloFuse: Cross-silo Synthetic Data Generation with Latent Tabular Diffusion Models</strong><br><button class=copy-to-clipboard title="SiloFuse: Cross-silo Synthetic Data Generation with Latent Tabular Diffusion Models" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-DB, cs-DC, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Diffusion Model, Autoencoder, Benchmarking, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03299v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03299v1.pdf filename=2404.03299v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Synthetic tabular data is crucial for sharing and augmenting data across silos, especially for enterprises with proprietary data. However, existing synthesizers are designed for centrally stored data. Hence, they struggle with real-world scenarios where features are distributed across multiple silos, necessitating on-premise data storage. We introduce SiloFuse, a novel generative framework for high-quality synthesis from cross-silo tabular data. To ensure privacy, SiloFuse utilizes a distributed latent tabular <b>diffusion</b> <b>architecture.</b> Through <b>autoencoders,</b> latent representations are learned for each client&rsquo;s features, masking their actual values. We employ stacked distributed training to improve communication efficiency, reducing the number of rounds to a single step. Under SiloFuse, we prove the impossibility of data reconstruction for vertically partitioned synthesis and quantify privacy risks through three attacks using our <b>benchmark</b> framework. Experimental results on nine datasets showcase SiloFuse&rsquo;s competence against centralized <b>diffusion-based</b> <b>synthesizers.</b> Notably, SiloFuse achieves 43.8 and 29.8 higher percentage points over <b>GANs</b> in resemblance and utility. Experiments on communication show stacked training&rsquo;s fixed cost compared to the growing costs of end-to-end training as the number of training iterations increases. Additionally, SiloFuse proves robust to feature permutations and varying numbers of clients.</p></p class="citation"></blockquote><h3 id=1442--67237-mitigating-the-impact-of-outlier-channels-for-language-model-quantization-with-activation-regularization-aniruddha-nrusimha-et-al-2024>(14/42 | 67/237) Mitigating the Impact of Outlier Channels for Language Model Quantization with Activation Regularization (Aniruddha Nrusimha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aniruddha Nrusimha, Mayank Mishra, Naigang Wang, Dan Alistarh, Rameswar Panda, Yoon Kim. (2024)<br><strong>Mitigating the Impact of Outlier Channels for Language Model Quantization with Activation Regularization</strong><br><button class=copy-to-clipboard title="Mitigating the Impact of Outlier Channels for Language Model Quantization with Activation Regularization" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Model Quantization, Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03605v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03605v1.pdf filename=2404.03605v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of accurate <b>quantization</b> for language <b>models,</b> <b>where</b> both the weights and activations are uniformly <b>quantized</b> to 4 bits per parameter, the lowest bitwidth format natively supported by GPU hardware. In this context, the key challenge is activation <b>quantization:</b> it is known that language <b>models</b> <b>contain</b> outlier channels whose values on average are orders of magnitude higher than than other channels, which prevents accurate low-bitwidth <b>quantization</b> with known techniques. We systematically study this phenomena and find that these outlier channels emerge early in training, and that they occur more frequently in layers with residual streams. We then propose a simple strategy which regularizes a layer&rsquo;s inputs via <b>quantization-aware</b> training (QAT) and its outputs via activation kurtosis regularization. We show that regularizing both the inputs and outputs is crucial for preventing a <b>model&rsquo;s</b> <b>&ldquo;migrating&rdquo;</b> the difficulty in input <b>quantization</b> to the weights, which makes post-training <b>quantization</b> (PTQ) of weights more difficult. When combined with weight PTQ, we show that our approach can obtain a W4A4 <b>model</b> <b>that</b> performs competitively to the standard-precision W16A16 baseline.</p></p class="citation"></blockquote><h3 id=1542--68237-self-organized-arrival-system-for-urban-air-mobility-martin-waltz-et-al-2024>(15/42 | 68/237) Self-organized arrival system for urban air mobility (Martin Waltz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Waltz, Ostap Okhrin, Michael Schultz. (2024)<br><strong>Self-organized arrival system for urban air mobility</strong><br><button class=copy-to-clipboard title="Self-organized arrival system for urban air mobility" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03710v1.pdf filename=2404.03710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Urban air mobility is an innovative mode of transportation in which electric vertical takeoff and landing (eVTOL) vehicles operate between nodes called vertiports. We outline a self-organized vertiport arrival system based on deep <b>reinforcement</b> <b>learning.</b> The airspace around the vertiport is assumed to be circular, and the vehicles can freely operate inside. Each aircraft is considered an individual agent and follows a shared policy, resulting in decentralized actions that are based on local information. We investigate the development of the <b>reinforcement</b> <b>learning</b> policy during training and illustrate how the algorithm moves from suboptimal local holding patterns to a safe and efficient final policy. The latter is validated in <b>simulation-based</b> scenarios and also deployed on small-scale unmanned aerial vehicles to showcase its real-world usability.</p></p class="citation"></blockquote><h3 id=1642--69237-investigating-the-robustness-of-counterfactual-learning-to-rank-models-a-reproducibility-study-zechun-niu-et-al-2024>(16/42 | 69/237) Investigating the Robustness of Counterfactual Learning to Rank Models: A Reproducibility Study (Zechun Niu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zechun Niu, Jiaxin Mao, Qingyao Ai, Ji-Rong Wen. (2024)<br><strong>Investigating the Robustness of Counterfactual Learning to Rank Models: A Reproducibility Study</strong><br><button class=copy-to-clipboard title="Investigating the Robustness of Counterfactual Learning to Rank Models: A Reproducibility Study" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-IR, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Counter-factual, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03707v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03707v1.pdf filename=2404.03707v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Counterfactual</b> learning to rank (CLTR) has attracted extensive attention in the IR community for its ability to leverage massive logged user interaction data to train ranking models. While the CLTR models can be theoretically unbiased when the user behavior assumption is correct and the propensity estimation is accurate, their effectiveness is usually empirically evaluated via <b>simulation-based</b> experiments due to a lack of widely-available, large-scale, real click logs. However, the mainstream <b>simulation-based</b> experiments are somewhat limited as they often feature a single, deterministic production ranker and simplified user <b>simulation</b> models to generate the synthetic click logs. As a result, the robustness of CLTR models in complex and diverse situations is largely unknown and needs further investigation. To address this problem, in this paper, we aim to investigate the robustness of existing CLTR models in a reproducibility study with extensive <b>simulation-based</b> experiments that (1) use both deterministic and stochastic production rankers, each with different ranking performance, and (2) leverage multiple user <b>simulation</b> models with different user behavior assumptions. We find that the DLA models and IPS-DCM show better robustness under various <b>simulation</b> settings than IPS-PBM and PRS with offline propensity estimation. Besides, the existing CLTR models often fail to outperform the naive click baselines when the production ranker has relatively high ranking performance or certain randomness, which suggests an urgent need for developing new CLTR algorithms that work for these settings.</p></p class="citation"></blockquote><h3 id=1742--70237-improvement-of-performance-in-freezing-of-gait-detection-in-parkinsons-disease-using-transformer-networks-and-a-single-waist-worn-triaxial-accelerometer-luis-sigcha-et-al-2024>(17/42 | 70/237) Improvement of Performance in Freezing of Gait detection in Parkinsons Disease using Transformer networks and a single waist worn triaxial accelerometer (Luis Sigcha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luis Sigcha, Luigi Borzì, Ignacio Pavón, Nélson Costa, Susana Costa, Pedro Arezes, Juan-Manuel López, Guillermo De Arcas. (2024)<br><strong>Improvement of Performance in Freezing of Gait detection in Parkinsons Disease using Transformer networks and a single waist worn triaxial accelerometer</strong><br><button class=copy-to-clipboard title="Improvement of Performance in Freezing of Gait detection in Parkinsons Disease using Transformer networks and a single waist worn triaxial accelerometer" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, eess-SP<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03704v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03704v1.pdf filename=2404.03704v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Freezing of gait (FOG) is one of the most incapacitating symptoms in Parkinsons disease, affecting more than 50 percent of patients in advanced stages of the disease. The presence of FOG may lead to falls and a loss of independence with a consequent reduction in the quality of life. Wearable technology and artificial intelligence have been used for automatic FOG detection to optimize monitoring. However, differences between laboratory and daily-life conditions present challenges for the implementation of reliable detection systems. Consequently, improvement of FOG detection methods remains important to provide accurate monitoring mechanisms intended for free-living and real-time use. This paper presents advances in automatic FOG detection using a single body-worn triaxial accelerometer and a novel classification algorithm based on <b>Transformers</b> and <b>convolutional</b> <b>networks.</b> This study was performed with data from 21 patients who manifested FOG episodes while performing activities of daily living in a home setting. Results indicate that the proposed FOG-Transformer can bring a significant improvement in FOG detection using leave-one-subject-out cross-validation (LOSO CV). These results bring opportunities for the implementation of accurate monitoring systems for use in ambulatory or home settings.</p></p class="citation"></blockquote><h3 id=1842--71237-goldfish-an-efficient-federated-unlearning-framework-houzhe-wang-et-al-2024>(18/42 | 71/237) Goldfish: An Efficient Federated Unlearning Framework (Houzhe Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Houzhe Wang, Xiaojie Zhu, Chi Chen, Paulo Esteves-Veríssimo. (2024)<br><strong>Goldfish: An Efficient Federated Unlearning Framework</strong><br><button class=copy-to-clipboard title="Goldfish: An Efficient Federated Unlearning Framework" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Machine Unlearning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03180v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03180v1.pdf filename=2404.03180v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With recent legislation on the right to be forgotten, <b>machine</b> <b>unlearning</b> has emerged as a crucial research area. It facilitates the removal of a user&rsquo;s data from federated trained <b>machine</b> <b>learning</b> models without the necessity for retraining from scratch. However, current <b>machine</b> <b>unlearning</b> algorithms are confronted with challenges of efficiency and validity.To address the above issues, we propose a new framework, named Goldfish. It comprises four modules: basic model, loss function, optimization, and extension. To address the challenge of low validity in existing <b>machine</b> <b>unlearning</b> algorithms, we propose a novel loss function. It takes into account the loss arising from the discrepancy between predictions and actual labels in the remaining dataset. Simultaneously, it takes into consideration the bias of predicted results on the removed dataset. Moreover, it accounts for the confidence level of predicted results. Additionally, to enhance efficiency, we adopt <b>knowledge</b> <b>distillation</b> technique in basic model and introduce an optimization module that encompasses the early termination mechanism guided by empirical risk and the data partition mechanism. Furthermore, to bolster the robustness of the aggregated model, we propose an extension module that incorporates a mechanism using adaptive <b>distillation</b> temperature to address the heterogeneity of user local data and a mechanism using adaptive weight to handle the variety in the quality of uploaded models. Finally, we conduct comprehensive experiments to illustrate the effectiveness of proposed approach.</p></p class="citation"></blockquote><h3 id=1942--72237-outlier-efficient-hopfield-layers-for-large-transformer-based-models-jerry-yao-chieh-hu-et-al-2024>(19/42 | 72/237) Outlier-Efficient Hopfield Layers for Large Transformer-Based Models (Jerry Yao-Chieh Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jerry Yao-Chieh Hu, Pei-Hsuan Chang, Robin Luo, Hong-Yu Chen, Weijian Li, Wei-Po Wang, Han Liu. (2024)<br><strong>Outlier-Efficient Hopfield Layers for Large Transformer-Based Models</strong><br><button class=copy-to-clipboard title="Outlier-Efficient Hopfield Layers for Large Transformer-Based Models" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, BERT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03828v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03828v1.pdf filename=2404.03828v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce an Outlier-Efficient Modern Hopfield Model (termed $\mathtt{OutEffHop}$) and use it to address the outlier-induced challenge of quantizing gigantic <b>transformer-based</b> models. Our main contribution is a novel associative memory model facilitating \textit{outlier-efficient} associative memory retrievals. Interestingly, this memory model manifests a model-based interpretation of an outlier-efficient attention mechanism ($\text{Softmax}_1$): it is an approximation of the memory retrieval process of $\mathtt{OutEffHop}$. Methodologically, this allows us to debut novel outlier-efficient Hopfield layers a powerful attention alternative with superior post-quantization performance. Theoretically, the Outlier-Efficient Modern Hopfield Model retains and improves the desirable properties of the standard modern Hopfield models, including fixed point convergence and exponential storage capacity. Empirically, we demonstrate the proposed model&rsquo;s efficacy across large-scale <b>transformer-based</b> and Hopfield-based models (including <b>BERT,</b> OPT, ViT and STanHop-Net), <b>benchmarking</b> against state-of-the-art methods including $\mathtt{Clipped_Softmax}$ and $\mathtt{Gated_Attention}$. Notably, $\mathtt{OutEffHop}$ achieves on average $\sim$22+% reductions in both average kurtosis and maximum infinity norm of model outputs accross 4 models.</p></p class="citation"></blockquote><h3 id=2042--73237-laser-learning-environment-a-new-environment-for-coordination-critical-multi-agent-tasks-yannick-molinghen-et-al-2024>(20/42 | 73/237) Laser Learning Environment: A new environment for coordination-critical multi-agent tasks (Yannick Molinghen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yannick Molinghen, Raphaël Avalos, Mark Van Achter, Ann Nowé, Tom Lenaerts. (2024)<br><strong>Laser Learning Environment: A new environment for coordination-critical multi-agent tasks</strong><br><button class=copy-to-clipboard title="Laser Learning Environment: A new environment for coordination-critical multi-agent tasks" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-MA, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Knowledge Distillation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03596v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03596v1.pdf filename=2404.03596v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the Laser Learning Environment (LLE), a collaborative multi-agent <b>reinforcement</b> <b>learning</b> environment in which coordination is central. In LLE, agents depend on each other to make progress (interdependence), must jointly take specific sequences of actions to succeed (perfect coordination), and accomplishing those joint actions does not yield any intermediate reward (zero-incentive dynamics). The challenge of such problems lies in the difficulty of escaping state space bottlenecks caused by interdependence steps since escaping those bottlenecks is not rewarded. We test multiple state-of-the-art value-based MARL algorithms against LLE and show that they consistently fail at the collaborative task because of their inability to escape state space bottlenecks, even though they successfully achieve perfect coordination. We show that Q-learning extensions such as prioritized experience replay and n-steps return hinder exploration in environments with zero-incentive dynamics, and find that intrinsic curiosity with random network <b>distillation</b> is not sufficient to escape those bottlenecks. We demonstrate the need for novel methods to solve this problem and the relevance of LLE as cooperative MARL <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=2142--74237-about-test-time-training-for-outlier-detection-simon-klüttermann-et-al-2024>(21/42 | 74/237) About Test-time training for outlier detection (Simon Klüttermann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Klüttermann, Emmanuel Müller. (2024)<br><strong>About Test-time training for outlier detection</strong><br><button class=copy-to-clipboard title="About Test-time training for outlier detection" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Outlier Detection, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03495v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03495v1.pdf filename=2404.03495v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce DOUST, our method applying test-time training for <b>outlier</b> <b>detection,</b> significantly improving the detection performance. After thoroughly evaluating our algorithm on common <b>benchmark</b> datasets, we discuss a common problem and show that it disappears with a large enough test set. Thus, we conclude that under reasonable conditions, our algorithm can reach almost <b>supervised</b> performance even when no labeled <b>outliers</b> <b>are</b> given.</p></p class="citation"></blockquote><h3 id=2242--75237-learning-from-simplicial-data-based-on-random-walks-and-1d-convolutions-florian-frantzen-et-al-2024>(22/42 | 75/237) Learning From Simplicial Data Based on Random Walks and 1D Convolutions (Florian Frantzen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian Frantzen, Michael T. Schaub. (2024)<br><strong>Learning From Simplicial Data Based on Random Walks and 1D Convolutions</strong><br><button class=copy-to-clipboard title="Learning From Simplicial Data Based on Random Walks and 1D Convolutions" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Message-Passing, Graph, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03434v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03434v1.pdf filename=2404.03434v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Triggered by limitations of <b>graph-based</b> deep learning methods in terms of computational expressivity and model flexibility, recent years have seen a surge of interest in computational models that operate on higher-order topological domains such as hypergraphs and simplicial complexes. While the increased expressivity of these models can indeed lead to a better classification performance and a more faithful representation of the underlying system, the computational cost of these higher-order models can increase dramatically. To this end, we here explore a simplicial complex neural network learning architecture based on random walks and fast 1D <b>convolutions</b> (SCRaWl), in which we can adjust the increase in computational cost by varying the length and number of random walks considered while accounting for higher-order relationships. Importantly, due to the random walk-based design, the expressivity of the proposed architecture is provably incomparable to that of existing <b>message-passing</b> simplicial neural networks. We empirically evaluate SCRaWl on real-world datasets and show that it outperforms other simplicial neural networks.</p></p class="citation"></blockquote><h3 id=2342--76237-future-proofing-class-incremental-learning-quentin-jodelet-et-al-2024>(23/42 | 76/237) Future-Proofing Class Incremental Learning (Quentin Jodelet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quentin Jodelet, Xin Liu, Yin Jun Phua, Tsuyoshi Murata. (2024)<br><strong>Future-Proofing Class Incremental Learning</strong><br><button class=copy-to-clipboard title="Future-Proofing Class Incremental Learning" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Diffusion Model, Benchmarking, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03200v1.pdf filename=2404.03200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exemplar-Free Class Incremental Learning is a highly challenging setting where replay memory is unavailable. Methods relying on frozen feature extractors have drawn attention recently in this setting due to their impressive performances and lower computational costs. However, those methods are highly dependent on the data used to train the feature extractor and may struggle when an insufficient amount of classes are available during the first incremental step. To overcome this limitation, we propose to use a pre-trained <b>text-to-image</b> <b>diffusion</b> <b>model</b> in order to generate synthetic images of future classes and use them to train the feature extractor. Experiments on the standard <b>benchmarks</b> CIFAR100 and ImageNet-Subset demonstrate that our proposed method can be used to improve state-of-the-art methods for exemplar-free class incremental learning, especially in the most difficult settings where the first incremental step only contains few classes. Moreover, we show that using synthetic samples of future classes achieves higher performance than using real data from different classes, paving the way for better and less costly pre-training methods for incremental learning.</p></p class="citation"></blockquote><h3 id=2442--77237-layerwise-early-stopping-for-test-time-adaptation-sabyasachi-sahoo-et-al-2024>(24/42 | 77/237) Layerwise Early Stopping for Test Time Adaptation (Sabyasachi Sahoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sabyasachi Sahoo, Mostafa ElAraby, Jonas Ngnawe, Yann Pequignot, Frederic Precioso, Christian Gagne. (2024)<br><strong>Layerwise Early Stopping for Test Time Adaptation</strong><br><button class=copy-to-clipboard title="Layerwise Early Stopping for Test Time Adaptation" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Distribution Shift, Distribution Shift, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03784v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03784v1.pdf filename=2404.03784v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Test Time Adaptation (TTA) addresses the problem of <b>distribution</b> <b>shift</b> by enabling pretrained models to learn new features on an unseen domain at test time. However, it poses a significant challenge to maintain a balance between learning new features and retaining useful pretrained features. In this paper, we propose Layerwise EArly STopping (LEAST) for TTA to address this problem. The key idea is to stop adapting individual layers during TTA if the features being learned do not appear beneficial for the new domain. For that purpose, we propose using a novel gradient-based metric to measure the relevance of the current learnt features to the new domain without the need for <b>supervised</b> labels. More specifically, we propose to use this metric to determine dynamically when to stop updating each layer during TTA. This enables a more balanced adaptation, restricted to layers benefiting from it, and only for a certain number of steps. Such an approach also has the added effect of limiting the forgetting of pretrained features useful for dealing with new domains. Through extensive experiments, we demonstrate that Layerwise Early Stopping improves the performance of existing TTA approaches across multiple datasets, domain shifts, model architectures, and TTA losses.</p></p class="citation"></blockquote><h3 id=2542--78237-on-the-efficiency-of-convolutional-neural-networks-andrew-lavin-2024>(25/42 | 78/237) On the Efficiency of Convolutional Neural Networks (Andrew Lavin, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Lavin. (2024)<br><strong>On the Efficiency of Convolutional Neural Networks</strong><br><button class=copy-to-clipboard title="On the Efficiency of Convolutional Neural Networks" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03617v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03617v1.pdf filename=2404.03617v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since the breakthrough performance of AlexNet in 2012, <b>convolutional</b> <b>neural</b> <b>networks</b> (convnets) have grown into extremely powerful vision models. Deep learning researchers have used convnets to produce accurate results that were unachievable a decade ago. Yet computer scientists make computational efficiency their primary objective. Accuracy with exorbitant cost is not acceptable; an algorithm must also minimize its computational requirements. Confronted with the daunting computation that convnets use, deep learning researchers also became interested in efficiency. Researchers applied tremendous effort to find the convnet architectures that have the greatest efficiency. However, skepticism grew among researchers and engineers alike about the relevance of arithmetic complexity. Contrary to the prevailing view that latency and arithmetic complexity are irreconcilable, a simple formula relates both through computational efficiency. This insight enabled us to co-optimize the separate factors that determine latency. We observed that the degenerate conv2d layers that produce the best accuracy-complexity trade-off also have low operational intensity. Therefore, kernels that implement these layers use significant memory resources. We solved this optimization problem with block-fusion kernels that implement all layers of a residual block, thereby creating temporal locality, avoiding communication, and reducing workspace size. Our ConvFirst model with block-fusion kernels ran approximately four times as fast as the ConvNeXt baseline with PyTorch Inductor, at equal accuracy on the ImageNet-1K classification task. Our unified approach to convnet efficiency envisions a new era of models and kernels that achieve greater accuracy at lower cost.</p></p class="citation"></blockquote><h3 id=2642--79237-distributionally-robust-reinforcement-learning-with-interactive-data-collection-fundamental-hardness-and-near-optimal-algorithm-miao-lu-et-al-2024>(26/42 | 79/237) Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithm (Miao Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miao Lu, Han Zhong, Tong Zhang, Jose Blanchet. (2024)<br><strong>Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithm</strong><br><button class=copy-to-clipboard title="Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithm" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03578v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03578v1.pdf filename=2404.03578v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The sim-to-real gap, which represents the disparity between training and testing environments, poses a significant challenge in <b>reinforcement</b> <b>learning</b> (RL). A promising approach to addressing this challenge is distributionally robust RL, often framed as a robust <b>Markov</b> <b>decision</b> <b>process</b> (RMDP). In this framework, the objective is to find a robust policy that achieves good performance under the worst-case scenario among all environments within a pre-specified uncertainty set centered around the training environment. Unlike previous work, which relies on a generative model or a pre-collected offline dataset enjoying good coverage of the deployment environment, we tackle robust RL via interactive data collection, where the learner interacts with the training environment only and refines the policy through trial and error. In this robust RL paradigm, two main challenges emerge: managing distributional robustness while striking a balance between exploration and exploitation during data collection. Initially, we establish that sample-efficient learning without additional assumptions is unattainable owing to the curse of support shift; i.e., the potential disjointedness of the distributional supports between the training and testing environments. To circumvent such a hardness result, we introduce the vanishing minimal value assumption to RMDPs with a total-variation (TV) distance robust set, postulating that the minimal value of the optimal robust value function is zero. We prove that such an assumption effectively eliminates the support shift issue for RMDPs with a TV distance robust set, and present an algorithm with a provable sample complexity guarantee. Our work makes the initial step to uncovering the inherent difficulty of robust RL via interactive data collection and sufficient conditions for designing a sample-efficient algorithm accompanied by sharp sample complexity analysis.</p></p class="citation"></blockquote><h3 id=2742--80237-gaussian-smoothed-sliced-probability-divergences-mokhtar-z-alaya-et-al-2024>(27/42 | 80/237) Gaussian-Smoothed Sliced Probability Divergences (Mokhtar Z. Alaya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mokhtar Z. Alaya, Alain Rakotomamonjy, Maxime Berar, Gilles Gasso. (2024)<br><strong>Gaussian-Smoothed Sliced Probability Divergences</strong><br><button class=copy-to-clipboard title="Gaussian-Smoothed Sliced Probability Divergences" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-ST, stat-ML, stat-TH<br>Keyword Score: 20<br>Keywords: Convolution, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03273v1.pdf filename=2404.03273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gaussian smoothed sliced Wasserstein distance has been recently introduced for comparing probability distributions, while preserving privacy on the data. It has been shown that it provides performances similar to its non-smoothed (non-private) counterpart. However, the computationaland statistical properties of such a metric have not yet been well-established. This work investigates the theoretical properties of this distance as well as those of generalized versions denoted as Gaussian-smoothed sliced divergences. We first show that smoothing and slicing preserve the metric property and the weak topology. To study the sample complexity of such divergences, we then introduce $\hat{\hat\mu}<em>{n}$ the double empirical distribution for the smoothed-projected $\mu$. The distribution $\hat{\hat\mu}</em>{n}$ is a result of a double sampling process: one from sampling according to the origin distribution $\mu$ and the second according to the <b>convolution</b> of the projection of $\mu$ on the unit sphere and the Gaussian smoothing. We particularly focus on the Gaussian smoothed sliced Wasserstein distance and prove that it converges with a rate $O(n^{-1/2})$. We also derive other properties, including continuity, of different divergences with respect to the smoothing parameter. We support our theoretical findings with empirical studies in the context of privacy-preserving <b>domain</b> <b>adaptation.</b></p></p class="citation"></blockquote><h3 id=2842--81237-enabling-clean-energy-resilience-with-machine-learning-empowered-underground-hydrogen-storage-alvaro-carbonero-et-al-2024>(28/42 | 81/237) Enabling Clean Energy Resilience with Machine Learning-Empowered Underground Hydrogen Storage (Alvaro Carbonero et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alvaro Carbonero, Shaowen Mao, Mohamed Mehana. (2024)<br><strong>Enabling Clean Energy Resilience with Machine Learning-Empowered Underground Hydrogen Storage</strong><br><button class=copy-to-clipboard title="Enabling Clean Energy Resilience with Machine Learning-Empowered Underground Hydrogen Storage" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03222v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03222v1.pdf filename=2404.03222v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To address the urgent challenge of climate change, there is a critical need to transition away from fossil fuels towards sustainable energy systems, with renewable energy sources playing a pivotal role. However, the inherent variability of renewable energy, without effective storage solutions, often leads to imbalances between energy supply and demand. Underground Hydrogen Storage (UHS) emerges as a promising long-term storage solution to bridge this gap, yet its widespread implementation is impeded by the high computational costs associated with high fidelity UHS <b>simulations.</b> This paper introduces UHS from a data-driven perspective and outlines a roadmap for integrating machine learning into UHS, thereby facilitating the large-scale deployment of UHS.</p></p class="citation"></blockquote><h3 id=2942--82237-information-theoretic-generalization-bounds-for-deep-neural-networks-haiyun-he-et-al-2024>(29/42 | 82/237) Information-Theoretic Generalization Bounds for Deep Neural Networks (Haiyun He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haiyun He, Christina Lee Yu, Ziv Goldfeld. (2024)<br><strong>Information-Theoretic Generalization Bounds for Deep Neural Networks</strong><br><button class=copy-to-clipboard title="Information-Theoretic Generalization Bounds for Deep Neural Networks" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03176v1.pdf filename=2404.03176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks (DNNs) exhibit an exceptional capacity for generalization in practical applications. This work aims to capture the effect and benefits of depth for <b>supervised</b> <b>learning</b> via information-theoretic generalization bounds. We first derive two hierarchical bounds on the generalization error in terms of the Kullback-Leibler (KL) divergence or the 1-Wasserstein distance between the train and test distributions of the network internal representations. The KL divergence bound shrinks as the layer index increases, while the Wasserstein bound implies the existence of a layer that serves as a generalization funnel, which attains a minimal 1-Wasserstein distance. Analytic expressions for both bounds are derived under the setting of binary Gaussian classification with linear DNNs. To quantify the contraction of the relevant information measures when moving deeper into the network, we analyze the strong data processing inequality (SDPI) coefficient between consecutive layers of three regularized DNN models: Dropout, DropConnect, and Gaussian noise injection. This enables refining our generalization bounds to capture the contraction as a function of the network architecture parameters. Specializing our results to DNNs with a finite parameter space and the Gibbs algorithm reveals that deeper yet narrower network architectures generalize better in those examples, although how broadly this statement applies remains a question.</p></p class="citation"></blockquote><h3 id=3042--83237-personalized-federated-learning-for-spatio-temporal-forecasting-a-dual-semantic-alignment-based-contrastive-approach-qingxiang-liu-et-al-2024>(30/42 | 83/237) Personalized Federated Learning for Spatio-Temporal Forecasting: A Dual Semantic Alignment-Based Contrastive Approach (Qingxiang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingxiang Liu, Sheng Sun, Yuxuan Liang, Jingjing Xue, Min Liu. (2024)<br><strong>Personalized Federated Learning for Spatio-Temporal Forecasting: A Dual Semantic Alignment-Based Contrastive Approach</strong><br><button class=copy-to-clipboard title="Personalized Federated Learning for Spatio-Temporal Forecasting: A Dual Semantic Alignment-Based Contrastive Approach" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03702v1.pdf filename=2404.03702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The existing <b>federated</b> <b>learning</b> (FL) methods for spatio-temporal forecasting fail to capture the inherent spatio-temporal heterogeneity, which calls for personalized FL (PFL) methods to model the spatio-temporally variant patterns. While <b>contrastive</b> <b>learning</b> approach is promising in addressing spatio-temporal heterogeneity, the existing methods are noneffective in determining negative pairs and can hardly apply to PFL paradigm. To tackle this limitation, we propose a novel PFL method, named <b>Federated</b> <b>dUal</b> sEmantic aLignment-based <b>contraStive</b> <b>learning</b> (FUELS), which can adaptively align positive and negative pairs based on semantic similarity, thereby injecting precise spatio-temporal heterogeneity into the latent representation space by auxiliary <b>contrastive</b> <b>tasks.</b> From temporal perspective, a hard negative filtering module is introduced to dynamically align heterogeneous temporal representations for the supplemented intra-client <b>contrastive</b> <b>task.</b> From spatial perspective, we design lightweight-but-efficient prototypes as client-level semantic representations, based on which the server evaluates spatial similarity and yields client-customized global prototypes for the supplemented inter-client <b>contrastive</b> <b>task.</b> Extensive experiments demonstrate that FUELS outperforms state-of-the-art methods, with communication cost decreasing by around 94%.</p></p class="citation"></blockquote><h3 id=3142--84237-integrating-hyperparameter-search-into-gramml-hernán-ceferino-vázquez-et-al-2024>(31/42 | 84/237) Integrating Hyperparameter Search into GramML (Hernán Ceferino Vázquez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hernán Ceferino Vázquez, Jorge Sanchez, Rafael Carrascosa. (2024)<br><strong>Integrating Hyperparameter Search into GramML</strong><br><button class=copy-to-clipboard title="Integrating Hyperparameter Search into GramML" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03419v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03419v1.pdf filename=2404.03419v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated Machine Learning (AutoML) has become increasingly popular in recent years due to its ability to reduce the amount of time and expertise required to design and develop machine learning systems. This is very important for the practice of machine learning, as it allows building strong baselines quickly, improving the efficiency of the data scientists, and reducing the time to production. However, despite the advantages of AutoML, it faces several challenges, such as defining the solutions space and exploring it efficiently. Recently, some approaches have been shown to be able to do it using tree-based search algorithms and context-free grammars. In particular, GramML presents a model-free <b>reinforcement</b> <b>learning</b> approach that leverages pipeline configuration grammars and operates using Monte Carlo tree search. However, one of the limitations of GramML is that it uses default hyperparameters, limiting the search problem to finding optimal pipeline structures for the available data preprocessors and models. In this work, we propose an extension to GramML that supports larger search spaces including hyperparameter search. We evaluated the approach using an OpenML <b>benchmark</b> and found significant improvements compared to other state-of-the-art techniques.</p></p class="citation"></blockquote><h3 id=3242--85237-an-explainablefair-framework-for-prediction-of-substance-use-disorder-treatment-completion-mary-m-lucas-et-al-2024>(32/42 | 85/237) An ExplainableFair Framework for Prediction of Substance Use Disorder Treatment Completion (Mary M. Lucas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mary M. Lucas, Xiaoyang Wang, Chia-Hsuan Chang, Christopher C. Yang, Jacqueline E. Braughton, Quyen M. Ngo. (2024)<br><strong>An ExplainableFair Framework for Prediction of Substance Use Disorder Treatment Completion</strong><br><button class=copy-to-clipboard title="An ExplainableFair Framework for Prediction of Substance Use Disorder Treatment Completion" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03833v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03833v1.pdf filename=2404.03833v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fairness</b> of machine learning models in healthcare has drawn increasing attention from clinicians, researchers, and even at the highest level of government. On the other hand, the importance of developing and deploying interpretable or explainable models has been demonstrated, and is essential to increasing the trustworthiness and likelihood of adoption of these models. The objective of this study was to develop and implement a framework for addressing both these issues - <b>fairness</b> and explainability. We propose an explainable <b>fairness</b> framework, first developing a model with optimized performance, and then using an in-processing approach to mitigate model biases relative to the sensitive attributes of race and sex. We then explore and visualize explanations of the model changes that lead to the <b>fairness</b> enhancement process through exploring the changes in importance of features. Our resulting-fairness enhanced models retain high sensitivity with improved <b>fairness</b> and explanations of the <b>fairness-enhancement</b> that may provide helpful insights for healthcare providers to guide clinical decision-making and resource allocation.</p></p class="citation"></blockquote><h3 id=3342--86237-localized-distributional-robustness-in-submodular-multi-task-subset-selection-ege-c-kaya-et-al-2024>(33/42 | 86/237) Localized Distributional Robustness in Submodular Multi-Task Subset Selection (Ege C. Kaya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ege C. Kaya, Abolfazl Hashemi. (2024)<br><strong>Localized Distributional Robustness in Submodular Multi-Task Subset Selection</strong><br><button class=copy-to-clipboard title="Localized Distributional Robustness in Submodular Multi-Task Subset Selection" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP, math-OC<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03759v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03759v1.pdf filename=2404.03759v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we approach the problem of multi-task submodular optimization with the perspective of local distributional robustness, within the neighborhood of a reference distribution which assigns an importance score to each task. We initially propose to introduce a regularization term which makes use of the relative entropy to the standard multi-task objective. We then demonstrate through duality that this novel formulation itself is equivalent to the maximization of a submodular function, which may be efficiently carried out through standard greedy selection methods. This approach bridges the existing gap in the optimization of performance-robustness trade-offs in multi-task subset selection. To numerically validate our theoretical results, we test the proposed method in two different setting, one involving the selection of satellites in low Earth orbit constellations in the context of a sensor selection problem, and the other involving an image <b>summarization</b> task using neural networks. Our method is compared with two other algorithms focused on optimizing the performance of the worst-case task, and on directly optimizing the performance on the reference distribution itself. We conclude that our novel formulation produces a solution that is locally distributional robust, and computationally inexpensive.</p></p class="citation"></blockquote><h3 id=3442--87237-explaining-explainability-understanding-concept-activation-vectors-angus-nicolson-et-al-2024>(34/42 | 87/237) Explaining Explainability: Understanding Concept Activation Vectors (Angus Nicolson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Angus Nicolson, Lisa Schut, J. Alison Noble, Yarin Gal. (2024)<br><strong>Explaining Explainability: Understanding Concept Activation Vectors</strong><br><button class=copy-to-clipboard title="Explaining Explainability: Understanding Concept Activation Vectors" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-6, cs-AI, cs-CV, cs-HC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03713v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03713v1.pdf filename=2404.03713v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent interpretability methods propose using concept-based explanations to translate the internal representations of deep learning models into a language that humans are familiar with: concepts. This requires understanding which concepts are present in the representation space of a neural network. One popular method for finding concepts is Concept Activation Vectors (CAVs), which are learnt using a probe dataset of concept exemplars. In this work, we investigate three properties of CAVs. CAVs may be: (1) inconsistent between layers, (2) entangled with different concepts, and (3) spatially dependent. Each property provides both challenges and opportunities in interpreting models. We introduce tools designed to detect the presence of these properties, provide insight into how they affect the derived explanations, and provide <b>recommendations</b> to minimise their impact. Understanding these properties can be used to our advantage. For example, we introduce spatially dependent CAVs to test if a model is translation invariant with respect to a specific concept and class. Our experiments are performed on ImageNet and a new synthetic dataset, Elements. Elements is designed to capture a known ground truth relationship between concepts and classes. We release this dataset to facilitate further research in understanding and evaluating interpretability methods.</p></p class="citation"></blockquote><h3 id=3542--88237-dida-denoised-imitation-learning-based-on-domain-adaptation-kaichen-huang-et-al-2024>(35/42 | 88/237) DIDA: Denoised Imitation Learning based on Domain Adaptation (Kaichen Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaichen Huang, Hai-Hang Sun, Shenghua Wan, Minghao Shao, Shuai Feng, Le Gan, De-Chuan Zhan. (2024)<br><strong>DIDA: Denoised Imitation Learning based on Domain Adaptation</strong><br><button class=copy-to-clipboard title="DIDA: Denoised Imitation Learning based on Domain Adaptation" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03382v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03382v1.pdf filename=2404.03382v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Imitating skills from low-quality datasets, such as sub-optimal demonstrations and observations with distractors, is common in real-world applications. In this work, we focus on the problem of Learning from Noisy Demonstrations (LND), where the imitator is required to learn from data with noise that often occurs during the processes of data collection or transmission. Previous IL methods improve the robustness of learned policies by injecting an adversarially learned Gaussian noise into pure expert data or utilizing additional ranking information, but they may fail in the LND setting. To alleviate the above problems, we propose Denoised Imitation learning based on <b>Domain</b> <b>Adaptation</b> (DIDA), which designs two discriminators to distinguish the noise level and expertise level of data, facilitating a feature encoder to learn task-related but <b>domain-agnostic</b> <b>representations.</b> Experiment results on MuJoCo demonstrate that DIDA can successfully handle challenging imitation tasks from demonstrations with various types of noise, outperforming most baseline methods.</p></p class="citation"></blockquote><h3 id=3642--89237-react-revealing-evolutionary-action-consequence-trajectories-for-interpretable-reinforcement-learning-philipp-altmann-et-al-2024>(36/42 | 89/237) REACT: Revealing Evolutionary Action Consequence Trajectories for Interpretable Reinforcement Learning (Philipp Altmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Altmann, Céline Davignon, Maximilian Zorn, Fabian Ritz, Claudia Linnhoff-Popien, Thomas Gabor. (2024)<br><strong>REACT: Revealing Evolutionary Action Consequence Trajectories for Interpretable Reinforcement Learning</strong><br><button class=copy-to-clipboard title="REACT: Revealing Evolutionary Action Consequence Trajectories for Interpretable Reinforcement Learning" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NE, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03359v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03359v1.pdf filename=2404.03359v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To enhance the interpretability of <b>Reinforcement</b> <b>Learning</b> (RL), we propose Revealing Evolutionary Action Consequence Trajectories (REACT). In contrast to the prevalent practice of validating RL models based on their optimal behavior learned during training, we posit that considering a range of edge-case trajectories provides a more comprehensive understanding of their inherent behavior. To induce such scenarios, we introduce a disturbance to the initial state, optimizing it through an evolutionary algorithm to generate a diverse population of demonstrations. To evaluate the fitness of trajectories, REACT incorporates a joint fitness function that encourages both local and global diversity in the encountered states and chosen actions. Through assessments with policies trained for varying durations in discrete and continuous environments, we demonstrate the descriptive power of REACT. Our results highlight its effectiveness in revealing nuanced aspects of RL models&rsquo; behavior beyond optimal performance, thereby contributing to improved interpretability.</p></p class="citation"></blockquote><h3 id=3742--90237-exploring-lightweight-federated-learning-for-distributed-load-forecasting-abhishek-duttagupta-et-al-2024>(37/42 | 90/237) Exploring Lightweight Federated Learning for Distributed Load Forecasting (Abhishek Duttagupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhishek Duttagupta, Jin Zhao, Shanker Shreejith. (2024)<br><strong>Exploring Lightweight Federated Learning for Distributed Load Forecasting</strong><br><button class=copy-to-clipboard title="Exploring Lightweight Federated Learning for Distributed Load Forecasting" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03320v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03320v1.pdf filename=2404.03320v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) is a distributed learning scheme that enables deep learning to be applied to sensitive data streams and applications in a privacy-preserving manner. This paper focuses on the use of FL for analyzing smart energy meter data with the aim to achieve comparable accuracy to state-of-the-art methods for load forecasting while ensuring the privacy of individual meter data. We show that with a lightweight fully connected deep neural network, we are able to achieve forecasting accuracy comparable to existing schemes, both at each meter source and at the aggregator, by utilising the FL framework. The use of lightweight models further reduces the energy and resource consumption caused by complex deep-learning models, making this approach ideally suited for deployment across resource-constrained smart meter systems. With our proposed lightweight model, we are able to achieve an overall average load forecasting RMSE of 0.17, with the model having a negligible energy overhead of 50 mWh when performing training and inference on an Arduino Uno platform.</p></p class="citation"></blockquote><h3 id=3842--91237-eigenpruning-tomás-vergara-browne-et-al-2024>(38/42 | 91/237) Eigenpruning (Tomás Vergara-Browne et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tomás Vergara-Browne, Álvaro Soto, Akiko Aizawa. (2024)<br><strong>Eigenpruning</strong><br><button class=copy-to-clipboard title=Eigenpruning index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03147v1.pdf filename=2404.03147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce eigenpruning, a method that removes singular values from weight matrices in an <b>LLM</b> to improve its performance in a particular task. This method is inspired by interpretability methods designed to automatically find subnetworks of a model which solve a specific task. In our tests, the pruned model outperforms the original model by a large margin, while only requiring minimal computation to prune the weight matrices. In the case of a small synthetic task in integer multiplication, the Phi-2 model can improve its accuracy in the test set from 13.75% to 97.50%. Interestingly, these results seem to indicate the existence of a computation path that can solve the task very effectively, but it was not being used by the original model. Finally, we plan to open-source our implementation in the camera-ready version of our work.</p></p class="citation"></blockquote><h3 id=3942--92237-multimodal-hierarchical-multi-task-deep-learning-framework-for-jointly-predicting-and-explaining-alzheimer-disease-progression-sayantan-kumar-et-al-2024>(39/42 | 92/237) Multimodal hierarchical multi-task deep learning framework for jointly predicting and explaining Alzheimer disease progression (Sayantan Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sayantan Kumar, Sean Yu, Thomas Kannampallil, Andrew Michelson, Aristeidis Sotiras, Philip Payne. (2024)<br><strong>Multimodal hierarchical multi-task deep learning framework for jointly predicting and explaining Alzheimer disease progression</strong><br><button class=copy-to-clipboard title="Multimodal hierarchical multi-task deep learning framework for jointly predicting and explaining Alzheimer disease progression" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03208v1.pdf filename=2404.03208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Early identification of Mild Cognitive Impairment (MCI) subjects who will eventually progress to Alzheimer Disease (AD) is challenging. Existing deep learning models are mostly single-modality single-task models predicting risk of disease progression at a fixed timepoint. We proposed a <b>multimodal</b> hierarchical multi-task learning approach which can monitor the risk of disease progression at each timepoint of the visit trajectory. Longitudinal visit data from multiple modalities (MRI, cognition, and clinical data) were collected from MCI individuals of the Alzheimer Disease Neuroimaging Initiative (ADNI) dataset. Our hierarchical model predicted at every timepoint a set of neuropsychological composite cognitive function scores as auxiliary tasks and used the forecasted scores at every timepoint to predict the future risk of disease. Relevance weights for each composite function provided explanations about potential factors for disease progression. Our proposed model performed better than state-of-the-art baselines in predicting AD progression risk and the composite scores. Ablation study on the number of modalities demonstrated that imaging and cognition data contributed most towards the outcome. Model explanations at each timepoint can inform clinicians 6 months in advance the potential cognitive function decline that can lead to progression to AD in future. Our model monitored their risk of AD progression every 6 months throughout the visit trajectory of individuals. The hierarchical learning of auxiliary tasks allowed better optimization and allowed longitudinal explanations for the outcome. Our framework is flexible with the number of input modalities and the selection of auxiliary tasks and hence can be generalized to other clinical problems too.</p></p class="citation"></blockquote><h3 id=4042--93237-bishop-bi-directional-cellular-learning-for-tabular-data-with-generalized-sparse-modern-hopfield-model-chenwei-xu-et-al-2024>(40/42 | 93/237) BiSHop: Bi-Directional Cellular Learning for Tabular Data with Generalized Sparse Modern Hopfield Model (Chenwei Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenwei Xu, Yu-Chao Huang, Jerry Yao-Chieh Hu, Weijian Li, Ammar Gilani, Hsi-Sheng Goan, Han Liu. (2024)<br><strong>BiSHop: Bi-Directional Cellular Learning for Tabular Data with Generalized Sparse Modern Hopfield Model</strong><br><button class=copy-to-clipboard title="BiSHop: Bi-Directional Cellular Learning for Tabular Data with Generalized Sparse Modern Hopfield Model" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03830v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03830v1.pdf filename=2404.03830v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the \textbf{B}i-Directional \textbf{S}parse \textbf{Hop}field Network (\textbf{BiSHop}), a novel end-to-end framework for deep tabular learning. BiSHop handles the two major challenges of deep tabular learning: non-rotationally invariant data structure and feature sparsity in tabular data. Our key motivation comes from the recent established connection between associative memory and attention mechanisms. Consequently, BiSHop uses a dual-component approach, sequentially processing data both column-wise and row-wise through two interconnected directional learning modules. Computationally, these modules house layers of generalized sparse modern Hopfield layers, a sparse extension of the modern Hopfield model with adaptable sparsity. Methodologically, BiSHop facilitates multi-scale <b>representation</b> <b>learning,</b> capturing both intra-feature and inter-feature interactions, with adaptive sparsity at each scale. Empirically, through experiments on diverse real-world datasets, we demonstrate that BiSHop surpasses current SOTA methods with significantly less HPO runs, marking it a robust solution for deep tabular learning.</p></p class="citation"></blockquote><h3 id=4142--94237-leveraging-interpolation-models-and-error-bounds-for-verifiable-scientific-machine-learning-tyler-chang-et-al-2024>(41/42 | 94/237) Leveraging Interpolation Models and Error Bounds for Verifiable Scientific Machine Learning (Tyler Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tyler Chang, Andrew Gillette, Romit Maulik. (2024)<br><strong>Leveraging Interpolation Models and Error Bounds for Verifiable Scientific Machine Learning</strong><br><button class=copy-to-clipboard title="Leveraging Interpolation Models and Error Bounds for Verifiable Scientific Machine Learning" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03586v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03586v1.pdf filename=2404.03586v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective verification and validation techniques for modern scientific machine learning workflows are challenging to devise. Statistical methods are abundant and easily deployed, but often rely on speculative assumptions about the data and methods involved. Error bounds for classical interpolation techniques can provide mathematically rigorous estimates of accuracy, but often are difficult or impractical to determine computationally. In this work, we present a best-of-both-worlds approach to verifiable scientific machine learning by demonstrating that (1) multiple standard interpolation techniques have informative error bounds that can be computed or estimated efficiently; (2) comparative performance among distinct interpolants can aid in validation goals; (3) deploying interpolation methods on latent spaces generated by deep learning techniques enables some interpretability for <b>black-box</b> <b>models.</b> We present a detailed case study of our approach for predicting lift-drag ratios from airfoil images. Code developed for this work is available in a public Github repository.</p></p class="citation"></blockquote><h3 id=4242--95237-mpofi-multichannel-partially-observed-functional-modeling-for-defect-classification-with-imbalanced-dataset-via-deep-metric-learning-yukun-xie-et-al-2024>(42/42 | 95/237) MPOFI: Multichannel Partially Observed Functional Modeling for Defect Classification with Imbalanced Dataset via Deep Metric Learning (Yukun Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yukun Xie, Juan Du, Chen Zhang. (2024)<br><strong>MPOFI: Multichannel Partially Observed Functional Modeling for Defect Classification with Imbalanced Dataset via Deep Metric Learning</strong><br><button class=copy-to-clipboard title="MPOFI: Multichannel Partially Observed Functional Modeling for Defect Classification with Imbalanced Dataset via Deep Metric Learning" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP, stat-ML<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03329v1.pdf filename=2404.03329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In modern manufacturing, most of the product lines are conforming. Few products are nonconforming but with different defect types. The identification of defect types can help further root cause diagnosis of production lines. With the sensing development, continuous signals of process variables can be collected in high resolution, which can be regarded as multichannel functional data. They have abundant information to characterize the process and help identify the defect types. Motivated by a real example from the pipe tightening process, we target at detect classification when each sample is a multichannel functional data. However, the available samples for each defect type are limited and imbalanced. Moreover, the functions are partially observed since the pre-tightening process before the pipe tightening process is unobserved. To classify the defect samples based on imbalanced, multichannel, and partially observed functional data is very important but challenging. Thus, we propose an innovative framework known as &ldquo;Multichannel Partially Observed Functional Modeling for Defect Classification with an Imbalanced Dataset&rdquo; (MPOFI). The framework leverages the power of deep metric learning in conjunction with a neural network specially crafted for processing functional data. This paper introduces a neural network explicitly tailored for handling multichannel and partially observed functional data, complemented by developing a corresponding loss function for training on imbalanced datasets. The results from a real-world case study demonstrate the superior accuracy of our framework when compared to existing <b>benchmarks.</b></p></p class="citation"></blockquote><h2 id=eessas-2>eess.AS (2)</h2><h3 id=12--96237-rall-e-robust-codec-language-modeling-with-chain-of-thought-prompting-for-text-to-speech-synthesis-detai-xin-et-al-2024>(1/2 | 96/237) RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis (Detai Xin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Detai Xin, Xu Tan, Kai Shen, Zeqian Ju, Dongchao Yang, Yuancheng Wang, Shinnosuke Takamichi, Hiroshi Saruwatari, Shujie Liu, Jinyu Li, Sheng Zhao. (2024)<br><strong>RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis</strong><br><button class=copy-to-clipboard title="RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-AI, cs-CL, cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 100<br>Keywords: Rerank, Zero-shot, Transformer, Text-to-speech, Text-to-speech, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03204v1.pdf filename=2404.03204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present RALL-E, a robust language modeling method for <b>text-to-speech</b> <b>(TTS)</b> synthesis. While previous work based on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> shows impressive performance on <b>zero-shot</b> <b>TTS,</b> such methods often suffer from poor robustness, such as unstable prosody (weird pitch and rhythm/duration) and a high word error rate (WER), due to the autoregressive prediction style of language models. The core idea behind RALL-E is <b>chain-of-thought</b> <b>(CoT)</b> <b>prompting,</b> which decomposes the task into simpler steps to enhance the robustness of <b>LLM-based</b> <b>TTS.</b> To accomplish this idea, RALL-E first predicts prosody features (pitch and duration) of the input text and uses them as intermediate conditions to predict speech tokens in a CoT style. Second, RALL-E utilizes the predicted duration <b>prompt</b> to guide the computing of <b>self-attention</b> weights in <b>Transformer</b> to enforce the model to focus on the corresponding phonemes and prosody features when predicting speech tokens. Results of comprehensive objective and subjective evaluations demonstrate that, compared to a powerful baseline method VALL-E, RALL-E significantly improves the WER of <b>zero-shot</b> <b>TTS</b> from $6.3%$ (without <b>reranking)</b> and $2.1%$ (with <b>reranking)</b> to $2.8%$ and $1.0%$, respectively. Furthermore, we demonstrate that RALL-E correctly synthesizes sentences that are hard for VALL-E and reduces the error rate from $68%$ to $4%$.</p></p class="citation"></blockquote><h3 id=22--97237-interpreting-end-to-end-deep-learning-models-for-speech-source-localization-using-layer-wise-relevance-propagation-luca-comanducci-et-al-2024>(2/2 | 97/237) Interpreting End-to-End Deep Learning Models for Speech Source Localization Using Layer-wise Relevance Propagation (Luca Comanducci et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Comanducci, Fabio Antonacci, Augusto Sarti. (2024)<br><strong>Interpreting End-to-End Deep Learning Models for Speech Source Localization Using Layer-wise Relevance Propagation</strong><br><button class=copy-to-clipboard title="Interpreting End-to-End Deep Learning Models for Speech Source Localization Using Layer-wise Relevance Propagation" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03436v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03436v1.pdf filename=2404.03436v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning models are widely applied in the signal processing community, yet their inner working procedure is often treated as a <b>black</b> <b>box.</b> In this paper, we investigate the use of eXplainable Artificial Intelligence (XAI) techniques to learning-based end-to-end speech source localization models. We consider the Layer-wise Relevance Propagation (LRP) technique, which aims to determine which parts of the input are more important for the output prediction. Using LRP we analyze two state-of-the-art models, of differing architectural complexity that map audio signals acquired by the microphones to the cartesian coordinates of the source. Specifically, we inspect the relevance associated with the input features of the two models and discover that both networks denoise and de-reverberate the microphone signals to compute more accurate statistical correlations between them and consequently localize the sources. To further demonstrate this fact, we estimate the Time-Difference of Arrivals (TDoAs) via the Generalized Cross Correlation with Phase Transform (GCC-PHAT) using both microphone signals and relevance signals extracted from the two networks and show that through the latter we obtain more accurate time-delay estimation results.</p></p class="citation"></blockquote><h2 id=cscv-65>cs.CV (65)</h2><h3 id=165--98237-diverse-and-tailored-image-generation-for-zero-shot-multi-label-classification-kaixin-zhang-et-al-2024>(1/65 | 98/237) Diverse and Tailored Image Generation for Zero-shot Multi-label Classification (Kaixin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaixin Zhang, Zhixiang Yuan, Tao Huang. (2024)<br><strong>Diverse and Tailored Image Generation for Zero-shot Multi-label Classification</strong><br><button class=copy-to-clipboard title="Diverse and Tailored Image Generation for Zero-shot Multi-label Classification" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 83<br>Keywords: Diffusion Model, Fine-tuning, Fine-tuning, Multi-modal, Zero-shot, Transformer, Text2image, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03144v1.pdf filename=2404.03144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>zero-shot</b> multi-label classification has garnered considerable attention for its capacity to operate predictions on unseen labels without human annotations. Nevertheless, prevailing approaches often use seen classes as imperfect proxies for unseen ones, resulting in suboptimal performance. Drawing inspiration from the success of <b>text-to-image</b> generation models in producing realistic images, we propose an innovative solution: generating synthetic data to construct a training set explicitly tailored for proxyless training on unseen labels. Our approach introduces a novel image generation framework that produces multi-label synthetic images of unseen classes for classifier training. To enhance diversity in the generated images, we leverage a pre-trained <b>large</b> <b>language</b> <b>model</b> to generate diverse <b>prompts.</b> Employing a pre-trained <b>multi-modal</b> CLIP model as a discriminator, we assess whether the generated images accurately represent the target classes. This enables automatic filtering of inaccurately generated images, preserving classifier accuracy. To refine text <b>prompts</b> for more precise and effective multi-label object generation, we introduce a CLIP score-based discriminative loss to <b>fine-tune</b> the text encoder in the <b>diffusion</b> <b>model.</b> Additionally, to enhance visual features on the target task while maintaining the generalization of original features and mitigating catastrophic forgetting resulting from <b>fine-tuning</b> the entire visual encoder, we propose a feature fusion module inspired by <b>transformer</b> attention mechanisms. This module aids in capturing global dependencies between multiple objects more effectively. Extensive experimental results validate the effectiveness of our approach, demonstrating significant improvements over state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=265--99237-tinyvqa-compact-multimodal-deep-neural-network-for-visual-question-answering-on-resource-constrained-devices-hasib-al-rashid-et-al-2024>(2/65 | 99/237) TinyVQA: Compact Multimodal Deep Neural Network for Visual Question Answering on Resource-Constrained Devices (Hasib-Al Rashid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hasib-Al Rashid, Argho Sarkar, Aryya Gangopadhyay, Maryam Rahnemoonfar, Tinoosh Mohsenin. (2024)<br><strong>TinyVQA: Compact Multimodal Deep Neural Network for Visual Question Answering on Resource-Constrained Devices</strong><br><button class=copy-to-clipboard title="TinyVQA: Compact Multimodal Deep Neural Network for Visual Question Answering on Resource-Constrained Devices" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 76<br>Keywords: Knowledge Distillation, Multi-modal, Multi-modal, Quantization, Supervised Learning, Question Answering, Visual Question Answering, Visual Question Answering, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03574v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03574v1.pdf filename=2404.03574v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional machine learning models often require powerful hardware, making them unsuitable for deployment on resource-limited devices. Tiny Machine Learning (tinyML) has emerged as a promising approach for running machine learning models on these devices, but integrating multiple data modalities into tinyML models still remains a challenge due to increased complexity, latency, and power consumption. This paper proposes TinyVQA, a novel <b>multimodal</b> deep neural network for <b>visual</b> <b>question</b> <b>answering</b> tasks that can be deployed on resource-constrained tinyML hardware. TinyVQA leverages a <b>supervised</b> attention-based model to learn how to answer <b>questions</b> <b>about</b> images using both vision and language modalities. <b>Distilled</b> knowledge from the <b>supervised</b> attention-based <b>VQA</b> model trains the memory aware compact TinyVQA model and low bit-width <b>quantization</b> technique is employed to further compress the model for deployment on tinyML devices. The TinyVQA model was evaluated on the FloodNet dataset, which is used for post-disaster damage assessment. The compact model achieved an accuracy of 79.5%, demonstrating the effectiveness of TinyVQA for real-world applications. Additionally, the model was deployed on a Crazyflie 2.0 drone, equipped with an AI deck and GAP8 microprocessor. The TinyVQA model achieved low latencies of 56 ms and consumes 693 mW power while deployed on the tiny drone, showcasing its suitability for resource-constrained embedded systems.</p></p class="citation"></blockquote><h3 id=365--100237-comat-aligning-text-to-image-diffusion-model-with-image-to-text-concept-matching-dongzhi-jiang-et-al-2024>(3/65 | 100/237) CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching (Dongzhi Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu, Hongsheng Li. (2024)<br><strong>CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching</strong><br><button class=copy-to-clipboard title="CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Diffusion Model, Benchmarking, Fine-tuning, Fine-tuning, Image2text, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03653v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03653v1.pdf filename=2404.03653v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have demonstrated great success in the field of <b>text-to-image</b> generation. However, alleviating the misalignment between the text <b>prompts</b> and images is still challenging. The root reason behind the misalignment has not been extensively investigated. We observe that the misalignment is caused by inadequate token attention activation. We further attribute this phenomenon to the <b>diffusion</b> <b>model&rsquo;s</b> insufficient condition utilization, which is caused by its training paradigm. To address the issue, we propose CoMat, an end-to-end <b>diffusion</b> <b>model</b> <b>fine-tuning</b> strategy with an <b>image-to-text</b> concept matching mechanism. We leverage an image captioning model to measure <b>image-to-text</b> alignment and guide the <b>diffusion</b> <b>model</b> to revisit ignored tokens. A novel attribute concentration module is also proposed to address the attribute binding problem. Without any image or human preference data, we use only 20K text <b>prompts</b> to <b>fine-tune</b> SDXL to obtain CoMat-SDXL. Extensive experiments show that CoMat-SDXL significantly outperforms the baseline model SDXL in two <b>text-to-image</b> alignment <b>benchmarks</b> and achieves start-of-the-art performance.</p></p class="citation"></blockquote><h3 id=465--101237-devide-faceted-medical-knowledge-for-improved-medical-vision-language-pre-training-haozhe-luo-et-al-2024>(4/65 | 101/237) DeViDe: Faceted medical knowledge for improved medical vision-language pre-training (Haozhe Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haozhe Luo, Ziyu Zhou, Corentin Royer, Anjany Sekuboyina, Bjoern Menze. (2024)<br><strong>DeViDe: Faceted medical knowledge for improved medical vision-language pre-training</strong><br><button class=copy-to-clipboard title="DeViDe: Faceted medical knowledge for improved medical vision-language pre-training" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Fine-tuning, Supervised Learning, Zero-shot, Transformer, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03618v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03618v1.pdf filename=2404.03618v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-language</b> pre-training for chest X-rays has made significant strides, primarily by utilizing paired radiographs and radiology reports. However, existing approaches often face challenges in encoding medical knowledge effectively. While radiology reports provide insights into the current disease manifestation, medical definitions (as used by contemporary methods) tend to be overly abstract, creating a gap in knowledge. To address this, we propose DeViDe, a novel <b>transformer-based</b> method that leverages radiographic descriptions from the open web. These descriptions outline general visual characteristics of diseases in radiographs, and when combined with abstract definitions and radiology reports, provide a holistic snapshot of knowledge. DeViDe incorporates three key features for knowledge-augmented vision language alignment: First, a <b>large-language</b> <b>model-based</b> <b>augmentation</b> is employed to homogenise medical knowledge from diverse sources. Second, this knowledge is aligned with image information at various levels of granularity. Third, a novel projection layer is proposed to handle the complexity of aligning each image with multiple descriptions arising in a multi-label setting. In <b>zero-shot</b> settings, DeViDe performs comparably to fully <b>supervised</b> models on external datasets and achieves state-of-the-art results on three <b>large-scale</b> <b>datasets.</b> <b>Additionally,</b> <b>fine-tuning</b> DeViDe on four downstream tasks and six segmentation tasks showcases its superior performance across data from diverse distributions.</p></p class="citation"></blockquote><h3 id=565--102237-performance-of-computer-vision-algorithms-for-fine-grained-classification-using-crowdsourced-insect-images-rita-pucci-et-al-2024>(5/65 | 102/237) Performance of computer vision algorithms for fine-grained classification using crowdsourced insect images (Rita Pucci et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rita Pucci, Vincent J. Kalkman, Dan Stowell. (2024)<br><strong>Performance of computer vision algorithms for fine-grained classification using crowdsourced insect images</strong><br><button class=copy-to-clipboard title="Performance of computer vision algorithms for fine-grained classification using crowdsourced insect images" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03474v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03474v1.pdf filename=2404.03474v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With fine-grained classification, we identify unique characteristics to distinguish among classes of the same super-class. We are focusing on species recognition in Insecta, as they are critical for biodiversity monitoring and at the base of many ecosystems. With citizen science campaigns, billions of images are collected in the wild. Once these are labelled, experts can use them to create distribution maps. However, the labelling process is time-consuming, which is where computer <b>vision</b> <b>comes</b> in. The field of computer <b>vision</b> <b>offers</b> a wide range of algorithms, each with its strengths and weaknesses; how do we identify the algorithm that is in line with our application? To answer this question, we provide a full and detailed evaluation of nine algorithms among deep <b>convolutional</b> <b>networks</b> <b>(CNN),</b> <b>vision</b> <b>transformers</b> (ViT), and locality-based <b>vision</b> <b>transformers</b> (LBVT) on 4 different aspects: classification performance, embedding quality, computational cost, and gradient activity. We offer insights that we haven&rsquo;t yet had in this domain proving to which extent these algorithms solve the fine-grained tasks in Insecta. We found that the ViT performs the best on inference speed and computational cost while the LBVT outperforms the others on performance and embedding quality; the <b>CNN</b> provide a trade-off among the metrics.</p></p class="citation"></blockquote><h3 id=665--103237-meta-invariance-defense-towards-generalizable-robustness-to-unknown-adversarial-attacks-lei-zhang-et-al-2024>(6/65 | 103/237) Meta Invariance Defense Towards Generalizable Robustness to Unknown Adversarial Attacks (Lei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Zhang, Yuhang Zhou, Yi Yang, Xinbo Gao. (2024)<br><strong>Meta Invariance Defense Towards Generalizable Robustness to Unknown Adversarial Attacks</strong><br><button class=copy-to-clipboard title="Meta Invariance Defense Towards Generalizable Robustness to Unknown Adversarial Attacks" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CR, cs-CV, cs-LG, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Knowledge Distillation, Supervised Learning, Zero-shot, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03340v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03340v1.pdf filename=2404.03340v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite providing high-performance solutions for computer vision tasks, the deep neural network (DNN) model has been proved to be extremely vulnerable to <b>adversarial</b> <b>attacks.</b> Current defense mainly focuses on the known attacks, but the <b>adversarial</b> <b>robustness</b> to the unknown attacks is seriously overlooked. Besides, commonly used adaptive learning and <b>fine-tuning</b> technique is unsuitable for <b>adversarial</b> <b>defense</b> since it is essentially a <b>zero-shot</b> problem when deployed. Thus, to tackle this challenge, we propose an attack-agnostic defense method named Meta Invariance Defense (MID). Specifically, various combinations of <b>adversarial</b> <b>attacks</b> are randomly sampled from a manually constructed Attacker Pool to constitute different defense tasks against unknown attacks, in which a student encoder is <b>supervised</b> by multi-consistency <b>distillation</b> to learn the attack-invariant features via a meta principle. The proposed MID has two merits: 1) Full <b>distillation</b> from pixel-, feature- and prediction-level between benign and <b>adversarial</b> <b>samples</b> facilitates the discovery of attack-invariance. 2) The model simultaneously achieves robustness to the imperceptible <b>adversarial</b> <b>perturbations</b> in high-level image classification and attack-suppression in low-level robust image regeneration. Theoretical and empirical studies on numerous <b>benchmarks</b> such as ImageNet verify the generalizable robustness and superiority of MID under various attacks.</p></p class="citation"></blockquote><h3 id=765--104237-factual-a-novel-framework-for-contrastive-learning-based-robust-sar-image-classification-xu-wang-et-al-2024>(7/65 | 104/237) FACTUAL: A Novel Framework for Contrastive Learning Based Robust SAR Image Classification (Xu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Wang, Tian Ye, Rajgopal Kannan, Viktor Prasanna. (2024)<br><strong>FACTUAL: A Novel Framework for Contrastive Learning Based Robust SAR Image Classification</strong><br><button class=copy-to-clipboard title="FACTUAL: A Novel Framework for Contrastive Learning Based Robust SAR Image Classification" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 53<br>Keywords: Adversarial Learning, Clustering, Contrastive Learning, Fine-tuning, Supervised Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03225v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03225v1.pdf filename=2404.03225v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Learning (DL) Models for Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR), while delivering improved performance, have been shown to be quite vulnerable to <b>adversarial</b> <b>attacks.</b> Existing works improve robustness by training models on <b>adversarial</b> <b>samples.</b> However, by focusing mostly on attacks that manipulate images randomly, they neglect the real-world feasibility of such attacks. In this paper, we propose FACTUAL, a novel <b>Contrastive</b> <b>Learning</b> framework for <b>Adversarial</b> <b>Training</b> and robust SAR classification. FACTUAL consists of two components: (1) Differing from existing works, a novel perturbation scheme that incorporates realistic physical <b>adversarial</b> <b>attacks</b> (such as OTSA) to build a <b>supervised</b> <b>adversarial</b> <b>pre-training</b> network. This network utilizes class labels for <b>clustering</b> clean and perturbed images together into a more informative feature space. (2) A linear classifier cascaded after the encoder to use the computed representations to predict the target labels. By pre-training and <b>fine-tuning</b> our model on both clean and <b>adversarial</b> <b>samples,</b> we show that our model achieves high prediction accuracy on both cases. Our model achieves 99.7% accuracy on clean samples, and 89.6% on perturbed samples, both outperforming previous state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=865--105237-longvlm-efficient-long-video-understanding-via-large-language-models-yuetian-weng-et-al-2024>(8/65 | 105/237) LongVLM: Efficient Long Video Understanding via Large Language Models (Yuetian Weng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, Bohan Zhuang. (2024)<br><strong>LongVLM: Efficient Long Video Understanding via Large Language Models</strong><br><button class=copy-to-clipboard title="LongVLM: Efficient Long Video Understanding via Large Language Models" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Zero-shot, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03384v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03384v1.pdf filename=2404.03384v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Empowered by <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> recent advancements in VideoLLMs have driven progress in various video understanding tasks. These models encode video representations through pooling or query aggregation over a vast number of visual tokens, making computational and memory costs affordable. Despite successfully providing an overall comprehension of video content, existing VideoLLMs still face challenges in achieving detailed understanding in videos due to overlooking local information in long-term videos. To tackle this challenge, we introduce LongVLM, a straightforward yet powerful VideoLLM for long video understanding, building upon the observation that long videos often consist of sequential key events, complex actions, and camera movements. Our approach proposes to decompose long videos into multiple short-term segments and encode local features for each local segment via a hierarchical token merging module. These features are concatenated in temporal order to maintain the storyline across sequential short-term segments. Additionally, we propose to integrate global semantics into each local feature to enhance context understanding. In this way, we encode video representations that incorporate both local and global information, enabling the <b>LLM</b> to generate comprehensive responses for long-term videos. Experimental results on the VideoChatGPT <b>benchmark</b> and <b>zero-shot</b> video <b>question-answering</b> <b>datasets</b> demonstrate the superior capabilities of our model over the previous state-of-the-art methods. Qualitative examples demonstrate that our model produces more precise responses for long videos understanding. Code is available at \url{https://github.com/ziplab/LongVLM}.</p></p class="citation"></blockquote><h3 id=965--106237-lcm-lookahead-for-encoder-based-text-to-image-personalization-rinon-gal-et-al-2024>(9/65 | 106/237) LCM-Lookahead for Encoder-based Text-to-Image Personalization (Rinon Gal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rinon Gal, Or Lichter, Elad Richardson, Or Patashnik, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or. (2024)<br><strong>LCM-Lookahead for Encoder-based Text-to-Image Personalization</strong><br><button class=copy-to-clipboard title="LCM-Lookahead for Encoder-based Text-to-Image Personalization" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Knowledge Distillation, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03620v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03620v1.pdf filename=2404.03620v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>diffusion</b> <b>models</b> have introduced fast sampling methods that can effectively produce high-quality images in just one or a few denoising steps. Interestingly, when these are <b>distilled</b> from existing <b>diffusion</b> <b>models,</b> they often maintain alignment with the original model, retaining similar outputs for similar <b>prompts</b> and seeds. These properties present opportunities to leverage fast sampling methods as a shortcut-mechanism, using them to create a preview of denoised outputs through which we can backpropagate image-space losses. In this work, we explore the potential of using such shortcut-mechanisms to guide the personalization of <b>text-to-image</b> models to specific facial identities. We focus on encoder-based personalization approaches, and demonstrate that by tuning them with a lookahead identity loss, we can achieve higher identity fidelity, without sacrificing layout diversity or <b>prompt</b> alignment. We further explore the use of attention sharing mechanisms and consistent data generation for the task of personalization, and find that encoder training can benefit from both.</p></p class="citation"></blockquote><h3 id=1065--107237-insectmamba-insect-pest-classification-with-state-space-model-qianning-wang-et-al-2024>(10/65 | 107/237) InsectMamba: Insect Pest Classification with State Space Model (Qianning Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianning Wang, Chenglin Wang, Zhixin Lai, Yucheng Zhou. (2024)<br><strong>InsectMamba: Insect Pest Classification with State Space Model</strong><br><button class=copy-to-clipboard title="InsectMamba: Insect Pest Classification with State Space Model" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03611v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03611v1.pdf filename=2404.03611v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The classification of insect pests is a critical task in agricultural technology, vital for ensuring food security and environmental sustainability. However, the complexity of pest identification, due to factors like high camouflage and species diversity, poses significant obstacles. Existing methods struggle with the fine-grained feature extraction needed to distinguish between closely related pest species. Although recent advancements have utilized modified network structures and combined deep learning approaches to improve accuracy, challenges persist due to the similarity between pests and their surroundings. To address this problem, we introduce InsectMamba, a novel approach that integrates State Space Models (SSMs), <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs),</b> Multi-Head <b>Self-Attention</b> mechanism (MSA), and Multilayer Perceptrons (MLPs) within Mix-SSM blocks. This integration facilitates the extraction of comprehensive visual features by leveraging the strengths of each encoding strategy. A selective module is also proposed to adaptively aggregate these features, enhancing the model&rsquo;s ability to discern pest characteristics. InsectMamba was evaluated against strong competitors across five insect pest classification datasets. The results demonstrate its superior performance and verify the significance of each model component by an ablation study.</p></p class="citation"></blockquote><h3 id=1165--108237-background-noise-reduction-of-attention-map-for-weakly-supervised-semantic-segmentation-izumi-fujimori-et-al-2024>(11/65 | 108/237) Background Noise Reduction of Attention Map for Weakly Supervised Semantic Segmentation (Izumi Fujimori et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Izumi Fujimori, Masaki Oono, Masami Shishibori. (2024)<br><strong>Background Noise Reduction of Attention Map for Weakly Supervised Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Background Noise Reduction of Attention Map for Weakly Supervised Semantic Segmentation" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolutional Neural Network, Supervised Learning, Weakly-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03394v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03394v1.pdf filename=2404.03394v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>weakly-supervised</b> semantic segmentation (WSSS) using only image-level class labels, a problem with <b>CNN-based</b> Class Activation Maps (CAM) is that they tend to activate the most discriminative local regions of objects. On the other hand, methods based on <b>Transformers</b> learn global features but suffer from the issue of background noise contamination. This paper focuses on addressing the issue of background noise in attention weights within the existing WSSS method based on Conformer, known as TransCAM. The proposed method successfully reduces background noise, leading to improved accuracy of pseudo labels. Experimental results demonstrate that our model achieves segmentation performance of 70.5% on the PASCAL VOC 2012 validation data, 71.1% on the test data, and 45.9% on MS COCO 2014 data, outperforming TransCAM in terms of segmentation performance.</p></p class="citation"></blockquote><h3 id=1265--109237-legrad-an-explainability-method-for-vision-transformers-via-feature-formation-sensitivity-walid-bousselham-et-al-2024>(12/65 | 109/237) LeGrad: An Explainability Method for Vision Transformers via Feature Formation Sensitivity (Walid Bousselham et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Walid Bousselham, Angie Boggust, Sofian Chaybouti, Hendrik Strobelt, Hilde Kuehne. (2024)<br><strong>LeGrad: An Explainability Method for Vision Transformers via Feature Formation Sensitivity</strong><br><button class=copy-to-clipboard title="LeGrad: An Explainability Method for Vision Transformers via Feature Formation Sensitivity" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Vision Transformer, Transformer, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03214v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03214v1.pdf filename=2404.03214v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision</b> <b>Transformers</b> (ViTs), with their ability to model long-range dependencies through <b>self-attention</b> mechanisms, have become a standard architecture in computer <b>vision.</b> <b>However,</b> the interpretability of these models remains a challenge. To address this, we propose LeGrad, an explainability method specifically designed for ViTs. LeGrad computes the gradient with respect to the attention maps of ViT layers, considering the gradient itself as the explainability signal. We aggregate the signal over all layers, combining the activations of the last as well as intermediate tokens to produce the merged explainability map. This makes LeGrad a conceptually simple and an easy-to-implement tool for enhancing the transparency of ViTs. We evaluate LeGrad in challenging segmentation, perturbation, and open-vocabulary settings, showcasing its versatility compared to other SotA explainability methods demonstrating its superior spatial fidelity and robustness to perturbations. A demo and the code is available at <a href=https://github.com/WalBouss/LeGrad>https://github.com/WalBouss/LeGrad</a>.</p></p class="citation"></blockquote><h3 id=1365--110237-minigpt4-video-advancing-multimodal-llms-for-video-understanding-with-interleaved-visual-textual-tokens-kirolos-ataallah-et-al-2024>(13/65 | 110/237) MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens (Kirolos Ataallah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, Mohamed Elhoseiny. (2024)<br><strong>MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens</strong><br><button class=copy-to-clipboard title="MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Image2text, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03413v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03413v1.pdf filename=2404.03413v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces MiniGPT4-Video, a <b>multimodal</b> <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> designed specifically for video understanding. The model is capable of processing both temporal visual and textual data, making it adept at understanding the complexities of videos. Building upon the success of MiniGPT-v2, which excelled in translating visual features into the <b>LLM</b> space for single images and achieved impressive results on various <b>image-text</b> <b>benchmarks,</b> this paper extends the model&rsquo;s capabilities to process a sequence of frames, enabling it to comprehend videos. MiniGPT4-video does not only consider visual content but also incorporates textual conversations, allowing the model to effectively answer queries involving both visual and text components. The proposed model outperforms existing state-of-the-art methods, registering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF, and TVQA <b>benchmarks</b> respectively. Our models and code have been made publicly available here <a href=https://vision-cair.github.io/MiniGPT4-video/>https://vision-cair.github.io/MiniGPT4-video/</a></p></p class="citation"></blockquote><h3 id=1465--111237-know-your-neighbors-improving-single-view-reconstruction-via-spatial-vision-language-reasoning-rui-li-et-al-2024>(14/65 | 111/237) Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning (Rui Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Li, Tobias Fischer, Mattia Segu, Marc Pollefeys, Luc Van Gool, Federico Tombari. (2024)<br><strong>Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning</strong><br><button class=copy-to-clipboard title="Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: Geometry, Zero-shot, Reasoning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03658v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03658v1.pdf filename=2404.03658v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recovering the 3D scene <b>geometry</b> from a single view is a fundamental yet ill-posed problem in computer vision. While classical depth estimation methods infer only a 2.5D scene representation limited to the image plane, recent approaches based on radiance fields reconstruct a full 3D representation. However, these methods still struggle with occluded regions since inferring <b>geometry</b> without visual observation requires (i) semantic knowledge of the surroundings, and (ii) <b>reasoning</b> about spatial context. We propose KYN, a novel method for single-view scene reconstruction that reasons about semantic and spatial context to predict each point&rsquo;s density. We introduce a <b>vision-language</b> modulation module to enrich point features with fine-grained semantic information. We aggregate point representations across the scene through a language-guided spatial attention mechanism to yield per-point density predictions aware of the 3D semantic context. We show that KYN improves 3D shape recovery compared to predicting density for each 3D point in isolation. We achieve state-of-the-art results in scene and object reconstruction on KITTI-360, and show improved <b>zero-shot</b> generalization compared to prior work. Project page: <a href=https://ruili3.github.io/kyn>https://ruili3.github.io/kyn</a>.</p></p class="citation"></blockquote><h3 id=1565--112237-language-guided-instance-aware-domain-adaptive-panoptic-segmentation-elham-amin-mansour-et-al-2024>(15/65 | 112/237) Language-Guided Instance-Aware Domain-Adaptive Panoptic Segmentation (Elham Amin Mansour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elham Amin Mansour, Ozan Unal, Suman Saha, Benjamin Bejar, Luc Van Gool. (2024)<br><strong>Language-Guided Instance-Aware Domain-Adaptive Panoptic Segmentation</strong><br><button class=copy-to-clipboard title="Language-Guided Instance-Aware Domain-Adaptive Panoptic Segmentation" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Unsupervised Learning, Domain Adaptation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03799v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03799v1.pdf filename=2404.03799v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing relevance of panoptic segmentation is tied to the advancements in autonomous driving and AR/VR applications. However, the deployment of such models has been limited due to the expensive nature of dense data annotation, giving rise to <b>unsupervised</b> <b>domain</b> <b>adaptation</b> (UDA). A key challenge in panoptic UDA is reducing the <b>domain</b> <b>gap</b> between a labeled source and an unlabeled target <b>domain</b> <b>while</b> harmonizing the subtasks of semantic and instance segmentation to limit catastrophic interference. While considerable progress has been achieved, existing approaches mainly focus on the adaptation of semantic segmentation. In this work, we focus on incorporating instance-level adaptation via a novel instance-aware cross-domain mixing strategy IMix. IMix significantly enhances the panoptic quality by improving instance segmentation performance. Specifically, we propose inserting high-confidence predicted instances from the target <b>domain</b> <b>onto</b> source images, retaining the exhaustiveness of the resulting pseudo-labels while reducing the injected confirmation bias. Nevertheless, such an enhancement comes at the cost of degraded semantic performance, attributed to catastrophic forgetting. To mitigate this issue, we regularize our semantic branch by employing CLIP-based <b>domain</b> <b>alignment</b> (CDA), exploiting the <b>domain-robustness</b> <b>of</b> natural language <b>prompts.</b> Finally, we present an end-to-end model incorporating these two mechanisms called LIDAPS, achieving state-of-the-art results on all popular panoptic UDA <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1665--113237-scaling-up-video-summarization-pretraining-with-large-language-models-dawit-mureja-argaw-et-al-2024>(16/65 | 113/237) Scaling Up Video Summarization Pretraining with Large Language Models (Dawit Mureja Argaw et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dawit Mureja Argaw, Seunghyun Yoon, Fabian Caba Heilbron, Hanieh Deilamsalehy, Trung Bui, Zhaowen Wang, Franck Dernoncourt, Joon Son Chung. (2024)<br><strong>Scaling Up Video Summarization Pretraining with Large Language Models</strong><br><button class=copy-to-clipboard title="Scaling Up Video Summarization Pretraining with Large Language Models" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03398v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03398v1.pdf filename=2404.03398v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Long-form video content constitutes a significant portion of internet traffic, making automated video <b>summarization</b> an essential research problem. However, existing video <b>summarization</b> datasets are notably limited in their size, constraining the effectiveness of state-of-the-art methods for generalization. Our work aims to overcome this limitation by capitalizing on the abundance of long-form videos with dense speech-to-video alignment and the remarkable capabilities of recent <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in summarizing long text. We introduce an automated and scalable pipeline for generating a <b>large-scale</b> <b>video</b> <b>summarization</b> dataset using <b>LLMs</b> as Oracle summarizers. By leveraging the generated dataset, we analyze the limitations of existing approaches and propose a new video <b>summarization</b> model that effectively addresses them. To facilitate further research in the field, our work also presents a new <b>benchmark</b> dataset that contains 1200 long videos each with high-quality summaries annotated by professionals. Extensive experiments clearly indicate that our proposed approach sets a new state-of-the-art in video <b>summarization</b> across several <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1765--114237-learning-transferable-negative-prompts-for-out-of-distribution-detection-tianqi-li-et-al-2024>(17/65 | 114/237) Learning Transferable Negative Prompts for Out-of-Distribution Detection (Tianqi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianqi Li, Guansong Pang, Xiao Bai, Wenjun Miao, Jin Zheng. (2024)<br><strong>Learning Transferable Negative Prompts for Out-of-Distribution Detection</strong><br><button class=copy-to-clipboard title="Learning Transferable Negative Prompts for Out-of-Distribution Detection" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Out-of-distribution, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03248v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03248v1.pdf filename=2404.03248v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>prompt</b> <b>learning</b> methods have shown certain capabilities in <b>Out-of-Distribution</b> (OOD) detection, but the lack of OOD images in the target dataset in their training can lead to mismatches between OOD images and In-Distribution (ID) categories, resulting in a high false positive rate. To address this issue, we introduce a novel OOD detection method, named &lsquo;NegPrompt&rsquo;, to learn a set of negative <b>prompts,</b> <b>each</b> representing a negative connotation of a given class label, for delineating the boundaries between ID and OOD images. It learns such negative <b>prompts</b> <b>with</b> ID data only, without any reliance on external outlier data. Further, current methods assume the availability of samples of all ID classes, rendering them ineffective in open-vocabulary learning scenarios where the inference stage can contain novel ID classes not present during training. In contrast, our learned negative <b>prompts</b> <b>are</b> transferable to novel class labels. Experiments on various ImageNet <b>benchmarks</b> show that NegPrompt surpasses state-of-the-art <b>prompt-learning-based</b> <b>OOD</b> detection methods and maintains a consistent lead in hard OOD detection in closed- and open-vocabulary classification scenarios. Code is available at <a href=https://github.com/mala-lab/negprompt>https://github.com/mala-lab/negprompt</a>.</p></p class="citation"></blockquote><h3 id=1865--115237-paris3d-reasoning-based-3d-part-segmentation-using-large-multimodal-model-amrin-kareem-et-al-2024>(18/65 | 115/237) PARIS3D: Reasoning-based 3D Part Segmentation Using Large Multimodal Model (Amrin Kareem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amrin Kareem, Jean Lahoud, Hisham Cholakkal. (2024)<br><strong>PARIS3D: Reasoning-based 3D Part Segmentation Using Large Multimodal Model</strong><br><button class=copy-to-clipboard title="PARIS3D: Reasoning-based 3D Part Segmentation Using Large Multimodal Model" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 32<br>Keywords: Benchmarking, Benchmarking, Multi-modal, Multi-modal, Natural Language Explanation, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03836v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03836v1.pdf filename=2404.03836v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in 3D perception systems have significantly improved their ability to perform visual recognition tasks such as segmentation. However, these systems still heavily rely on explicit human instruction to identify target objects or categories, lacking the capability to actively reason and comprehend implicit user intentions. We introduce a novel segmentation task known as <b>reasoning</b> part segmentation for 3D objects, aiming to output a segmentation mask based on complex and implicit textual queries about specific parts of a 3D object. To facilitate evaluation and <b>benchmarking,</b> we present a large 3D dataset comprising over 60k instructions paired with corresponding ground-truth part segmentation annotations specifically curated for <b>reasoning-based</b> 3D part segmentation. We propose a model that is capable of segmenting parts of 3D objects based on implicit textual queries and generating <b>natural</b> <b>language</b> <b>explanations</b> corresponding to 3D object segmentation requests. Experiments show that our method achieves competitive performance to models that use explicit queries, with the additional abilities to identify part concepts, reason about them, and complement them with world knowledge. Our source code, dataset, and trained models are available at <a href=https://github.com/AmrinKareem/PARIS3D>https://github.com/AmrinKareem/PARIS3D</a>.</p></p class="citation"></blockquote><h3 id=1965--116237-semgrasp-semantic-grasp-generation-via-language-aligned-discretization-kailin-li-et-al-2024>(19/65 | 116/237) SemGrasp: Semantic Grasp Generation via Language Aligned Discretization (Kailin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kailin Li, Jingbo Wang, Lixin Yang, Cewu Lu, Bo Dai. (2024)<br><strong>SemGrasp: Semantic Grasp Generation via Language Aligned Discretization</strong><br><button class=copy-to-clipboard title="SemGrasp: Semantic Grasp Generation via Language Aligned Discretization" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 31<br>Keywords: Fine-tuning, Geometry, Multi-modal, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03590v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03590v1.pdf filename=2404.03590v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating natural human grasps necessitates consideration of not just object <b>geometry</b> but also semantic information. Solely depending on object shape for grasp generation confines the applications of prior methods in downstream tasks. This paper presents a novel semantic-based grasp generation method, termed SemGrasp, which generates a static human grasp pose by incorporating semantic information into the grasp representation. We introduce a discrete representation that aligns the grasp space with semantic space, enabling the generation of grasp postures in accordance with language instructions. A <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Model</b> (MLLM) is subsequently <b>fine-tuned,</b> integrating object, grasp, and language within a unified semantic space. To facilitate the training of SemGrasp, we have compiled a <b>large-scale,</b> <b>grasp-text-aligned</b> <b>dataset</b> named CapGrasp, featuring about 260k detailed captions and 50k diverse grasps. Experimental findings demonstrate that SemGrasp efficiently generates natural human grasps in alignment with linguistic intentions. Our code, models, and dataset are available publicly at: <a href=https://kailinli.github.io/SemGrasp>https://kailinli.github.io/SemGrasp</a>.</p></p class="citation"></blockquote><h3 id=2065--117237-robust-concept-erasure-using-task-vectors-minh-pham-et-al-2024>(20/65 | 117/237) Robust Concept Erasure Using Task Vectors (Minh Pham et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minh Pham, Kelly O. Marshall, Chinmay Hegde, Niv Cohen. (2024)<br><strong>Robust Concept Erasure Using Task Vectors</strong><br><button class=copy-to-clipboard title="Robust Concept Erasure Using Task Vectors" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Text2image, Prompt, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03631v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03631v1.pdf filename=2404.03631v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid growth of <b>text-to-image</b> models, a variety of techniques have been suggested to prevent undesirable image generations. Yet, these methods often only protect against specific user <b>prompts</b> and have been shown to allow unsafe generations with other inputs. Here we focus on unconditionally erasing a concept from a <b>text-to-image</b> model rather than conditioning the erasure on the user&rsquo;s <b>prompt.</b> We first show that compared to input-dependent erasure methods, concept erasure that uses Task Vectors (TV) is more robust to unexpected user inputs, not seen during training. However, TV-based erasure can also affect the core performance of the edited model, particularly when the required edit strength is unknown. To this end, we propose a method called Diverse Inversion, which we use to estimate the required strength of the TV edit. Diverse Inversion finds within the model input space a large set of <b>word</b> <b>embeddings,</b> each of which induces the generation of the target concept. We find that encouraging diversity in the set makes our estimation more robust to unexpected <b>prompts.</b> Finally, we show that Diverse Inversion enables us to apply a TV edit only to a subset of the model weights, enhancing the erasure capabilities while better maintaining the core functionality of the model.</p></p class="citation"></blockquote><h3 id=2165--118237-hapnet-toward-superior-rgb-thermal-scene-parsing-via-hybrid-asymmetric-and-progressive-heterogeneous-feature-fusion-jiahang-li-et-al-2024>(21/65 | 118/237) HAPNet: Toward Superior RGB-Thermal Scene Parsing via Hybrid, Asymmetric, and Progressive Heterogeneous Feature Fusion (Jiahang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahang Li, Peng Yun, Qijun Chen, Rui Fan. (2024)<br><strong>HAPNet: Toward Superior RGB-Thermal Scene Parsing via Hybrid, Asymmetric, and Progressive Heterogeneous Feature Fusion</strong><br><button class=copy-to-clipboard title="HAPNet: Toward Superior RGB-Thermal Scene Parsing via Hybrid, Asymmetric, and Progressive Heterogeneous Feature Fusion" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03527v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03527v1.pdf filename=2404.03527v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data-fusion networks have shown significant promise for RGB-thermal scene parsing. However, the majority of existing studies have relied on symmetric duplex encoders for heterogeneous feature extraction and fusion, paying inadequate attention to the inherent differences between RGB and thermal modalities. Recent progress in vision <b>foundation</b> <b>models</b> (VFMs) trained through self-supervision on vast amounts of unlabeled data has proven their ability to extract informative, general-purpose features. However, this potential has yet to be fully leveraged in the domain. In this study, we take one step toward this new research area by exploring a feasible strategy to fully exploit VFM features for RGB-thermal scene parsing. Specifically, we delve deeper into the unique characteristics of RGB and thermal modalities, thereby designing a hybrid, asymmetric encoder that incorporates both a VFM and a <b>convolutional</b> <b>neural</b> <b>network.</b> This design allows for more effective extraction of complementary heterogeneous features, which are subsequently fused in a dual-path, progressive manner. Moreover, we introduce an auxiliary task to further enrich the local semantics of the fused features, thereby improving the overall performance of RGB-thermal scene parsing. Our proposed HAPNet, equipped with all these components, demonstrates superior performance compared to all other state-of-the-art RGB-thermal scene parsing networks, achieving top ranks across three widely used public RGB-thermal scene parsing datasets. We believe this new paradigm has opened up new opportunities for future developments in data-fusion scene parsing approaches.</p></p class="citation"></blockquote><h3 id=2265--119237-towards-automated-movie-trailer-generation-dawit-mureja-argaw-et-al-2024>(22/65 | 119/237) Towards Automated Movie Trailer Generation (Dawit Mureja Argaw et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dawit Mureja Argaw, Mattia Soldan, Alejandro Pardo, Chen Zhao, Fabian Caba Heilbron, Joon Son Chung, Bernard Ghanem. (2024)<br><strong>Towards Automated Movie Trailer Generation</strong><br><button class=copy-to-clipboard title="Towards Automated Movie Trailer Generation" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Transformer, Neural Machine Translation, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03477v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03477v1.pdf filename=2404.03477v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Movie trailers are an essential tool for promoting films and attracting audiences. However, the process of creating trailers can be time-consuming and expensive. To streamline this process, we propose an automatic trailer generation framework that generates plausible trailers from a full movie by automating shot selection and composition. Our approach draws inspiration from <b>machine</b> <b>translation</b> techniques and models the movies and trailers as sequences of shots, thus formulating the trailer generation problem as a sequence-to-sequence task. We introduce Trailer Generation <b>Transformer</b> (TGT), a deep-learning framework utilizing an encoder-decoder architecture. TGT movie encoder is tasked with contextualizing each movie shot representation via <b>self-attention,</b> while the autoregressive trailer decoder predicts the feature representation of the next trailer shot, accounting for the relevance of shots&rsquo; temporal order in trailers. Our TGT significantly outperforms previous methods on a comprehensive suite of metrics.</p></p class="citation"></blockquote><h3 id=2365--120237-dreamwalk-style-space-exploration-using-diffusion-guidance-michelle-shu-et-al-2024>(23/65 | 120/237) DreamWalk: Style Space Exploration using Diffusion Guidance (Michelle Shu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michelle Shu, Charles Herrmann, Richard Strong Bowen, Forrester Cole, Ramin Zabih. (2024)<br><strong>DreamWalk: Style Space Exploration using Diffusion Guidance</strong><br><button class=copy-to-clipboard title="DreamWalk: Style Space Exploration using Diffusion Guidance" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Fine-tuning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03145v1.pdf filename=2404.03145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-conditioned <b>diffusion</b> <b>models</b> can generate impressive images, but fall short when it comes to fine-grained control. Unlike direct-editing tools like Photoshop, text conditioned models require the artist to perform <b>&ldquo;prompt</b> engineering,&rdquo; constructing special text sentences to control the style or amount of a particular subject present in the output image. Our goal is to provide fine-grained control over the style and substance specified by the <b>prompt,</b> for example to adjust the intensity of styles in different regions of the image (Figure 1). Our approach is to decompose the text <b>prompt</b> into conceptual elements, and apply a separate guidance term for each element in a single <b>diffusion</b> <b>process.</b> We introduce guidance scale functions to control when in the <b>diffusion</b> <b>process</b> and \emph{where} in the image to intervene. Since the method is based solely on adjusting <b>diffusion</b> <b>guidance,</b> it does not require <b>fine-tuning</b> or manipulating the internal layers of the <b>diffusion</b> <b>model&rsquo;s</b> neural network, and can be used in conjunction with LoRA- or DreamBooth-trained models (Figure2). Project page: <a href=https://mshu1.github.io/dreamwalk.github.io/>https://mshu1.github.io/dreamwalk.github.io/</a></p></p class="citation"></blockquote><h3 id=2465--121237-is-clip-the-main-roadblock-for-fine-grained-open-world-perception-lorenzo-bianchi-et-al-2024>(24/65 | 121/237) Is CLIP the main roadblock for fine-grained open-world perception? (Lorenzo Bianchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lorenzo Bianchi, Fabio Carrara, Nicola Messina, Fabrizio Falchi. (2024)<br><strong>Is CLIP the main roadblock for fine-grained open-world perception?</strong><br><button class=copy-to-clipboard title="Is CLIP the main roadblock for fine-grained open-world perception?" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 29<br>Keywords: Object Detection, Benchmarking, Multi-modal, Multi-modal, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03539v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03539v1.pdf filename=2404.03539v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern applications increasingly demand flexible computer vision models that adapt to novel concepts not encountered during training. This necessity is pivotal in emerging domains like extended reality, robotics, and autonomous driving, which require the ability to respond to open-world stimuli. A key ingredient is the ability to identify <b>objects</b> <b>based</b> on free-form textual queries defined at inference time - a task known as open-vocabulary <b>object</b> <b>detection.</b> <b>Multimodal</b> backbones like CLIP are the main enabling technology for current open-world perception solutions. Despite performing well on generic queries, recent studies highlighted limitations on the fine-grained recognition capabilities in open-vocabulary settings - i.e., for distinguishing subtle <b>object</b> <b>features</b> like color, shape, and material. In this paper, we perform a detailed examination of these open-vocabulary <b>object</b> <b>recognition</b> limitations to find the root cause. We evaluate the performance of CLIP, the most commonly used <b>vision-language</b> backbone, against a fine-grained <b>object-matching</b> <b>benchmark,</b> revealing interesting analogies between the limitations of open-vocabulary <b>object</b> <b>detectors</b> and their backbones. Experiments suggest that the lack of fine-grained understanding is caused by the poor separability of <b>object</b> <b>characteristics</b> in the CLIP latent space. Therefore, we try to understand whether fine-grained knowledge is present in CLIP embeddings but not exploited at inference time due, for example, to the unsuitability of the cosine similarity matching function, which may discard important <b>object</b> <b>characteristics.</b> Our preliminary experiments show that simple CLIP latent-space re-projections help separate fine-grained concepts, paving the way towards the development of backbones inherently able to process fine-grained details. The code for reproducing these experiments is available at <a href=https://github.com/lorebianchi98/FG-CLIP>https://github.com/lorebianchi98/FG-CLIP</a>.</p></p class="citation"></blockquote><h3 id=2565--122237-sparse-concept-bottleneck-models-gumbel-tricks-in-contrastive-learning-andrei-semenov-et-al-2024>(25/65 | 122/237) Sparse Concept Bottleneck Models: Gumbel Tricks in Contrastive Learning (Andrei Semenov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrei Semenov, Vladimir Ivanov, Aleksandr Beznosikov, Alexander Gasnikov. (2024)<br><strong>Sparse Concept Bottleneck Models: Gumbel Tricks in Contrastive Learning</strong><br><button class=copy-to-clipboard title="Sparse Concept Bottleneck Models: Gumbel Tricks in Contrastive Learning" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-2-6, I-2-10, I-4-10, I-5-1, I-5-4, I-5-5, I-2-6; I-2-10; I-4-10; I-5-1; I-5-4; I-5-5, cs-AI, cs-CV, cs.CV<br>Keyword Score: 28<br>Keywords: Black Box, Contrastive Learning, Fine-tuning, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03323v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03323v1.pdf filename=2404.03323v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel architecture and method of explainable classification with Concept Bottleneck Models (CBMs). While SOTA approaches to Image Classification task work as a <b>black</b> <b>box,</b> there is a growing demand for models that would provide interpreted results. Such a models often learn to predict the distribution over class labels using additional description of this target instances, called concepts. However, existing Bottleneck methods have a number of limitations: their accuracy is lower than that of a standard model and CBMs require an additional set of concepts to leverage. We provide a framework for creating Concept Bottleneck Model from pre-trained <b>multi-modal</b> encoder and new CLIP-like architectures. By introducing a new type of layers known as Concept Bottleneck Layers, we outline three methods for training them: with $\ell_1$-loss, <b>contrastive</b> <b>loss</b> and loss function based on Gumbel-Softmax distribution (Sparse-CBM), while final FC layer is still trained with Cross-Entropy. We show a significant increase in accuracy using sparse hidden layers in CLIP-based bottleneck models. Which means that sparse representation of concepts activation vector is meaningful in Concept Bottleneck Models. Moreover, with our Concept Matrix Search algorithm we can improve CLIP predictions on complex datasets without any additional training or <b>fine-tuning.</b> The code is available at: <a href=https://github.com/Andron00e/SparseCBM>https://github.com/Andron00e/SparseCBM</a>.</p></p class="citation"></blockquote><h3 id=2665--123237-diffbody-human-body-restoration-by-imagining-with-generative-diffusion-prior-yiming-zhang-et-al-2024>(26/65 | 123/237) DiffBody: Human Body Restoration by Imagining with Generative Diffusion Prior (Yiming Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiming Zhang, Zhe Wang, Xinjie Li, Yunchen Yuan, Chengsong Zhang, Xiao Sun, Zhihang Zhong, Jian Wang. (2024)<br><strong>DiffBody: Human Body Restoration by Imagining with Generative Diffusion Prior</strong><br><button class=copy-to-clipboard title="DiffBody: Human Body Restoration by Imagining with Generative Diffusion Prior" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Diffusion Model, Benchmarking, Benchmarking, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03642v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03642v1.pdf filename=2404.03642v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human body restoration plays a vital role in various applications related to the human body. Despite recent advances in general image restoration using generative models, their performance in human body restoration remains mediocre, often resulting in foreground and background blending, over-smoothing surface textures, missing accessories, and distorted limbs. Addressing these challenges, we propose a novel approach by constructing a human body-aware <b>diffusion</b> <b>model</b> that leverages domain-specific knowledge to enhance performance. Specifically, we employ a pretrained body attention module to guide the <b>diffusion</b> <b>model&rsquo;s</b> focus on the foreground, addressing issues caused by blending between the subject and background. We also demonstrate the value of revisiting the language modality of the <b>diffusion</b> <b>model</b> in restoration tasks by seamlessly incorporating text <b>prompt</b> to improve the quality of surface texture and additional clothing and accessories details. Additionally, we introduce a <b>diffusion</b> <b>sampler</b> tailored for fine-grained human body parts, utilizing local semantic information to rectify limb distortions. Lastly, we collect a comprehensive dataset for <b>benchmarking</b> and advancing the field of human body restoration. Extensive experimental validation showcases the superiority of our approach, both quantitatively and qualitatively, over existing methods.</p></p class="citation"></blockquote><h3 id=2765--124237-mvd-fusion-single-view-3d-via-depth-consistent-multi-view-generation-hanzhe-hu-et-al-2024>(27/65 | 124/237) MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation (Hanzhe Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanzhe Hu, Zhizhuo Zhou, Varun Jampani, Shubham Tulsiani. (2024)<br><strong>MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation</strong><br><button class=copy-to-clipboard title="MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Diffusion Model, Geometry, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03656v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03656v1.pdf filename=2404.03656v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present MVD-Fusion: a method for single-view 3D inference via generative modeling of multi-view-consistent RGB-D images. While recent methods pursuing 3D inference advocate learning novel-view generative models, these generations are not 3D-consistent and require a <b>distillation</b> process to generate a 3D output. We instead cast the task of 3D inference as directly generating mutually-consistent multiple views and build on the insight that additionally inferring depth can provide a mechanism for enforcing this consistency. Specifically, we train a denoising <b>diffusion</b> <b>model</b> to generate multi-view RGB-D images given a single RGB input image and leverage the (intermediate noisy) depth estimates to obtain reprojection-based conditioning to maintain multi-view consistency. We train our model using large-scale synthetic dataset Obajverse as well as the real-world CO3D dataset comprising of generic camera viewpoints. We demonstrate that our approach can yield more accurate synthesis compared to recent state-of-the-art, including <b>distillation-based</b> 3D inference and prior multi-view generation methods. We also evaluate the <b>geometry</b> induced by our multi-view depth prediction and find that it yields a more accurate representation than other direct 3D inference approaches.</p></p class="citation"></blockquote><h3 id=2865--125237-opennerf-open-set-3d-neural-scene-segmentation-with-pixel-wise-features-and-rendered-novel-views-francis-engelmann-et-al-2024>(28/65 | 125/237) OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views (Francis Engelmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francis Engelmann, Fabian Manhardt, Michael Niemeyer, Keisuke Tateno, Marc Pollefeys, Federico Tombari. (2024)<br><strong>OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views</strong><br><button class=copy-to-clipboard title="OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Convolution, Geometry, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03650v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03650v1.pdf filename=2404.03650v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large visual-language models (VLMs), like CLIP, enable open-set image segmentation to segment arbitrary concepts from an image in a <b>zero-shot</b> manner. This goes beyond the traditional closed-set assumption, i.e., where models can only segment classes from a pre-defined training set. More recently, first works on open-set segmentation in 3D scenes have appeared in the literature. These methods are heavily influenced by closed-set 3D <b>convolutional</b> approaches that process point clouds or polygon meshes. However, these 3D scene representations do not align well with the image-based nature of the visual-language models. Indeed, point cloud and 3D meshes typically have a lower resolution than images and the reconstructed 3D scene <b>geometry</b> might not project well to the underlying 2D image sequences used to compute pixel-aligned CLIP features. To address these challenges, we propose OpenNeRF which naturally operates on posed images and directly encodes the VLM features within the NeRF. This is similar in spirit to LERF, however our work shows that using pixel-wise VLM features (instead of global CLIP features) results in an overall less complex architecture without the need for additional DINO regularization. Our OpenNeRF further leverages NeRF&rsquo;s ability to render novel views and extract open-set VLM features from areas that are not well observed in the initial posed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF outperforms recent open-vocabulary methods such as LERF and OpenScene by at least +4.9 mIoU.</p></p class="citation"></blockquote><h3 id=2965--126237-reference-based-3d-aware-image-editing-with-triplane-bahri-batuhan-bilecen-et-al-2024>(29/65 | 126/237) Reference-Based 3D-Aware Image Editing with Triplane (Bahri Batuhan Bilecen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bahri Batuhan Bilecen, Yigit Yalin, Ning Yu, Aysegul Dundar. (2024)<br><strong>Reference-Based 3D-Aware Image Editing with Triplane</strong><br><button class=copy-to-clipboard title="Reference-Based 3D-Aware Image Editing with Triplane" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03632v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03632v1.pdf filename=2404.03632v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> have emerged as powerful tools not only for high-quality image generation but also for real image editing through manipulation of their interpretable latent spaces. Recent advancements in <b>GANs</b> include the development of 3D-aware models such as EG3D, characterized by efficient triplane-based architectures enabling the reconstruction of 3D <b>geometry</b> from single images. However, scant attention has been devoted to providing an integrated framework for high-quality reference-based 3D-aware image editing within this domain. This study addresses this gap by exploring and demonstrating the effectiveness of EG3D&rsquo;s triplane space for achieving advanced reference-based edits, presenting a unique perspective on 3D-aware image editing through our novel pipeline. Our approach integrates the encoding of triplane features, spatial disentanglement and automatic localization of features in the triplane domain, and fusion learning for desired image editing. Moreover, our framework demonstrates versatility across domains, extending its effectiveness to animal face edits and partial stylization of cartoon portraits. The method shows significant improvements over relevant 3D-aware latent editing and 2D reference-based editing methods, both qualitatively and quantitatively. Project page: <a href=https://three-bee.github.io/triplane_edit>https://three-bee.github.io/triplane_edit</a></p></p class="citation"></blockquote><h3 id=3065--127237-effective-lymph-nodes-detection-in-ct-scans-using-location-debiased-query-selection-and-contrastive-query-representation-in-transformer-qinji-yu-et-al-2024>(30/65 | 127/237) Effective Lymph Nodes Detection in CT Scans Using Location Debiased Query Selection and Contrastive Query Representation in Transformer (Qinji Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qinji Yu, Yirui Wang, Ke Yan, Haoshen Li, Dazhou Guo, Li Zhang, Le Lu, Na Shen, Qifeng Wang, Xiaowei Ding, Xianghua Ye, Dakai Jin. (2024)<br><strong>Effective Lymph Nodes Detection in CT Scans Using Location Debiased Query Selection and Contrastive Query Representation in Transformer</strong><br><button class=copy-to-clipboard title="Effective Lymph Nodes Detection in CT Scans Using Location Debiased Query Selection and Contrastive Query Representation in Transformer" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Contrastive Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03819v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03819v1.pdf filename=2404.03819v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lymph node (LN) assessment is a critical, indispensable yet very challenging task in the routine clinical workflow of radiology and oncology. Accurate LN analysis is essential for cancer diagnosis, staging, and treatment planning. Finding scatteredly distributed, low-contrast clinically relevant LNs in 3D CT is difficult even for experienced physicians under high inter-observer variations. Previous automatic LN detection works typically yield limited recall and high false positives (FPs) due to adjacent anatomies with similar image intensities, shapes, or textures (vessels, muscles, esophagus, etc). In this work, we propose a new LN DEtection <b>TRansformer,</b> named LN-DETR, to achieve more accurate performance. By enhancing the 2D backbone with a multi-scale 2.5D feature fusion to incorporate 3D context explicitly, more importantly, we make two main contributions to improve the representation quality of LN queries. 1) Considering that LN boundaries are often unclear, an IoU prediction head and a location debiased query selection are proposed to select LN queries of higher localization accuracy as the decoder query&rsquo;s initialization. 2) To reduce FPs, query <b>contrastive</b> <b>learning</b> is employed to explicitly reinforce LN queries towards their best-matched ground-truth queries over unmatched query predictions. Trained and tested on 3D CT scans of 1067 patients (with 10,000+ labeled LNs) via combining seven LN datasets from different body parts (neck, chest, and abdomen) and pathologies/cancers, our method significantly improves the performance of previous leading methods by > 4-5% average recall at the same FP rates in both internal and external testing. We further evaluate on the universal lesion detection task using NIH DeepLesion <b>benchmark,</b> and our method achieves the top performance of 88.46% averaged recall across 0.5 to 4 FPs per image, compared with other leading reported results.</p></p class="citation"></blockquote><h3 id=3165--128237-the-more-you-see-in-2d-the-more-you-perceive-in-3d-xinyang-han-et-al-2024>(31/65 | 128/237) The More You See in 2D, the More You Perceive in 3D (Xinyang Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyang Han, Zelin Gao, Angjoo Kanazawa, Shubham Goel, Yossi Gandelsman. (2024)<br><strong>The More You See in 2D, the More You Perceive in 3D</strong><br><button class=copy-to-clipboard title="The More You See in 2D, the More You Perceive in 3D" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Diffusion Model, Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03652v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03652v1.pdf filename=2404.03652v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans can infer 3D structure from 2D images of an object based on past experience and improve their 3D understanding as they see more images. Inspired by this behavior, we introduce SAP3D, a system for 3D reconstruction and novel view synthesis from an arbitrary number of unposed images. Given a few unposed images of an object, we adapt a pre-trained view-conditioned <b>diffusion</b> <b>model</b> together with the camera poses of the images via test-time <b>fine-tuning.</b> The adapted <b>diffusion</b> <b>model</b> and the obtained camera poses are then utilized as instance-specific priors for 3D reconstruction and novel view synthesis. We show that as the number of input images increases, the performance of our approach improves, bridging the gap between optimization-based prior-less 3D reconstruction methods and single-image-to-3D <b>diffusion-based</b> <b>methods.</b> We demonstrate our system on real images as well as standard synthetic <b>benchmarks.</b> Our ablation studies confirm that this adaption behavior is key for more accurate 3D understanding.</p></p class="citation"></blockquote><h3 id=3265--129237-uniav-unified-audio-visual-perception-for-multi-task-video-localization-tiantian-geng-et-al-2024>(32/65 | 129/237) UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization (Tiantian Geng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tiantian Geng, Teng Wang, Yanfu Zhang, Jinming Duan, Weili Guan, Feng Zheng. (2024)<br><strong>UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization</strong><br><button class=copy-to-clipboard title="UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs-SD, cs.CV, eess-AS<br>Keyword Score: 23<br>Keywords: Benchmarking, Event Detection, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03179v1.pdf filename=2404.03179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video localization tasks aim to temporally locate specific instances in videos, including temporal action localization (TAL), sound <b>event</b> <b>detection</b> (SED) and audio-visual <b>event</b> <b>localization</b> (AVEL). Existing methods over-specialize on each task, overlooking the fact that these instances often occur in the same video to form the complete video content. In this work, we present UniAV, a Unified Audio-Visual perception network, to achieve joint learning of TAL, SED and AVEL tasks for the first time. UniAV can leverage diverse data available in task-specific datasets, allowing the model to learn and share mutually beneficial knowledge across tasks and modalities. To tackle the challenges posed by substantial variations in datasets (size/domain/duration) and distinct task characteristics, we propose to uniformly encode visual and audio modalities of all videos to derive generic representations, while also designing task-specific experts to capture unique knowledge for each task. Besides, we develop a unified language-aware classifier by utilizing a pre-trained text encoder, enabling the model to flexibly detect various types of instances and previously unseen ones by simply changing <b>prompts</b> during inference. UniAV outperforms its single-task counterparts by a large margin with fewer parameters, achieving on-par or superior performances compared to state-of-the-art task-specific methods across ActivityNet 1.3, DESED and UnAV-100 <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=3365--130237-no-zero-shot-without-exponential-data-pretraining-concept-frequency-determines-multimodal-model-performance-vishaal-udandarao-et-al-2024>(33/65 | 130/237) No &lsquo;Zero-Shot&rsquo; Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance (Vishaal Udandarao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H. S. Torr, Adel Bibi, Samuel Albanie, Matthias Bethge. (2024)<br><strong>No &lsquo;Zero-Shot&rsquo; Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance</strong><br><button class=copy-to-clipboard title="No 'Zero-Shot' Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 22<br>Keywords: Benchmarking, Benchmarking, Multi-modal, Multi-modal, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04125v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04125v1.pdf filename=2404.04125v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Web-crawled pretraining datasets underlie the impressive <b>&ldquo;zero-shot&rdquo;</b> evaluation performance of <b>multimodal</b> models, such as CLIP for classification/retrieval and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of <b>&ldquo;zero-shot&rdquo;</b> generalization is for such <b>multimodal</b> models, as it is not known to what extent their pretraining datasets encompass the downstream concepts targeted for during <b>&ldquo;zero-shot&rdquo;</b> evaluation. In this work, we ask: How is the performance of <b>multimodal</b> models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets? We comprehensively investigate this question across 34 models and five standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts. We consistently find that, far from exhibiting <b>&ldquo;zero-shot&rdquo;</b> generalization, <b>multimodal</b> models require exponentially more data to achieve linear improvements in downstream <b>&ldquo;zero-shot&rdquo;</b> performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and downstream datasets, and testing on purely synthetic data distributions. Furthermore, upon <b>benchmarking</b> models on long-tailed data sampled based on our analysis, we demonstrate that <b>multimodal</b> models across the board perform poorly. We contribute this long-tail test set as the &ldquo;Let it Wag!&rdquo; <b>benchmark</b> to further research in this direction. Taken together, our study reveals an exponential need for training data which implies that the key to <b>&ldquo;zero-shot&rdquo;</b> generalization capabilities under large-scale training paradigms remains to be found.</p></p class="citation"></blockquote><h3 id=3465--131237-rafe-generative-radiance-fields-restoration-zhongkai-wu-et-al-2024>(34/65 | 131/237) RaFE: Generative Radiance Fields Restoration (Zhongkai Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu. (2024)<br><strong>RaFE: Generative Radiance Fields Restoration</strong><br><button class=copy-to-clipboard title="RaFE: Generative Radiance Fields Restoration" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03654v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03654v1.pdf filename=2404.03654v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel view synthesis and 3D reconstruction, but its performance is sensitive to input image quality, which struggles to achieve high-fidelity rendering when provided with low-quality sparse input viewpoints. Previous methods for NeRF restoration are tailored for specific degradation type, ignoring the generality of restoration. To overcome this limitation, we propose a generic radiance fields restoration pipeline, named RaFE, which applies to various types of degradations, such as low resolution, blurriness, noise, compression artifacts, or their combinations. Our approach leverages the success of off-the-shelf 2D restoration methods to recover the multi-view images individually. Instead of reconstructing a blurred NeRF by averaging inconsistencies, we introduce a novel approach using <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> for NeRF generation to better accommodate the geometric and appearance inconsistencies present in the multi-view images. Specifically, we adopt a two-level tri-plane architecture, where the coarse level remains fixed to represent the low-quality NeRF, and a fine-level residual tri-plane to be added to the coarse level is modeled as a distribution with <b>GAN</b> to capture potential variations in restoration. We validate RaFE on both synthetic and real cases for various restoration tasks, demonstrating superior performance in both quantitative and qualitative evaluations, surpassing other 3D restoration methods specific to single task. Please see our project website <a href=https://zkaiwu.github.io/RaFE-Project/>https://zkaiwu.github.io/RaFE-Project/</a>.</p></p class="citation"></blockquote><h3 id=3565--132237-decoupling-static-and-hierarchical-motion-perception-for-referring-video-segmentation-shuting-he-et-al-2024>(35/65 | 132/237) Decoupling Static and Hierarchical Motion Perception for Referring Video Segmentation (Shuting He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuting He, Henghui Ding. (2024)<br><strong>Decoupling Static and Hierarchical Motion Perception for Referring Video Segmentation</strong><br><button class=copy-to-clipboard title="Decoupling Static and Hierarchical Motion Perception for Referring Video Segmentation" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Sentence Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03645v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03645v1.pdf filename=2404.03645v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Referring video segmentation relies on natural language expressions to identify and segment objects, often emphasizing motion clues. Previous works treat a <b>sentence</b> <b>as</b> a whole and directly perform identification at the video-level, mixing up static image-level cues with temporal motion cues. However, image-level features cannot well comprehend motion cues in <b>sentences,</b> <b>and</b> static cues are not crucial for temporal perception. In fact, static cues can sometimes interfere with temporal perception by overshadowing motion cues. In this work, we propose to decouple video-level referring expression understanding into static and motion perception, with a specific emphasis on enhancing temporal comprehension. Firstly, we introduce an expression-decoupling module to make static cues and motion cues perform their distinct role, alleviating the issue of <b>sentence</b> <b>embeddings</b> overlooking motion cues. Secondly, we propose a hierarchical motion perception module to capture temporal information effectively across varying timescales. Furthermore, we employ <b>contrastive</b> <b>learning</b> to distinguish the motions of visually similar objects. These contributions yield state-of-the-art performance across five datasets, including a remarkable $\textbf{9.2%}$ $\mathcal{J&amp;F}$ improvement on the challenging $\textbf{MeViS}$ dataset. Code is available at <a href=https://github.com/heshuting555/DsHmp>https://github.com/heshuting555/DsHmp</a>.</p></p class="citation"></blockquote><h3 id=3665--133237-pointinfinity-resolution-invariant-point-diffusion-models-zixuan-huang-et-al-2024>(36/65 | 133/237) PointInfinity: Resolution-Invariant Point Diffusion Models (Zixuan Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zixuan Huang, Justin Johnson, Shoubhik Debnath, James M. Rehg, Chao-Yuan Wu. (2024)<br><strong>PointInfinity: Resolution-Invariant Point Diffusion Models</strong><br><button class=copy-to-clipboard title="PointInfinity: Resolution-Invariant Point Diffusion Models" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03566v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03566v1.pdf filename=2404.03566v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present PointInfinity, an efficient family of point cloud <b>diffusion</b> <b>models.</b> Our core idea is to use a <b>transformer-based</b> architecture with a fixed-size, resolution-invariant latent representation. This enables efficient training with low-resolution point clouds, while allowing high-resolution point clouds to be generated during inference. More importantly, we show that scaling the test-time resolution beyond the training resolution improves the fidelity of generated point clouds and surfaces. We analyze this phenomenon and draw a link to classifier-free guidance commonly used in <b>diffusion</b> <b>models,</b> demonstrating that both allow trading off fidelity and variability during inference. Experiments on CO3D show that PointInfinity can efficiently generate high-resolution point clouds (up to 131k points, 31 times more than Point-E) with state-of-the-art quality.</p></p class="citation"></blockquote><h3 id=3765--134237-if-its-not-enough-make-it-so-reducing-authentic-data-demand-in-face-recognition-through-synthetic-faces-andrea-atzori-et-al-2024>(37/65 | 134/237) If It&rsquo;s Not Enough, Make It So: Reducing Authentic Data Demand in Face Recognition through Synthetic Faces (Andrea Atzori et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Atzori, Fadi Boutros, Naser Damer, Gianni Fenu, Mirko Marras. (2024)<br><strong>If It&rsquo;s Not Enough, Make It So: Reducing Authentic Data Demand in Face Recognition through Synthetic Faces</strong><br><button class=copy-to-clipboard title="If It's Not Enough, Make It So: Reducing Authentic Data Demand in Face Recognition through Synthetic Faces" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Face Recognition, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03537v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03537v1.pdf filename=2404.03537v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in deep <b>face</b> <b>recognition</b> have spurred a growing demand for large, diverse, and manually annotated <b>face</b> <b>datasets.</b> Acquiring authentic, high-quality <b>data</b> <b>for</b> <b>face</b> <b>recognition</b> has proven to be a challenge, primarily due to privacy concerns. Large <b>face</b> <b>datasets</b> are primarily sourced from web-based images, lacking explicit user consent. In this paper, we examine whether and how synthetic <b>face</b> <b>data</b> <b>can</b> be used to train effective <b>face</b> <b>recognition</b> models with reduced reliance on authentic images, thereby mitigating <b>data</b> <b>collection</b> concerns. First, we explored the performance gap among recent state-of-the-art <b>face</b> <b>recognition</b> models, trained with synthetic <b>data</b> <b>only</b> and authentic (scarce) <b>data</b> <b>only.</b> Then, we deepened our analysis by training a state-of-the-art backbone with various combinations of synthetic and authentic <b>data,</b> <b>gaining</b> insights into optimizing the limited use of the latter for verification accuracy. Finally, we assessed the effectiveness of <b>data</b> <b>augmentation</b> approaches on synthetic and authentic <b>data,</b> <b>with</b> the same goal in mind. Our results highlighted the effectiveness of FR trained on combined datasets, particularly when combined with appropriate augmentation techniques.</p></p class="citation"></blockquote><h3 id=3865--135237-sdpose-tokenized-pose-estimation-via-circulation-guide-self-distillation-sichen-chen-et-al-2024>(38/65 | 135/237) SDPose: Tokenized Pose Estimation via Circulation-Guide Self-Distillation (Sichen Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sichen Chen, Yingyi Zhang, Siming Huang, Ran Yi, Ke Fan, Ruixin Zhang, Peixian Chen, Jun Wang, Shouhong Ding, Lizhuang Ma. (2024)<br><strong>SDPose: Tokenized Pose Estimation via Circulation-Guide Self-Distillation</strong><br><button class=copy-to-clipboard title="SDPose: Tokenized Pose Estimation via Circulation-Guide Self-Distillation" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Self-Distillation, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03518v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03518v1.pdf filename=2404.03518v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>transformer-based</b> methods have achieved state-of-the-art prediction quality on human pose estimation(HPE). Nonetheless, most of these top-performing <b>transformer-based</b> models are too computation-consuming and storage-demanding to deploy on edge computing platforms. Those <b>transformer-based</b> models that require fewer resources are prone to under-fitting due to their smaller scale and thus perform notably worse than their larger counterparts. Given this conundrum, we introduce SDPose, a new <b>self-distillation</b> method for improving the performance of small <b>transformer-based</b> models. To mitigate the problem of under-fitting, we design a <b>transformer</b> module named Multi-Cycled Transformer(MCT) based on multiple-cycled forwards to more fully exploit the potential of small model parameters. Further, in order to prevent the additional inference compute-consuming brought by MCT, we introduce a <b>self-distillation</b> scheme, extracting the knowledge from the MCT module to a naive forward model. Specifically, on the MSCOCO validation dataset, SDPose-T obtains 69.7% mAP with 4.4M parameters and 1.8 GFLOPs. Furthermore, SDPose-S-V2 obtains 73.5% mAP on the MSCOCO validation dataset with 6.2M parameters and 4.7 GFLOPs, achieving a new state-of-the-art among predominant tiny neural network methods. Our code is available at <a href=https://github.com/MartyrPenink/SDPose>https://github.com/MartyrPenink/SDPose</a>.</p></p class="citation"></blockquote><h3 id=3965--136237-dq-detr-detr-with-dynamic-query-for-tiny-object-detection-yi-xin-huang-et-al-2024>(39/65 | 136/237) DQ-DETR: DETR with Dynamic Query for Tiny Object Detection (Yi-Xin Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi-Xin Huang, Hou-I Liu, Hong-Han Shuai, Wen-Huang Cheng. (2024)<br><strong>DQ-DETR: DETR with Dynamic Query for Tiny Object Detection</strong><br><button class=copy-to-clipboard title="DQ-DETR: DETR with Dynamic Query for Tiny Object Detection" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03507v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03507v1.pdf filename=2404.03507v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite previous DETR-like methods having performed successfully in generic <b>object</b> <b>detection,</b> tiny <b>object</b> <b>detection</b> is still a challenging task for them since the positional information of <b>object</b> <b>queries</b> is not customized for detecting tiny <b>objects,</b> <b>whose</b> scale is extraordinarily smaller than general <b>objects.</b> <b>Also,</b> DETR-like methods using a fixed number of queries make them unsuitable for aerial datasets, which only contain tiny <b>objects,</b> <b>and</b> the numbers of instances are imbalanced between different images. Thus, we present a simple yet effective model, named DQ-DETR, which consists of three different components: categorical counting module, counting-guided feature enhancement, and dynamic query selection to solve the above-mentioned problems. DQ-DETR uses the prediction and density maps from the categorical counting module to dynamically adjust the number of <b>object</b> <b>queries</b> and improve the positional information of queries. Our model DQ-DETR outperforms previous <b>CNN-based</b> and DETR-like methods, achieving state-of-the-art mAP 30.2% on the AI-TOD-V2 dataset, which mostly consists of tiny objects.</p></p class="citation"></blockquote><h3 id=4065--137237-di-retinex-digital-imaging-retinex-theory-for-low-light-image-enhancement-shangquan-sun-et-al-2024>(40/65 | 137/237) DI-Retinex: Digital-Imaging Retinex Theory for Low-Light Image Enhancement (Shangquan Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shangquan Sun, Wenqi Ren, Jingyang Peng, Fenglong Song, Xiaochun Cao. (2024)<br><strong>DI-Retinex: Digital-Imaging Retinex Theory for Low-Light Image Enhancement</strong><br><button class=copy-to-clipboard title="DI-Retinex: Digital-Imaging Retinex Theory for Low-Light Image Enhancement" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Quantization, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03327v1.pdf filename=2404.03327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many existing methods for low-light image enhancement (LLIE) based on Retinex theory ignore important factors that affect the validity of this theory in digital imaging, such as noise, <b>quantization</b> error, non-linearity, and dynamic range overflow. In this paper, we propose a new expression called Digital-Imaging Retinex theory (DI-Retinex) through theoretical and experimental analysis of Retinex theory in digital imaging. Our new expression includes an offset term in the enhancement model, which allows for pixel-wise brightness contrast adjustment with a non-linear mapping function. In addition, to solve the lowlight enhancement problem in an <b>unsupervised</b> manner, we propose an image-adaptive masked reverse degradation loss in Gamma space. We also design a variance suppression loss for regulating the additional offset term. Extensive experiments show that our proposed method outperforms all existing <b>unsupervised</b> methods in terms of visual quality, model size, and speed. Our algorithm can also assist downstream face detectors in low-light, as it shows the most performance gain after the low-light enhancement compared to other methods.</p></p class="citation"></blockquote><h3 id=4165--138237-adabm-on-the-fly-adaptive-bit-mapping-for-image-super-resolution-cheeun-hong-et-al-2024>(41/65 | 138/237) AdaBM: On-the-Fly Adaptive Bit Mapping for Image Super-Resolution (Cheeun Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheeun Hong, Kyoung Mu Lee. (2024)<br><strong>AdaBM: On-the-Fly Adaptive Bit Mapping for Image Super-Resolution</strong><br><button class=copy-to-clipboard title="AdaBM: On-the-Fly Adaptive Bit Mapping for Image Super-Resolution" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03296v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03296v1.pdf filename=2404.03296v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although image super-resolution (SR) problem has experienced unprecedented restoration accuracy with deep neural networks, it has yet limited versatile applications due to the substantial computational costs. Since different input images for SR face different restoration difficulties, adapting computational costs based on the input image, referred to as adaptive inference, has emerged as a promising solution to compress SR networks. Specifically, adapting the <b>quantization</b> bit-widths has successfully reduced the inference and memory cost without sacrificing the accuracy. However, despite the benefits of the resultant adaptive network, existing works rely on time-intensive <b>quantization-aware</b> training with full access to the original training pairs to learn the appropriate bit allocation policies, which limits its ubiquitous usage. To this end, we introduce the first on-the-fly adaptive <b>quantization</b> framework that accelerates the processing time from hours to seconds. We formulate the bit allocation problem with only two bit mapping modules: one to map the input image to the image-wise bit adaptation factor and one to obtain the layer-wise adaptation factors. These bit mappings are calibrated and <b>fine-tuned</b> using only a small number of calibration images. We achieve competitive performance with the previous adaptive <b>quantization</b> methods, while the processing time is accelerated by x2000. Codes are available at <a href=https://github.com/Cheeun/AdaBM>https://github.com/Cheeun/AdaBM</a>.</p></p class="citation"></blockquote><h3 id=4265--139237-would-deep-generative-models-amplify-bias-in-future-models-tianwei-chen-et-al-2024>(42/65 | 139/237) Would Deep Generative Models Amplify Bias in Future Models? (Tianwei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianwei Chen, Yusuke Hirota, Mayu Otani, Noa Garcia, Yuta Nakashima. (2024)<br><strong>Would Deep Generative Models Amplify Bias in Future Models?</strong><br><button class=copy-to-clipboard title="Would Deep Generative Models Amplify Bias in Future Models?" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03242v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03242v1.pdf filename=2404.03242v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the impact of deep generative models on potential social biases in upcoming computer vision models. As the internet witnesses an increasing influx of AI-generated images, concerns arise regarding inherent biases that may accompany them, potentially leading to the dissemination of harmful content. This paper explores whether a detrimental feedback loop, resulting in bias amplification, would occur if generated images were used as the training data for future models. We conduct <b>simulations</b> by progressively substituting original images in COCO and CC3M datasets with images generated through Stable Diffusion. The modified datasets are used to train OpenCLIP and image captioning models, which we evaluate in terms of quality and bias. Contrary to expectations, our findings indicate that introducing generated images during training does not uniformly amplify bias. Instead, instances of bias mitigation across specific tasks are observed. We further explore the factors that may influence these phenomena, such as artifacts in image generation (e.g., blurry faces) or pre-existing biases in the original datasets.</p></p class="citation"></blockquote><h3 id=4365--140237-hdr-imaging-for-dynamic-scenes-with-events-li-xiaopeng-et-al-2024>(43/65 | 140/237) HDR Imaging for Dynamic Scenes with Events (Li Xiaopeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Xiaopeng, Zeng Zhaoyuan, Fan Cien, Zhao Chen, Deng Lei, Yu Lei. (2024)<br><strong>HDR Imaging for Dynamic Scenes with Events</strong><br><button class=copy-to-clipboard title="HDR Imaging for Dynamic Scenes with Events" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03210v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03210v1.pdf filename=2404.03210v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High dynamic range imaging (HDRI) for real-world dynamic scenes is challenging because moving objects may lead to hybrid degradation of low dynamic range and motion blur. Existing event-based approaches only focus on a separate task, while cascading HDRI and motion deblurring would lead to sub-optimal solutions, and unavailable ground-truth sharp HDR images aggravate the predicament. To address these challenges, we propose an Event-based HDRI framework within a <b>Self-supervised</b> <b>learning</b> paradigm, i.e., Self-EHDRI, which generalizes HDRI performance in real-world dynamic scenarios. Specifically, a <b>self-supervised</b> <b>learning</b> strategy is carried out by learning cross-domain conversions from blurry LDR images to sharp LDR images, which enables sharp HDR images to be accessible in the intermediate process even though ground-truth sharp HDR images are missing. Then, we formulate the event-based HDRI and motion deblurring model and conduct a unified network to recover the intermediate sharp HDR results, where both the high dynamic range and high temporal resolution of events are leveraged simultaneously for compensation. We construct large-scale synthetic and real-world datasets to evaluate the effectiveness of our method. Comprehensive experiments demonstrate that the proposed Self-EHDRI outperforms state-of-the-art approaches by a large margin. The codes, datasets, and results are available at <a href=https://lxp-whu.github.io/Self-EHDRI>https://lxp-whu.github.io/Self-EHDRI</a>.</p></p class="citation"></blockquote><h3 id=4465--141237-adaptive-discrete-disparity-volume-for-self-supervised-monocular-depth-estimation-jianwei-ren-2024>(44/65 | 141/237) Adaptive Discrete Disparity Volume for Self-supervised Monocular Depth Estimation (Jianwei Ren, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianwei Ren. (2024)<br><strong>Adaptive Discrete Disparity Volume for Self-supervised Monocular Depth Estimation</strong><br><button class=copy-to-clipboard title="Adaptive Discrete Disparity Volume for Self-supervised Monocular Depth Estimation" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03190v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03190v1.pdf filename=2404.03190v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>self-supervised</b> monocular depth estimation tasks, discrete disparity prediction has been proven to attain higher quality depth maps than common continuous methods. However, current discretization strategies often divide depth ranges of scenes into bins in a handcrafted and rigid manner, limiting model performance. In this paper, we propose a learnable module, Adaptive Discrete Disparity Volume (ADDV), which is capable of dynamically sensing depth distributions in different RGB images and generating adaptive bins for them. Without any extra supervision, this module can be integrated into existing <b>CNN</b> architectures, allowing networks to produce representative values for bins and a probability volume over them. Furthermore, we introduce novel training strategies - uniformizing and sharpening - through a loss term and temperature parameter, respectively, to provide regularizations under <b>self-supervised</b> conditions, preventing model degradation or collapse. Empirical results demonstrate that ADDV effectively processes global information, generating appropriate bins for various scenes and producing higher quality depth maps compared to handcrafted methods.</p></p class="citation"></blockquote><h3 id=4565--142237-biovl-qr-egocentric-biochemical-video-and-language-dataset-using-micro-qr-codes-taichi-nishimura-et-al-2024>(45/65 | 142/237) BioVL-QR: Egocentric Biochemical Video-and-Language Dataset Using Micro QR Codes (Taichi Nishimura et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taichi Nishimura, Koki Yamamoto, Yuto Haneji, Keiya Kajimura, Chihiro Nishiwaki, Eriko Daikoku, Natsuko Okuda, Fumihito Ono, Hirotaka Kameko, Shinsuke Mori. (2024)<br><strong>BioVL-QR: Egocentric Biochemical Video-and-Language Dataset Using Micro QR Codes</strong><br><button class=copy-to-clipboard title="BioVL-QR: Egocentric Biochemical Video-and-Language Dataset Using Micro QR Codes" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-MM, cs.CV<br>Keyword Score: 20<br>Keywords: Video-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03161v1.pdf filename=2404.03161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a biochemical <b>vision-and-language</b> dataset, which consists of 24 egocentric experiment videos, corresponding protocols, and <b>video-and-language</b> alignments. The key challenge in the wet-lab domain is detecting equipment, reagents, and containers is difficult because the lab environment is scattered by filling objects on the table and some objects are indistinguishable. Therefore, previous studies assume that objects are manually annotated and given for downstream tasks, but this is costly and time-consuming. To address this issue, this study focuses on Micro QR Codes to detect objects automatically. From our preliminary study, we found that detecting objects only using Micro QR Codes is still difficult because the researchers manipulate objects, causing blur and occlusion frequently. To address this, we also propose a novel object labeling method by combining a Micro QR Code detector and an off-the-shelf hand object detector. As one of the applications of our dataset, we conduct the task of generating protocols from experiment videos and find that our approach can generate accurate protocols.</p></p class="citation"></blockquote><h3 id=4665--143237-test-time-training-for-industrial-anomaly-segmentation-alex-costanzino-et-al-2024>(46/65 | 143/237) Test Time Training for Industrial Anomaly Segmentation (Alex Costanzino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alex Costanzino, Pierluigi Zama Ramirez, Mirko Del Moro, Agostino Aiezzo, Giuseppe Lisanti, Samuele Salti, Luigi Di Stefano. (2024)<br><strong>Test Time Training for Industrial Anomaly Segmentation</strong><br><button class=copy-to-clipboard title="Test Time Training for Industrial Anomaly Segmentation" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Anomaly Detection, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03743v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03743v1.pdf filename=2404.03743v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Anomaly</b> <b>Detection</b> and Segmentation (AD&amp;S) is crucial for industrial quality control. While existing methods excel in generating <b>anomaly</b> <b>scores</b> for each pixel, practical applications require producing a binary segmentation to identify anomalies. Due to the absence of labeled anomalies in many real scenarios, standard practices binarize these maps based on some statistics derived from a validation set containing only nominal samples, resulting in poor segmentation performance. This paper addresses this problem by proposing a test time training strategy to improve the segmentation performance. Indeed, at test time, we can extract rich features directly from anomalous samples to train a classifier that can discriminate defects effectively. Our general approach can work downstream to any AD&amp;S method that provides an <b>anomaly</b> <b>score</b> map as output, even in <b>multimodal</b> settings. We demonstrate the effectiveness of our approach over baselines through extensive experimentation and evaluation on MVTec AD and MVTec 3D-AD.</p></p class="citation"></blockquote><h3 id=4765--144237-aigiqa-20k-a-large-database-for-ai-generated-image-quality-assessment-chunyi-li-et-al-2024>(47/65 | 144/237) AIGIQA-20K: A Large Database for AI-Generated Image Quality Assessment (Chunyi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chunyi Li, Tengchuan Kou, Yixuan Gao, Yuqin Cao, Wei Sun, Zicheng Zhang, Yingjie Zhou, Zhichao Zhang, Weixia Zhang, Haoning Wu, Xiaohong Liu, Xiongkuo Min, Guangtao Zhai. (2024)<br><strong>AIGIQA-20K: A Large Database for AI-Generated Image Quality Assessment</strong><br><button class=copy-to-clipboard title="AIGIQA-20K: A Large Database for AI-Generated Image Quality Assessment" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03407v1.pdf filename=2404.03407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid advancements in AI-Generated Content (AIGC), AI-Generated Images (AIGIs) have been widely applied in entertainment, education, and social media. However, due to the significant variance in quality among different AIGIs, there is an urgent need for models that consistently match human subjective ratings. To address this issue, we organized a challenge towards AIGC quality assessment on NTIRE 2024 that extensively considers 15 popular generative models, utilizing dynamic hyper-parameters (including classifier-free guidance, iteration epochs, and output image resolution), and gather subjective scores that consider perceptual quality and <b>text-to-image</b> alignment altogether comprehensively involving 21 subjects. This approach culminates in the creation of the largest fine-grained AIGI subjective quality database to date with 20,000 AIGIs and 420,000 subjective ratings, known as AIGIQA-20K. Furthermore, we conduct <b>benchmark</b> experiments on this database to assess the correspondence between 16 mainstream AIGI quality models and human perception. We anticipate that this large-scale quality database will inspire robust quality indicators for AIGIs and propel the evolution of AIGC for vision. The database is released on <a href=https://www.modelscope.cn/datasets/lcysyzxdxc/AIGCQA-30K-Image>https://www.modelscope.cn/datasets/lcysyzxdxc/AIGCQA-30K-Image</a>.</p></p class="citation"></blockquote><h3 id=4865--145237-two-tricks-to-improve-unsupervised-segmentation-learning-alp-eren-sari-et-al-2024>(48/65 | 145/237) Two Tricks to Improve Unsupervised Segmentation Learning (Alp Eren Sari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alp Eren Sari, Francesco Locatello, Paolo Favar. (2024)<br><strong>Two Tricks to Improve Unsupervised Segmentation Learning</strong><br><button class=copy-to-clipboard title="Two Tricks to Improve Unsupervised Segmentation Learning" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03392v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03392v1.pdf filename=2404.03392v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present two practical improvement techniques for <b>unsupervised</b> segmentation learning. These techniques address limitations in the resolution and accuracy of predicted segmentation maps of recent state-of-the-art methods. Firstly, we leverage image post-processing techniques such as guided filtering to refine the output masks, improving accuracy while avoiding substantial computational costs. Secondly, we introduce a multi-scale consistency criterion, based on a teacher-student training scheme. This criterion matches segmentation masks predicted from regions of the input image extracted at different resolutions to each other. Experimental results on several <b>benchmarks</b> used in <b>unsupervised</b> segmentation learning demonstrate the effectiveness of our proposed techniques.</p></p class="citation"></blockquote><h3 id=4965--146237-monocd-monocular-3d-object-detection-with-complementary-depths-longfei-yan-et-al-2024>(49/65 | 146/237) MonoCD: Monocular 3D Object Detection with Complementary Depths (Longfei Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Longfei Yan, Pei Yan, Shengzhou Xiong, Xuanyu Xiang, Yihua Tan. (2024)<br><strong>MonoCD: Monocular 3D Object Detection with Complementary Depths</strong><br><button class=copy-to-clipboard title="MonoCD: Monocular 3D Object Detection with Complementary Depths" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Object Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03181v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03181v1.pdf filename=2404.03181v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monocular 3D <b>object</b> <b>detection</b> has attracted widespread attention due to its potential to accurately obtain <b>object</b> <b>3D</b> localization from a single image at a low cost. Depth estimation is an essential but challenging subtask of monocular 3D <b>object</b> <b>detection</b> due to the ill-posedness of 2D to 3D mapping. Many methods explore multiple local depth clues such as <b>object</b> <b>heights</b> and keypoints and then formulate the <b>object</b> <b>depth</b> estimation as an ensemble of multiple depth predictions to mitigate the insufficiency of single-depth information. However, the errors of existing multiple depths tend to have the same sign, which hinders them from neutralizing each other and limits the overall accuracy of combined depth. To alleviate this problem, we propose to increase the complementarity of depths with two novel designs. First, we add a new depth prediction branch named complementary depth that utilizes global and efficient depth clues from the entire image rather than the local clues to reduce the correlation of depth predictions. Second, we propose to fully exploit the geometric relations between multiple depth clues to achieve complementarity in form. Benefiting from these designs, our method achieves higher complementarity. Experiments on the KITTI <b>benchmark</b> demonstrate that our method achieves state-of-the-art performance without introducing extra data. In addition, complementary depth can also be a lightweight and plug-and-play module to boost multiple existing monocular 3d <b>object</b> <b>detectors.</b> Code is available at <a href=https://github.com/elvintanhust/MonoCD>https://github.com/elvintanhust/MonoCD</a>.</p></p class="citation"></blockquote><h3 id=5065--147237-handdiff-3d-hand-pose-estimation-with-diffusion-on-image-point-cloud-wencan-cheng-et-al-2024>(50/65 | 147/237) HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud (Wencan Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wencan Cheng, Hao Tang, Luc Van Gool, Jong Hwan Ko. (2024)<br><strong>HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud</strong><br><button class=copy-to-clipboard title="HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03159v1.pdf filename=2404.03159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Extracting keypoint locations from input hand frames, known as 3D hand pose estimation, is a critical task in various human-computer interaction applications. Essentially, the 3D hand pose estimation can be regarded as a 3D point subset generative problem conditioned on input frames. Thanks to the recent significant progress on <b>diffusion-based</b> <b>generative</b> models, hand pose estimation can also benefit from the <b>diffusion</b> <b>model</b> to estimate keypoint locations with high quality. However, directly deploying the existing <b>diffusion</b> <b>models</b> to solve hand pose estimation is non-trivial, since they cannot achieve the complex permutation mapping and precise localization. Based on this motivation, this paper proposes HandDiff, a <b>diffusion-based</b> <b>hand</b> pose estimation model that iteratively denoises accurate hand pose conditioned on hand-shaped image-point clouds. In order to recover keypoint permutation and accurate location, we further introduce joint-wise condition and local detail condition. Experimental results demonstrate that the proposed HandDiff significantly outperforms the existing approaches on four challenging hand pose <b>benchmark</b> datasets. Codes and pre-trained models are publicly available at <a href=https://github.com/cwc1260/HandDiff>https://github.com/cwc1260/HandDiff</a>.</p></p class="citation"></blockquote><h3 id=5165--148237-sleepvst-sleep-staging-from-near-infrared-video-signals-using-pre-trained-transformers-jonathan-f-carter-et-al-2024>(51/65 | 148/237) SleepVST: Sleep Staging from Near-Infrared Video Signals using Pre-Trained Transformers (Jonathan F. Carter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan F. Carter, João Jorge, Oliver Gibson, Lionel Tarassenko. (2024)<br><strong>SleepVST: Sleep Staging from Near-Infrared Video Signals using Pre-Trained Transformers</strong><br><button class=copy-to-clipboard title="SleepVST: Sleep Staging from Near-Infrared Video Signals using Pre-Trained Transformers" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-HC, cs.CV, q-bio-NC<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03831v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03831v1.pdf filename=2404.03831v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances in camera-based physiological monitoring have enabled the robust, non-contact measurement of respiration and the cardiac pulse, which are known to be indicative of the sleep stage. This has led to research into camera-based sleep monitoring as a promising alternative to &ldquo;gold-standard&rdquo; polysomnography, which is cumbersome, expensive to administer, and hence unsuitable for longer-term clinical studies. In this paper, we introduce SleepVST, a <b>transformer</b> model which enables state-of-the-art performance in camera-based sleep stage classification (sleep staging). After pre-training on contact sensor data, SleepVST outperforms existing methods for cardio-respiratory sleep staging on the SHHS and MESA datasets, achieving total Cohen&rsquo;s kappa scores of 0.75 and 0.77 respectively. We then show that SleepVST can be successfully transferred to cardio-respiratory waveforms extracted from video, enabling fully contact-free sleep staging. Using a video dataset of 50 nights, we achieve a total accuracy of 78.8% and a Cohen&rsquo;s $\kappa$ of 0.71 in four-class video-based sleep staging, setting a new state-of-the-art in the domain.</p></p class="citation"></blockquote><h3 id=5265--149237-quantifying-uncertainty-in-motion-prediction-with-variational-bayesian-mixture-juanwu-lu-et-al-2024>(52/65 | 149/237) Quantifying Uncertainty in Motion Prediction with Variational Bayesian Mixture (Juanwu Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juanwu Lu, Can Cui, Yunsheng Ma, Aniket Bera, Ziran Wang. (2024)<br><strong>Quantifying Uncertainty in Motion Prediction with Variational Bayesian Mixture</strong><br><button class=copy-to-clipboard title="Quantifying Uncertainty in Motion Prediction with Variational Bayesian Mixture" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03789v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03789v1.pdf filename=2404.03789v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Safety and robustness are crucial factors in developing trustworthy autonomous vehicles. One essential aspect of addressing these factors is to equip vehicles with the capability to predict future trajectories for all moving objects in the surroundings and quantify prediction uncertainties. In this paper, we propose the Sequential Neural Variational Agent (SeNeVA), a generative model that describes the distribution of future trajectories for a single moving object. Our approach can distinguish <b>Out-of-Distribution</b> data while quantifying uncertainty and achieving competitive performance compared to state-of-the-art methods on the Argoverse 2 and INTERACTION datasets. Specifically, a 0.446 meters minimum Final Displacement Error, a 0.203 meters minimum Average Displacement Error, and a 5.35% Miss Rate are achieved on the INTERACTION test set. Extensive qualitative and quantitative analysis is also provided to evaluate the proposed model. Our open-source code is available at <a href=https://github.com/PurdueDigitalTwin/seneva>https://github.com/PurdueDigitalTwin/seneva</a>.</p></p class="citation"></blockquote><h3 id=5365--150237-flattening-the-parent-bias-hierarchical-semantic-segmentation-in-the-poincaré-ball-simon-weber-et-al-2024>(53/65 | 150/237) Flattening the Parent Bias: Hierarchical Semantic Segmentation in the Poincaré Ball (Simon Weber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Weber, Barış Zöngür, Nikita Araslanov, Daniel Cremers. (2024)<br><strong>Flattening the Parent Bias: Hierarchical Semantic Segmentation in the Poincaré Ball</strong><br><button class=copy-to-clipboard title="Flattening the Parent Bias: Hierarchical Semantic Segmentation in the Poincaré Ball" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03778v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03778v1.pdf filename=2404.03778v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hierarchy is a natural representation of semantic taxonomies, including the ones routinely used in image segmentation. Indeed, recent work on semantic segmentation reports improved accuracy from <b>supervised</b> training leveraging hierarchical label structures. Encouraged by these results, we revisit the fundamental assumptions behind that work. We postulate and then empirically verify that the reasons for the observed improvement in segmentation accuracy may be entirely unrelated to the use of the semantic hierarchy. To demonstrate this, we design a range of cross-domain experiments with a representative hierarchical approach. We find that on the new testing domains, a flat (non-hierarchical) segmentation network, in which the parents are inferred from the children, has superior segmentation accuracy to the hierarchical approach across the board. Complementing these findings and inspired by the intrinsic properties of hyperbolic spaces, we study a more principled approach to hierarchical segmentation using the Poincar'e ball model. The hyperbolic representation largely outperforms the previous (Euclidean) hierarchical approach as well and is on par with our flat Euclidean baseline in terms of segmentation accuracy. However, it additionally exhibits surprisingly strong calibration quality of the parent nodes in the semantic hierarchy, especially on the more challenging domains. Our combined analysis suggests that the established practice of hierarchical segmentation may be limited to in-domain settings, whereas flat classifiers generalize substantially better, especially if they are modeled in the hyperbolic space.</p></p class="citation"></blockquote><h3 id=5465--151237-sc4d-sparse-controlled-video-to-4d-generation-and-motion-transfer-zijie-wu-et-al-2024>(54/65 | 151/237) SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer (Zijie Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijie Wu, Chaohui Yu, Yanqin Jiang, Chenjie Cao, Fan Wang, Xiang Bai. (2024)<br><strong>SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer</strong><br><button class=copy-to-clipboard title="SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03736v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03736v1.pdf filename=2404.03736v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in 2D/3D generative models enable the generation of dynamic 3D objects from a single-view video. Existing approaches utilize score <b>distillation</b> sampling to form the dynamic scene as dynamic NeRF or dense 3D Gaussians. However, these methods struggle to strike a balance among reference view alignment, spatio-temporal consistency, and motion fidelity under single-view conditions due to the implicit nature of NeRF or the intricate dense Gaussian motion prediction. To address these issues, this paper proposes an efficient, sparse-controlled video-to-4D framework named SC4D, that decouples motion and appearance to achieve superior video-to-4D generation. Moreover, we introduce Adaptive Gaussian (AG) initialization and Gaussian Alignment (GA) loss to mitigate shape degeneration issue, ensuring the fidelity of the learned motion and shape. Comprehensive experimental results demonstrate that our method surpasses existing methods in both quality and efficiency. In addition, facilitated by the disentangled modeling of motion and appearance of SC4D, we devise a novel application that seamlessly transfers the learned motion onto a diverse array of 4D entities according to textual descriptions.</p></p class="citation"></blockquote><h3 id=5565--152237-ow-viscap-open-world-video-instance-segmentation-and-captioning-anwesa-choudhuri-et-al-2024>(55/65 | 152/237) OW-VISCap: Open-World Video Instance Segmentation and Captioning (Anwesa Choudhuri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anwesa Choudhuri, Girish Chowdhary, Alexander G. Schwing. (2024)<br><strong>OW-VISCap: Open-World Video Instance Segmentation and Captioning</strong><br><button class=copy-to-clipboard title="OW-VISCap: Open-World Video Instance Segmentation and Captioning" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03657v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03657v1.pdf filename=2404.03657v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-world video instance segmentation is an important video understanding task. Yet most methods either operate in a closed-world setting, require an additional user-input, or use classic region-based proposals to identify never before seen objects. Further, these methods only assign a one-word label to detected objects, and don&rsquo;t generate rich object-centric descriptions. They also often suffer from highly overlapping predictions. To address these issues, we propose Open-World Video Instance Segmentation and Captioning (OW-VISCap), an approach to jointly segment, track, and caption previously seen or unseen objects in a video. For this, we introduce open-world object queries to discover never before seen objects without additional user-input. We generate rich and descriptive object-centric captions for each detected object via a masked attention augmented <b>LLM</b> input. We introduce an inter-query contrastive loss to ensure that the object queries differ from one another. Our generalized approach matches or surpasses state-of-the-art on three tasks: open-world video instance segmentation on the BURST dataset, dense video object captioning on the VidSTG dataset, and closed-world video instance segmentation on the OVIS dataset.</p></p class="citation"></blockquote><h3 id=5665--153237-adaglimpse-active-visual-exploration-with-arbitrary-glimpse-position-and-scale-adam-pardyl-et-al-2024>(56/65 | 153/237) AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position and Scale (Adam Pardyl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adam Pardyl, Michał Wronka, Maciej Wołczyk, Kamil Adamczewski, Tomasz Trzciński, Bartosz Zieliński. (2024)<br><strong>AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position and Scale</strong><br><button class=copy-to-clipboard title="AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position and Scale" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03482v1.pdf filename=2404.03482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Active Visual Exploration (AVE) is a task that involves dynamically selecting observations (glimpses), which is critical to facilitate comprehension and navigation within an environment. While modern AVE methods have demonstrated impressive performance, they are constrained to fixed-scale glimpses from rigid grids. In contrast, existing mobile platforms equipped with optical zoom capabilities can capture glimpses of arbitrary positions and scales. To address this gap between software and hardware capabilities, we introduce AdaGlimpse. It uses Soft Actor-Critic, a <b>reinforcement</b> <b>learning</b> algorithm tailored for exploration tasks, to select glimpses of arbitrary position and scale. This approach enables our model to rapidly establish a general awareness of the environment before zooming in for detailed analysis. Experimental results demonstrate that AdaGlimpse surpasses previous methods across various visual tasks while maintaining greater applicability in realistic AVE scenarios.</p></p class="citation"></blockquote><h3 id=5765--154237-nmf-based-analysis-of-mobile-eye-tracking-data-daniel-klötzl-et-al-2024>(57/65 | 154/237) NMF-Based Analysis of Mobile Eye-Tracking Data (Daniel Klötzl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Klötzl, Tim Krake, Frank Heyen, Michael Becher, Maurice Koch, Daniel Weiskopf, Kuno Kurzhals. (2024)<br><strong>NMF-Based Analysis of Mobile Eye-Tracking Data</strong><br><button class=copy-to-clipboard title="NMF-Based Analysis of Mobile Eye-Tracking Data" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03417v1.pdf filename=2404.03417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The depiction of scanpaths from mobile eye-tracking recordings by thumbnails from the stimulus allows the application of visual computing to detect areas of interest in an <b>unsupervised</b> way. We suggest using nonnegative matrix factorization (NMF) to identify such areas in stimuli. For a user-defined integer k, NMF produces an explainable decomposition into k components, each consisting of a spatial representation associated with a temporal indicator. In the context of multiple eye-tracking recordings, this leads to k spatial representations, where the temporal indicator highlights the appearance within recordings. The choice of k provides an opportunity to control the refinement of the decomposition, i.e., the number of areas to detect. We combine our NMF-based approach with visualization techniques to enable an exploratory analysis of multiple recordings. Finally, we demonstrate the usefulness of our approach with mobile eye-tracking data of an art gallery.</p></p class="citation"></blockquote><h3 id=5865--155237-design-and-development-of-a-framework-for-stroke-based-handwritten-gujarati-font-generation-preeti-p-bhatt-et-al-2024>(58/65 | 155/237) Design and Development of a Framework For Stroke-Based Handwritten Gujarati Font Generation (Preeti P. Bhatt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Preeti P. Bhatt, Jitendra V. Nasriwala, Rakesh R. Savant. (2024)<br><strong>Design and Development of a Framework For Stroke-Based Handwritten Gujarati Font Generation</strong><br><button class=copy-to-clipboard title="Design and Development of a Framework For Stroke-Based Handwritten Gujarati Font Generation" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Optical Character Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03277v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03277v1.pdf filename=2404.03277v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Handwritten font generation is important for preserving cultural heritage and creating personalized designs. It adds an authentic and expressive touch to printed materials, making them visually appealing and establishing a stronger connection with the audience. This paper aims to design a framework for generating handwritten fonts in the Gujarati script, mimicking the variation of human handwriting. The proposed font generation model consists of a learning phase and a generation phase. In the learning phase, Gujarati scripts are analyzed, and rules for designing each character are formulated. This ruleset involves the concatenation of strokes in a stroke-based manner, ensuring visual consistency in the resulting glyphs. The generation phase involves the user providing a small subset of characters, and the system automatically generates the remaining character glyphs based on extracted strokes and learned rules, resulting in handwritten Gujarati fonts. The resulting character glyphs are converted into an open-type font using the FontForge tool, making them compatible with any Gujarati editor. Both subjective and objective evaluations are conducted to assess the synthesized images and fonts. Subjective evaluation through user studies provides feedback on quality and visual appeal, achieving an overall accuracy of 84.84%. Notably, eleven characters demonstrated a success ratio above 90%. Objective evaluation using an existing recognition system achieves an overall accuracy of 84.28% in <b>OCR</b> evaluation. Notably, fifteen characters had a success ratio of 80% or higher.</p></p class="citation"></blockquote><h3 id=5965--156237-multi-positive-contrastive-learning-with-pose-consistent-generated-images-sho-inayoshi-et-al-2024>(59/65 | 156/237) Multi Positive Contrastive Learning with Pose-Consistent Generated Images (Sho Inayoshi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sho Inayoshi, Aji Resindra Widya, Satoshi Ozaki, Junji Otsuka, Takeshi Ohashi. (2024)<br><strong>Multi Positive Contrastive Learning with Pose-Consistent Generated Images</strong><br><button class=copy-to-clipboard title="Multi Positive Contrastive Learning with Pose-Consistent Generated Images" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03256v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03256v1.pdf filename=2404.03256v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model pre-training has become essential in various recognition tasks. Meanwhile, with the remarkable advancements in image generation models, pre-training methods utilizing generated images have also emerged given their ability to produce unlimited training data. However, while existing methods utilizing generated images excel in classification, they fall short in more practical tasks, such as human pose estimation. In this paper, we have experimentally demonstrated it and propose the generation of visually distinct images with identical human poses. We then propose a novel multi-positive <b>contrastive</b> <b>learning,</b> which optimally utilize the previously generated images to learn structural features of the human body. We term the entire learning pipeline as GenPoCCL. Despite using only less than 1% amount of data compared to current state-of-the-art method, GenPoCCL captures structural features of the human body more effectively, surpassing existing methods in a variety of human-centric perception tasks.</p></p class="citation"></blockquote><h3 id=6065--157237-iseg-interactive-3d-segmentation-via-interactive-attention-itai-lang-et-al-2024>(60/65 | 157/237) iSeg: Interactive 3D Segmentation via Interactive Attention (Itai Lang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Itai Lang, Fei Xu, Dale Decatur, Sudarshan Babu, Rana Hanocka. (2024)<br><strong>iSeg: Interactive 3D Segmentation via Interactive Attention</strong><br><button class=copy-to-clipboard title="iSeg: Interactive 3D Segmentation via Interactive Attention" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 10<br>Keywords: Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03219v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03219v1.pdf filename=2404.03219v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present iSeg, a new interactive technique for segmenting 3D shapes. Previous works have focused mainly on leveraging pre-trained 2D <b>foundation</b> <b>models</b> for 3D segmentation based on text. However, text may be insufficient for accurately describing fine-grained spatial segmentations. Moreover, achieving a consistent 3D segmentation using a 2D model is challenging since occluded areas of the same semantic region may not be visible together from any 2D view. Thus, we design a segmentation method conditioned on fine user clicks, which operates entirely in 3D. Our system accepts user clicks directly on the shape&rsquo;s surface, indicating the inclusion or exclusion of regions from the desired shape partition. To accommodate various click settings, we propose a novel interactive attention module capable of processing different numbers and types of clicks, enabling the training of a single unified interactive segmentation model. We apply iSeg to a myriad of shapes from different domains, demonstrating its versatility and faithfulness to the user&rsquo;s specifications. Our project page is at <a href=https://threedle.github.io/iSeg/>https://threedle.github.io/iSeg/</a>.</p></p class="citation"></blockquote><h3 id=6165--158237-corp-a-multi-modal-dataset-for-campus-oriented-roadside-perception-tasks-beibei-wang-et-al-2024>(61/65 | 158/237) CORP: A Multi-Modal Dataset for Campus-Oriented Roadside Perception Tasks (Beibei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Beibei Wang, Lu Zhang, Shuang Meng, Chenjie Wang, Jingjing Huang, Yao Li, Haojie Ren, Yuxuan Xiao, Yuru Peng, Jianmin Ji, Yu Zhang, Yanyong Zhang. (2024)<br><strong>CORP: A Multi-Modal Dataset for Campus-Oriented Roadside Perception Tasks</strong><br><button class=copy-to-clipboard title="CORP: A Multi-Modal Dataset for Campus-Oriented Roadside Perception Tasks" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03191v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03191v1.pdf filename=2404.03191v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Numerous roadside perception datasets have been introduced to propel advancements in autonomous driving and intelligent transportation systems research and development. However, it has been observed that the majority of their concentrates is on urban arterial roads, inadvertently overlooking residential areas such as parks and campuses that exhibit entirely distinct characteristics. In light of this gap, we propose CORP, which stands as the first public <b>benchmark</b> dataset tailored for <b>multi-modal</b> roadside perception tasks under campus scenarios. Collected in a university campus, CORP consists of over 205k images plus 102k point clouds captured from 18 cameras and 9 LiDAR sensors. These sensors with different configurations are mounted on roadside utility poles to provide diverse viewpoints within the campus region. The annotations of CORP encompass multi-dimensional information beyond 2D and 3D bounding boxes, providing extra support for 3D seamless tracking and instance segmentation with unique IDs and pixel masks for identifying targets, to enhance the understanding of objects and their behaviors distributed across the campus premises. Unlike other roadside datasets about urban traffic, CORP extends the spectrum to highlight the challenges for <b>multi-modal</b> perception in campuses and other residential areas.</p></p class="citation"></blockquote><h3 id=6265--159237-como-compact-mapping-and-odometry-eric-dexheimer-et-al-2024>(62/65 | 159/237) COMO: Compact Mapping and Odometry (Eric Dexheimer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric Dexheimer, Andrew J. Davison. (2024)<br><strong>COMO: Compact Mapping and Odometry</strong><br><button class=copy-to-clipboard title="COMO: Compact Mapping and Odometry" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03531v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03531v1.pdf filename=2404.03531v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present COMO, a real-time monocular mapping and odometry system that encodes dense <b>geometry</b> via a compact set of 3D anchor points. Decoding anchor point projections into dense <b>geometry</b> via per-keyframe depth covariance functions guarantees that depth maps are joined together at visible anchor points. The representation enables joint optimization of camera poses and dense <b>geometry,</b> intrinsic 3D consistency, and efficient second-order inference. To maintain a compact yet expressive map, we introduce a frontend that leverages the covariance function for tracking and initializing potentially visually indistinct 3D points across frames. Altogether, we introduce a real-time system capable of estimating accurate poses and consistent <b>geometry.</b></p></p class="citation"></blockquote><h3 id=6365--160237-you-only-scan-once-a-dynamic-scene-reconstruction-pipeline-for-6-dof-robotic-grasping-of-novel-objects-lei-zhou-et-al-2024>(63/65 | 160/237) You Only Scan Once: A Dynamic Scene Reconstruction Pipeline for 6-DoF Robotic Grasping of Novel Objects (Lei Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Zhou, Haozhe Wang, Zhengshen Zhang, Zhiyang Liu, Francis EH Tay, adn Marcelo H. Ang. Jr. (2024)<br><strong>You Only Scan Once: A Dynamic Scene Reconstruction Pipeline for 6-DoF Robotic Grasping of Novel Objects</strong><br><button class=copy-to-clipboard title="You Only Scan Once: A Dynamic Scene Reconstruction Pipeline for 6-DoF Robotic Grasping of Novel Objects" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03462v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03462v1.pdf filename=2404.03462v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of robotic grasping, achieving accurate and reliable interactions with the environment is a pivotal challenge. Traditional methods of grasp planning methods utilizing partial point clouds derived from depth image often suffer from reduced scene understanding due to occlusion, ultimately impeding their grasping accuracy. Furthermore, scene reconstruction methods have primarily relied upon static techniques, which are susceptible to environment change during manipulation process limits their efficacy in real-time grasping tasks. To address these limitations, this paper introduces a novel two-stage pipeline for dynamic scene reconstruction. In the first stage, our approach takes scene scanning as input to register each target object with mesh reconstruction and novel object pose tracking. In the second stage, pose tracking is still performed to provide object poses in real-time, enabling our approach to transform the reconstructed object point clouds back into the scene. Unlike conventional methodologies, which rely on static scene snapshots, our method continuously captures the evolving scene <b>geometry,</b> resulting in a comprehensive and up-to-date point cloud representation. By circumventing the constraints posed by occlusion, our method enhances the overall grasp planning process and empowers state-of-the-art 6-DoF robotic grasping algorithms to exhibit markedly improved accuracy.</p></p class="citation"></blockquote><h3 id=6465--161237-towards-more-realistic-human-motion-prediction-with-attention-to-motion-coordination-pengxiang-ding-et-al-2024>(64/65 | 161/237) Towards more realistic human motion prediction with attention to motion coordination (Pengxiang Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengxiang Ding, Jianqin Yin. (2024)<br><strong>Towards more realistic human motion prediction with attention to motion coordination</strong><br><button class=copy-to-clipboard title="Towards more realistic human motion prediction with attention to motion coordination" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03584v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03584v1.pdf filename=2404.03584v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Joint relation modeling is a curial component in human motion prediction. Most existing methods rely on skeletal-based <b>graphs</b> to build the joint relations, where local interactive relations between joint pairs are well learned. However, the motion coordination, a global joint relation reflecting the simultaneous cooperation of all joints, is usually weakened because it is learned from part to whole progressively and asynchronously. Thus, the final predicted motions usually appear unrealistic. To tackle this issue, we learn a medium, called coordination attractor (CA), from the spatiotemporal features of motion to characterize the global motion features, which is subsequently used to build new relative joint relations. Through the CA, all joints are related simultaneously, and thus the motion coordination of all joints can be better learned. Based on this, we further propose a novel joint relation modeling module, Comprehensive Joint Relation Extractor (CJRE), to combine this motion coordination with the local interactions between joint pairs in a unified manner. Additionally, we also present a Multi-timescale Dynamics Extractor (MTDE) to extract enriched dynamics from the raw position information for effective prediction. Extensive experiments show that the proposed framework outperforms state-of-the-art methods in both short- and long-term predictions on H3.6M, CMU-Mocap, and 3DPW.</p></p class="citation"></blockquote><h3 id=6565--162237-sp2ot-semantic-regularized-progressive-partial-optimal-transport-for-imbalanced-clustering-chuyu-zhang-et-al-2024>(65/65 | 162/237) SP$^2$OT: Semantic-Regularized Progressive Partial Optimal Transport for Imbalanced Clustering (Chuyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuyu Zhang, Hui Ren, Xuming He. (2024)<br><strong>SP$^2$OT: Semantic-Regularized Progressive Partial Optimal Transport for Imbalanced Clustering</strong><br><button class=copy-to-clipboard title="SP$^2$OT: Semantic-Regularized Progressive Partial Optimal Transport for Imbalanced Clustering" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03446v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03446v1.pdf filename=2404.03446v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>clustering,</b> which learns representation and semantic <b>clustering</b> without labels information, poses a great challenge for deep learning-based approaches. Despite significant progress in recent years, most existing methods focus on uniformly distributed datasets, significantly limiting the practical applicability of their methods. In this paper, we propose a more practical problem setting named deep imbalanced <b>clustering,</b> where the underlying classes exhibit an imbalance distribution. To address this challenge, we introduce a novel optimal transport-based pseudo-label learning framework. Our framework formulates pseudo-label generation as a Semantic-regularized Progressive Partial Optimal Transport (SP$^2$OT) problem, which progressively transports each sample to imbalanced clusters under several prior distribution and semantic relation constraints, thus generating high-quality and imbalance-aware pseudo-labels. To solve SP$^2$OT, we develop a Majorization-Minimization-based optimization algorithm. To be more precise, we employ the strategy of majorization to reformulate the SP$^2$OT problem into a Progressive Partial Optimal Transport problem, which can be transformed into an unbalanced optimal transport problem with augmented constraints and can be solved efficiently by a fast matrix scaling algorithm. Experiments on various datasets, including a human-curated long-tailed CIFAR100, challenging ImageNet-R, and large-scale subsets of fine-grained iNaturalist2018 datasets, demonstrate the superiority of our method.</p></p class="citation"></blockquote><h2 id=mathoc-2>math.OC (2)</h2><h3 id=12--163237-capabilities-of-large-language-models-in-control-engineering-a-benchmark-study-on-gpt-4-claude-3-opus-and-gemini-10-ultra-darioush-kevian-et-al-2024>(1/2 | 163/237) Capabilities of Large Language Models in Control Engineering: A Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra (Darioush Kevian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Darioush Kevian, Usman Syed, Xingang Guo, Aaron Havens, Geir Dullerud, Peter Seiler, Lianhui Qin, Bin Hu. (2024)<br><strong>Capabilities of Large Language Models in Control Engineering: A Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra</strong><br><button class=copy-to-clipboard title="Capabilities of Large Language Models in Control Engineering: A Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-AI, cs-LG, math-OC, math.OC<br>Keyword Score: 73<br>Keywords: Benchmarking, Claude, GPT, GPT-4, Gemini, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03647v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03647v1.pdf filename=2404.03647v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we explore the capabilities of state-of-the-art <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> such as <b>GPT-4,</b> <b>Claude</b> 3 Opus, and <b>Gemini</b> 1.0 Ultra in solving undergraduate-level control problems. Controls provides an interesting case study for <b>LLM</b> <b>reasoning</b> due to its combination of mathematical theory and engineering design. We introduce ControlBench, a <b>benchmark</b> dataset tailored to reflect the breadth, depth, and complexity of classical control design. We use this dataset to study and evaluate the problem-solving abilities of these <b>LLMs</b> in the context of control engineering. We present evaluations conducted by a panel of human experts, providing insights into the accuracy, <b>reasoning,</b> and explanatory prowess of <b>LLMs</b> in control engineering. Our analysis reveals the strengths and limitations of each <b>LLM</b> in the context of classical control, and our results imply that <b>Claude</b> 3 Opus has become the state-of-the-art <b>LLM</b> for solving undergraduate control problems. Our study serves as an initial step towards the broader goal of employing artificial general intelligence in control engineering.</p></p class="citation"></blockquote><h3 id=22--164237-elementary-analysis-of-policy-gradient-methods-jiacai-liu-et-al-2024>(2/2 | 164/237) Elementary Analysis of Policy Gradient Methods (Jiacai Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiacai Liu, Wenye Li, Ke Wei. (2024)<br><strong>Elementary Analysis of Policy Gradient Methods</strong><br><button class=copy-to-clipboard title="Elementary Analysis of Policy Gradient Methods" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03372v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03372v1.pdf filename=2404.03372v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Projected policy gradient under the simplex parameterization, policy gradient and natural policy gradient under the softmax parameterization, are fundamental algorithms in <b>reinforcement</b> <b>learning.</b> There have been a flurry of recent activities in studying these algorithms from the theoretical aspect. Despite this, their convergence behavior is still not fully understood, even given the access to exact policy evaluations. In this paper, we focus on the discounted MDP setting and conduct a systematic study of the aforementioned policy optimization methods. Several novel results are presented, including 1) global linear convergence of projected policy gradient for any constant step size, 2) sublinear convergence of softmax policy gradient for any constant step size, 3) global linear convergence of softmax natural policy gradient for any constant step size, 4) global linear convergence of entropy regularized softmax policy gradient for a wider range of constant step sizes than existing result, 5) tight local linear convergence rate of entropy regularized natural policy gradient, and 6) a new and concise local quadratic convergence rate of soft policy iteration without the assumption on the stationary distribution under the optimal policy. New and elementary analysis techniques have been developed to establish these results.</p></p class="citation"></blockquote><h2 id=csse-3>cs.SE (3)</h2><h3 id=13--165237-codeeditorbench-evaluating-code-editing-capability-of-large-language-models-jiawei-guo-et-al-2024>(1/3 | 165/237) CodeEditorBench: Evaluating Code Editing Capability of Large Language Models (Jiawei Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Guo, Ziming Li, Xueling Liu, Kaijing Ma, Tianyu Zheng, Zhouliang Yu, Ding Pan, Yizhi LI, Ruibo Liu, Yue Wang, Shuyue Guo, Xingwei Qu, Xiang Yue, Ge Zhang, Wenhu Chen, Jie Fu. (2024)<br><strong>CodeEditorBench: Evaluating Code Editing Capability of Large Language Models</strong><br><button class=copy-to-clipboard title="CodeEditorBench: Evaluating Code Editing Capability of Large Language Models" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-CL, cs-LG, cs-SE, cs.SE<br>Keyword Score: 73<br>Keywords: Benchmarking, GPT, GPT-4, Gemini, Code Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03543v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03543v1.pdf filename=2404.03543v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for <b>code</b> <b>are</b> rapidly evolving, with <b>code</b> <b>editing</b> emerging as a critical capability. We introduce CodeEditorBench, an evaluation framework designed to rigorously assess the performance of <b>LLMs</b> in <b>code</b> <b>editing</b> tasks, including debugging, translating, polishing, and requirement switching. Unlike existing <b>benchmarks</b> focusing solely on <b>code</b> <b>generation,</b> CodeEditorBench emphasizes real-world scenarios and practical aspects of software development. We curate diverse coding challenges and scenarios from five sources, covering various programming languages, complexity levels, and editing tasks. Evaluation of 19 <b>LLMs</b> reveals that closed-source models (particularly <b>Gemini-Ultra</b> and <b>GPT-4),</b> outperform open-source models in CodeEditorBench, highlighting differences in model performance based on problem types and <b>prompt</b> sensitivities. CodeEditorBench aims to catalyze advancements in <b>LLMs</b> by providing a robust platform for assessing <b>code</b> <b>editing</b> capabilities. We will release all <b>prompts</b> and datasets to enable the community to expand the dataset and <b>benchmark</b> emerging <b>LLMs.</b> By introducing CodeEditorBench, we contribute to the advancement of <b>LLMs</b> in <b>code</b> <b>editing</b> and provide a valuable resource for researchers and practitioners.</p></p class="citation"></blockquote><h3 id=23--166237-multi-modal-learning-for-webassembly-reverse-engineering-hanxian-huang-et-al-2024>(2/3 | 166/237) Multi-modal Learning for WebAssembly Reverse Engineering (Hanxian Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanxian Huang, Jishen Zhao. (2024)<br><strong>Multi-modal Learning for WebAssembly Reverse Engineering</strong><br><button class=copy-to-clipboard title="Multi-modal Learning for WebAssembly Reverse Engineering" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-LG, cs-PL, cs-SE, cs.SE<br>Keyword Score: 73<br>Keywords: Few-shot, Fine-tuning, Fine-tuning, Multi-modal, Self-supervised Learning, Self-supervised Learning, Pre-trained Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03171v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03171v1.pdf filename=2404.03171v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing adoption of WebAssembly (Wasm) for performance-critical and security-sensitive tasks drives the demand for WebAssembly program comprehension and reverse engineering. Recent studies have introduced machine learning (ML)-based WebAssembly reverse engineering tools. Yet, the generalization of task-specific ML solutions remains challenging, because their effectiveness hinges on the availability of an ample supply of high-quality task-specific labeled data. Moreover, previous works overlook the high-level semantics present in source code and its documentation. Acknowledging the abundance of available source code with documentation, which can be compiled into WebAssembly, we propose to learn representations of them concurrently and harness their mutual relationships for effective WebAssembly reverse engineering. In this paper, we present WasmRev, the first <b>multi-modal</b> <b>pre-trained</b> <b>language</b> <b>model</b> for WebAssembly reverse engineering. WasmRev is <b>pre-trained</b> <b>using</b> <b>self-supervised</b> <b>learning</b> on a large-scale <b>multi-modal</b> corpus encompassing source code, code documentation and the compiled WebAssembly, without requiring labeled data. WasmRev incorporates three tailored <b>multi-modal</b> pre-training tasks to capture various characteristics of WebAssembly and cross-modal relationships. WasmRev is only trained once to produce general-purpose representations that can broadly support WebAssembly reverse engineering tasks through <b>few-shot</b> <b>fine-tuning</b> with much less labeled data, improving data efficiency. We <b>fine-tune</b> WasmRev onto three important reverse engineering tasks: type recovery, function purpose identification and WebAssembly <b>summarization.</b> Our results show that WasmRev <b>pre-trained</b> <b>on</b> <b>the</b> corpus of <b>multi-modal</b> samples establishes a robust foundation for these tasks, achieving high task accuracy and outperforming the state-of-the-art ML methods for WebAssembly reverse engineering.</p></p class="citation"></blockquote><h3 id=33--167237-towards-standards-compliant-assistive-technology-product-specifications-via-llms-chetan-arora-et-al-2024>(3/3 | 167/237) Towards Standards-Compliant Assistive Technology Product Specifications via LLMs (Chetan Arora et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chetan Arora, John Grundy, Louise Puli, Natasha Layton. (2024)<br><strong>Towards Standards-Compliant Assistive Technology Product Specifications via LLMs</strong><br><button class=copy-to-clipboard title="Towards Standards-Compliant Assistive Technology Product Specifications via LLMs" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03122v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03122v1.pdf filename=2404.03122v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving field of assistive technology (AT), ensuring that products meet national and international standards is essential for user safety, efficacy, and accessibility. In this vision paper, we introduce CompliAT, a pioneering framework designed to streamline the compliance process of AT product specifications with these standards through the innovative use of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> CompliAT addresses three critical tasks: checking terminology consistency, classifying products according to standards, and tracing key product specifications to standard requirements. We tackle the challenge of terminology consistency to ensure that the language used in product specifications aligns with relevant standards, reducing misunderstandings and non-compliance risks. We propose a novel approach for product classification, leveraging a <b>retrieval-augmented</b> <b>generation</b> <b>model</b> to accurately categorize AT products aligning to international standards, despite the sparse availability of training data. Finally, CompliAT implements a traceability and compliance mechanism from key product specifications to standard requirements, ensuring all aspects of an AT product are thoroughly vetted against the corresponding standards. By semi-automating these processes, CompliAT aims to significantly reduce the time and effort required for AT product standards compliance and uphold quality and safety standards. We outline our planned implementation and evaluation plan for CompliAT.</p></p class="citation"></blockquote><h2 id=eessiv-4>eess.IV (4)</h2><h3 id=14--168237-changemamba-remote-sensing-change-detection-with-spatio-temporal-state-space-model-hongruixuan-chen-et-al-2024>(1/4 | 168/237) ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State Space Model (Hongruixuan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongruixuan Chen, Jian Song, Chengxi Han, Junshi Xia, Naoto Yokoya. (2024)<br><strong>ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State Space Model</strong><br><button class=copy-to-clipboard title="ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State Space Model" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, eess-IV, eess.IV<br>Keyword Score: 43<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03425v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03425v1.pdf filename=2404.03425v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Convolutional</b> <b>neural</b> <b>networks</b> <b>(CNN)</b> and <b>Transformers</b> have made impressive progress in the field of remote sensing change detection (CD). However, both architectures have their inherent shortcomings. Recently, the Mamba architecture, based on spatial state models, has shown remarkable performance in a series of natural language processing tasks, which can effectively compensate for the shortcomings of the above two architectures. In this paper, we explore for the first time the potential of the Mamba architecture for remote sensing change detection tasks. We tailor the corresponding frameworks, called MambaBCD, MambaSCD, and MambaBDA, for binary change detection (BCD), semantic change detection (SCD), and building damage assessment (BDA), respectively. All three frameworks adopt the cutting-edge visual Mamba architecture as the encoder, which allows full learning of global spatial contextual information from the input images. For the change decoder, which is available in all three architectures, we propose three spatio-temporal relationship modeling mechanisms, which can be naturally combined with the Mamba architecture and fully utilize its attribute to achieve spatio-temporal interaction of multi-temporal features and obtain accurate change information. On five <b>benchmark</b> datasets, our proposed frameworks outperform current CNN- and <b>Transformer-based</b> approaches without using any complex strategies or tricks, fully demonstrating the potential of the Mamba architecture. Specifically, we obtained 83.11%, 88.39% and 94.19% F1 scores on the three BCD datasets SYSU, LEVIR-CD+, and WHU-CD; on the SCD dataset SECOND, we obtained 24.04% SeK; and on the xBD dataset, we obtained 81.41% overall F1 score. The source code will be available in <a href=https://github.com/ChenHongruixuan/MambaCD>https://github.com/ChenHongruixuan/MambaCD</a></p></p class="citation"></blockquote><h3 id=24--169237-mitigating-analytical-variability-in-fmri-results-with-style-transfer-elodie-germani-et-al-2024>(2/4 | 169/237) Mitigating analytical variability in fMRI results with style transfer (Elodie Germani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elodie Germani, Elisa Fromont, Camille Maumet. (2024)<br><strong>Mitigating analytical variability in fMRI results with style transfer</strong><br><button class=copy-to-clipboard title="Mitigating analytical variability in fMRI results with style transfer" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Data Augmentation, Unsupervised Learning, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03703v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03703v1.pdf filename=2404.03703v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel approach to improve the reproducibility of neuroimaging results by converting statistic maps across different functional MRI pipelines. We make the assumption that pipelines can be considered as a <b>style</b> <b>component</b> of <b>data</b> <b>and</b> propose to use different generative models, among which, <b>Diffusion</b> <b>Models</b> (DM) to convert <b>data</b> <b>between</b> pipelines. We design a new DM-based <b>unsupervised</b> multi-domain image-to-image transition framework and constrain the generation of 3D fMRI statistic maps using the latent space of an auxiliary classifier that distinguishes statistic maps from different pipelines. We extend traditional sampling techniques used in DM to improve the transition performance. Our experiments demonstrate that our proposed methods are successful: pipelines can indeed be transferred, providing an important source of <b>data</b> <b>augmentation</b> for future medical studies.</p></p class="citation"></blockquote><h3 id=34--170237-bi-level-guided-diffusion-models-for-zero-shot-medical-imaging-inverse-problems-hossein-askari-et-al-2024>(3/4 | 170/237) Bi-level Guided Diffusion Models for Zero-Shot Medical Imaging Inverse Problems (Hossein Askari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hossein Askari, Fred Roosta, Hongfu Sun. (2024)<br><strong>Bi-level Guided Diffusion Models for Zero-Shot Medical Imaging Inverse Problems</strong><br><button class=copy-to-clipboard title="Bi-level Guided Diffusion Models for Zero-Shot Medical Imaging Inverse Problems" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03706v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03706v1.pdf filename=2404.03706v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of medical imaging, inverse problems aim to infer high-quality images from incomplete, noisy measurements, with the objective of minimizing expenses and risks to patients in clinical settings. The <b>Diffusion</b> <b>Models</b> have recently emerged as a promising approach to such practical challenges, proving particularly useful for the <b>zero-shot</b> inference of images from partially acquired measurements in Magnetic Resonance Imaging (MRI) and Computed Tomography (CT). A central challenge in this approach, however, is how to guide an unconditional prediction to conform to the measurement information. Existing methods rely on deficient projection or inefficient posterior score approximation guidance, which often leads to suboptimal performance. In this paper, we propose \underline{\textbf{B}}i-level \underline{G}uided \underline{D}iffusion \underline{M}odels ({BGDM}), a <b>zero-shot</b> imaging framework that efficiently steers the initial unconditional prediction through a \emph{bi-level} guidance strategy. Specifically, BGDM first approximates an \emph{inner-level} conditional posterior mean as an initial measurement-consistent reference point and then solves an \emph{outer-level} proximal optimization objective to reinforce the measurement consistency. Our experimental findings, using publicly available MRI and CT medical datasets, reveal that BGDM is more effective and efficient compared to the baselines, faithfully generating high-fidelity medical images and substantially reducing hallucinatory artifacts in cases of severe degradation.</p></p class="citation"></blockquote><h3 id=44--171237-segmentation-guided-knee-radiograph-generation-using-conditional-diffusion-models-siyuan-mei-et-al-2024>(4/4 | 171/237) Segmentation-Guided Knee Radiograph Generation using Conditional Diffusion Models (Siyuan Mei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyuan Mei, Fuxin Fan, Fabian Wagner, Mareike Thies, Mingxuan Gu, Yipeng Sun, Andreas Maier. (2024)<br><strong>Segmentation-Guided Knee Radiograph Generation using Conditional Diffusion Models</strong><br><button class=copy-to-clipboard title="Segmentation-Guided Knee Radiograph Generation using Conditional Diffusion Models" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03541v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03541v1.pdf filename=2404.03541v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning-based medical image processing algorithms require representative data during development. In particular, surgical data might be difficult to obtain, and high-quality public datasets are limited. To overcome this limitation and augment datasets, a widely adopted solution is the generation of synthetic images. In this work, we employ conditional <b>diffusion</b> <b>models</b> to generate knee radiographs from contour and bone segmentations. Remarkably, two distinct strategies are presented by incorporating the segmentation as a condition into the sampling and training process, namely, conditional sampling and conditional training. The results demonstrate that both methods can generate realistic images while adhering to the conditioning segmentation. The conditional training method outperforms the conditional sampling method and the conventional U-Net.</p></p class="citation"></blockquote><h2 id=cscr-7>cs.CR (7)</h2><h3 id=17--172237-an-investigation-into-misuse-of-java-security-apis-by-large-language-models-zahra-mousavi-et-al-2024>(1/7 | 172/237) An Investigation into Misuse of Java Security APIs by Large Language Models (Zahra Mousavi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zahra Mousavi, Chadni Islam, Kristen Moore, Alsharif Abuadbba, Muhammad Ali Babar. (2024)<br><strong>An Investigation into Misuse of Java Security APIs by Large Language Models</strong><br><button class=copy-to-clipboard title="An Investigation into Misuse of Java Security APIs by Large Language Models" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs-CY, cs.CR<br>Keyword Score: 40<br>Keywords: ChatGPT, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03823v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03823v1.pdf filename=2404.03823v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing trend of using <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for <b>code</b> <b>generation</b> raises the question of their capability to generate trustworthy <b>code.</b> <b>While</b> many researchers are exploring the utility of <b>code</b> <b>generation</b> for uncovering software vulnerabilities, one crucial but often overlooked aspect is the security Application Programming Interfaces (APIs). APIs play an integral role in upholding software security, yet effectively integrating security APIs presents substantial challenges. This leads to inadvertent misuse by developers, thereby exposing software to vulnerabilities. To overcome these challenges, developers may seek assistance from <b>LLMs.</b> In this paper, we systematically assess <b>ChatGPT&rsquo;s</b> trustworthiness in <b>code</b> <b>generation</b> for security API use cases in Java. To conduct a thorough evaluation, we compile an extensive collection of 48 programming tasks for 5 widely used security APIs. We employ both automated and manual approaches to effectively detect security API misuse in the <b>code</b> <b>generated</b> by <b>ChatGPT</b> for these tasks. Our findings are concerning: around 70% of the <b>code</b> <b>instances</b> across 30 attempts per task contain security API misuse, with 20 distinct misuse types identified. Moreover, for roughly half of the tasks, this rate reaches 100%, indicating that there is a long way to go before developers can rely on <b>ChatGPT</b> to securely implement security API code.</p></p class="citation"></blockquote><h3 id=27--173237-learn-what-you-want-to-unlearn-unlearning-inversion-attacks-against-machine-unlearning-hongsheng-hu-et-al-2024>(2/7 | 173/237) Learn What You Want to Unlearn: Unlearning Inversion Attacks against Machine Unlearning (Hongsheng Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongsheng Hu, Shuo Wang, Tian Dong, Minhui Xue. (2024)<br><strong>Learn What You Want to Unlearn: Unlearning Inversion Attacks against Machine Unlearning</strong><br><button class=copy-to-clipboard title="Learn What You Want to Unlearn: Unlearning Inversion Attacks against Machine Unlearning" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 13<br>Keywords: Benchmarking, Machine Unlearning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03233v1.pdf filename=2404.03233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Machine</b> <b>unlearning</b> has become a promising solution for fulfilling the &ldquo;right to be forgotten&rdquo;, under which individuals can request the deletion of their data from <b>machine</b> <b>learning</b> models. However, existing studies of <b>machine</b> <b>unlearning</b> mainly focus on the efficacy and efficiency of unlearning methods, while neglecting the investigation of the privacy vulnerability during the unlearning process. With two versions of a model available to an adversary, that is, the original model and the unlearned model, <b>machine</b> <b>unlearning</b> opens up a new attack surface. In this paper, we conduct the first investigation to understand the extent to which <b>machine</b> <b>unlearning</b> can leak the confidential content of the unlearned data. Specifically, under the <b>Machine</b> <b>Learning</b> as a Service setting, we propose unlearning inversion attacks that can reveal the feature and label information of an unlearned sample by only accessing the original and unlearned model. The effectiveness of the proposed unlearning inversion attacks is evaluated through extensive experiments on <b>benchmark</b> datasets across various model architectures and on both exact and approximate representative unlearning approaches. The experimental results indicate that the proposed attack can reveal the sensitive information of the unlearned data. As such, we identify three possible defenses that help to mitigate the proposed attacks, while at the cost of reducing the utility of the unlearned model. The study in this paper uncovers an underexplored gap between <b>machine</b> <b>unlearning</b> and the privacy of unlearned data, highlighting the need for the careful design of mechanisms for implementing unlearning without leaking the information of the unlearned data.</p></p class="citation"></blockquote><h3 id=37--174237-ltrdetector-exploring-long-term-relationship-for-advanced-persistent-threats-detection-xiaoxiao-liu-et-al-2024>(3/7 | 174/237) LTRDetector: Exploring Long-Term Relationship for Advanced Persistent Threats Detection (Xiaoxiao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoxiao Liu, Fan Xu, Nan Wang, Qinxin Zhao, Dalin Zhang, Xibin Zhao, Jiqiang Liu. (2024)<br><strong>LTRDetector: Exploring Long-Term Relationship for Advanced Persistent Threats Detection</strong><br><button class=copy-to-clipboard title="LTRDetector: Exploring Long-Term Relationship for Advanced Persistent Threats Detection" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 13<br>Keywords: Graph, Graph Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03162v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03162v1.pdf filename=2404.03162v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advanced Persistent Threat (APT) is challenging to detect due to prolonged duration, infrequent occurrence, and adept concealment techniques. Existing approaches primarily concentrate on the observable traits of attack behaviors, neglecting the intricate relationships formed throughout the persistent attack lifecycle. Thus, we present an innovative APT detection framework named LTRDetector, implementing an end-to-end holistic operation. LTRDetector employs an innovative <b>graph</b> <b>embedding</b> technique to retain comprehensive contextual information, then derives long-term features from these embedded provenance <b>graphs.</b> <b>During</b> the process, we compress the data of the system provenance <b>graph</b> <b>for</b> effective feature learning. Furthermore, in order to detect attacks conducted by using zero-day exploits, we captured the system&rsquo;s regular behavior and detects abnormal activities without relying on predefined attack signatures. We also conducted extensive evaluations using five prominent datasets, the efficacy evaluation of which underscores the superiority of LTRDetector compared to existing state-of-the-art techniques.</p></p class="citation"></blockquote><h3 id=47--175237-r5detect-detecting-control-flow-attacks-from-standard-risc-v-enclaves-davide-bove-et-al-2024>(4/7 | 175/237) R5Detect: Detecting Control-Flow Attacks from Standard RISC-V Enclaves (Davide Bove et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Davide Bove, Lukas Panzer. (2024)<br><strong>R5Detect: Detecting Control-Flow Attacks from Standard RISC-V Enclaves</strong><br><button class=copy-to-clipboard title="R5Detect: Detecting Control-Flow Attacks from Standard RISC-V Enclaves" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Mirai<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03771v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03771v1.pdf filename=2404.03771v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Embedded and Internet-of-Things (IoT) devices are ubiquitous today, and the uprising of several botnets based on them (e.g., <b>Mirai,</b> Ripple20) raises issues about the security of such devices. Especially low-power devices often lack support for modern system security measures, such as stack integrity, Non-eXecutable bits or strong cryptography. In this work, we present R5Detect, a security monitoring software that detects and prevents control-flow attacks on unmodified RISC-V standard architectures. With a novel combination of different protection techniques, it can run on embedded and low-power IoT devices, which may lack proper security features. R5Detect implements a memory-protected shadow stack to prevent runtime modifications, as well as a heuristics detection based on Hardware Performance Counters to detect control-flow integrity violations. Our results indicate that regular software can be protected against different degrees of control-flow manipulations with an average performance overhead of below 5 %. We implement and evaluate R5Detect on standard low-power RISC-V devices and show that such security features can be effectively used with minimal hardware support.</p></p class="citation"></blockquote><h3 id=57--176237-heckler-breaking-confidential-vms-with-malicious-interrupts-benedict-schlüter-et-al-2024>(5/7 | 176/237) Heckler: Breaking Confidential VMs with Malicious Interrupts (Benedict Schlüter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benedict Schlüter, Supraja Sridhara, Mark Kuhne, Andrin Bertschi, Shweta Shinde. (2024)<br><strong>Heckler: Breaking Confidential VMs with Malicious Interrupts</strong><br><button class=copy-to-clipboard title="Heckler: Breaking Confidential VMs with Malicious Interrupts" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Text Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03387v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03387v1.pdf filename=2404.03387v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hardware-based Trusted execution environments (TEEs) offer an isolation granularity of virtual machine abstraction. They provide confidential VMs (CVMs) that host security-sensitive code and data. AMD SEV-SNP and Intel TDX enable CVMs and are now available on popular cloud platforms. The untrusted hypervisor in these settings is in control of several resource management and configuration tasks, including interrupts. We present Heckler, a new attack wherein the hypervisor injects malicious non-timer interrupts to break the confidentiality and integrity of CVMs. Our insight is to use the interrupt handlers that have global effects, such that we can manipulate a CVM&rsquo;s register states to change the data and control flow. With AMD SEV-SNP and Intel TDX, we demonstrate Heckler on OpenSSH and sudo to bypass authentication. On AMD SEV-SNP we break execution integrity of C, Java, and Julia applications that perform statistical and <b>text</b> <b>analysis.</b> We explain the gaps in current defenses and outline guidelines for future defenses.</p></p class="citation"></blockquote><h3 id=67--177237-a-deep-reinforcement-learning-approach-for-security-aware-service-acquisition-in-iot-marco-arazzi-et-al-2024>(6/7 | 177/237) A Deep Reinforcement Learning Approach for Security-Aware Service Acquisition in IoT (Marco Arazzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Arazzi, Serena Nicolazzo, Antonino Nocera. (2024)<br><strong>A Deep Reinforcement Learning Approach for Security-Aware Service Acquisition in IoT</strong><br><button class=copy-to-clipboard title="A Deep Reinforcement Learning Approach for Security-Aware Service Acquisition in IoT" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03276v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03276v1.pdf filename=2404.03276v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The novel Internet of Things (IoT) paradigm is composed of a growing number of heterogeneous smart objects and services that are transforming architectures and applications, increasing systems&rsquo; complexity, and the need for reliability and autonomy. In this context, both smart objects and services are often provided by third parties which do not give full transparency regarding the security and privacy of the features offered. Although machine-based Service Level Agreements (SLA) have been recently leveraged to establish and share policies in Cloud-based scenarios, and also in the IoT context, the issue of making end users aware of the overall system security levels and the fulfillment of their privacy requirements through the provision of the requested service remains a challenging task. To tackle this problem, we propose a complete framework that defines suitable levels of privacy and security requirements in the acquisition of services in IoT, according to the user needs. Through the use of a <b>Reinforcement</b> <b>Learning</b> based solution, a user agent, inside the environment, is trained to choose the best smart objects granting access to the target services. Moreover, the solution is designed to guarantee deadline requirements and user security and privacy needs. Finally, to evaluate the correctness and the performance of the proposed approach we illustrate an extensive experimental analysis.</p></p class="citation"></blockquote><h3 id=77--178237-accurate-low-degree-polynomial-approximation-of-non-polynomial-operators-for-fast-private-inference-in-homomorphic-encryption-jianming-tong-et-al-2024>(7/7 | 178/237) Accurate Low-Degree Polynomial Approximation of Non-polynomial Operators for Fast Private Inference in Homomorphic Encryption (Jianming Tong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianming Tong, Jingtian Dang, Anupam Golder, Callie Hao, Arijit Raychowdhury, Tushar Krishna. (2024)<br><strong>Accurate Low-Degree Polynomial Approximation of Non-polynomial Operators for Fast Private Inference in Homomorphic Encryption</strong><br><button class=copy-to-clipboard title="Accurate Low-Degree Polynomial Approximation of Non-polynomial Operators for Fast Private Inference in Homomorphic Encryption" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03216v1.pdf filename=2404.03216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As machine learning (ML) permeates fields like healthcare, facial recognition, and blockchain, the need to protect sensitive data intensifies. Fully Homomorphic Encryption (FHE) allows inference on encrypted data, preserving the privacy of both data and the ML model. However, it slows down non-secure inference by up to five magnitudes, with a root cause of replacing non-polynomial operators (ReLU and MaxPooling) with high-degree Polynomial Approximated Function (PAF). We propose SmartPAF, a framework to replace non-polynomial operators with low-degree PAF and then recover the accuracy of PAF-approximated model through four techniques: (1) Coefficient Tuning (CT) &ndash; adjust PAF coefficients based on the input distributions before training, (2) Progressive Approximation (PA) &ndash; progressively replace one non-polynomial operator at a time followed by a <b>fine-tuning,</b> (3) Alternate Training (AT) &ndash; alternate the training between PAFs and other linear operators in the decoupled manner, and (4) Dynamic Scale (DS) / Static Scale (SS) &ndash; dynamically scale PAF input value within (-1, 1) in training, and fix the scale as the running max value in FHE deployment. The synergistic effect of CT, PA, AT, and DS/SS enables SmartPAF to enhance the accuracy of the various models approximated by PAFs with various low degrees under multiple datasets. For ResNet-18 under ImageNet-1k, the Pareto-frontier spotted by SmartPAF in latency-accuracy tradeoff space achieves 1.42x ~ 13.64x accuracy improvement and 6.79x ~ 14.9x speedup than prior works. Further, SmartPAF enables a 14-degree PAF (f1^2 g_1^2) to achieve 7.81x speedup compared to the 27-degree PAF obtained by minimax approximation with the same 69.4% post-replacement accuracy. Our code is available at <a href=https://github.com/TorchFHE/SmartPAF>https://github.com/TorchFHE/SmartPAF</a>.</p></p class="citation"></blockquote><h2 id=csro-16>cs.RO (16)</h2><h3 id=116--179237-embodied-ai-with-two-arms-zero-shot-learning-safety-and-modularity-jake-varley-et-al-2024>(1/16 | 179/237) Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity (Jake Varley et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jake Varley, Sumeet Singh, Deepali Jain, Krzysztof Choromanski, Andy Zeng, Somnath Basu Roy Chowdhury, Avinava Dubey, Vikas Sindhwani. (2024)<br><strong>Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity</strong><br><button class=copy-to-clipboard title="Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Zero-shot, Transformer, Large Language Model, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03570v1.pdf filename=2404.03570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present an embodied AI system which receives open-ended natural language instructions from a human, and controls two arms to collaboratively accomplish potentially long-horizon tasks over a <b>large</b> <b>workspace.</b> <b>Our</b> system is modular: it deploys state of the art <b>Large</b> <b>Language</b> <b>Models</b> for task planning,Vision-Language models for semantic perception, and Point Cloud <b>transformers</b> for grasping. With semantic and physical safety in mind, these modules are interfaced with a real-time trajectory optimizer and a compliant tracking controller to enable human-robot proximity. We demonstrate performance for the following tasks: bi-arm sorting, bottle opening, and trash disposal tasks. These are done <b>zero-shot</b> <b>where</b> the models used have not been trained with any real world data from this bi-arm robot, scenes or workspace.Composing both learning- and non-learning-based components in a modular fashion with interpretable inputs and outputs allows the user to easily debug points of failures and fragilities. One may also in-place swap modules to improve the robustness of the overall platform, for instance with imitation-learned policies.</p></p class="citation"></blockquote><h3 id=216--180237-delta-decomposed-efficient-long-term-robot-task-planning-using-large-language-models-yuchen-liu-et-al-2024>(2/16 | 180/237) DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large Language Models (Yuchen Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuchen Liu, Luigi Palmieri, Sebastian Koch, Ilche Georgievski, Marco Aiello. (2024)<br><strong>DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large Language Models</strong><br><button class=copy-to-clipboard title="DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large Language Models" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Graph, Grounding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03275v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03275v1.pdf filename=2404.03275v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have sparked a revolution across various research fields. In particular, the integration of common-sense knowledge from <b>LLMs</b> into robot task and motion planning has been proven to be a game-changer, elevating performance in terms of explainability and downstream task efficiency to unprecedented heights. However, managing the vast knowledge encapsulated within these <b>large</b> <b>models</b> <b>has</b> posed challenges, often resulting in infeasible plans generated by <b>LLM-based</b> planning systems due to hallucinations or missing domain information. To overcome these challenges and obtain even greater planning feasibility and computational efficiency, we propose a novel <b>LLM-driven</b> task planning approach called DELTA. For achieving better <b>grounding</b> from environmental topology into actionable knowledge, DELTA leverages the power of scene <b>graphs</b> as environment representations within <b>LLMs,</b> enabling the fast generation of precise planning problem descriptions. For obtaining higher planning performance, we use <b>LLMs</b> to decompose the long-term task goals into an autoregressive sequence of sub-goals for an automated task planner to solve. Our contribution enables a more efficient and fully automatic task planning pipeline, achieving higher planning success rates and significantly shorter planning times compared to the state of the art.</p></p class="citation"></blockquote><h3 id=316--181237-a-high-fidelity-simulation-framework-for-grasping-stability-analysis-in-human-casualty-manipulation-qianwen-zhao-et-al-2024>(3/16 | 181/237) A High-Fidelity Simulation Framework for Grasping Stability Analysis in Human Casualty Manipulation (Qianwen Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianwen Zhao, Rajarshi Roy, Chad Spurlock, Kevin Lister, Long Wang. (2024)<br><strong>A High-Fidelity Simulation Framework for Grasping Stability Analysis in Human Casualty Manipulation</strong><br><button class=copy-to-clipboard title="A High-Fidelity Simulation Framework for Grasping Stability Analysis in Human Casualty Manipulation" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Human Intervention, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03741v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03741v1.pdf filename=2404.03741v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been a growing interest in rescue robots due to their vital role in addressing emergency scenarios and providing crucial support in challenging or hazardous situations where <b>human</b> <b>intervention</b> is difficult. However, very few of these robots are capable of actively engaging with <b>humans</b> <b>and</b> undertaking physical manipulation tasks. This limitation is largely attributed to the absence of tools that can realistically simulate physical interactions, especially the contact mechanisms between a robotic gripper and a <b>human</b> <b>body.</b> In this letter, we aim to address key limitations in current developments towards robotic casualty manipulation. Firstly, we present an integrative <b>simulation</b> framework for casualty manipulation. We adapt a finite element method (FEM) tool into the grasping and manipulation scenario, and the developed framework can provide accurate biomechanical reactions resulting from manipulation. Secondly, we conduct a detailed assessment of grasping stability during casualty grasping and manipulation <b>simulations.</b> To validate the necessity and superior performance of the proposed high-fidelity <b>simulation</b> framework, we conducted a qualitative and quantitative comparison of grasping stability analyses between the proposed framework and the state-of-the-art multi-body physics <b>simulations.</b> Through these efforts, we have taken the first step towards a feasible solution for robotic casualty manipulation.</p></p class="citation"></blockquote><h3 id=416--182237-legible-and-proactive-robot-planning-for-prosocial-human-robot-interactions-jasper-geldenbott-et-al-2024>(4/16 | 182/237) Legible and Proactive Robot Planning for Prosocial Human-Robot Interactions (Jasper Geldenbott et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jasper Geldenbott, Karen Leung. (2024)<br><strong>Legible and Proactive Robot Planning for Prosocial Human-Robot Interactions</strong><br><button class=copy-to-clipboard title="Legible and Proactive Robot Planning for Prosocial Human-Robot Interactions" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03734v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03734v1.pdf filename=2404.03734v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans have a remarkable ability to fluently engage in joint collision avoidance in crowded navigation tasks despite the complexities and uncertainties inherent in human behavior. Underlying these interactions is a mutual understanding that (i) individuals are prosocial, that is, there is equitable responsibility in avoiding collisions, and (ii) individuals should behave legibly, that is, move in a way that clearly conveys their intent to reduce ambiguity in how they intend to avoid others. Toward building robots that can safely and seamlessly interact with humans, we propose a general robot trajectory planning framework for synthesizing legible and proactive behaviors and demonstrate that our robot planner naturally leads to prosocial interactions. Specifically, we introduce the notion of a markup factor to incentivize legible and proactive behaviors and an inconvenience budget constraint to ensure equitable collision avoidance responsibility. We evaluate our approach against well-established multi-agent planning algorithms and show that using our approach produces safe, fluent, and prosocial interactions. We demonstrate the real-time feasibility of our approach with <b>human-in-the-loop</b> <b>simulations.</b> Project page can be found at <a href=https://uw-ctrl.github.io/phri/>https://uw-ctrl.github.io/phri/</a>.</p></p class="citation"></blockquote><h3 id=516--183237-juicer-data-efficient-imitation-learning-for-robotic-assembly-lars-ankile-et-al-2024>(5/16 | 183/237) JUICER: Data-Efficient Imitation Learning for Robotic Assembly (Lars Ankile et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lars Ankile, Anthony Simeonov, Idan Shenfeld, Pulkit Agrawal. (2024)<br><strong>JUICER: Data-Efficient Imitation Learning for Robotic Assembly</strong><br><button class=copy-to-clipboard title="JUICER: Data-Efficient Imitation Learning for Robotic Assembly" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Data Augmentation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03729v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03729v1.pdf filename=2404.03729v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While learning from demonstrations is powerful for acquiring visuomotor policies, high-performance imitation without large demonstration datasets remains challenging for tasks requiring precise, long-horizon manipulation. This paper proposes a pipeline for improving imitation learning performance with a small human demonstration budget. We apply our approach to assembly tasks that require precisely grasping, reorienting, and inserting multiple parts over long horizons and multiple task phases. Our pipeline combines expressive policy architectures and various techniques for dataset expansion and <b>simulation-based</b> <b>data</b> <b>augmentation.</b> These help expand dataset support and supervise the model with locally corrective actions near bottleneck regions requiring high precision. We demonstrate our pipeline on four furniture assembly tasks in <b>simulation,</b> enabling a manipulator to assemble up to five parts over nearly 2500 time steps directly from RGB images, outperforming imitation and <b>data</b> <b>augmentation</b> baselines.</p></p class="citation"></blockquote><h3 id=616--184237-anticipate--collab-data-driven-task-anticipation-and-knowledge-driven-planning-for-human-robot-collaboration-shivam-singh-et-al-2024>(6/16 | 184/237) Anticipate & Collab: Data-driven Task Anticipation and Knowledge-driven Planning for Human-robot Collaboration (Shivam Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shivam Singh, Karthik Swaminathan, Raghav Arora, Ramandeep Singh, Ahana Datta, Dipanjan Das, Snehasis Banerjee, Mohan Sridharan, Madhava Krishna. (2024)<br><strong>Anticipate & Collab: Data-driven Task Anticipation and Knowledge-driven Planning for Human-robot Collaboration</strong><br><button class=copy-to-clipboard title="Anticipate & Collab: Data-driven Task Anticipation and Knowledge-driven Planning for Human-robot Collaboration" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03587v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03587v1.pdf filename=2404.03587v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An agent assisting humans in daily living activities can collaborate more effectively by anticipating upcoming tasks. Data-driven methods represent the state of the art in task anticipation, planning, and related problems, but these methods are resource-hungry and opaque. Our prior work introduced a proof of concept framework that used an <b>LLM</b> to anticipate 3 high-level tasks that served as goals for a classical planning system that computed a sequence of low-level actions for the agent to achieve these goals. This paper describes DaTAPlan, our framework that significantly extends our prior work toward human-robot collaboration. Specifically, DaTAPlan planner computes actions for an agent and a human to collaboratively and jointly achieve the tasks anticipated by the <b>LLM,</b> and the agent automatically adapts to unexpected changes in human action outcomes and preferences. We evaluate DaTAPlan capabilities in a realistic <b>simulation</b> environment, demonstrating accurate task anticipation, effective human-robot collaboration, and the ability to adapt to unexpected changes. Project website: <a href=https://dataplan-hrc.github.io>https://dataplan-hrc.github.io</a></p></p class="citation"></blockquote><h3 id=716--185237-scaling-population-based-reinforcement-learning-with-gpu-accelerated-simulation-asad-ali-shahid-et-al-2024>(7/16 | 185/237) Scaling Population-Based Reinforcement Learning with GPU Accelerated Simulation (Asad Ali Shahid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Asad Ali Shahid, Yashraj Narang, Vincenzo Petrone, Enrico Ferrentino, Ankur Handa, Dieter Fox, Marco Pavone, Loris Roveda. (2024)<br><strong>Scaling Population-Based Reinforcement Learning with GPU Accelerated Simulation</strong><br><button class=copy-to-clipboard title="Scaling Population-Based Reinforcement Learning with GPU Accelerated Simulation" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03336v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03336v1.pdf filename=2404.03336v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, deep <b>reinforcement</b> <b>learning</b> (RL) has shown its effectiveness in solving complex continuous control tasks like locomotion and dexterous manipulation. However, this comes at the cost of an enormous amount of experience required for training, exacerbated by the sensitivity of learning efficiency and the policy performance to hyperparameter selection, which often requires numerous trials of time-consuming experiments. This work introduces a Population-Based <b>Reinforcement</b> <b>Learning</b> (PBRL) approach that exploits a GPU-accelerated physics simulator to enhance the exploration capabilities of RL by concurrently training multiple policies in parallel. The PBRL framework is applied to three state-of-the-art RL algorithms &ndash; PPO, SAC, and DDPG &ndash; dynamically adjusting hyperparameters based on the performance of learning agents. The experiments are performed on four challenging tasks in Isaac Gym &ndash; Anymal Terrain, Shadow Hand, Humanoid, Franka Nut Pick &ndash; by analyzing the effect of population size and mutation mechanisms for hyperparameters. The results show that PBRL agents achieve superior performance, in terms of cumulative reward, compared to non-evolutionary baseline agents. The trained agents are finally deployed in the real world for a Franka Nut Pick} task, demonstrating successful sim-to-real transfer. Code and videos of the learned policies are available on our project website.</p></p class="citation"></blockquote><h3 id=816--186237-integrating-large-language-models-with-multimodal-virtual-reality-interfaces-to-support-collaborative-human-robot-construction-work-somin-park-et-al-2024>(8/16 | 186/237) Integrating Large Language Models with Multimodal Virtual Reality Interfaces to Support Collaborative Human-Robot Construction Work (Somin Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Somin Park, Carol C. Menassa, Vineet R. Kamat. (2024)<br><strong>Integrating Large Language Models with Multimodal Virtual Reality Interfaces to Support Collaborative Human-Robot Construction Work</strong><br><button class=copy-to-clipboard title="Integrating Large Language Models with Multimodal Virtual Reality Interfaces to Support Collaborative Human-Robot Construction Work" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-HC, cs-RO, cs.RO<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03498v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03498v1.pdf filename=2404.03498v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the construction industry, where work environments are complex, unstructured and often dangerous, the implementation of Human-Robot Collaboration (HRC) is emerging as a promising advancement. This underlines the critical need for intuitive communication interfaces that enable construction workers to collaborate seamlessly with robotic assistants. This study introduces a conversational Virtual Reality (VR) interface integrating <b>multimodal</b> interaction to enhance intuitive communication between construction workers and robots. By integrating voice and controller inputs with the Robot Operating System (ROS), Building Information Modeling (BIM), and a game engine featuring a chat interface powered by a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM),</b> the proposed system enables intuitive and precise interaction within a VR setting. Evaluated by twelve construction workers through a drywall installation case study, the proposed system demonstrated its low workload and high usability with succinct command inputs. The proposed <b>multimodal</b> interaction system suggests that such technological integration can substantially advance the integration of robotic assistants in the construction industry.</p></p class="citation"></blockquote><h3 id=916--187237-a-bimanual-teleoperation-framework-for-light-duty-underwater-vehicle-manipulator-systems-justin-sitler-et-al-2024>(9/16 | 187/237) A Bimanual Teleoperation Framework for Light Duty Underwater Vehicle-Manipulator Systems (Justin Sitler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Justin Sitler, Srikarran Sowrirajan, Brendan Englot, Long Wang. (2024)<br><strong>A Bimanual Teleoperation Framework for Light Duty Underwater Vehicle-Manipulator Systems</strong><br><button class=copy-to-clipboard title="A Bimanual Teleoperation Framework for Light Duty Underwater Vehicle-Manipulator Systems" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03790v1.pdf filename=2404.03790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In an effort to lower the barrier to entry in underwater manipulation, this paper presents an open-source, user-friendly framework for bimanual teleoperation of a light-duty underwater vehicle-manipulator system (UVMS). This framework allows for the control of the vehicle along with two manipulators and their end-effectors using two low-cost haptic devices. The UVMS kinematics are derived in order to create an independent resolved motion rate controller for each manipulator, which optimally controls the joint positions to achieve a desired end-effector pose. This desired pose is computed in real-time using a teleoperation controller developed to process the dual haptic device input from the user. A physics-based <b>simulation</b> environment is used to implement this framework for two example tasks as well as provide data for error analysis of user commands. The first task illustrates the functionality of the framework through motion control of the vehicle and manipulators using only the haptic devices. The second task is to grasp an object using both manipulators simultaneously, demonstrating precision and coordination using the framework. The framework code is available at <a href=https://github.com/stevens-armlab/uvms_bimanual_sim>https://github.com/stevens-armlab/uvms_bimanual_sim</a>.</p></p class="citation"></blockquote><h3 id=1016--188237-simultaneous-state-estimation-and-contact-detection-for-legged-robots-by-multiple-model-kalman-filtering-marcel-menner-et-al-2024>(10/16 | 188/237) Simultaneous State Estimation and Contact Detection for Legged Robots by Multiple-Model Kalman Filtering (Marcel Menner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcel Menner, Karl Berntorp. (2024)<br><strong>Simultaneous State Estimation and Contact Detection for Legged Robots by Multiple-Model Kalman Filtering</strong><br><button class=copy-to-clipboard title="Simultaneous State Estimation and Contact Detection for Legged Robots by Multiple-Model Kalman Filtering" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03444v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03444v1.pdf filename=2404.03444v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes an algorithm for combined contact detection and state estimation for legged robots. The proposed algorithm models the robot&rsquo;s movement as a switched system, in which different modes relate to different feet being in contact with the ground. The key element in the proposed algorithm is an interacting multiple-model Kalman filter, which identifies the currently-active mode defining contacts, while estimating the state. The rationale for the proposed estimation framework is that contacts (and contact forces) impact the robot&rsquo;s state and vice versa. This paper presents validation studies with a quadruped using (i) the high-fidelity simulator Gazebo for a comparison with ground truth values and a baseline estimator, and (ii) hardware experiments with the Unitree A1 robot. The <b>simulation</b> study shows that the proposed algorithm outperforms the baseline estimator, which does not simultaneous detect contacts. The hardware experiments showcase the applicability of the proposed algorithm and highlights the ability to detect contacts.</p></p class="citation"></blockquote><h3 id=1116--189237-gmmcalib-extrinsic-calibration-of-lidar-sensors-using-gmm-based-joint-registration-ilir-tahiraj-et-al-2024>(11/16 | 189/237) GMMCalib: Extrinsic Calibration of LiDAR Sensors using GMM-based Joint Registration (Ilir Tahiraj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ilir Tahiraj, Felix Fent, Philipp Hafemann, Egon Ye, Markus Lienkamp. (2024)<br><strong>GMMCalib: Extrinsic Calibration of LiDAR Sensors using GMM-based Joint Registration</strong><br><button class=copy-to-clipboard title="GMMCalib: Extrinsic Calibration of LiDAR Sensors using GMM-based Joint Registration" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03427v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03427v1.pdf filename=2404.03427v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>State-of-the-art LiDAR calibration frameworks mainly use non-probabilistic registration methods such as Iterative Closest Point (ICP) and its variants. These methods suffer from biased results due to their pair-wise registration procedure as well as their sensitivity to initialization and parameterization. This often leads to misalignments in the calibration process. Probabilistic registration methods compensate for these drawbacks by specifically modeling the probabilistic nature of the observations. This paper presents GMMCalib, an automatic target-based extrinsic calibration approach for multi-LiDAR systems. Using an implementation of a Gaussian Mixture Model (GMM)-based registration method that allows joint registration of multiple point clouds, this data-driven approach is compared to ICP algorithms. We perform <b>simulation</b> experiments using the digital twin of the EDGAR research vehicle and validate the results in a real-world environment. We also address the local minima problem of local registration methods for extrinsic sensor calibration and use a distance-based metric to evaluate the calibration results. Our results show that an increase in robustness against sensor miscalibrations can be achieved by using GMM-based registration algorithms. The code is open source and available on GitHub.</p></p class="citation"></blockquote><h3 id=1216--190237-radium-predicting-and-repairing-end-to-end-robot-failures-using-gradient-accelerated-sampling-charles-dawson-et-al-2024>(12/16 | 190/237) RADIUM: Predicting and Repairing End-to-End Robot Failures using Gradient-Accelerated Sampling (Charles Dawson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Charles Dawson, Anjali Parashar, Chuchu Fan. (2024)<br><strong>RADIUM: Predicting and Repairing End-to-End Robot Failures using Gradient-Accelerated Sampling</strong><br><button class=copy-to-clipboard title="RADIUM: Predicting and Repairing End-to-End Robot Failures using Gradient-Accelerated Sampling" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03412v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03412v1.pdf filename=2404.03412v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Before autonomous systems can be deployed in safety-critical applications, we must be able to understand and verify the safety of these systems. For cases where the risk or cost of real-world testing is prohibitive, we propose a <b>simulation-based</b> framework for a) predicting ways in which an autonomous system is likely to fail and b) automatically adjusting the system&rsquo;s design and control policy to preemptively mitigate those failures. Existing tools for failure prediction struggle to search over high-dimensional environmental parameters, cannot efficiently handle end-to-end testing for systems with vision in the loop, and provide little guidance on how to mitigate failures once they are discovered. We approach this problem through the lens of approximate Bayesian inference and use differentiable <b>simulation</b> and rendering for efficient failure case prediction and repair. For cases where a differentiable simulator is not available, we provide a gradient-free version of our algorithm, and we include a theoretical and empirical evaluation of the trade-offs between gradient-based and gradient-free methods. We apply our approach on a range of robotics and control problems, including optimizing search patterns for robot swarms, UAV formation control, and robust network control. Compared to optimization-based falsification methods, our method predicts a more diverse, representative set of failure modes, and we find that our use of differentiable <b>simulation</b> yields solutions that have up to 10x lower cost and requires up to 2x fewer iterations to converge relative to gradient-free techniques. In hardware experiments, we find that repairing control policies using our method leads to a 5x robustness improvement. Accompanying code and video can be found at <a href=https://mit-realm.github.io/radium/>https://mit-realm.github.io/radium/</a></p></p class="citation"></blockquote><h3 id=1316--191237-traversability-aware-adaptive-optimization-for-path-planning-and-control-in-mountainous-terrain-se-wook-yoo-et-al-2024>(13/16 | 191/237) Traversability-aware Adaptive Optimization for Path Planning and Control in Mountainous Terrain (Se-Wook Yoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Se-Wook Yoo, E In Son, Seung-Woo Seo. (2024)<br><strong>Traversability-aware Adaptive Optimization for Path Planning and Control in Mountainous Terrain</strong><br><button class=copy-to-clipboard title="Traversability-aware Adaptive Optimization for Path Planning and Control in Mountainous Terrain" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03274v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03274v1.pdf filename=2404.03274v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous navigation in extreme mountainous terrains poses challenges due to the presence of mobility-stressing elements and undulating surfaces, making it particularly difficult compared to conventional off-road driving scenarios. In such environments, estimating traversability solely based on exteroceptive sensors often leads to the inability to reach the goal due to a high prevalence of non-traversable areas. In this paper, we consider traversability as a relative value that integrates the robot&rsquo;s internal state, such as speed and torque to exhibit resilient behavior to reach its goal successfully. We separate traversability into apparent traversability and relative traversability, then incorporate these distinctions in the optimization process of sampling-based planning and motion predictive control. Our method enables the robots to execute the desired behaviors more accurately while avoiding hazardous regions and getting stuck. Experiments conducted on <b>simulation</b> with 27 diverse types of mountainous terrain and real-world demonstrate the robustness of the proposed framework, with increasingly better performance observed in more complex environments.</p></p class="citation"></blockquote><h3 id=1416--192237-factored-task-and-motion-planning-with-combined-optimization-sampling-and-learning-joaquim-ortiz-haro-2024>(14/16 | 192/237) Factored Task and Motion Planning with Combined Optimization, Sampling and Learning (Joaquim Ortiz-Haro, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joaquim Ortiz-Haro. (2024)<br><strong>Factored Task and Motion Planning with Combined Optimization, Sampling and Learning</strong><br><button class=copy-to-clipboard title="Factored Task and Motion Planning with Combined Optimization, Sampling and Learning" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03567v1.pdf filename=2404.03567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this thesis, we aim to improve the performance of TAMP algorithms from three complementary perspectives. First, we investigate the integration of discrete task planning with continuous trajectory optimization. Our main contribution is a conflict-based solver that automatically discovers why a task plan might fail when considering the constraints of the physical world. This information is then fed back into the task planner, resulting in an efficient, bidirectional, and intuitive interface between task and motion, capable of solving TAMP problems with multiple objects, robots, and tight physical constraints. In the second part, we first illustrate that, given the wide range of tasks and environments within TAMP, neither sampling nor optimization is superior in all settings. To combine the strengths of both approaches, we have designed meta-solvers for TAMP, adaptive solvers that automatically select which algorithms and computations to use and how to best decompose each problem to find a solution faster. In the third part, we combine deep learning architectures with model-based <b>reasoning</b> to accelerate computations within our TAMP solver. Specifically, we target infeasibility detection and nonlinear optimization, focusing on generalization, accuracy, compute time, and data efficiency. At the core of our contributions is a refined, factored representation of the trajectory optimization problems inside TAMP. This structure not only facilitates more efficient planning, encoding of geometric infeasibility, and meta-reasoning but also provides better generalization in neural architectures.</p></p class="citation"></blockquote><h3 id=1516--193237-fast-k-connectivity-restoration-in-multi-robot-systems-for-robust-communication-maintenance-md-ishat-e-rabban-et-al-2024>(15/16 | 193/237) Fast k-connectivity Restoration in Multi-Robot Systems for Robust Communication Maintenance (Md Ishat-E-Rabban et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Ishat-E-Rabban, Guangyao Shi, Griffin Bonner, Pratap Tokekar. (2024)<br><strong>Fast k-connectivity Restoration in Multi-Robot Systems for Robust Communication Maintenance</strong><br><button class=copy-to-clipboard title="Fast k-connectivity Restoration in Multi-Robot Systems for Robust Communication Maintenance" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03834v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03834v1.pdf filename=2404.03834v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Maintaining a robust communication network plays an important role in the success of a multi-robot team jointly performing an optimization task. A key characteristic of a robust cooperative multi-robot system is the ability to repair the communication topology in the case of robot failure. In this paper, we focus on the Fast k-connectivity Restoration (FCR) problem, which aims to repair a network to make it k-connected with minimum robot movement. We develop a Quadratically Constrained Program (QCP) formulation of the FCR problem, which provides a way to optimally solve the problem, but cannot handle large instances due to high computational overhead. We therefore present a scalable algorithm, called EA-SCR, for the FCR problem using <b>graph</b> theoretic concepts. By conducting empirical studies, we demonstrate that the EA-SCR algorithm performs within 10 percent of the optimal while being orders of magnitude faster. We also show that EA-SCR outperforms existing solutions by 30 percent in terms of the FCR distance metric.</p></p class="citation"></blockquote><h3 id=1616--194237-embodied-neuromorphic-artificial-intelligence-for-robotics-perspectives-challenges-and-research-development-stack-rachmad-vidya-wicaksana-putra-et-al-2024>(16/16 | 194/237) Embodied Neuromorphic Artificial Intelligence for Robotics: Perspectives, Challenges, and Research Development Stack (Rachmad Vidya Wicaksana Putra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rachmad Vidya Wicaksana Putra, Alberto Marchisio, Fakhreddine Zayer, Jorge Dias, Muhammad Shafique. (2024)<br><strong>Embodied Neuromorphic Artificial Intelligence for Robotics: Perspectives, Challenges, and Research Development Stack</strong><br><button class=copy-to-clipboard title="Embodied Neuromorphic Artificial Intelligence for Robotics: Perspectives, Challenges, and Research Development Stack" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-NE, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03325v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03325v1.pdf filename=2404.03325v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic technologies have been an indispensable part for improving human productivity since they have been helping humans in completing diverse, complex, and intensive tasks in a fast yet accurate and efficient way. Therefore, robotic technologies have been deployed in a wide range of applications, ranging from personal to industrial use-cases. However, current robotic technologies and their computing paradigm still lack embodied intelligence to efficiently interact with operational environments, respond with correct/expected actions, and adapt to changes in the environments. Toward this, recent advances in neuromorphic computing with Spiking Neural Networks (SNN) have demonstrated the potential to enable the embodied intelligence for robotics through bio-plausible computing paradigm that mimics how the biological brain works, known as &ldquo;neuromorphic artificial intelligence (AI)&rdquo;. However, the field of neuromorphic AI-based robotics is still at an early stage, therefore its development and deployment for solving real-world problems expose new challenges in different design aspects, such as accuracy, adaptability, efficiency, reliability, and security. To address these challenges, this paper will discuss how we can enable embodied neuromorphic AI for robotic systems through our perspectives: (P1) Embodied intelligence based on effective learning rule, training mechanism, and adaptability; (P2) Cross-layer optimizations for energy-efficient neuromorphic computing; (P3) Representative and fair <b>benchmarks;</b> (P4) Low-cost reliability and safety enhancements; (P5) Security and privacy for neuromorphic computing; and (P6) A synergistic development for energy-efficient and robust neuromorphic-based robotics. Furthermore, this paper identifies research challenges and opportunities, as well as elaborates our vision for future research development toward embodied neuromorphic AI for robotics.</p></p class="citation"></blockquote><h2 id=csai-3>cs.AI (3)</h2><h3 id=13--195237-benchmarking-chatgpt-on-algorithmic-reasoning-sean-mcleish-et-al-2024>(1/3 | 195/237) Benchmarking ChatGPT on Algorithmic Reasoning (Sean McLeish et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sean McLeish, Avi Schwarzschild, Tom Goldstein. (2024)<br><strong>Benchmarking ChatGPT on Algorithmic Reasoning</strong><br><button class=copy-to-clipboard title="Benchmarking ChatGPT on Algorithmic Reasoning" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 36<br>Keywords: Graph Neural Network, Benchmarking, Benchmarking, ChatGPT, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03441v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03441v1.pdf filename=2404.03441v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We evaluate <b>ChatGPT&rsquo;s</b> ability to solve algorithm problems from the CLRS <b>benchmark</b> suite that is designed for <b>GNNs.</b> The <b>benchmark</b> requires the use of a specified classical algorithm to solve a given problem. We find that <b>ChatGPT</b> outperforms specialist <b>GNN</b> models, using Python to successfully solve these problems. This raises new points in the discussion about learning algorithms with neural networks.</p></p class="citation"></blockquote><h3 id=23--196237-ai-and-the-problem-of-knowledge-collapse-andrew-j-peterson-2024>(2/3 | 196/237) AI and the Problem of Knowledge Collapse (Andrew J. Peterson, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew J. Peterson. (2024)<br><strong>AI and the Problem of Knowledge Collapse</strong><br><button class=copy-to-clipboard title="AI and the Problem of Knowledge Collapse" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-0, cs-AI, cs-CY, cs.AI<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03502v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03502v1.pdf filename=2404.03502v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While artificial intelligence has the potential to process vast amounts of data, generate new insights, and unlock greater productivity, its widespread adoption may entail unforeseen consequences. We identify conditions under which AI, by reducing the cost of access to certain modes of knowledge, can paradoxically harm public understanding. While <b>large</b> <b>language</b> <b>models</b> are trained on vast amounts of diverse data, they naturally generate output towards the &lsquo;center&rsquo; of the distribution. This is generally useful, but widespread reliance on recursive AI systems could lead to a process we define as &ldquo;knowledge collapse&rdquo;, and argue this could harm innovation and the richness of human understanding and culture. However, unlike AI models that cannot choose what data they are trained on, humans may strategically seek out diverse forms of knowledge if they perceive them to be worthwhile. To investigate this, we provide a simple model in which a community of learners or innovators choose to use traditional methods or to rely on a discounted AI-assisted process and identify conditions under which knowledge collapse occurs. In our default model, a 20% discount on AI-generated content generates public beliefs 2.3 times further from the truth than when there is no discount. Finally, based on the results, we consider further research directions to counteract such outcomes.</p></p class="citation"></blockquote><h3 id=33--197237-comprehensible-artificial-intelligence-on-knowledge-graphs-a-survey-simon-schramm-et-al-2024>(3/3 | 197/237) Comprehensible Artificial Intelligence on Knowledge Graphs: A survey (Simon Schramm et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Schramm, Christoph Wehner, Ute Schmid. (2024)<br><strong>Comprehensible Artificial Intelligence on Knowledge Graphs: A survey</strong><br><button class=copy-to-clipboard title="Comprehensible Artificial Intelligence on Knowledge Graphs: A survey" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 8<br>Keywords: Graph, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03499v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03499v1.pdf filename=2404.03499v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial Intelligence applications gradually move outside the safe walls of research labs and invade our daily lives. This is also true for Machine Learning methods on <b>Knowledge</b> <b>Graphs,</b> which has led to a steady increase in their application since the beginning of the 21st century. However, in many applications, users require an explanation of the Artificial Intelligences decision. This led to increased demand for Comprehensible Artificial Intelligence. <b>Knowledge</b> <b>Graphs</b> epitomize fertile soil for Comprehensible Artificial Intelligence, due to their ability to display connected data, i.e. <b>knowledge,</b> <b>in</b> a human- as well as machine-readable way. This survey gives a short history to Comprehensible Artificial Intelligence on <b>Knowledge</b> <b>Graphs.</b> Furthermore, we contribute by arguing that the concept Explainable Artificial Intelligence is overloaded and overlapping with Interpretable Machine Learning. By introducing the parent concept Comprehensible Artificial Intelligence, we provide a clear-cut distinction of both concepts while accounting for their similarities. Thus, we provide in this survey a case for Comprehensible Artificial Intelligence on <b>Knowledge</b> <b>Graphs</b> consisting of Interpretable Machine Learning on <b>Knowledge</b> <b>Graphs</b> and Explainable Artificial Intelligence on <b>Knowledge</b> <b>Graphs.</b> This leads to the introduction of a novel taxonomy for Comprehensible Artificial Intelligence on <b>Knowledge</b> <b>Graphs.</b> In addition, a comprehensive overview of the research on Comprehensible Artificial Intelligence on <b>Knowledge</b> <b>Graphs</b> is presented and put into the context of the taxonomy. Finally, research gaps in the field of Comprehensible Artificial Intelligence on <b>Knowledge</b> <b>Graphs</b> are identified for future research.</p></p class="citation"></blockquote><h2 id=eesssp-2>eess.SP (2)</h2><h3 id=12--198237-alzheimers-disease-detection-in-psg-signals-lorena-gallego-viñarás-et-al-2024>(1/2 | 198/237) Alzheimer&rsquo;s disease detection in PSG signals (Lorena Gallego-Viñarás et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lorena Gallego-Viñarás, Juan Miguel Mira-Tomás, Anna Michela-Gaeta, Gerard Pinol-Ripoll, Ferrán Barbé, Pablo M. Olmos, Arrate Muñoz-Barrutia. (2024)<br><strong>Alzheimer&rsquo;s disease detection in PSG signals</strong><br><button class=copy-to-clipboard title="Alzheimer's disease detection in PSG signals" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: 68T07 (Primary), 68T05, 92B20 (Secondary), I-2-1, cs-AI, eess-SP, eess.SP<br>Keyword Score: 33<br>Keywords: Benchmarking, Semi-Supervised Learning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03549v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03549v1.pdf filename=2404.03549v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Alzheimer&rsquo;s disease (AD) and sleep disorders exhibit a close association, where disruptions in sleep patterns often precede the onset of Mild Cognitive Impairment (MCI) and early-stage AD. This study delves into the potential of utilizing sleep-related electroencephalography (EEG) signals acquired through polysomnography (PSG) for the early detection of AD. Our primary focus is on exploring <b>semi-supervised</b> <b>Deep</b> Learning techniques for the classification of EEG signals due to the clinical scenario characterized by the limited data availability. The methodology entails testing and comparing the performance of <b>semi-supervised</b> <b>SMATE</b> and TapNet models, <b>benchmarked</b> against the <b>supervised</b> XCM model, and <b>unsupervised</b> Hidden Markov Models (HMMs). The study highlights the significance of spatial and temporal analysis capabilities, conducting independent analyses of each sleep stage. Results demonstrate the effectiveness of SMATE in leveraging limited labeled data, achieving stable metrics across all sleep stages, and reaching 90% accuracy in its <b>supervised</b> form. Comparative analyses reveal SMATE&rsquo;s superior performance over TapNet and HMM, while XCM excels in <b>supervised</b> scenarios with an accuracy range of 92 - 94%. These findings underscore the potential of <b>semi-supervised</b> <b>models</b> in early AD detection, particularly in overcoming the challenges associated with the scarcity of labeled data. Ablation tests affirm the critical role of spatio-temporal feature extraction in <b>semi-supervised</b> <b>predictive</b> performance, and t-SNE visualizations validate the model&rsquo;s proficiency in distinguishing AD patterns. Overall, this research contributes to the advancement of AD detection through innovative Deep Learning approaches, highlighting the crucial role of <b>semi-supervised</b> <b>learning</b> in addressing data limitations.</p></p class="citation"></blockquote><h3 id=22--199237-decentralized-learning-strategies-for-estimation-error-minimization-with-graph-neural-networks-xingran-chen-et-al-2024>(2/2 | 199/237) Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks (Xingran Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingran Chen, Navid NaderiAlizadeh, Alejandro Ribeiro, Shirin Saeedi Bidokhti. (2024)<br><strong>Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, eess-SP, eess.SP<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03227v1.pdf filename=2404.03227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address the challenge of sampling and remote estimation for autoregressive Markovian processes in a multi-hop wireless network with statistically-identical agents. Agents cache the most recent samples from others and communicate over wireless collision channels governed by an underlying <b>graph</b> <b>topology.</b> <b>Our</b> goal is to minimize time-average estimation error and/or age of information with decentralized scalable sampling and transmission policies, considering both oblivious (where decision-making is independent of the physical processes) and non-oblivious policies (where decision-making depends on physical processes). We prove that in oblivious policies, minimizing estimation error is equivalent to minimizing the age of information. The complexity of the problem, especially the multi-dimensional action spaces and arbitrary network topologies, makes theoretical methods for finding optimal transmission policies intractable. We optimize the policies using a graphical multi-agent <b>reinforcement</b> <b>learning</b> framework, where each agent employs a permutation-equivariant <b>graph</b> <b>neural</b> <b>network</b> architecture. Theoretically, we prove that our proposed framework exhibits desirable transferability properties, allowing transmission policies trained on small- or moderate-size networks to be executed effectively on large-scale topologies. Numerical experiments demonstrate that (i) Our proposed framework outperforms state-of-the-art baselines; (ii) The trained policies are transferable to larger networks, and their performance gains increase with the number of agents; (iii) The training procedure withstands non-stationarity even if we utilize independent learning techniques; and, (iv) Recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity in independent learning.</p></p class="citation"></blockquote><h2 id=statml-2>stat.ML (2)</h2><h3 id=12--200237-transformerlsr-attentive-joint-model-of-longitudinal-data-survival-and-recurrent-events-with-concurrent-latent-structure-zhiyue-zhang-et-al-2024>(1/2 | 200/237) TransformerLSR: Attentive Joint Model of Longitudinal Data, Survival, and Recurrent Events with Concurrent Latent Structure (Zhiyue Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyue Zhang, Yao Zhao, Yanxun Xu. (2024)<br><strong>TransformerLSR: Attentive Joint Model of Longitudinal Data, Survival, and Recurrent Events with Concurrent Latent Structure</strong><br><button class=copy-to-clipboard title="TransformerLSR: Attentive Joint Model of Longitudinal Data, Survival, and Recurrent Events with Concurrent Latent Structure" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-AP, stat-ME, stat-ML, stat.ML<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03804v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03804v1.pdf filename=2404.03804v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In applications such as biomedical studies, epidemiology, and social sciences, recurrent events often co-occur with longitudinal measurements and a terminal event, such as death. Therefore, jointly modeling longitudinal measurements, recurrent events, and survival data while accounting for their dependencies is critical. While joint models for the three components exist in statistical literature, many of these approaches are limited by heavy parametric assumptions and scalability issues. Recently, incorporating deep learning techniques into joint modeling has shown promising results. However, current methods only address joint modeling of longitudinal measurements at regularly-spaced observation times and survival events, neglecting recurrent events. In this paper, we develop TransformerLSR, a flexible <b>transformer-based</b> deep modeling and inference framework to jointly model all three components simultaneously. TransformerLSR integrates deep temporal point processes into the joint modeling framework, treating recurrent and terminal events as two competing processes dependent on past longitudinal measurements and recurrent event times. Additionally, TransformerLSR introduces a novel trajectory representation and model architecture to potentially incorporate a priori knowledge of known latent structures among concurrent longitudinal variables. We demonstrate the effectiveness and necessity of TransformerLSR through <b>simulation</b> studies and analyzing a real-world medical dataset on patients after kidney transplantation.</p></p class="citation"></blockquote><h3 id=22--201237-countarfactuals----generating-plausible-model-agnostic-counterfactual-explanations-with-adversarial-random-forests-susanne-dandl-et-al-2024>(2/2 | 201/237) CountARFactuals &ndash; Generating plausible model-agnostic counterfactual explanations with adversarial random forests (Susanne Dandl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Susanne Dandl, Kristin Blesch, Timo Freiesleben, Gunnar König, Jan Kapar, Bernd Bischl, Marvin Wright. (2024)<br><strong>CountARFactuals &ndash; Generating plausible model-agnostic counterfactual explanations with adversarial random forests</strong><br><button class=copy-to-clipboard title="CountARFactuals -- Generating plausible model-agnostic counterfactual explanations with adversarial random forests" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03506v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03506v1.pdf filename=2404.03506v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Counterfactual</b> explanations elucidate algorithmic decisions by pointing to scenarios that would have led to an alternative, desired outcome. Giving insight into the model&rsquo;s behavior, they hint users towards possible actions and give grounds for contesting decisions. As a crucial factor in achieving these goals, <b>counterfactuals</b> must be plausible, i.e., describing realistic alternative scenarios within the data manifold. This paper leverages a recently developed generative modeling technique &ndash; adversarial random forests (ARFs) &ndash; to efficiently generate plausible <b>counterfactuals</b> in a model-agnostic way. ARFs can serve as a plausibility measure or directly generate <b>counterfactual</b> explanations. Our ARF-based approach surpasses the limitations of existing methods that aim to generate plausible <b>counterfactual</b> explanations: It is easy to train and computationally highly efficient, handles continuous and categorical data naturally, and allows integrating additional desiderata such as sparsity in a straightforward manner.</p></p class="citation"></blockquote><h2 id=cshc-4>cs.HC (4)</h2><h3 id=14--202237-fakes-of-varying-shades-how-warning-affects-human-perception-and-engagement-regarding-llm-hallucinations-mahjabin-nahar-et-al-2024>(1/4 | 202/237) Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations (Mahjabin Nahar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahjabin Nahar, Haeseung Seo, Eun-Ju Lee, Aiping Xiong, Dongwon Lee. (2024)<br><strong>Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations</strong><br><button class=copy-to-clipboard title="Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CL, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Hallucination Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03745v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03745v1.pdf filename=2404.03745v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread adoption and transformative effects of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have sparked concerns regarding their capacity to produce inaccurate and fictitious content, referred to as `hallucinations&rsquo;. Given the potential risks associated with <b>hallucinations,</b> <b>humans</b> should be able to identify them. This research aims to understand the human perception of <b>LLM</b> <b>hallucinations</b> <b>by</b> systematically varying the degree of <b>hallucination</b> <b>(genuine,</b> minor <b>hallucination,</b> <b>major</b> <b>hallucination)</b> <b>and</b> examining its interaction with warning (i.e., a warning of potential inaccuracies: absent vs. present). Participants (N=419) from Prolific rated the perceived accuracy and engaged with content (e.g., like, dislike, share) in a Q/A format. Results indicate that humans rank content as truthful in the order genuine > minor <b>hallucination</b> <b>></b> major <b>hallucination</b> <b>and</b> user engagement behaviors mirror this pattern. More importantly, we observed that warning improves <b>hallucination</b> <b>detection</b> without significantly affecting the perceived truthfulness of genuine content. We conclude by offering insights for future tools to aid human detection of hallucinations.</p></p class="citation"></blockquote><h3 id=24--203237-biodegradable-interactive-materials-zhihan-zhang-et-al-2024>(2/4 | 203/237) Biodegradable Interactive Materials (Zhihan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihan Zhang, Mallory Parker, Kuotian Liao, Jerry Cao, Anandghan Waghmare, Joseph Breda, Chris Matsumura, Serena Eley, Eleftheria Roumeli, Shwetak Patel, Vikram Iyer. (2024)<br><strong>Biodegradable Interactive Materials</strong><br><button class=copy-to-clipboard title="Biodegradable Interactive Materials" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03130v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03130v1.pdf filename=2404.03130v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The sense of touch is fundamental to how we interact with the physical and digital world. Conventional interactive surfaces and tactile interfaces use electronic sensors embedded into objects, however this approach poses serious challenges both for environmental sustainability and a future of truly ubiquitous interaction systems where information is encoded into everyday objects. In this work, we present Biodegradable Interactive Materials: backyard-compostable interactive interfaces that leverage information encoded in material properties. Inspired by natural systems, we propose an architecture that programmatically encodes multidimensional information into materials themselves and combines them with wearable devices that extend human senses to perceive the embedded data. We combine unrefined biological matter from plants and algae like chlorella with natural minerals like graphite and magnetite to produce materials with varying electrical, magnetic, and surface properties. We perform in-depth analysis using physics models, computational <b>simulations,</b> and real-world experiments to characterize their information density and develop decoding methods. Our passive, chip-less materials can robustly encode 12 bits of information, equivalent to 4096 unique classes. We further develop wearable device prototypes that can decode this information during touch interactions using off-the-shelf sensors. We demonstrate sample applications such as customized buttons, tactile maps, and interactive surfaces. We further demonstrate the natural degradation of these interactive materials in degrade outdoors within 21 days and perform a comparative environmental analysis of the benefits of this approach.</p></p class="citation"></blockquote><h3 id=34--204237-exploring-emotions-in-multi-componential-space-using-interactive-vr-games-rukshani-somarathna-et-al-2024>(3/4 | 204/237) Exploring Emotions in Multi-componential Space using Interactive VR Games (Rukshani Somarathna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rukshani Somarathna, Gelareh Mohammadi. (2024)<br><strong>Exploring Emotions in Multi-componential Space using Interactive VR Games</strong><br><button class=copy-to-clipboard title="Exploring Emotions in Multi-componential Space using Interactive VR Games" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs-LG, cs.HC<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03239v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03239v1.pdf filename=2404.03239v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Emotion</b> <b>understanding</b> is a complex process that involves multiple components. The ability to recognise <b>emotions</b> <b>not</b> only leads to new context awareness methods but also enhances system interaction&rsquo;s effectiveness by perceiving and expressing <b>emotions.</b> <b>Despite</b> the attention to discrete and dimensional models, neuroscientific evidence supports those <b>emotions</b> <b>as</b> being complex and multi-faceted. One framework that resonated well with such findings is the Component Process Model (CPM), a theory that considers the complexity of <b>emotions</b> <b>with</b> five interconnected components: appraisal, expression, motivation, physiology and feeling. However, the relationship between CPM and discrete <b>emotions</b> <b>has</b> not yet been fully explored. Therefore, to better understand <b>emotions</b> <b>underlying</b> processes, we operationalised a data-driven approach using interactive Virtual Reality (VR) games and collected <b>multimodal</b> measures (self-reports, physiological and facial signals) from 39 participants. We used Machine Learning (ML) methods to identify the unique contributions of each component to <b>emotion</b> <b>differentiation.</b> Our results showed the role of different components in <b>emotion</b> <b>differentiation,</b> with the model including all components demonstrating the most significant contribution. Moreover, we found that at least five dimensions are needed to represent the variation of <b>emotions</b> <b>in</b> our dataset. These findings also have implications for using VR environments in <b>emotion</b> <b>research</b> and highlight the role of physiological signals in <b>emotion</b> <b>recognition</b> within such environments.</p></p class="citation"></blockquote><h3 id=44--205237-nlp4gov-a-comprehensive-library-for-computational-policy-analysis-mahasweta-chakraborti-et-al-2024>(4/4 | 205/237) NLP4Gov: A Comprehensive Library for Computational Policy Analysis (Mahasweta Chakraborti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahasweta Chakraborti, Sailendra Akash Bonagiri, Santiago Virgüez-Ruiz, Seth Frey. (2024)<br><strong>NLP4Gov: A Comprehensive Library for Computational Policy Analysis</strong><br><button class=copy-to-clipboard title="NLP4Gov: A Comprehensive Library for Computational Policy Analysis" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03206v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03206v1.pdf filename=2404.03206v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Formal rules and policies are fundamental in formally specifying a social system: its operation, boundaries, processes, and even ontology. Recent scholarship has highlighted the role of formal policy in collective knowledge creation, game communities, the production of digital public goods, and national social media governance. Researchers have shown interest in how online communities convene tenable self-governance mechanisms to regulate member activities and distribute rights and privileges by designating responsibilities, roles, and hierarchies. We present NLP4Gov, an interactive kit to train and aid scholars and practitioners alike in computational policy analysis. The library explores and integrates methods and capabilities from computational linguistics and NLP to generate semantic and symbolic representations of community policies from text records. Versatile, documented, and accessible, NLP4Gov provides granular and comparative views into institutional structures and interactions, along with other <b>information</b> <b>extraction</b> capabilities for downstream analysis.</p></p class="citation"></blockquote><h2 id=quant-ph-4>quant-ph (4)</h2><h3 id=14--206237-direct-interpolative-construction-of-the-discrete-fourier-transform-as-a-matrix-product-operator-jielun-chen-et-al-2024>(1/4 | 206/237) Direct interpolative construction of the discrete Fourier transform as a matrix product operator (Jielun Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jielun Chen, Michael Lindsey. (2024)<br><strong>Direct interpolative construction of the discrete Fourier transform as a matrix product operator</strong><br><button class=copy-to-clipboard title="Direct interpolative construction of the discrete Fourier transform as a matrix product operator" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-NA, math-NA, quant-ph, quant-ph<br>Keyword Score: 30<br>Keywords: Quantization, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03182v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03182v1.pdf filename=2404.03182v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The quantum Fourier transform (QFT), which can be viewed as a reindexing of the discrete Fourier transform (DFT), has been shown to be compressible as a low-rank matrix product operator (MPO) or <b>quantized</b> tensor train (QTT) operator. However, the original proof of this fact does not furnish a construction of the MPO with a guaranteed error bound. Meanwhile, the existing practical construction of this MPO, based on the compression of a quantum circuit, is not as efficient as possible. We present a simple closed-form construction of the QFT MPO using the interpolative decomposition, with guaranteed near-optimal compression error for a given rank. This construction can speed up the application of the QFT and the DFT, respectively, in quantum circuit <b>simulations</b> and QTT applications. We also connect our interpolative construction to the approximate quantum Fourier transform (AQFT) by demonstrating that the AQFT can be viewed as an MPO constructed using a different interpolation scheme.</p></p class="citation"></blockquote><h3 id=24--207237-faster-and-shorter-synthesis-of-hamiltonian-simulation-circuits-timothée-goubault-de-brugière-et-al-2024>(2/4 | 207/237) Faster and shorter synthesis of Hamiltonian simulation circuits (Timothée Goubault de Brugière et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Timothée Goubault de Brugière, Simon Martiel. (2024)<br><strong>Faster and shorter synthesis of Hamiltonian simulation circuits</strong><br><button class=copy-to-clipboard title="Faster and shorter synthesis of Hamiltonian simulation circuits" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: 68Q99, cs-DM, quant-ph, quant-ph<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03280v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03280v1.pdf filename=2404.03280v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We devise greedy heuristics tailored for synthesizing quantum circuits that implement a specified set of Pauli rotations. Our heuristics are designed to minimize either the count of entangling gates or the depth of entangling gates, and they can be adjusted to either maintain or loosen the ordering of rotations. We present <b>benchmark</b> results demonstrating a depth reduction of up to a factor of 4 compared to the current state-of-the-art heuristics for synthesizing Hamiltonian <b>simulation</b> circuits. We also show that these heuristics can be used to optimize generic quantum circuits by decomposing and resynthesizing them.</p></p class="citation"></blockquote><h3 id=34--208237-the-power-of-a-single-haar-random-state-constructing-and-separating-quantum-pseudorandomness-boyang-chen-et-al-2024>(3/4 | 208/237) The power of a single Haar random state: constructing and separating quantum pseudorandomness (Boyang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boyang Chen, Andrea Coladangelo, Or Sattath. (2024)<br><strong>The power of a single Haar random state: constructing and separating quantum pseudorandomness</strong><br><button class=copy-to-clipboard title="The power of a single Haar random state: constructing and separating quantum pseudorandomness" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-CR, quant-ph, quant-ph<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03295v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03295v1.pdf filename=2404.03295v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we focus on the following question: what are the cryptographic implications of having access to an oracle that provides a single Haar random quantum state? We show, perhaps surprisingly, that such an oracle is sufficient to construct quantum pseudorandomness. Pseudorandom states (PRS) are a family of states for which it is hard to distinguish between polynomially many copies of either a state sampled uniformly from the family or a Haar random state. A weaker notion, called single-copy pseudorandom states (1PRS), satisfies this property with respect to a single copy. Our main result is that 1PRS (as well as bit-commitments) exist relative to an oracle that provides a single Haar random state. We build on this result to show the existence of an oracle relative to which 1PRS exist, but PRS do not. This provides one of the first <b>black-box</b> <b>separations</b> between different forms of quantum pseudorandomness.</p></p class="citation"></blockquote><h3 id=44--209237-circuit-knitting-faces-exponential-sampling-overhead-scaling-bounded-by-entanglement-cost-mingrui-jing-et-al-2024>(4/4 | 209/237) Circuit Knitting Faces Exponential Sampling Overhead Scaling Bounded by Entanglement Cost (Mingrui Jing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingrui Jing, Chengkai Zhu, Xin Wang. (2024)<br><strong>Circuit Knitting Faces Exponential Sampling Overhead Scaling Bounded by Entanglement Cost</strong><br><button class=copy-to-clipboard title="Circuit Knitting Faces Exponential Sampling Overhead Scaling Bounded by Entanglement Cost" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-IT, math-IT, quant-ph, quant-ph<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03619v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03619v1.pdf filename=2404.03619v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Circuit knitting, a method for connecting quantum circuits across multiple processors to simulate nonlocal quantum operations, is a promising approach for distributed quantum computing. While various techniques have been developed for circuit knitting, we uncover fundamental limitations to the scalability of this technology. We prove that the sampling overhead of circuit knitting is exponentially lower bounded by the exact entanglement cost of the target bipartite dynamic, even for asymptotic overhead in the parallel cut regime. Specifically, we prove that the regularized sampling overhead assisted with local operations and classical communication (LOCC), of any bipartite quantum channel is lower bounded by the exponential of its exact entanglement cost under separable preserving operations. Furthermore, we show that the regularized sampling overhead for simulating a general bipartite channel via LOCC is lower bounded by $\kappa$-entanglement and max-Rains information, providing efficiently computable <b>benchmarks.</b> Our work reveals a profound connection between virtual quantum information processing via quasi-probability decomposition and quantum Shannon theory, highlighting the critical role of entanglement in distributed quantum computing.</p></p class="citation"></blockquote><h2 id=cslo-3>cs.LO (3)</h2><h3 id=13--210237-a-reinforcement-learning-based-reset-policy-for-cdcl-sat-solvers-chunxiao-li-et-al-2024>(1/3 | 210/237) A Reinforcement Learning based Reset Policy for CDCL SAT Solvers (Chunxiao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chunxiao Li, Charlie Liu, Jonathan Chung, Zhengyang, Lu, Piyush Jha, Vijay Ganesh. (2024)<br><strong>A Reinforcement Learning based Reset Policy for CDCL SAT Solvers</strong><br><button class=copy-to-clipboard title="A Reinforcement Learning based Reset Policy for CDCL SAT Solvers" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-AI, cs-LG, cs-LO, cs.LO<br>Keyword Score: 23<br>Keywords: Bandit Algorithm, Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03753v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03753v1.pdf filename=2404.03753v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Restart policy is an important technique used in modern Conflict-Driven Clause Learning (CDCL) solvers, wherein some parts of the solver state are erased at certain intervals during the run of the solver. In most solvers, variable activities are preserved across restart boundaries, resulting in solvers continuing to search parts of the assignment tree that are not far from the one immediately prior to a restart. To enable the solver to search possibly &ldquo;distant&rdquo; parts of the assignment tree, we study the effect of resets, a variant of restarts which not only erases the assignment trail, but also randomizes the activity scores of the variables of the input formula after reset, thus potentially enabling a better global exploration of the search space. In this paper, we model the problem of whether to trigger reset as a multi-armed <b>bandit</b> (MAB) problem, and propose two <b>reinforcement</b> <b>learning</b> (RL) based adaptive reset policies using the Upper Confidence Bound (UCB) and Thompson sampling algorithms. These two algorithms balance the exploration-exploitation tradeoff by adaptively choosing arms (reset vs. no reset) based on their estimated rewards during the solver&rsquo;s run. We implement our reset policies in four baseline SOTA CDCL solvers and compare the baselines against the reset versions on Satcoin <b>benchmarks</b> and SAT Competition instances. Our results show that RL-based reset versions outperform the corresponding baseline solvers on both Satcoin and the SAT competition instances, suggesting that our RL policy helps to dynamically and profitably adapt the reset frequency for any given input instance. We also introduce the concept of a partial reset, where at least a constant number of variable activities are retained across reset boundaries. Building on previous results, we show that there is an exponential separation between O(1) vs. $\Omega(n)$-length partial resets.</p></p class="citation"></blockquote><h3 id=23--211237-parametricity-via-cohesion-c-b-aberlé-2024>(2/3 | 211/237) Parametricity via Cohesion (C. B. Aberlé, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>C. B. Aberlé. (2024)<br><strong>Parametricity via Cohesion</strong><br><button class=copy-to-clipboard title="Parametricity via Cohesion" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs-PL, cs.LO, math-CT<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03825v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03825v1.pdf filename=2404.03825v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parametricity is a key metatheoretic property of type systems, which implies strong uniformity & modularity properties of the structure of types within systems possessing it. In recent years, various systems of dependent type theory have emerged with the aim of expressing such parametric <b>reasoning</b> in their internal logic, toward the end of solving various problems arising from the complexity of higher-dimensional coherence conditions in type theory. This paper presents a first step toward the unification, simplification, and extension of these various methods for internalizing parametricity. Specifically, I argue that there is an essentially modal aspect of parametricity, which is intimately connected with the category-theoretic concept of cohesion. On this basis, I describe a general categorical semantics for modal parametricity, develop a corresponding framework of axioms (with computational interpretations) in dependent type theory that can be used to internally represent and reason about such parametricity, and show this in practice by implementing these axioms in Agda and using them to verify parametricity theorems therein. I then demonstrate the utility of these axioms in managing the complexity of higher-dimensional coherence by deriving induction principles for higher inductive types, and in closing, I sketch the outlines of a more general synthetic theory of parametricity, with applications in domains ranging from homotopy type theory to the analysis of program modules.</p></p class="citation"></blockquote><h3 id=33--212237-model-checking-recursive-probabilistic-programs-with-conditioning-francesco-pontiggia-et-al-2024>(3/3 | 212/237) Model Checking Recursive Probabilistic Programs with Conditioning (Francesco Pontiggia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Pontiggia, Ezio Bartocci, Michele Chiari. (2024)<br><strong>Model Checking Recursive Probabilistic Programs with Conditioning</strong><br><button class=copy-to-clipboard title="Model Checking Recursive Probabilistic Programs with Conditioning" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: F-3-1; D-2-4, cs-LO, cs-PL, cs.LO<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03515v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03515v1.pdf filename=2404.03515v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address the problem of model checking temporal logic specifications for <b>probabilistic</b> <b>programs</b> with recursive procedures, nested queries, and conditioning expressed with observe statements. We introduce <b>probabilistic</b> <b>Operator</b> Precedence Automata (pOPA), a new class of <b>probabilistic</b> <b>pushdown</b> automata suitable to model constructs and behaviors of <b>probabilistic</b> <b>programs.</b> We develop a model checking algorithm that can verify requirements expressed in a fragment of Precedence Oriented Temporal Logic (POTL$^f_\mathcal{X}$) on a pOPA in single EXPTIME. POTL$^f_\mathcal{X}$ is a temporal logic based on Operator Precedence Languages, which features modalities that interact with the context-free structure of program traces, matching procedure calls with returns or observe statements. We provide the first <b>probabilistic</b> <b>model</b> checking implementation of context-free language properties for <b>probabilistic</b> <b>pushdown</b> systems.</p></p class="citation"></blockquote><h2 id=csdc-3>cs.DC (3)</h2><h3 id=13--213237-wilkins-hpc-in-situ-workflows-made-easy-orcun-yildiz-et-al-2024>(1/3 | 213/237) Wilkins: HPC In Situ Workflows Made Easy (Orcun Yildiz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Orcun Yildiz, Dmitriy Morozov, Arnur Nigmetov, Bogdan Nicolae, Tom Peterka. (2024)<br><strong>Wilkins: HPC In Situ Workflows Made Easy</strong><br><button class=copy-to-clipboard title="Wilkins: HPC In Situ Workflows Made Easy" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03591v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03591v1.pdf filename=2404.03591v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In situ approaches can accelerate the pace of scientific discoveries by allowing scientists to perform data analysis at <b>simulation</b> time. Current in situ workflow systems, however, face challenges in handling the growing complexity and diverse computational requirements of scientific tasks. In this work, we present Wilkins, an in situ workflow system that is designed for ease-of-use while providing scalable and efficient execution of workflow tasks. Wilkins provides a flexible workflow description interface, employs a high-performance data transport layer based on HDF5, and supports tasks with disparate data rates by providing a flow control mechanism. Wilkins seamlessly couples scientific tasks that already use HDF5, without requiring task code modifications. We demonstrate the above features using both synthetic <b>benchmarks</b> and two science use cases in materials science and cosmology.</p></p class="citation"></blockquote><h3 id=23--214237-use-cases-for-high-performance-research-desktops-robert-henschel-et-al-2024>(2/3 | 214/237) Use Cases for High Performance Research Desktops (Robert Henschel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert Henschel, Jonas Lindemann, Anders Follin, Bernd Dammann, Cicada Dennis, Abhinav Thota. (2024)<br><strong>Use Cases for High Performance Research Desktops</strong><br><button class=copy-to-clipboard title="Use Cases for High Performance Research Desktops" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03298v1.pdf filename=2404.03298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High Performance Research Desktops are used by HPC centers and research computing organizations to lower the barrier of entry to HPC systems. These Linux desktops are deployed alongside HPC systems, leveraging the investments in HPC compute and storage infrastructure. By serving as a gateway to HPC systems they provide users with an environment to perform setup and infrastructure tasks related to the actual HPC work. Such tasks can take significant amounts of time, are vital to the successful use of HPC systems, and can benefit from a graphical desktop environment. In addition to serving as a gateway to HPC systems, High Performance Research Desktops are also used to run interactive graphical applications like MATLAB, RStudio or VMD. This paper defines the concept of High Performance Research Desktops and <b>summarizes</b> use cases from Indiana University, Lund University and Technical University of Denmark, which have implemented and operated such a system for more than 10 years. Based on these use cases, possible future directions are presented.</p></p class="citation"></blockquote><h3 id=33--215237-inspirit-optimizing-heterogeneous-task-scheduling-through-adaptive-priority-in-task-based-runtime-systems-yiqing-wang-et-al-2024>(3/3 | 215/237) INSPIRIT: Optimizing Heterogeneous Task Scheduling through Adaptive Priority in Task-based Runtime Systems (Yiqing Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiqing Wang, Xiaoyan Liu, Hailong Yang, Xinyu Yang, Pengbo Wang, Yi Liu, Zhongzhi Luan, Depei Qian. (2024)<br><strong>INSPIRIT: Optimizing Heterogeneous Task Scheduling through Adaptive Priority in Task-based Runtime Systems</strong><br><button class=copy-to-clipboard title="INSPIRIT: Optimizing Heterogeneous Task Scheduling through Adaptive Priority in Task-based Runtime Systems" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03226v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03226v1.pdf filename=2404.03226v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As modern HPC computing platforms become increasingly heterogeneous, it is challenging for programmers to fully leverage the computation power of massive parallelism offered by such heterogeneity. Consequently, task-based runtime systems have been proposed as an intermediate layer to hide the complex heterogeneity from the application programmers. The core functionality of these systems is to realize efficient task-to-resource mapping in the form of Directed Acyclic <b>Graph</b> (DAG) scheduling. However, existing scheduling schemes face several drawbacks to determine task priorities due to the heavy reliance on domain knowledge or failure to efficiently exploit the interaction of application and hardware characteristics. In this paper, we propose INSPIRIT, an efficient and lightweight scheduling framework with adaptive priority designed for task-based runtime systems. INSPIRIT introduces two novel task attributes \textit{inspiring ability} and \textit{inspiring efficiency} for dictating scheduling, eliminating the need for application domain knowledge. In addition, INSPIRIT jointly considers runtime information such as ready tasks in worker queues to guide task scheduling. This approach exposes more performance opportunities in heterogeneous hardware at runtime while effectively reducing the overhead for adjusting task priorities. Our evaluation results demonstrate that INSPIRIT achieves superior performance compared to cutting edge scheduling schemes on both synthesized and real-world task DAGs.</p></p class="citation"></blockquote><h2 id=statme-1>stat.ME (1)</h2><h3 id=11--216237-multi-task-learning-via-robust-regularized-clustering-with-non-convex-group-penalties-akira-okazaki-et-al-2024>(1/1 | 216/237) Multi-task learning via robust regularized clustering with non-convex group penalties (Akira Okazaki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akira Okazaki, Shuichi Kawano. (2024)<br><strong>Multi-task learning via robust regularized clustering with non-convex group penalties</strong><br><button class=copy-to-clipboard title="Multi-task learning via robust regularized clustering with non-convex group penalties" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-LG, stat-ME, stat-ML, stat.ME<br>Keyword Score: 23<br>Keywords: Clustering, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03250v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03250v1.pdf filename=2404.03250v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-task learning (MTL) aims to improve estimation and prediction performance by sharing common information among related tasks. One natural assumption in MTL is that tasks are classified into clusters based on their characteristics. However, existing MTL methods based on this assumption often ignore outlier tasks that have large task-specific components or no relation to other tasks. To address this issue, we propose a novel MTL method called Multi-Task Learning via Robust Regularized <b>Clustering</b> (MTLRRC). MTLRRC incorporates robust regularization terms inspired by robust convex <b>clustering,</b> which is further extended to handle non-convex and group-sparse penalties. The extension allows MTLRRC to simultaneously perform robust task <b>clustering</b> and outlier task detection. The connection between the extended robust <b>clustering</b> and the multivariate M-estimator is also established. This provides an interpretation of the robustness of MTLRRC against outlier tasks. An efficient algorithm based on a modified alternating direction method of multipliers is developed for the estimation of the parameters. The effectiveness of MTLRRC is demonstrated through <b>simulation</b> studies and application to real data.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--217237-towards-trustworthy-automated-program-verifiers-formally-validating-translations-into-an-intermediate-verification-language-extended-version-gaurav-parthasarathy-et-al-2024>(1/1 | 217/237) Towards Trustworthy Automated Program Verifiers: Formally Validating Translations into an Intermediate Verification Language (extended version) (Gaurav Parthasarathy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gaurav Parthasarathy, Thibault Dardinier, Benjamin Bonneau, Peter Müller, Alexander J. Summers. (2024)<br><strong>Towards Trustworthy Automated Program Verifiers: Formally Validating Translations into an Intermediate Verification Language (extended version)</strong><br><button class=copy-to-clipboard title="Towards Trustworthy Automated Program Verifiers: Formally Validating Translations into an Intermediate Verification Language (extended version)" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03614v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03614v1.pdf filename=2404.03614v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated program verifiers are typically implemented using an intermediate verification language (IVL), such as Boogie or Why3. A verifier front-end translates the input program and specification into an IVL program, while the back-end generates proof obligations for the IVL program and employs an SMT solver to discharge them. Soundness of such verifiers therefore requires that the front-end translation faithfully captures the semantics of the input program and specification in the IVL program, and that the back-end reports success only if the IVL program is actually correct. For a verification tool to be trustworthy, these soundness conditions must be satisfied by its actual implementation, not just the program logic it uses. In this paper, we present a novel validation methodology that, given a formal semantics for the input language and IVL, provides formal soundness guarantees for front-end implementations. For each run of the verifier, we automatically generate a proof in Isabelle showing that the correctness of the produced IVL program implies the correctness of the input program. This proof can be checked independently from the verifier in Isabelle and can be combined with existing work on validating back-ends to obtain an end-to-end soundness result. Our methodology based on forward <b>simulation</b> employs several modularisation strategies to handle the large semantic gap between the input language and the IVL, as well as the intricacies of practical, optimised translations. We present our methodology for the widely-used Viper and Boogie languages. Our evaluation shows that it is effective in validating the translations performed by the existing Viper implementation.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--218237-integrating-generative-ai-into-financial-market-prediction-for-improved-decision-making-chang-che-et-al-2024>(1/1 | 218/237) Integrating Generative AI into Financial Market Prediction for Improved Decision Making (Chang Che et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chang Che, Zengyi Huang, Chen Li, Haotian Zheng, Xinyu Tian. (2024)<br><strong>Integrating Generative AI into Financial Market Prediction for Improved Decision Making</strong><br><button class=copy-to-clipboard title="Integrating Generative AI into Financial Market Prediction for Improved Decision Making" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 20<br>Keywords: Generative AI, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03523v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03523v1.pdf filename=2404.03523v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study provides an in-depth analysis of the model architecture and key technologies of <b>generative</b> <b>artificial</b> <b>intelligence,</b> combined with specific application cases, and uses conditional <b>generative</b> <b>adversarial</b> <b>networks</b> ( cGAN ) and time series analysis methods to simulate and predict dynamic changes in financial markets. The research results show that the cGAN model can effectively capture the complexity of financial market data, and the deviation between the prediction results and the actual market performance is minimal, showing a high degree of accuracy.</p></p class="citation"></blockquote><h2 id=csit-4>cs.IT (4)</h2><h3 id=14--219237-movable-antennas-assisted-secure-transmission-without-eavesdroppers-instantaneous-csi-guojie-hu-et-al-2024>(1/4 | 219/237) Movable Antennas-Assisted Secure Transmission Without Eavesdroppers&rsquo; Instantaneous CSI (Guojie Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guojie Hu, Qingqing Wu, Donghui Xu, Kui Xu, Jiangbo Si, Yunlong Cai, Naofal Al-Dhahir. (2024)<br><strong>Movable Antennas-Assisted Secure Transmission Without Eavesdroppers&rsquo; Instantaneous CSI</strong><br><button class=copy-to-clipboard title="Movable Antennas-Assisted Secure Transmission Without Eavesdroppers' Instantaneous CSI" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-ET, cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03395v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03395v1.pdf filename=2404.03395v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Movable antenna (MA) technology is highly promising for improving communication performance, due to its advantage of flexibly adjusting positions of antennas to reconfigure channel conditions. In this paper, we investigate MAs-assisted secure transmission under a legitimate transmitter Alice, a legitimate receiver Bob and multiple eavesdroppers. Specifically, we consider a practical scenario where Alice has no any knowledge about the instantaneous non-line-of-sight component of the wiretap channel. Under this setup, we evaluate the secrecy performance by adopting the secrecy outage probability metric, the tight approximation of which is first derived by interpreting the Rician fading as a special case of Nakagami fading and concurrently exploiting the Laguerre series approximation. Then, we minimize the secrecy outage probability by jointly optimizing the transmit beamforming and positions of antennas at Alice. However, the problem is highly non-convex because the objective includes the complex incomplete gamma function. To tackle this challenge, we, for the first time, effectively approximate the inverse of the incomplete gamma function as a simple linear model. Based on this approximation, we arrive at a simplified problem with a clear structure, which can be solved via the developed alternating projected gradient ascent (APGA) algorithm. Considering the high complexity of the APGA, we further design another scheme where the zero-forcing based beamforming is adopted by Alice, and then we transform the problem into minimizing a simple function which is only related to positions of antennas at Alice.As demonstrated by <b>simulations,</b> our proposed schemes achieve significant performance gains compared to conventional schemes based on fixed-position antennas.</p></p class="citation"></blockquote><h3 id=24--220237-optimization-of-resources-for-digital-radio-transmission-over-iboc-fm-through-max-min-fairness-mónica-rico-martínez-et-al-2024>(2/4 | 220/237) Optimization of resources for digital radio transmission over IBOC FM through max-min fairness (Mónica Rico Martínez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mónica Rico Martínez, Juan Carlos Vesga Ferreira, Joel Carroll Vargas, María Consuelo Rodríguez Niño, Andrés Alejandro Diaz Toro, William Alexander Cuevas Carrero. (2024)<br><strong>Optimization of resources for digital radio transmission over IBOC FM through max-min fairness</strong><br><button class=copy-to-clipboard title="Optimization of resources for digital radio transmission over IBOC FM through max-min fairness" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03795v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03795v1.pdf filename=2404.03795v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The equitable distribution of resources in a network is a complex process, considering that not all nodes have the same requirements, and the In-Band On-Channel (IBOC) hybrid transmission system is no exception. The IBOC system utilizes a hybrid in-band transmission to simultaneously broadcast analog and digital audio over the FM band. This article proposes the use of a Max-Min <b>Fairness</b> (MMF) algorithm, with a strategy to optimize resource allocation for IBOC FM transmission in a multiservice scenario. Additionally, the MMF algorithm offers low computational complexity for implementation in low-cost embedded systems, aiming to achieve fair resource distribution and provide adequate Quality of Service (QoS) levels for each node in the RF network, considering channel conditions and traffic types. The article explores a scenario under saturated traffic conditions to assess the optimization capabilities of the MMF algorithm under well-defined traffic and channel conditions. The evaluation process yielded highly favorable results, indicating that theMMF algorithm can be considered a viable alternative for bandwidth optimization in digital broadcasting over IBOC on FM with 95% confidence, and it holds potential for implementation in other digital broadcasting system.</p></p class="citation"></blockquote><h3 id=34--221237-design-and-optimization-of-cooperative-sensing-with-limited-backhaul-capacity-wenrui-li-et-al-2024>(3/4 | 221/237) Design and Optimization of Cooperative Sensing With Limited Backhaul Capacity (Wenrui Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenrui Li, Min Li, An Liu, Tony Xiao Han. (2024)<br><strong>Design and Optimization of Cooperative Sensing With Limited Backhaul Capacity</strong><br><button class=copy-to-clipboard title="Design and Optimization of Cooperative Sensing With Limited Backhaul Capacity" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03440v1.pdf filename=2404.03440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a cooperative sensing framework designed for integrated sensing and communication cellular networks. The framework comprises one base station (BS) functioning as the sensing transmitter, while several nearby BSs act as sensing receivers. The primary objective is to facilitate cooperative target localization by enabling each receiver to share specific information with a fusion center (FC) over a limited capacity backhaul link. To achieve this goal, we propose an advanced cooperative sensing design that enhances the communication process between the receivers and the FC. Each receiver independently estimates the time delay and the reflecting coefficient associated with the reflected path from the target. Subsequently, each receiver transmits the estimated values and the received signal samples centered around the estimated time delay to the FC. To efficiently <b>quantize</b> the signal samples, a Karhunen-Lo`eve Transform coding scheme is employed. Furthermore, an optimization problem is formulated to allocate backhaul resources for quantizing different samples, improving target localization. Numerical results validate the effectiveness of our proposed advanced design and demonstrate its superiority over a baseline design, where only the locally estimated values are transmitted from each receiver to the FC.</p></p class="citation"></blockquote><h3 id=44--222237-mmse-channel-estimation-in-large-scale-mimo-improved-robustness-with-reduced-complexity-giacomo-bacci-et-al-2024>(4/4 | 222/237) MMSE Channel Estimation in Large-Scale MIMO: Improved Robustness with Reduced Complexity (Giacomo Bacci et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giacomo Bacci, Antonio Alberto D&rsquo;Amico, Luca Sanguinetti. (2024)<br><strong>MMSE Channel Estimation in Large-Scale MIMO: Improved Robustness with Reduced Complexity</strong><br><button class=copy-to-clipboard title="MMSE Channel Estimation in Large-Scale MIMO: Improved Robustness with Reduced Complexity" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03279v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03279v1.pdf filename=2404.03279v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale MIMO systems with a massive number N of individually controlled antennas pose significant challenges for minimum mean square error (MMSE) channel estimation, based on uplink pilots. The major ones arise from the computational complexity, which scales with $N^3$, and from the need for accurate knowledge of the channel statistics. This paper aims to address both challenges by introducing reduced-complexity channel estimation methods that achieve the performance of MMSE in terms of estimation accuracy and uplink spectral efficiency while demonstrating improved robustness in practical scenarios where channel statistics must be estimated. This is achieved by exploiting the inherent structure of the spatial correlation matrix induced by the array <b>geometry.</b> Specifically, we use a Kronecker decomposition for uniform planar arrays and a well-suited circulant approximation for uniform linear arrays. By doing so, a significantly lower computational complexity is achieved, scaling as $N\sqrt{N}$ and $N\log N$ for squared planar arrays and linear arrays, respectively.</p></p class="citation"></blockquote><h2 id=eesssy-2>eess.SY (2)</h2><h3 id=12--223237-implementation-of-complex-valued-sliding-mode-controllers-in-three-phase-power-converters-arnau-dòria-cerezo-et-al-2024>(1/2 | 223/237) Implementation of complex-valued sliding mode controllers in three-phase power converters (Arnau Dòria-Cerezo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arnau Dòria-Cerezo, Pau Boira, Víctor Repecho, Domingo Biel. (2024)<br><strong>Implementation of complex-valued sliding mode controllers in three-phase power converters</strong><br><button class=copy-to-clipboard title="Implementation of complex-valued sliding mode controllers in three-phase power converters" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03358v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03358v1.pdf filename=2404.03358v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents two methods for implementing complex-valued sliding mode controllers in three-phase power converters. The paper includes the description of the algorithms and a detailed analysis of the proposed implementations. The methods, that are easy to code and have a low computational burden, retain the sliding mode properties of robustness and fast response and do not require any additional processing often used to decouple the dynamics of the three-phase system. The performance of the methods is compared in numerical <b>simulations,</b> and the algorithms are experimentally tested in a microcontroller using a Hardware-in-the-Loop platform.</p></p class="citation"></blockquote><h3 id=22--224237-a-rolling-horizon-restoration-framework-for-post-disaster-restoration-of-electrical-distribution-networks-ran-wei-et-al-2024>(2/2 | 224/237) A Rolling Horizon Restoration Framework for Post-disaster Restoration of Electrical Distribution Networks (Ran Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ran Wei, Arindam K. Das, Payman Arabshahi, Daniel S. Kirschen. (2024)<br><strong>A Rolling Horizon Restoration Framework for Post-disaster Restoration of Electrical Distribution Networks</strong><br><button class=copy-to-clipboard title="A Rolling Horizon Restoration Framework for Post-disaster Restoration of Electrical Distribution Networks" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03197v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03197v1.pdf filename=2404.03197v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Severe weather events such as floods, hurricanes, earthquakes, and large wind or ice storms can cause extensive damage to electrical distribution networks, requiring a multi-day restoration effort. Complicating the recovery process is the lack of complete and accurate information regarding the extent and locations of damages, at least during the initial part of the recovery process. These factors make workforce planning challenging. In this paper, we adopt a rolling horizon restoration framework whereby repairs are planned for adjustable finite length restoration windows. Considering both repair times as well as travel times, we show that the optimal scheduling problem with multiple crews, each with their own time budget, can be recast in terms of a cost constrained reward maximizing mTSP (traveling salesman problem) on doubly weighted <b>graphs,</b> where the objective is to maximize the aggregate reward earned during the upcoming restoration window, provided no crew violates its time budget and certain electrical continuity constraints are met. We propose a mixed integer linear programming (MILP) model for solving the above problem which is validated on standard IEEE PES test feeder networks.</p></p class="citation"></blockquote><h2 id=econem-1>econ.EM (1)</h2><h3 id=11--225237-early-warning-systems-for-financial-markets-of-emerging-economies-artem-kraevskiy-et-al-2024>(1/1 | 225/237) Early warning systems for financial markets of emerging economies (Artem Kraevskiy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Artem Kraevskiy, Artem Prokhorov, Evgeniy Sokolovskiy. (2024)<br><strong>Early warning systems for financial markets of emerging economies</strong><br><button class=copy-to-clipboard title="Early warning systems for financial markets of emerging economies" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.EM<br>Categories: cs-IT, econ-EM, econ.EM, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03319v1.pdf filename=2404.03319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We develop and apply a new online early warning system (EWS) for what is known in machine learning as concept drift, in economics as a regime shift and in statistics as a change point. The system goes beyond linearity assumed in many conventional methods, and is robust to heavy tails and tail-dependence in the data, making it particularly suitable for emerging markets. The key component is an effective change-point detection mechanism for conditional entropy of the data, rather than for a particular indicator of interest. Combined with recent advances in machine learning methods for high-dimensional random forests, the mechanism is capable of finding significant shifts in information transfer between interdependent time series when traditional methods fail. We explore when this happens using <b>simulations</b> and we provide illustrations by applying the method to Uzbekistan&rsquo;s commodity and equity markets as well as to Russia&rsquo;s equity market in 2021-2023.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--226237-a-classification-of-well-behaved-graph-clustering-schemes-vilhelm-agdur-2024>(1/1 | 226/237) A classification of well-behaved graph clustering schemes (Vilhelm Agdur, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vilhelm Agdur. (2024)<br><strong>A classification of well-behaved graph clustering schemes</strong><br><button class=copy-to-clipboard title="A classification of well-behaved graph clustering schemes" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-SI, math-CO, math-CT, math.CO<br>Keyword Score: 16<br>Keywords: Graph, Clustering, Hierarchical Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03332v1.pdf filename=2404.03332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Community detection in <b>graphs</b> is a problem that is likely to be relevant whenever network data appears, and consequently the problem has received much attention with many different methods and algorithms applied. However, many of these methods are hard to study theoretically, and they optimise for somewhat different goals. A general and rigorous account of the problem and possible methods remains elusive. We study the class of all <b>clustering</b> methods that are monotone under addition of vertices and edges, phrasing this as a functoriality notion. We show that if additionally we require the methods to have no resolution limit in a strong sense, this is equivalent to a notion of representability, which requires them to be explainable and determined by a representing set of <b>graphs.</b> We show that representable <b>clustering</b> methods are always computable in polynomial time, and in any nowhere dense class they are computable in roughly quadratic time. Finally, we extend our definitions to the case of <b>hierarchical</b> <b>clustering,</b> and give a notion of representability for <b>hierarchical</b> <b>clustering</b> schemes.</p></p class="citation"></blockquote><h2 id=csne-2>cs.NE (2)</h2><h3 id=12--227237-spikeexplorer-hardware-oriented-design-space-exploration-for-spiking-neural-networks-on-fpga-dario-padovano-et-al-2024>(1/2 | 227/237) SpikeExplorer: hardware-oriented Design Space Exploration for Spiking Neural Networks on FPGA (Dario Padovano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dario Padovano, Alessio Carpegna, Alessandro Savino, Stefano Di Carlo. (2024)<br><strong>SpikeExplorer: hardware-oriented Design Space Exploration for Spiking Neural Networks on FPGA</strong><br><button class=copy-to-clipboard title="SpikeExplorer: hardware-oriented Design Space Exploration for Spiking Neural Networks on FPGA" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 13<br>Keywords: MNIST, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03714v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03714v1.pdf filename=2404.03714v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One of today&rsquo;s main concerns is to bring Artificial Intelligence power to embedded systems for edge applications. The hardware resources and power consumption required by state-of-the-art models are incompatible with the constrained environments observed in edge systems, such as IoT nodes and wearable devices. Spiking Neural Networks (SNNs) can represent a solution in this sense: inspired by neuroscience, they reach unparalleled power and resource efficiency when run on dedicated hardware accelerators. However, when designing such accelerators, the amount of choices that can be taken is huge. This paper presents SpikExplorer, a modular and flexible Python tool for hardware-oriented Automatic Design Space Exploration to automate the configuration of FPGA accelerators for SNNs. Using Bayesian optimizations, SpikerExplorer enables hardware-centric multi-objective optimization, supporting factors such as accuracy, area, latency, power, and various combinations during the exploration process. The tool searches the optimal network architecture, neuron model, and internal and training parameters, trying to reach the desired constraints imposed by the user. It allows for a straightforward network configuration, providing the full set of explored points for the user to pick the trade-off that best fits the needs. The potential of SpikExplorer is showcased using three <b>benchmark</b> datasets. It reaches 95.8% accuracy on the <b>MNIST</b> dataset, with a power consumption of 180mW/image and a latency of 0.12 ms/image, making it a powerful tool for automatically optimizing SNNs.</p></p class="citation"></blockquote><h3 id=22--228237-benchmarking-parameter-control-methods-in-differential-evolution-for-mixed-integer-black-box-optimization-ryoji-tanabe-2024>(2/2 | 228/237) Benchmarking Parameter Control Methods in Differential Evolution for Mixed-Integer Black-Box Optimization (Ryoji Tanabe, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryoji Tanabe. (2024)<br><strong>Benchmarking Parameter Control Methods in Differential Evolution for Mixed-Integer Black-Box Optimization</strong><br><button class=copy-to-clipboard title="Benchmarking Parameter Control Methods in Differential Evolution for Mixed-Integer Black-Box Optimization" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 11<br>Keywords: Benchmarking, Benchmarking, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03303v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03303v1.pdf filename=2404.03303v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Differential evolution (DE) generally requires parameter control methods (PCMs) for the scale factor and crossover rate. Although a better understanding of PCMs provides a useful clue to designing an efficient DE, their effectiveness is poorly understood in mixed-integer <b>black-box</b> <b>optimization.</b> In this context, this paper <b>benchmarks</b> PCMs in DE on the mixed-integer <b>black-box</b> <b>optimization</b> <b>benchmarking</b> function (bbob-mixint) suite in a component-wise manner. First, we demonstrate that the best PCM significantly depends on the combination of the mutation strategy and repair method. Although the PCM of SHADE is state-of-the-art for numerical <b>black-box</b> <b>optimization,</b> our results show its poor performance for mixed-integer <b>black-box</b> <b>optimization.</b> In contrast, our results show that some simple PCMs (e.g., the PCM of CoDE) perform the best in most cases. Then, we demonstrate that a DE with a suitable PCM performs significantly better than CMA-ES with integer handling for larger budgets of function evaluations. Finally, we show how the adaptation in the PCM of SHADE fails.</p></p class="citation"></blockquote><h2 id=physicsao-ph-1>physics.ao-ph (1)</h2><h3 id=11--229237-site-specific-deterministic-temperature-and-humidity-forecasts-with-explainable-and-reliable-machine-learning-mengmeng-han-et-al-2024>(1/1 | 229/237) Site-specific Deterministic Temperature and Humidity Forecasts with Explainable and Reliable Machine Learning (MengMeng Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>MengMeng Han, Tennessee Leeuwenburg, Brad Murphy. (2024)<br><strong>Site-specific Deterministic Temperature and Humidity Forecasts with Explainable and Reliable Machine Learning</strong><br><button class=copy-to-clipboard title="Site-specific Deterministic Temperature and Humidity Forecasts with Explainable and Reliable Machine Learning" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.ao-ph<br>Categories: cs-LG, physics-ao-ph, physics.ao-ph<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03310v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03310v1.pdf filename=2404.03310v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Site-specific weather forecasts are essential to accurate prediction of power demand and are consequently of great interest to energy operators. However, weather forecasts from current numerical weather prediction (NWP) models lack the fine-scale detail to capture all important characteristics of localised real-world sites. Instead they provide weather information representing a rectangular gridbox (usually kilometres in size). Even after post-processing and bias correction, area-averaged information is usually not optimal for specific sites. Prior work on site optimised forecasts has focused on linear methods, weighted consensus averaging, time-series methods, and others. Recent developments in machine learning (ML) have <b>prompted</b> increasing interest in applying ML as a novel approach towards this problem. In this study, we investigate the feasibility of optimising forecasts at sites by adopting the popular machine learning model gradient boosting decision tree, supported by the Python version of the XGBoost package. Regression trees have been trained with historical NWP and site observations as training data, aimed at predicting temperature and dew point at multiple site locations across Australia. We developed a working ML framework, named &lsquo;Multi-SiteBoost&rsquo; and initial testing results show a significant improvement compared with gridded values from bias-corrected NWP models. The improvement from XGBoost is found to be comparable with non-ML methods reported in literature. With the insights provided by SHapley Additive exPlanations (SHAP), this study also tests various approaches to understand the ML predictions and increase the reliability of the forecasts generated by ML.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--230237-foundation-model-for-advancing-healthcare-challenges-opportunities-and-future-directions-yuting-he-et-al-2024>(1/1 | 230/237) Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions (Yuting He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuting He, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, Hao Chen. (2024)<br><strong>Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions</strong><br><button class=copy-to-clipboard title="Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03264v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03264v1.pdf filename=2404.03264v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>model,</b> which is pre-trained on broad data and is able to adapt to a wide range of tasks, is advancing healthcare. It promotes the development of healthcare artificial intelligence (AI) models, breaking the contradiction between limited AI models and diverse healthcare practices. Much more widespread healthcare scenarios will benefit from the development of a healthcare <b>foundation</b> <b>model</b> (HFM), improving their advanced intelligent healthcare services. Despite the impending widespread deployment of HFMs, there is currently a lack of clear understanding about how they work in the healthcare field, their current challenges, and where they are headed in the future. To answer these questions, a comprehensive and deep survey of the challenges, opportunities, and future directions of HFMs is presented in this survey. It first conducted a comprehensive overview of the HFM including the methods, data, and applications for a quick grasp of the current progress. Then, it made an in-depth exploration of the challenges present in data, algorithms, and computing infrastructures for constructing and widespread application of <b>foundation</b> <b>models</b> in healthcare. This survey also identifies emerging and promising directions in this field for future development. We believe that this survey will enhance the community&rsquo;s comprehension of the current progress of HFM and serve as a valuable source of guidance for future development in this field. The latest HFM papers and related resources are maintained on our website: <a href=https://github.com/YutingHe-list/Awesome-Foundation-Models-for-Advancing-Healthcare>https://github.com/YutingHe-list/Awesome-Foundation-Models-for-Advancing-Healthcare</a>.</p></p class="citation"></blockquote><h2 id=mathna-1>math.NA (1)</h2><h3 id=11--231237-computational-analysis-of-a-contraction-rheometer-for-the-grade-two-fluid-model-sara-pollock-et-al-2024>(1/1 | 231/237) Computational analysis of a contraction rheometer for the grade-two fluid model (Sara Pollock et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sara Pollock, L. Ridgway Scott. (2024)<br><strong>Computational analysis of a contraction rheometer for the grade-two fluid model</strong><br><button class=copy-to-clipboard title="Computational analysis of a contraction rheometer for the grade-two fluid model" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 76A05, 65N30, cs-NA, math-NA, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03450v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03450v1.pdf filename=2404.03450v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explore the possibility of simulating the grade-two fluid model in a <b>geometry</b> related to a contraction rheometer, and we provide details on several key aspects of the computation. We show how the results can be used to determine the viscosity $\nu$ from experimental data. We also explore the identifiability of the grade-two parameters $\alpha_1$ and $\alpha_2$ from experimental data. In particular, as the flow rate varies, force data appears to be nearly the same for certain distinct pairs of values $\alpha_1$ and $\alpha_2$; however we determine a regime for $\alpha_1$ and $\alpha_2$ for which the parameters may be identifiable with a contraction rheometer.</p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--232237-additive-approximation-algorithm-for-geodesic-centers-in-δ-hyperbolic-graphs-dibyayan-chakraborty-et-al-2024>(1/2 | 232/237) Additive approximation algorithm for geodesic centers in $δ$-hyperbolic graphs (Dibyayan Chakraborty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dibyayan Chakraborty, Yann Vaxès. (2024)<br><strong>Additive approximation algorithm for geodesic centers in $δ$-hyperbolic graphs</strong><br><button class=copy-to-clipboard title="Additive approximation algorithm for geodesic centers in $δ$-hyperbolic graphs" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-CC, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03812v1.pdf filename=2404.03812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For an integer $k\geq 1$, the objective of \textsc{$k$-Geodesic Center} is to find a set $\mathcal{C}$ of $k$ isometric paths such that the maximum distance between any vertex $v$ and $\mathcal{C}$ is minimised. Introduced by Gromov, \emph{$\delta$-hyperbolicity} measures how treelike a <b>graph</b> is from a metric point of view. Our main contribution in this paper is to provide an additive $O(\delta)$-approximation algorithm for \textsc{$k$-Geodesic Center} on $\delta$-hyperbolic <b>graphs.</b> On the way, we define a coarse version of the pairing property introduced by Gerstel & Zaks (Networks, 1994) and show it holds for $\delta$-hyperbolic <b>graphs.</b> This result allows to reduce the \textsc{$k$-Geodesic Center} problem to its rooted counterpart, a main idea behind our algorithm. We also adapt a technique of Dragan & Leitert, (TCS, 2017) to show that for every $k\geq 1$, $k$-\textsc{Geodesic Center} is NP-hard even on partial grids.</p></p class="citation"></blockquote><h3 id=22--233237-lower-bounds-for-graph-reconstruction-with-maximal-independent-set-queries-lukas-michel-et-al-2024>(2/2 | 233/237) Lower bounds for graph reconstruction with maximal independent set queries (Lukas Michel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lukas Michel, Alex Scott. (2024)<br><strong>Lower bounds for graph reconstruction with maximal independent set queries</strong><br><button class=copy-to-clipboard title="Lower bounds for graph reconstruction with maximal independent set queries" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03472v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03472v1.pdf filename=2404.03472v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the number of maximal independent set queries required to reconstruct the edges of a hidden <b>graph.</b> We show that randomised adaptive algorithms need at least $\Omega(\Delta^2 \log(n / \Delta) / \log \Delta)$ queries to reconstruct $n$-vertex <b>graphs</b> of maximum degree $\Delta$ with success probability at least $1/2$, and we further improve this lower bound to $\Omega(\Delta^2 \log(n / \Delta))$ for randomised non-adaptive algorithms. We also prove that deterministic non-adaptive algorithms require at least $\Omega(\Delta^3 \log n / \log \Delta)$ queries. This improves bounds of Konrad, O&rsquo;Sullivan, and Traistaru, and answers one of their questions. The proof of the lower bound for deterministic non-adaptive algorithms relies on a connection to cover-free families, for which we also improve known bounds.</p></p class="citation"></blockquote><h2 id=cscg-2>cs.CG (2)</h2><h3 id=12--234237-the-maximum-clique-problem-in-a-disk-graph-made-easy-j-mark-keil-et-al-2024>(1/2 | 234/237) The Maximum Clique Problem in a Disk Graph Made Easy (J. Mark Keil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>J. Mark Keil, Debajyoti Mondal. (2024)<br><strong>The Maximum Clique Problem in a Disk Graph Made Easy</strong><br><button class=copy-to-clipboard title="The Maximum Clique Problem in a Disk Graph Made Easy" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: 52C99, 68Q25, F-2-2; I-3-5, cs-CG, cs-DS, cs.CG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03751v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03751v1.pdf filename=2404.03751v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A disk <b>graph</b> is an intersection <b>graph</b> of disks in $\mathbb{R}^2$. Determining the computational complexity of finding a maximum clique in a disk <b>graph</b> is a long-standing open problem. In 1990, Clark, Colbourn, and Johnson gave a polynomial-time algorithm for computing a maximum clique in a unit disk <b>graph.</b> However, finding a maximum clique when disks are of arbitrary size is widely believed to be a challenging open problem. The problem is open even if we restrict the disks to have at most two different sizes of radii, or restrict the radii to be within $[1,1+\varepsilon]$ for some $\epsilon>0$. In this paper, we provide a new perspective to examine adjacencies in a disk <b>graph</b> that helps obtain the following results. - We design an $O(2^k n^{2k} poly(n))$-time algorithm to find a maximum clique in a $n$-vertex disk <b>graph</b> with $k$ different sizes of radii. This is polynomial for every fixed $k$, and thus settles the open question for the case when $k=2$. - Given a set of $n$ unit disks, we show how to compute a maximum clique inside each possible axis-aligned rectangle determined by the disk centers in $O(n^5\log n)$-time. This is at least a factor of $n^{4/3}$ faster than applying the fastest known algorithm for finding a maximum clique in a unit disk <b>graph</b> for each rectangle independently. - We give an $O(2^kn^{2rk} poly(n,r))$-time algorithm to find a maximum clique in a $n$-vertex ball <b>graph</b> with $k$ different sizes of radii where the ball centers lie on $r$ parallel planes. This is polynomial for every fixed $k$ and $r$, and thus contrasts the previously known NP-hardness result for finding a maximum clique in an arbitrary ball <b>graph.</b></p></p class="citation"></blockquote><h3 id=22--235237-improved-total-domination-and-total-roman-domination-in-unit-disk-graphs-sasmita-rout-et-al-2024>(2/2 | 235/237) Improved Total Domination and Total Roman Domination in Unit Disk Graphs (Sasmita Rout et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sasmita Rout, Gautam Kumar Das. (2024)<br><strong>Improved Total Domination and Total Roman Domination in Unit Disk Graphs</strong><br><button class=copy-to-clipboard title="Improved Total Domination and Total Roman Domination in Unit Disk Graphs" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs.CG, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03511v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03511v1.pdf filename=2404.03511v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Let $G=(V, E)$ be a simple undirected <b>graph</b> with no isolated vertex. A set $D_t\subseteq V$ is a total dominating set of $G$ if $(i)$ $D_t$ is a dominating set, and $(ii)$ the set $D_t$ induces a subgraph with no isolated vertex. The total dominating set of minimum cardinality is called the minimum total dominating set, and the size of the minimum total dominating set is called the total domination number ($\gamma_t(G)$). Given a <b>graph</b> $G$, the total dominating set (TDS) problem is to find a total dominating set of minimum cardinality. A Roman dominating function (RDF) on a <b>graph</b> $G$ is a function $f:V\rightarrow {0,1,2}$ such that each vertex $v\in V$ with $f(v)=0$ is adjacent to at least one vertex $u\in V$ with $f(u)=2$. A RDF $f$ of a <b>graph</b> $G$ is said to be a total Roman dominating function (TRDF) if the induced subgraph of $V_1\cup V_2$ does not contain any isolated vertex, where $V_i={u\in V|f(u)=i}$. Given a <b>graph</b> $G$, the total Roman dominating set (TRDS) problem is to minimize the weight, $W(f)=\sum_{u\in V} f(u)$, called the total Roman domination number ($\gamma_{tR}(G)$). In this paper, we are the first to show that the TRDS problem is NP-complete in unit disk <b>graphs</b> (UDGs). Furthermore, we propose a $7.17\operatorname{-}$ factor approximation algorithm for the TDS problem and a $6.03\operatorname{-}$ factor approximation algorithm for the TRDS problem in geometric unit disk <b>graphs.</b> The running time for both algorithms is notably bounded by $O(n\log{k})$, where $n$ represents the number of vertices in the given UDG and $k$ represents the size of the independent set in (i.e., $D$ and $V_2$ in TDS and TRDS problems, respectively) the given UDG.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--236237-reservoir-sampling-over-joins-binyang-dai-et-al-2024>(1/1 | 236/237) Reservoir Sampling over Joins (Binyang Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Binyang Dai, Xiao Hu, Ke Yi. (2024)<br><strong>Reservoir Sampling over Joins</strong><br><button class=copy-to-clipboard title="Reservoir Sampling over Joins" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03194v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03194v1.pdf filename=2404.03194v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sampling over joins is a fundamental task in large-scale data analytics. Instead of computing the full join results, which could be massive, a uniform sample of the join results would suffice for many purposes, such as answering analytical queries or training machine learning models. In this paper, we study the problem of how to maintain a random sample over joins while the tuples are streaming in. Without the join, this problem can be solved by some simple and classical reservoir sampling algorithms. However, the join operator makes the problem significantly harder, as the join size can be polynomially larger than the input. We present a new algorithm for this problem that achieves a near-linear complexity. The key technical components are a generalized reservoir sampling algorithm that supports a predicate, and a dynamic index for sampling over joins. We also conduct extensive experiments on both <b>graph</b> and relational data over various join queries, and the experimental results demonstrate significant performance improvement over the state of the art.</p></p class="citation"></blockquote><h2 id=cset-1>cs.ET (1)</h2><h3 id=11--237237-tegra----scaling-up-terascale-graph-processing-with-disaggregated-computing-william-shaddix-et-al-2024>(1/1 | 237/237) TEGRA &ndash; Scaling Up Terascale Graph Processing with Disaggregated Computing (William Shaddix et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>William Shaddix, Mahyar Samani, Marjan Fariborz, S. J. Ben Yoo, Jason Lowe-Power, Venkatesh Akella. (2024)<br><strong>TEGRA &ndash; Scaling Up Terascale Graph Processing with Disaggregated Computing</strong><br><button class=copy-to-clipboard title="TEGRA -- Scaling Up Terascale Graph Processing with Disaggregated Computing" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.ET<br>Categories: cs-ET, cs.ET<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03155v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03155v1.pdf filename=2404.03155v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graphs</b> are essential for representing relationships in various domains, driving modern AI applications such as <b>graph</b> analytics and neural networks across science, engineering, cybersecurity, transportation, and economics. However, the size of modern <b>graphs</b> are rapidly expanding, posing challenges for traditional CPUs and GPUs in meeting real-time processing demands. As a result, hardware accelerators for <b>graph</b> processing have been proposed. However, the largest <b>graphs</b> that can be handled by these systems is still modest often targeting Twitter graph(1.4B edges approximately). This paper aims to address this limitation by developing a <b>graph</b> accelerator capable of terascale <b>graph</b> processing. Scale out architectures, architectures where nodes are replicated to expand to larger datasets, are natural for handling larger <b>graphs.</b> We argue that this approach is not appropriate for very large-scale <b>graphs</b> because it leads to under utilization of both memory resources and compute resources. Additionally, vertex and edge processing have different access patterns. Communication overheads also pose further challenges in designing scalable architectures. To overcome these issues, this paper proposes TEGRA, a scale-up architecture for terascale <b>graph</b> processing. TEGRA leverages a composable computing system with disaggregated resources and a communication architecture inspired by Active Messages. By employing direct communication between cores and optimizing memory interconnect utilization, TEGRA effectively reduces communication overhead and improves resource utilization, therefore enabling efficient processing of terascale <b>graphs.</b></p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.04.05</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.04.07</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-45>cs.CL (45)</a><ul><li><a href=#145--1237-enhancing-the-performance-of-aspect-based-sentiment-analysis-systems-chen-li-et-al-2024>(1/45 | 1/237) Enhancing the Performance of Aspect-Based Sentiment Analysis Systems (Chen Li et al., 2024)</a></li><li><a href=#245--2237-evaluating-llms-at-detecting-errors-in-llm-responses-ryo-kamoi-et-al-2024>(2/45 | 2/237) Evaluating LLMs at Detecting Errors in LLM Responses (Ryo Kamoi et al., 2024)</a></li><li><a href=#345--3237-intent-detection-and-entity-extraction-from-biomedical-literature-ankan-mullick-et-al-2024>(3/45 | 3/237) Intent Detection and Entity Extraction from BioMedical Literature (Ankan Mullick et al., 2024)</a></li><li><a href=#445--4237-how-does-multi-task-training-affect-transformer-in-context-capabilities-investigations-with-function-classes-harmon-bhasin-et-al-2024>(4/45 | 4/237) How does Multi-Task Training Affect Transformer In-Context Capabilities? Investigations with Function Classes (Harmon Bhasin et al., 2024)</a></li><li><a href=#545--5237-banglaautokg-automatic-bangla-knowledge-graph-construction-with-semantic-neural-graph-filtering-azmine-toushik-wasi-et-al-2024>(5/45 | 5/237) BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with Semantic Neural Graph Filtering (Azmine Toushik Wasi et al., 2024)</a></li><li><a href=#645--6237-canttalkaboutthis-aligning-language-models-to-stay-on-topic-in-dialogues-makesh-narsimhan-sreedhar-et-al-2024>(6/45 | 6/237) CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues (Makesh Narsimhan Sreedhar et al., 2024)</a></li><li><a href=#745--7237-can-small-language-models-help-large-language-models-reason-better-lm-guided-chain-of-thought-jooyoung-lee-et-al-2024>(7/45 | 7/237) Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought (Jooyoung Lee et al., 2024)</a></li><li><a href=#845--8237-shroom-indelab-at-semeval-2024-task-6-zero--and-few-shot-llm-based-classification-for-hallucination-detection-bradley-p-allen-et-al-2024>(8/45 | 8/237) SHROOM-INDElab at SemEval-2024 Task 6: Zero- and Few-Shot LLM-Based Classification for Hallucination Detection (Bradley P. Allen et al., 2024)</a></li><li><a href=#945--9237-evaluating-generative-language-models-in-information-extraction-as-subjective-question-correction-yuchen-fan-et-al-2024>(9/45 | 9/237) Evaluating Generative Language Models in Information Extraction as Subjective Question Correction (Yuchen Fan et al., 2024)</a></li><li><a href=#1045--10237-untangle-the-knot-interweaving-conflicting-knowledge-and-reasoning-skills-in-large-language-models-yantao-liu-et-al-2024>(10/45 | 10/237) Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models (Yantao Liu et al., 2024)</a></li><li><a href=#1145--11237-the-probabilities-also-matter-a-more-faithful-metric-for-faithfulness-of-free-text-explanations-in-large-language-models-noah-y-siegel-et-al-2024>(11/45 | 11/237) The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models (Noah Y. Siegel et al., 2024)</a></li><li><a href=#1245--12237-nlp-at-uc-santa-cruz-at-semeval-2024-task-5-legal-answer-validation-using-few-shot-multi-choice-qa-anish-pahilajani-et-al-2024>(12/45 | 12/237) NLP at UC Santa Cruz at SemEval-2024 Task 5: Legal Answer Validation using Few-Shot Multi-Choice QA (Anish Pahilajani et al., 2024)</a></li><li><a href=#1345--13237-a-cause-effect-look-at-alleviating-hallucination-of-knowledge-grounded-dialogue-generation-jifan-yu-et-al-2024>(13/45 | 13/237) A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation (Jifan Yu et al., 2024)</a></li><li><a href=#1445--14237-unveiling-llms-the-evolution-of-latent-representations-in-a-temporal-knowledge-graph-marco-bronzini-et-al-2024>(14/45 | 14/237) Unveiling LLMs: The Evolution of Latent Representations in a Temporal Knowledge Graph (Marco Bronzini et al., 2024)</a></li><li><a href=#1545--15237-autowebglm-bootstrap-and-reinforce-a-large-language-model-based-web-navigating-agent-hanyu-lai-et-al-2024>(15/45 | 15/237) AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent (Hanyu Lai et al., 2024)</a></li><li><a href=#1645--16237-reft-representation-finetuning-for-language-models-zhengxuan-wu-et-al-2024>(16/45 | 16/237) ReFT: Representation Finetuning for Language Models (Zhengxuan Wu et al., 2024)</a></li><li><a href=#1745--17237-nicolay-r-at-semeval-2024-task-3-using-flan-t5-for-reasoning-emotion-cause-in-conversations-with-chain-of-thought-on-emotion-states-nicolay-rusnachenko-et-al-2024>(17/45 | 17/237) nicolay-r at SemEval-2024 Task 3: Using Flan-T5 for Reasoning Emotion Cause in Conversations with Chain-of-Thought on Emotion States (Nicolay Rusnachenko et al., 2024)</a></li><li><a href=#1845--18237-visualization-of-thought-elicits-spatial-reasoning-in-large-language-models-wenshan-wu-et-al-2024>(18/45 | 18/237) Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models (Wenshan Wu et al., 2024)</a></li><li><a href=#1945--19237-sailor-open-language-models-for-south-east-asia-longxu-dou-et-al-2024>(19/45 | 19/237) Sailor: Open Language Models for South-East Asia (Longxu Dou et al., 2024)</a></li><li><a href=#2045--20237-okay-lets-do-this-modeling-event-coreference-with-generated-rationales-and-knowledge-distillation-abhijnan-nath-et-al-2024>(20/45 | 20/237) Okay, Let&rsquo;s Do This! Modeling Event Coreference with Generated Rationales and Knowledge Distillation (Abhijnan Nath et al., 2024)</a></li><li><a href=#2145--21237-personalized-llm-response-generation-with-parameterized-memory-injection-kai-zhang-et-al-2024>(21/45 | 21/237) Personalized LLM Response Generation with Parameterized Memory Injection (Kai Zhang et al., 2024)</a></li><li><a href=#2245--22237-learn-when-not-to-trust-language-models-a-privacy-centric-adaptive-model-aware-approach-chengkai-huang-et-al-2024>(22/45 | 22/237) Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive Model-Aware Approach (Chengkai Huang et al., 2024)</a></li><li><a href=#2345--23237-generative-ai-and-teachers----for-us-or-against-us-a-case-study-jenny-pettersson-et-al-2024>(23/45 | 23/237) Generative AI and Teachers &ndash; For Us or Against Us? A Case Study (Jenny Pettersson et al., 2024)</a></li><li><a href=#2445--24237-concept----an-evaluation-protocol-on-conversation-recommender-systems-with-system--and-user-centric-factors-chen-huang-et-al-2024>(24/45 | 24/237) Concept &ndash; An Evaluation Protocol on Conversation Recommender Systems with System- and User-centric Factors (Chen Huang et al., 2024)</a></li><li><a href=#2545--25237-uncertainty-in-language-models-assessment-through-rank-calibration-xinmeng-huang-et-al-2024>(25/45 | 25/237) Uncertainty in Language Models: Assessment through Rank-Calibration (Xinmeng Huang et al., 2024)</a></li><li><a href=#2645--26237-scaffolding-language-learning-via-multi-modal-tutoring-systems-with-pedagogical-instructions-zhengyuan-liu-et-al-2024>(26/45 | 26/237) Scaffolding Language Learning via Multi-modal Tutoring Systems with Pedagogical Instructions (Zhengyuan Liu et al., 2024)</a></li><li><a href=#2745--27237-training-llms-over-neurally-compressed-text-brian-lester-et-al-2024>(27/45 | 27/237) Training LLMs over Neurally Compressed Text (Brian Lester et al., 2024)</a></li><li><a href=#2845--28237-select-and-summarize-scene-saliency-for-movie-script-summarization-rohit-saxena-et-al-2024>(28/45 | 28/237) Select and Summarize: Scene Saliency for Movie Script Summarization (Rohit Saxena et al., 2024)</a></li><li><a href=#2945--29237-reevaluating-bias-detection-in-language-models-the-role-of-implicit-norm-farnaz-kohankhaki-et-al-2024>(29/45 | 29/237) Reevaluating Bias Detection in Language Models: The Role of Implicit Norm (Farnaz Kohankhaki et al., 2024)</a></li><li><a href=#3045--30237-learning-to-plan-and-generate-text-with-citations-constanza-fierro-et-al-2024>(30/45 | 30/237) Learning to Plan and Generate Text with Citations (Constanza Fierro et al., 2024)</a></li><li><a href=#3145--31237-probing-large-language-models-for-scalar-adjective-lexical-semantics-and-scalar-diversity-pragmatics-fangru-lin-et-al-2024>(31/45 | 31/237) Probing Large Language Models for Scalar Adjective Lexical Semantics and Scalar Diversity Pragmatics (Fangru Lin et al., 2024)</a></li><li><a href=#3245--32237-robust-pronoun-use-fidelity-with-english-llms-are-they-reasoning-repeating-or-just-biased-vagrant-gautam-et-al-2024>(32/45 | 32/237) Robust Pronoun Use Fidelity with English LLMs: Are they Reasoning, Repeating, or Just Biased? (Vagrant Gautam et al., 2024)</a></li><li><a href=#3345--33237-a-comparative-analysis-of-word-level-metric-differential-privacy-benchmarking-the-privacy-utility-trade-off-stephen-meisenbacher-et-al-2024>(33/45 | 33/237) A Comparative Analysis of Word-Level Metric Differential Privacy: Benchmarking The Privacy-Utility Trade-off (Stephen Meisenbacher et al., 2024)</a></li><li><a href=#3445--34237-probelm-plausibility-ranking-evaluation-for-language-models-zhangdie-yuan-et-al-2024>(34/45 | 34/237) PRobELM: Plausibility Ranking Evaluation for Language Models (Zhangdie Yuan et al., 2024)</a></li><li><a href=#3545--35237-towards-pareto-optimal-throughput-in-small-language-model-serving-pol-g-recasens-et-al-2024>(35/45 | 35/237) Towards Pareto Optimal Throughput in Small Language Model Serving (Pol G. Recasens et al., 2024)</a></li><li><a href=#3645--36237-locating-and-editing-factual-associations-in-mamba-arnab-sen-sharma-et-al-2024>(36/45 | 36/237) Locating and Editing Factual Associations in Mamba (Arnab Sen Sharma et al., 2024)</a></li><li><a href=#3745--37237-easse-de-easier-automatic-sentence-simplification-evaluation-for-german-regina-stodden-2024>(37/45 | 37/237) EASSE-DE: Easier Automatic Sentence Simplification Evaluation for German (Regina Stodden, 2024)</a></li><li><a href=#3845--38237-edisum-summarizing-and-explaining-wikipedia-edits-at-scale-marija-šakota-et-al-2024>(38/45 | 38/237) Edisum: Summarizing and Explaining Wikipedia Edits at Scale (Marija Šakota et al., 2024)</a></li><li><a href=#3945--39237-how-easily-do-irrelevant-inputs-skew-the-responses-of-large-language-models-siye-wu-et-al-2024>(39/45 | 39/237) How Easily do Irrelevant Inputs Skew the Responses of Large Language Models? (Siye Wu et al., 2024)</a></li><li><a href=#4045--40237-the-death-of-feature-engineering-bert-with-linguistic-features-on-squad-20-jiawei-li-et-al-2024>(40/45 | 40/237) The Death of Feature Engineering? BERT with Linguistic Features on SQuAD 2.0 (Jiawei Li et al., 2024)</a></li><li><a href=#4145--41237-from-news-to-summaries-building-a-hungarian-corpus-for-extractive-and-abstractive-summarization-botond-barta-et-al-2024>(41/45 | 41/237) From News to Summaries: Building a Hungarian Corpus for Extractive and Abstractive Summarization (Botond Barta et al., 2024)</a></li><li><a href=#4245--42237-evaluating-document-simplification-on-the-importance-of-separately-assessing-simplicity-and-meaning-preservation-liam-cripwell-et-al-2024>(42/45 | 42/237) Evaluating Document Simplification: On the Importance of Separately Assessing Simplicity and Meaning Preservation (Liam Cripwell et al., 2024)</a></li><li><a href=#4345--43237-knowledge-graph-representation-for-political-information-sources-tinatin-osmonova-et-al-2024>(43/45 | 43/237) Knowledge Graph Representation for Political Information Sources (Tinatin Osmonova et al., 2024)</a></li><li><a href=#4445--44237-schroedingers-threshold-when-the-auc-doesnt-predict-accuracy-juri-opitz-2024>(44/45 | 44/237) Schroedinger&rsquo;s Threshold: When the AUC doesn&rsquo;t predict Accuracy (Juri Opitz, 2024)</a></li><li><a href=#4545--45237-m3tcm-multi-modal-multi-task-context-model-for-utterance-classification-in-motivational-interviews-sayed-muddashir-hossain-et-al-2024>(45/45 | 45/237) M3TCM: Multi-modal Multi-task Context Model for Utterance Classification in Motivational Interviews (Sayed Muddashir Hossain et al., 2024)</a></li></ul></li><li><a href=#csir-8>cs.IR (8)</a><ul><li><a href=#18--46237-a-comprehensive-survey-on-self-supervised-learning-for-recommendation-xubin-ren-et-al-2024>(1/8 | 46/237) A Comprehensive Survey on Self-Supervised Learning for Recommendation (Xubin Ren et al., 2024)</a></li><li><a href=#28--47237-do-large-language-models-rank-fairly-an-empirical-study-on-the-fairness-of-llms-as-rankers-yuan-wang-et-al-2024>(2/8 | 47/237) Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers (Yuan Wang et al., 2024)</a></li><li><a href=#38--48237-understanding-language-modeling-paradigm-adaptations-in-recommender-systems-lessons-learned-and-open-challenges-lemei-zhang-et-al-2024>(3/8 | 48/237) Understanding Language Modeling Paradigm Adaptations in Recommender Systems: Lessons Learned and Open Challenges (Lemei Zhang et al., 2024)</a></li><li><a href=#48--49237-sequential-recommendation-for-optimizing-both-immediate-feedback-and-long-term-retention-ziru-liu-et-al-2024>(4/8 | 49/237) Sequential Recommendation for Optimizing Both Immediate Feedback and Long-term Retention (Ziru Liu et al., 2024)</a></li><li><a href=#58--50237-a-directional-diffusion-graph-transformer-for-recommendation-zixuan-yi-et-al-2024>(5/8 | 50/237) A Directional Diffusion Graph Transformer for Recommendation (Zixuan Yi et al., 2024)</a></li><li><a href=#68--51237-genqrensemble-zero-shot-llm-ensemble-prompting-for-generative-query-reformulation-kaustubh-dhole-et-al-2024>(6/8 | 51/237) GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation (Kaustubh Dhole et al., 2024)</a></li><li><a href=#78--52237-does-knowledge-graph-really-matter-for-recommender-systems-haonan-zhang-et-al-2024>(7/8 | 52/237) Does Knowledge Graph Really Matter for Recommender Systems? (Haonan Zhang et al., 2024)</a></li><li><a href=#88--53237-to-search-or-to-recommend-predicting-open-app-motivation-with-neural-hawkes-process-zhongxiang-sun-et-al-2024>(8/8 | 53/237) To Search or to Recommend: Predicting Open-App Motivation with Neural Hawkes Process (Zhongxiang Sun et al., 2024)</a></li></ul></li><li><a href=#cslg-42>cs.LG (42)</a><ul><li><a href=#142--54237-direct-nash-optimization-teaching-language-models-to-self-improve-with-general-preferences-corby-rosset-et-al-2024>(1/42 | 54/237) Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences (Corby Rosset et al., 2024)</a></li><li><a href=#242--55237-knowledge-distillation-based-model-extraction-attack-using-private-counterfactual-explanations-fatima-ezzeddine-et-al-2024>(2/42 | 55/237) Knowledge Distillation-Based Model Extraction Attack using Private Counterfactual Explanations (Fatima Ezzeddine et al., 2024)</a></li><li><a href=#342--56237-on-the-surprising-efficacy-of-distillation-as-an-alternative-to-pre-training-small-models-sean-farhat-et-al-2024>(3/42 | 56/237) On the Surprising Efficacy of Distillation as an Alternative to Pre-Training Small Models (Sean Farhat et al., 2024)</a></li><li><a href=#442--57237-red-teaming-gpt-4v-are-gpt-4v-safe-against-unimulti-modal-jailbreak-attacks-shuo-chen-et-al-2024>(4/42 | 57/237) Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks? (Shuo Chen et al., 2024)</a></li><li><a href=#542--58237-theoretical-and-empirical-insights-into-the-origins-of-degree-bias-in-graph-neural-networks-arjun-subramonian-et-al-2024>(5/42 | 58/237) Theoretical and Empirical Insights into the Origins of Degree Bias in Graph Neural Networks (Arjun Subramonian et al., 2024)</a></li><li><a href=#642--59237-learning-social-fairness-preferences-from-non-expert-stakeholder-opinions-in-kidney-placement-mukund-telukunta-et-al-2024>(6/42 | 59/237) Learning Social Fairness Preferences from Non-Expert Stakeholder Opinions in Kidney Placement (Mukund Telukunta et al., 2024)</a></li><li><a href=#742--60237-exploration-is-harder-than-prediction-cryptographically-separating-reinforcement-learning-from-supervised-learning-noah-golowich-et-al-2024>(7/42 | 60/237) Exploration is Harder than Prediction: Cryptographically Separating Reinforcement Learning from Supervised Learning (Noah Golowich et al., 2024)</a></li><li><a href=#842--61237-approximate-gradient-coding-for-privacy-flexible-federated-learning-with-non-iid-data-okko-makkonen-et-al-2024>(8/42 | 61/237) Approximate Gradient Coding for Privacy-Flexible Federated Learning with Non-IID Data (Okko Makkonen et al., 2024)</a></li><li><a href=#942--62237-knowledge-based-convolutional-neural-network-for-the-simulation-and-prediction-of-two-phase-darcy-flows-zakaria-elabid-et-al-2024>(9/42 | 62/237) Knowledge-Based Convolutional Neural Network for the Simulation and Prediction of Two-Phase Darcy Flows (Zakaria Elabid et al., 2024)</a></li><li><a href=#1042--63237-generalization-bounds-for-message-passing-networks-on-mixture-of-graphons-sohir-maskey-et-al-2024>(10/42 | 63/237) Generalization Bounds for Message Passing Networks on Mixture of Graphons (Sohir Maskey et al., 2024)</a></li><li><a href=#1142--64237-on-the-theoretical-expressive-power-and-the-design-space-of-higher-order-graph-transformers-cai-zhou-et-al-2024>(11/42 | 64/237) On the Theoretical Expressive Power and the Design Space of Higher-Order Graph Transformers (Cai Zhou et al., 2024)</a></li><li><a href=#1242--65237-graph-neural-networks-for-electric-and-hydraulic-data-fusion-to-enhance-short-term-forecasting-of-pumped-storage-hydroelectricity-raffael-theiler-et-al-2024>(12/42 | 65/237) Graph Neural Networks for Electric and Hydraulic Data Fusion to Enhance Short-term Forecasting of Pumped-storage Hydroelectricity (Raffael Theiler et al., 2024)</a></li><li><a href=#1342--66237-silofuse-cross-silo-synthetic-data-generation-with-latent-tabular-diffusion-models-aditya-shankar-et-al-2024>(13/42 | 66/237) SiloFuse: Cross-silo Synthetic Data Generation with Latent Tabular Diffusion Models (Aditya Shankar et al., 2024)</a></li><li><a href=#1442--67237-mitigating-the-impact-of-outlier-channels-for-language-model-quantization-with-activation-regularization-aniruddha-nrusimha-et-al-2024>(14/42 | 67/237) Mitigating the Impact of Outlier Channels for Language Model Quantization with Activation Regularization (Aniruddha Nrusimha et al., 2024)</a></li><li><a href=#1542--68237-self-organized-arrival-system-for-urban-air-mobility-martin-waltz-et-al-2024>(15/42 | 68/237) Self-organized arrival system for urban air mobility (Martin Waltz et al., 2024)</a></li><li><a href=#1642--69237-investigating-the-robustness-of-counterfactual-learning-to-rank-models-a-reproducibility-study-zechun-niu-et-al-2024>(16/42 | 69/237) Investigating the Robustness of Counterfactual Learning to Rank Models: A Reproducibility Study (Zechun Niu et al., 2024)</a></li><li><a href=#1742--70237-improvement-of-performance-in-freezing-of-gait-detection-in-parkinsons-disease-using-transformer-networks-and-a-single-waist-worn-triaxial-accelerometer-luis-sigcha-et-al-2024>(17/42 | 70/237) Improvement of Performance in Freezing of Gait detection in Parkinsons Disease using Transformer networks and a single waist worn triaxial accelerometer (Luis Sigcha et al., 2024)</a></li><li><a href=#1842--71237-goldfish-an-efficient-federated-unlearning-framework-houzhe-wang-et-al-2024>(18/42 | 71/237) Goldfish: An Efficient Federated Unlearning Framework (Houzhe Wang et al., 2024)</a></li><li><a href=#1942--72237-outlier-efficient-hopfield-layers-for-large-transformer-based-models-jerry-yao-chieh-hu-et-al-2024>(19/42 | 72/237) Outlier-Efficient Hopfield Layers for Large Transformer-Based Models (Jerry Yao-Chieh Hu et al., 2024)</a></li><li><a href=#2042--73237-laser-learning-environment-a-new-environment-for-coordination-critical-multi-agent-tasks-yannick-molinghen-et-al-2024>(20/42 | 73/237) Laser Learning Environment: A new environment for coordination-critical multi-agent tasks (Yannick Molinghen et al., 2024)</a></li><li><a href=#2142--74237-about-test-time-training-for-outlier-detection-simon-klüttermann-et-al-2024>(21/42 | 74/237) About Test-time training for outlier detection (Simon Klüttermann et al., 2024)</a></li><li><a href=#2242--75237-learning-from-simplicial-data-based-on-random-walks-and-1d-convolutions-florian-frantzen-et-al-2024>(22/42 | 75/237) Learning From Simplicial Data Based on Random Walks and 1D Convolutions (Florian Frantzen et al., 2024)</a></li><li><a href=#2342--76237-future-proofing-class-incremental-learning-quentin-jodelet-et-al-2024>(23/42 | 76/237) Future-Proofing Class Incremental Learning (Quentin Jodelet et al., 2024)</a></li><li><a href=#2442--77237-layerwise-early-stopping-for-test-time-adaptation-sabyasachi-sahoo-et-al-2024>(24/42 | 77/237) Layerwise Early Stopping for Test Time Adaptation (Sabyasachi Sahoo et al., 2024)</a></li><li><a href=#2542--78237-on-the-efficiency-of-convolutional-neural-networks-andrew-lavin-2024>(25/42 | 78/237) On the Efficiency of Convolutional Neural Networks (Andrew Lavin, 2024)</a></li><li><a href=#2642--79237-distributionally-robust-reinforcement-learning-with-interactive-data-collection-fundamental-hardness-and-near-optimal-algorithm-miao-lu-et-al-2024>(26/42 | 79/237) Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithm (Miao Lu et al., 2024)</a></li><li><a href=#2742--80237-gaussian-smoothed-sliced-probability-divergences-mokhtar-z-alaya-et-al-2024>(27/42 | 80/237) Gaussian-Smoothed Sliced Probability Divergences (Mokhtar Z. Alaya et al., 2024)</a></li><li><a href=#2842--81237-enabling-clean-energy-resilience-with-machine-learning-empowered-underground-hydrogen-storage-alvaro-carbonero-et-al-2024>(28/42 | 81/237) Enabling Clean Energy Resilience with Machine Learning-Empowered Underground Hydrogen Storage (Alvaro Carbonero et al., 2024)</a></li><li><a href=#2942--82237-information-theoretic-generalization-bounds-for-deep-neural-networks-haiyun-he-et-al-2024>(29/42 | 82/237) Information-Theoretic Generalization Bounds for Deep Neural Networks (Haiyun He et al., 2024)</a></li><li><a href=#3042--83237-personalized-federated-learning-for-spatio-temporal-forecasting-a-dual-semantic-alignment-based-contrastive-approach-qingxiang-liu-et-al-2024>(30/42 | 83/237) Personalized Federated Learning for Spatio-Temporal Forecasting: A Dual Semantic Alignment-Based Contrastive Approach (Qingxiang Liu et al., 2024)</a></li><li><a href=#3142--84237-integrating-hyperparameter-search-into-gramml-hernán-ceferino-vázquez-et-al-2024>(31/42 | 84/237) Integrating Hyperparameter Search into GramML (Hernán Ceferino Vázquez et al., 2024)</a></li><li><a href=#3242--85237-an-explainablefair-framework-for-prediction-of-substance-use-disorder-treatment-completion-mary-m-lucas-et-al-2024>(32/42 | 85/237) An ExplainableFair Framework for Prediction of Substance Use Disorder Treatment Completion (Mary M. Lucas et al., 2024)</a></li><li><a href=#3342--86237-localized-distributional-robustness-in-submodular-multi-task-subset-selection-ege-c-kaya-et-al-2024>(33/42 | 86/237) Localized Distributional Robustness in Submodular Multi-Task Subset Selection (Ege C. Kaya et al., 2024)</a></li><li><a href=#3442--87237-explaining-explainability-understanding-concept-activation-vectors-angus-nicolson-et-al-2024>(34/42 | 87/237) Explaining Explainability: Understanding Concept Activation Vectors (Angus Nicolson et al., 2024)</a></li><li><a href=#3542--88237-dida-denoised-imitation-learning-based-on-domain-adaptation-kaichen-huang-et-al-2024>(35/42 | 88/237) DIDA: Denoised Imitation Learning based on Domain Adaptation (Kaichen Huang et al., 2024)</a></li><li><a href=#3642--89237-react-revealing-evolutionary-action-consequence-trajectories-for-interpretable-reinforcement-learning-philipp-altmann-et-al-2024>(36/42 | 89/237) REACT: Revealing Evolutionary Action Consequence Trajectories for Interpretable Reinforcement Learning (Philipp Altmann et al., 2024)</a></li><li><a href=#3742--90237-exploring-lightweight-federated-learning-for-distributed-load-forecasting-abhishek-duttagupta-et-al-2024>(37/42 | 90/237) Exploring Lightweight Federated Learning for Distributed Load Forecasting (Abhishek Duttagupta et al., 2024)</a></li><li><a href=#3842--91237-eigenpruning-tomás-vergara-browne-et-al-2024>(38/42 | 91/237) Eigenpruning (Tomás Vergara-Browne et al., 2024)</a></li><li><a href=#3942--92237-multimodal-hierarchical-multi-task-deep-learning-framework-for-jointly-predicting-and-explaining-alzheimer-disease-progression-sayantan-kumar-et-al-2024>(39/42 | 92/237) Multimodal hierarchical multi-task deep learning framework for jointly predicting and explaining Alzheimer disease progression (Sayantan Kumar et al., 2024)</a></li><li><a href=#4042--93237-bishop-bi-directional-cellular-learning-for-tabular-data-with-generalized-sparse-modern-hopfield-model-chenwei-xu-et-al-2024>(40/42 | 93/237) BiSHop: Bi-Directional Cellular Learning for Tabular Data with Generalized Sparse Modern Hopfield Model (Chenwei Xu et al., 2024)</a></li><li><a href=#4142--94237-leveraging-interpolation-models-and-error-bounds-for-verifiable-scientific-machine-learning-tyler-chang-et-al-2024>(41/42 | 94/237) Leveraging Interpolation Models and Error Bounds for Verifiable Scientific Machine Learning (Tyler Chang et al., 2024)</a></li><li><a href=#4242--95237-mpofi-multichannel-partially-observed-functional-modeling-for-defect-classification-with-imbalanced-dataset-via-deep-metric-learning-yukun-xie-et-al-2024>(42/42 | 95/237) MPOFI: Multichannel Partially Observed Functional Modeling for Defect Classification with Imbalanced Dataset via Deep Metric Learning (Yukun Xie et al., 2024)</a></li></ul></li><li><a href=#eessas-2>eess.AS (2)</a><ul><li><a href=#12--96237-rall-e-robust-codec-language-modeling-with-chain-of-thought-prompting-for-text-to-speech-synthesis-detai-xin-et-al-2024>(1/2 | 96/237) RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis (Detai Xin et al., 2024)</a></li><li><a href=#22--97237-interpreting-end-to-end-deep-learning-models-for-speech-source-localization-using-layer-wise-relevance-propagation-luca-comanducci-et-al-2024>(2/2 | 97/237) Interpreting End-to-End Deep Learning Models for Speech Source Localization Using Layer-wise Relevance Propagation (Luca Comanducci et al., 2024)</a></li></ul></li><li><a href=#cscv-65>cs.CV (65)</a><ul><li><a href=#165--98237-diverse-and-tailored-image-generation-for-zero-shot-multi-label-classification-kaixin-zhang-et-al-2024>(1/65 | 98/237) Diverse and Tailored Image Generation for Zero-shot Multi-label Classification (Kaixin Zhang et al., 2024)</a></li><li><a href=#265--99237-tinyvqa-compact-multimodal-deep-neural-network-for-visual-question-answering-on-resource-constrained-devices-hasib-al-rashid-et-al-2024>(2/65 | 99/237) TinyVQA: Compact Multimodal Deep Neural Network for Visual Question Answering on Resource-Constrained Devices (Hasib-Al Rashid et al., 2024)</a></li><li><a href=#365--100237-comat-aligning-text-to-image-diffusion-model-with-image-to-text-concept-matching-dongzhi-jiang-et-al-2024>(3/65 | 100/237) CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching (Dongzhi Jiang et al., 2024)</a></li><li><a href=#465--101237-devide-faceted-medical-knowledge-for-improved-medical-vision-language-pre-training-haozhe-luo-et-al-2024>(4/65 | 101/237) DeViDe: Faceted medical knowledge for improved medical vision-language pre-training (Haozhe Luo et al., 2024)</a></li><li><a href=#565--102237-performance-of-computer-vision-algorithms-for-fine-grained-classification-using-crowdsourced-insect-images-rita-pucci-et-al-2024>(5/65 | 102/237) Performance of computer vision algorithms for fine-grained classification using crowdsourced insect images (Rita Pucci et al., 2024)</a></li><li><a href=#665--103237-meta-invariance-defense-towards-generalizable-robustness-to-unknown-adversarial-attacks-lei-zhang-et-al-2024>(6/65 | 103/237) Meta Invariance Defense Towards Generalizable Robustness to Unknown Adversarial Attacks (Lei Zhang et al., 2024)</a></li><li><a href=#765--104237-factual-a-novel-framework-for-contrastive-learning-based-robust-sar-image-classification-xu-wang-et-al-2024>(7/65 | 104/237) FACTUAL: A Novel Framework for Contrastive Learning Based Robust SAR Image Classification (Xu Wang et al., 2024)</a></li><li><a href=#865--105237-longvlm-efficient-long-video-understanding-via-large-language-models-yuetian-weng-et-al-2024>(8/65 | 105/237) LongVLM: Efficient Long Video Understanding via Large Language Models (Yuetian Weng et al., 2024)</a></li><li><a href=#965--106237-lcm-lookahead-for-encoder-based-text-to-image-personalization-rinon-gal-et-al-2024>(9/65 | 106/237) LCM-Lookahead for Encoder-based Text-to-Image Personalization (Rinon Gal et al., 2024)</a></li><li><a href=#1065--107237-insectmamba-insect-pest-classification-with-state-space-model-qianning-wang-et-al-2024>(10/65 | 107/237) InsectMamba: Insect Pest Classification with State Space Model (Qianning Wang et al., 2024)</a></li><li><a href=#1165--108237-background-noise-reduction-of-attention-map-for-weakly-supervised-semantic-segmentation-izumi-fujimori-et-al-2024>(11/65 | 108/237) Background Noise Reduction of Attention Map for Weakly Supervised Semantic Segmentation (Izumi Fujimori et al., 2024)</a></li><li><a href=#1265--109237-legrad-an-explainability-method-for-vision-transformers-via-feature-formation-sensitivity-walid-bousselham-et-al-2024>(12/65 | 109/237) LeGrad: An Explainability Method for Vision Transformers via Feature Formation Sensitivity (Walid Bousselham et al., 2024)</a></li><li><a href=#1365--110237-minigpt4-video-advancing-multimodal-llms-for-video-understanding-with-interleaved-visual-textual-tokens-kirolos-ataallah-et-al-2024>(13/65 | 110/237) MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens (Kirolos Ataallah et al., 2024)</a></li><li><a href=#1465--111237-know-your-neighbors-improving-single-view-reconstruction-via-spatial-vision-language-reasoning-rui-li-et-al-2024>(14/65 | 111/237) Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning (Rui Li et al., 2024)</a></li><li><a href=#1565--112237-language-guided-instance-aware-domain-adaptive-panoptic-segmentation-elham-amin-mansour-et-al-2024>(15/65 | 112/237) Language-Guided Instance-Aware Domain-Adaptive Panoptic Segmentation (Elham Amin Mansour et al., 2024)</a></li><li><a href=#1665--113237-scaling-up-video-summarization-pretraining-with-large-language-models-dawit-mureja-argaw-et-al-2024>(16/65 | 113/237) Scaling Up Video Summarization Pretraining with Large Language Models (Dawit Mureja Argaw et al., 2024)</a></li><li><a href=#1765--114237-learning-transferable-negative-prompts-for-out-of-distribution-detection-tianqi-li-et-al-2024>(17/65 | 114/237) Learning Transferable Negative Prompts for Out-of-Distribution Detection (Tianqi Li et al., 2024)</a></li><li><a href=#1865--115237-paris3d-reasoning-based-3d-part-segmentation-using-large-multimodal-model-amrin-kareem-et-al-2024>(18/65 | 115/237) PARIS3D: Reasoning-based 3D Part Segmentation Using Large Multimodal Model (Amrin Kareem et al., 2024)</a></li><li><a href=#1965--116237-semgrasp-semantic-grasp-generation-via-language-aligned-discretization-kailin-li-et-al-2024>(19/65 | 116/237) SemGrasp: Semantic Grasp Generation via Language Aligned Discretization (Kailin Li et al., 2024)</a></li><li><a href=#2065--117237-robust-concept-erasure-using-task-vectors-minh-pham-et-al-2024>(20/65 | 117/237) Robust Concept Erasure Using Task Vectors (Minh Pham et al., 2024)</a></li><li><a href=#2165--118237-hapnet-toward-superior-rgb-thermal-scene-parsing-via-hybrid-asymmetric-and-progressive-heterogeneous-feature-fusion-jiahang-li-et-al-2024>(21/65 | 118/237) HAPNet: Toward Superior RGB-Thermal Scene Parsing via Hybrid, Asymmetric, and Progressive Heterogeneous Feature Fusion (Jiahang Li et al., 2024)</a></li><li><a href=#2265--119237-towards-automated-movie-trailer-generation-dawit-mureja-argaw-et-al-2024>(22/65 | 119/237) Towards Automated Movie Trailer Generation (Dawit Mureja Argaw et al., 2024)</a></li><li><a href=#2365--120237-dreamwalk-style-space-exploration-using-diffusion-guidance-michelle-shu-et-al-2024>(23/65 | 120/237) DreamWalk: Style Space Exploration using Diffusion Guidance (Michelle Shu et al., 2024)</a></li><li><a href=#2465--121237-is-clip-the-main-roadblock-for-fine-grained-open-world-perception-lorenzo-bianchi-et-al-2024>(24/65 | 121/237) Is CLIP the main roadblock for fine-grained open-world perception? (Lorenzo Bianchi et al., 2024)</a></li><li><a href=#2565--122237-sparse-concept-bottleneck-models-gumbel-tricks-in-contrastive-learning-andrei-semenov-et-al-2024>(25/65 | 122/237) Sparse Concept Bottleneck Models: Gumbel Tricks in Contrastive Learning (Andrei Semenov et al., 2024)</a></li><li><a href=#2665--123237-diffbody-human-body-restoration-by-imagining-with-generative-diffusion-prior-yiming-zhang-et-al-2024>(26/65 | 123/237) DiffBody: Human Body Restoration by Imagining with Generative Diffusion Prior (Yiming Zhang et al., 2024)</a></li><li><a href=#2765--124237-mvd-fusion-single-view-3d-via-depth-consistent-multi-view-generation-hanzhe-hu-et-al-2024>(27/65 | 124/237) MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation (Hanzhe Hu et al., 2024)</a></li><li><a href=#2865--125237-opennerf-open-set-3d-neural-scene-segmentation-with-pixel-wise-features-and-rendered-novel-views-francis-engelmann-et-al-2024>(28/65 | 125/237) OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views (Francis Engelmann et al., 2024)</a></li><li><a href=#2965--126237-reference-based-3d-aware-image-editing-with-triplane-bahri-batuhan-bilecen-et-al-2024>(29/65 | 126/237) Reference-Based 3D-Aware Image Editing with Triplane (Bahri Batuhan Bilecen et al., 2024)</a></li><li><a href=#3065--127237-effective-lymph-nodes-detection-in-ct-scans-using-location-debiased-query-selection-and-contrastive-query-representation-in-transformer-qinji-yu-et-al-2024>(30/65 | 127/237) Effective Lymph Nodes Detection in CT Scans Using Location Debiased Query Selection and Contrastive Query Representation in Transformer (Qinji Yu et al., 2024)</a></li><li><a href=#3165--128237-the-more-you-see-in-2d-the-more-you-perceive-in-3d-xinyang-han-et-al-2024>(31/65 | 128/237) The More You See in 2D, the More You Perceive in 3D (Xinyang Han et al., 2024)</a></li><li><a href=#3265--129237-uniav-unified-audio-visual-perception-for-multi-task-video-localization-tiantian-geng-et-al-2024>(32/65 | 129/237) UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization (Tiantian Geng et al., 2024)</a></li><li><a href=#3365--130237-no-zero-shot-without-exponential-data-pretraining-concept-frequency-determines-multimodal-model-performance-vishaal-udandarao-et-al-2024>(33/65 | 130/237) No &lsquo;Zero-Shot&rsquo; Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance (Vishaal Udandarao et al., 2024)</a></li><li><a href=#3465--131237-rafe-generative-radiance-fields-restoration-zhongkai-wu-et-al-2024>(34/65 | 131/237) RaFE: Generative Radiance Fields Restoration (Zhongkai Wu et al., 2024)</a></li><li><a href=#3565--132237-decoupling-static-and-hierarchical-motion-perception-for-referring-video-segmentation-shuting-he-et-al-2024>(35/65 | 132/237) Decoupling Static and Hierarchical Motion Perception for Referring Video Segmentation (Shuting He et al., 2024)</a></li><li><a href=#3665--133237-pointinfinity-resolution-invariant-point-diffusion-models-zixuan-huang-et-al-2024>(36/65 | 133/237) PointInfinity: Resolution-Invariant Point Diffusion Models (Zixuan Huang et al., 2024)</a></li><li><a href=#3765--134237-if-its-not-enough-make-it-so-reducing-authentic-data-demand-in-face-recognition-through-synthetic-faces-andrea-atzori-et-al-2024>(37/65 | 134/237) If It&rsquo;s Not Enough, Make It So: Reducing Authentic Data Demand in Face Recognition through Synthetic Faces (Andrea Atzori et al., 2024)</a></li><li><a href=#3865--135237-sdpose-tokenized-pose-estimation-via-circulation-guide-self-distillation-sichen-chen-et-al-2024>(38/65 | 135/237) SDPose: Tokenized Pose Estimation via Circulation-Guide Self-Distillation (Sichen Chen et al., 2024)</a></li><li><a href=#3965--136237-dq-detr-detr-with-dynamic-query-for-tiny-object-detection-yi-xin-huang-et-al-2024>(39/65 | 136/237) DQ-DETR: DETR with Dynamic Query for Tiny Object Detection (Yi-Xin Huang et al., 2024)</a></li><li><a href=#4065--137237-di-retinex-digital-imaging-retinex-theory-for-low-light-image-enhancement-shangquan-sun-et-al-2024>(40/65 | 137/237) DI-Retinex: Digital-Imaging Retinex Theory for Low-Light Image Enhancement (Shangquan Sun et al., 2024)</a></li><li><a href=#4165--138237-adabm-on-the-fly-adaptive-bit-mapping-for-image-super-resolution-cheeun-hong-et-al-2024>(41/65 | 138/237) AdaBM: On-the-Fly Adaptive Bit Mapping for Image Super-Resolution (Cheeun Hong et al., 2024)</a></li><li><a href=#4265--139237-would-deep-generative-models-amplify-bias-in-future-models-tianwei-chen-et-al-2024>(42/65 | 139/237) Would Deep Generative Models Amplify Bias in Future Models? (Tianwei Chen et al., 2024)</a></li><li><a href=#4365--140237-hdr-imaging-for-dynamic-scenes-with-events-li-xiaopeng-et-al-2024>(43/65 | 140/237) HDR Imaging for Dynamic Scenes with Events (Li Xiaopeng et al., 2024)</a></li><li><a href=#4465--141237-adaptive-discrete-disparity-volume-for-self-supervised-monocular-depth-estimation-jianwei-ren-2024>(44/65 | 141/237) Adaptive Discrete Disparity Volume for Self-supervised Monocular Depth Estimation (Jianwei Ren, 2024)</a></li><li><a href=#4565--142237-biovl-qr-egocentric-biochemical-video-and-language-dataset-using-micro-qr-codes-taichi-nishimura-et-al-2024>(45/65 | 142/237) BioVL-QR: Egocentric Biochemical Video-and-Language Dataset Using Micro QR Codes (Taichi Nishimura et al., 2024)</a></li><li><a href=#4665--143237-test-time-training-for-industrial-anomaly-segmentation-alex-costanzino-et-al-2024>(46/65 | 143/237) Test Time Training for Industrial Anomaly Segmentation (Alex Costanzino et al., 2024)</a></li><li><a href=#4765--144237-aigiqa-20k-a-large-database-for-ai-generated-image-quality-assessment-chunyi-li-et-al-2024>(47/65 | 144/237) AIGIQA-20K: A Large Database for AI-Generated Image Quality Assessment (Chunyi Li et al., 2024)</a></li><li><a href=#4865--145237-two-tricks-to-improve-unsupervised-segmentation-learning-alp-eren-sari-et-al-2024>(48/65 | 145/237) Two Tricks to Improve Unsupervised Segmentation Learning (Alp Eren Sari et al., 2024)</a></li><li><a href=#4965--146237-monocd-monocular-3d-object-detection-with-complementary-depths-longfei-yan-et-al-2024>(49/65 | 146/237) MonoCD: Monocular 3D Object Detection with Complementary Depths (Longfei Yan et al., 2024)</a></li><li><a href=#5065--147237-handdiff-3d-hand-pose-estimation-with-diffusion-on-image-point-cloud-wencan-cheng-et-al-2024>(50/65 | 147/237) HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud (Wencan Cheng et al., 2024)</a></li><li><a href=#5165--148237-sleepvst-sleep-staging-from-near-infrared-video-signals-using-pre-trained-transformers-jonathan-f-carter-et-al-2024>(51/65 | 148/237) SleepVST: Sleep Staging from Near-Infrared Video Signals using Pre-Trained Transformers (Jonathan F. Carter et al., 2024)</a></li><li><a href=#5265--149237-quantifying-uncertainty-in-motion-prediction-with-variational-bayesian-mixture-juanwu-lu-et-al-2024>(52/65 | 149/237) Quantifying Uncertainty in Motion Prediction with Variational Bayesian Mixture (Juanwu Lu et al., 2024)</a></li><li><a href=#5365--150237-flattening-the-parent-bias-hierarchical-semantic-segmentation-in-the-poincaré-ball-simon-weber-et-al-2024>(53/65 | 150/237) Flattening the Parent Bias: Hierarchical Semantic Segmentation in the Poincaré Ball (Simon Weber et al., 2024)</a></li><li><a href=#5465--151237-sc4d-sparse-controlled-video-to-4d-generation-and-motion-transfer-zijie-wu-et-al-2024>(54/65 | 151/237) SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer (Zijie Wu et al., 2024)</a></li><li><a href=#5565--152237-ow-viscap-open-world-video-instance-segmentation-and-captioning-anwesa-choudhuri-et-al-2024>(55/65 | 152/237) OW-VISCap: Open-World Video Instance Segmentation and Captioning (Anwesa Choudhuri et al., 2024)</a></li><li><a href=#5665--153237-adaglimpse-active-visual-exploration-with-arbitrary-glimpse-position-and-scale-adam-pardyl-et-al-2024>(56/65 | 153/237) AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position and Scale (Adam Pardyl et al., 2024)</a></li><li><a href=#5765--154237-nmf-based-analysis-of-mobile-eye-tracking-data-daniel-klötzl-et-al-2024>(57/65 | 154/237) NMF-Based Analysis of Mobile Eye-Tracking Data (Daniel Klötzl et al., 2024)</a></li><li><a href=#5865--155237-design-and-development-of-a-framework-for-stroke-based-handwritten-gujarati-font-generation-preeti-p-bhatt-et-al-2024>(58/65 | 155/237) Design and Development of a Framework For Stroke-Based Handwritten Gujarati Font Generation (Preeti P. Bhatt et al., 2024)</a></li><li><a href=#5965--156237-multi-positive-contrastive-learning-with-pose-consistent-generated-images-sho-inayoshi-et-al-2024>(59/65 | 156/237) Multi Positive Contrastive Learning with Pose-Consistent Generated Images (Sho Inayoshi et al., 2024)</a></li><li><a href=#6065--157237-iseg-interactive-3d-segmentation-via-interactive-attention-itai-lang-et-al-2024>(60/65 | 157/237) iSeg: Interactive 3D Segmentation via Interactive Attention (Itai Lang et al., 2024)</a></li><li><a href=#6165--158237-corp-a-multi-modal-dataset-for-campus-oriented-roadside-perception-tasks-beibei-wang-et-al-2024>(61/65 | 158/237) CORP: A Multi-Modal Dataset for Campus-Oriented Roadside Perception Tasks (Beibei Wang et al., 2024)</a></li><li><a href=#6265--159237-como-compact-mapping-and-odometry-eric-dexheimer-et-al-2024>(62/65 | 159/237) COMO: Compact Mapping and Odometry (Eric Dexheimer et al., 2024)</a></li><li><a href=#6365--160237-you-only-scan-once-a-dynamic-scene-reconstruction-pipeline-for-6-dof-robotic-grasping-of-novel-objects-lei-zhou-et-al-2024>(63/65 | 160/237) You Only Scan Once: A Dynamic Scene Reconstruction Pipeline for 6-DoF Robotic Grasping of Novel Objects (Lei Zhou et al., 2024)</a></li><li><a href=#6465--161237-towards-more-realistic-human-motion-prediction-with-attention-to-motion-coordination-pengxiang-ding-et-al-2024>(64/65 | 161/237) Towards more realistic human motion prediction with attention to motion coordination (Pengxiang Ding et al., 2024)</a></li><li><a href=#6565--162237-sp2ot-semantic-regularized-progressive-partial-optimal-transport-for-imbalanced-clustering-chuyu-zhang-et-al-2024>(65/65 | 162/237) SP$^2$OT: Semantic-Regularized Progressive Partial Optimal Transport for Imbalanced Clustering (Chuyu Zhang et al., 2024)</a></li></ul></li><li><a href=#mathoc-2>math.OC (2)</a><ul><li><a href=#12--163237-capabilities-of-large-language-models-in-control-engineering-a-benchmark-study-on-gpt-4-claude-3-opus-and-gemini-10-ultra-darioush-kevian-et-al-2024>(1/2 | 163/237) Capabilities of Large Language Models in Control Engineering: A Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra (Darioush Kevian et al., 2024)</a></li><li><a href=#22--164237-elementary-analysis-of-policy-gradient-methods-jiacai-liu-et-al-2024>(2/2 | 164/237) Elementary Analysis of Policy Gradient Methods (Jiacai Liu et al., 2024)</a></li></ul></li><li><a href=#csse-3>cs.SE (3)</a><ul><li><a href=#13--165237-codeeditorbench-evaluating-code-editing-capability-of-large-language-models-jiawei-guo-et-al-2024>(1/3 | 165/237) CodeEditorBench: Evaluating Code Editing Capability of Large Language Models (Jiawei Guo et al., 2024)</a></li><li><a href=#23--166237-multi-modal-learning-for-webassembly-reverse-engineering-hanxian-huang-et-al-2024>(2/3 | 166/237) Multi-modal Learning for WebAssembly Reverse Engineering (Hanxian Huang et al., 2024)</a></li><li><a href=#33--167237-towards-standards-compliant-assistive-technology-product-specifications-via-llms-chetan-arora-et-al-2024>(3/3 | 167/237) Towards Standards-Compliant Assistive Technology Product Specifications via LLMs (Chetan Arora et al., 2024)</a></li></ul></li><li><a href=#eessiv-4>eess.IV (4)</a><ul><li><a href=#14--168237-changemamba-remote-sensing-change-detection-with-spatio-temporal-state-space-model-hongruixuan-chen-et-al-2024>(1/4 | 168/237) ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State Space Model (Hongruixuan Chen et al., 2024)</a></li><li><a href=#24--169237-mitigating-analytical-variability-in-fmri-results-with-style-transfer-elodie-germani-et-al-2024>(2/4 | 169/237) Mitigating analytical variability in fMRI results with style transfer (Elodie Germani et al., 2024)</a></li><li><a href=#34--170237-bi-level-guided-diffusion-models-for-zero-shot-medical-imaging-inverse-problems-hossein-askari-et-al-2024>(3/4 | 170/237) Bi-level Guided Diffusion Models for Zero-Shot Medical Imaging Inverse Problems (Hossein Askari et al., 2024)</a></li><li><a href=#44--171237-segmentation-guided-knee-radiograph-generation-using-conditional-diffusion-models-siyuan-mei-et-al-2024>(4/4 | 171/237) Segmentation-Guided Knee Radiograph Generation using Conditional Diffusion Models (Siyuan Mei et al., 2024)</a></li></ul></li><li><a href=#cscr-7>cs.CR (7)</a><ul><li><a href=#17--172237-an-investigation-into-misuse-of-java-security-apis-by-large-language-models-zahra-mousavi-et-al-2024>(1/7 | 172/237) An Investigation into Misuse of Java Security APIs by Large Language Models (Zahra Mousavi et al., 2024)</a></li><li><a href=#27--173237-learn-what-you-want-to-unlearn-unlearning-inversion-attacks-against-machine-unlearning-hongsheng-hu-et-al-2024>(2/7 | 173/237) Learn What You Want to Unlearn: Unlearning Inversion Attacks against Machine Unlearning (Hongsheng Hu et al., 2024)</a></li><li><a href=#37--174237-ltrdetector-exploring-long-term-relationship-for-advanced-persistent-threats-detection-xiaoxiao-liu-et-al-2024>(3/7 | 174/237) LTRDetector: Exploring Long-Term Relationship for Advanced Persistent Threats Detection (Xiaoxiao Liu et al., 2024)</a></li><li><a href=#47--175237-r5detect-detecting-control-flow-attacks-from-standard-risc-v-enclaves-davide-bove-et-al-2024>(4/7 | 175/237) R5Detect: Detecting Control-Flow Attacks from Standard RISC-V Enclaves (Davide Bove et al., 2024)</a></li><li><a href=#57--176237-heckler-breaking-confidential-vms-with-malicious-interrupts-benedict-schlüter-et-al-2024>(5/7 | 176/237) Heckler: Breaking Confidential VMs with Malicious Interrupts (Benedict Schlüter et al., 2024)</a></li><li><a href=#67--177237-a-deep-reinforcement-learning-approach-for-security-aware-service-acquisition-in-iot-marco-arazzi-et-al-2024>(6/7 | 177/237) A Deep Reinforcement Learning Approach for Security-Aware Service Acquisition in IoT (Marco Arazzi et al., 2024)</a></li><li><a href=#77--178237-accurate-low-degree-polynomial-approximation-of-non-polynomial-operators-for-fast-private-inference-in-homomorphic-encryption-jianming-tong-et-al-2024>(7/7 | 178/237) Accurate Low-Degree Polynomial Approximation of Non-polynomial Operators for Fast Private Inference in Homomorphic Encryption (Jianming Tong et al., 2024)</a></li></ul></li><li><a href=#csro-16>cs.RO (16)</a><ul><li><a href=#116--179237-embodied-ai-with-two-arms-zero-shot-learning-safety-and-modularity-jake-varley-et-al-2024>(1/16 | 179/237) Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity (Jake Varley et al., 2024)</a></li><li><a href=#216--180237-delta-decomposed-efficient-long-term-robot-task-planning-using-large-language-models-yuchen-liu-et-al-2024>(2/16 | 180/237) DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large Language Models (Yuchen Liu et al., 2024)</a></li><li><a href=#316--181237-a-high-fidelity-simulation-framework-for-grasping-stability-analysis-in-human-casualty-manipulation-qianwen-zhao-et-al-2024>(3/16 | 181/237) A High-Fidelity Simulation Framework for Grasping Stability Analysis in Human Casualty Manipulation (Qianwen Zhao et al., 2024)</a></li><li><a href=#416--182237-legible-and-proactive-robot-planning-for-prosocial-human-robot-interactions-jasper-geldenbott-et-al-2024>(4/16 | 182/237) Legible and Proactive Robot Planning for Prosocial Human-Robot Interactions (Jasper Geldenbott et al., 2024)</a></li><li><a href=#516--183237-juicer-data-efficient-imitation-learning-for-robotic-assembly-lars-ankile-et-al-2024>(5/16 | 183/237) JUICER: Data-Efficient Imitation Learning for Robotic Assembly (Lars Ankile et al., 2024)</a></li><li><a href=#616--184237-anticipate--collab-data-driven-task-anticipation-and-knowledge-driven-planning-for-human-robot-collaboration-shivam-singh-et-al-2024>(6/16 | 184/237) Anticipate & Collab: Data-driven Task Anticipation and Knowledge-driven Planning for Human-robot Collaboration (Shivam Singh et al., 2024)</a></li><li><a href=#716--185237-scaling-population-based-reinforcement-learning-with-gpu-accelerated-simulation-asad-ali-shahid-et-al-2024>(7/16 | 185/237) Scaling Population-Based Reinforcement Learning with GPU Accelerated Simulation (Asad Ali Shahid et al., 2024)</a></li><li><a href=#816--186237-integrating-large-language-models-with-multimodal-virtual-reality-interfaces-to-support-collaborative-human-robot-construction-work-somin-park-et-al-2024>(8/16 | 186/237) Integrating Large Language Models with Multimodal Virtual Reality Interfaces to Support Collaborative Human-Robot Construction Work (Somin Park et al., 2024)</a></li><li><a href=#916--187237-a-bimanual-teleoperation-framework-for-light-duty-underwater-vehicle-manipulator-systems-justin-sitler-et-al-2024>(9/16 | 187/237) A Bimanual Teleoperation Framework for Light Duty Underwater Vehicle-Manipulator Systems (Justin Sitler et al., 2024)</a></li><li><a href=#1016--188237-simultaneous-state-estimation-and-contact-detection-for-legged-robots-by-multiple-model-kalman-filtering-marcel-menner-et-al-2024>(10/16 | 188/237) Simultaneous State Estimation and Contact Detection for Legged Robots by Multiple-Model Kalman Filtering (Marcel Menner et al., 2024)</a></li><li><a href=#1116--189237-gmmcalib-extrinsic-calibration-of-lidar-sensors-using-gmm-based-joint-registration-ilir-tahiraj-et-al-2024>(11/16 | 189/237) GMMCalib: Extrinsic Calibration of LiDAR Sensors using GMM-based Joint Registration (Ilir Tahiraj et al., 2024)</a></li><li><a href=#1216--190237-radium-predicting-and-repairing-end-to-end-robot-failures-using-gradient-accelerated-sampling-charles-dawson-et-al-2024>(12/16 | 190/237) RADIUM: Predicting and Repairing End-to-End Robot Failures using Gradient-Accelerated Sampling (Charles Dawson et al., 2024)</a></li><li><a href=#1316--191237-traversability-aware-adaptive-optimization-for-path-planning-and-control-in-mountainous-terrain-se-wook-yoo-et-al-2024>(13/16 | 191/237) Traversability-aware Adaptive Optimization for Path Planning and Control in Mountainous Terrain (Se-Wook Yoo et al., 2024)</a></li><li><a href=#1416--192237-factored-task-and-motion-planning-with-combined-optimization-sampling-and-learning-joaquim-ortiz-haro-2024>(14/16 | 192/237) Factored Task and Motion Planning with Combined Optimization, Sampling and Learning (Joaquim Ortiz-Haro, 2024)</a></li><li><a href=#1516--193237-fast-k-connectivity-restoration-in-multi-robot-systems-for-robust-communication-maintenance-md-ishat-e-rabban-et-al-2024>(15/16 | 193/237) Fast k-connectivity Restoration in Multi-Robot Systems for Robust Communication Maintenance (Md Ishat-E-Rabban et al., 2024)</a></li><li><a href=#1616--194237-embodied-neuromorphic-artificial-intelligence-for-robotics-perspectives-challenges-and-research-development-stack-rachmad-vidya-wicaksana-putra-et-al-2024>(16/16 | 194/237) Embodied Neuromorphic Artificial Intelligence for Robotics: Perspectives, Challenges, and Research Development Stack (Rachmad Vidya Wicaksana Putra et al., 2024)</a></li></ul></li><li><a href=#csai-3>cs.AI (3)</a><ul><li><a href=#13--195237-benchmarking-chatgpt-on-algorithmic-reasoning-sean-mcleish-et-al-2024>(1/3 | 195/237) Benchmarking ChatGPT on Algorithmic Reasoning (Sean McLeish et al., 2024)</a></li><li><a href=#23--196237-ai-and-the-problem-of-knowledge-collapse-andrew-j-peterson-2024>(2/3 | 196/237) AI and the Problem of Knowledge Collapse (Andrew J. Peterson, 2024)</a></li><li><a href=#33--197237-comprehensible-artificial-intelligence-on-knowledge-graphs-a-survey-simon-schramm-et-al-2024>(3/3 | 197/237) Comprehensible Artificial Intelligence on Knowledge Graphs: A survey (Simon Schramm et al., 2024)</a></li></ul></li><li><a href=#eesssp-2>eess.SP (2)</a><ul><li><a href=#12--198237-alzheimers-disease-detection-in-psg-signals-lorena-gallego-viñarás-et-al-2024>(1/2 | 198/237) Alzheimer&rsquo;s disease detection in PSG signals (Lorena Gallego-Viñarás et al., 2024)</a></li><li><a href=#22--199237-decentralized-learning-strategies-for-estimation-error-minimization-with-graph-neural-networks-xingran-chen-et-al-2024>(2/2 | 199/237) Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks (Xingran Chen et al., 2024)</a></li></ul></li><li><a href=#statml-2>stat.ML (2)</a><ul><li><a href=#12--200237-transformerlsr-attentive-joint-model-of-longitudinal-data-survival-and-recurrent-events-with-concurrent-latent-structure-zhiyue-zhang-et-al-2024>(1/2 | 200/237) TransformerLSR: Attentive Joint Model of Longitudinal Data, Survival, and Recurrent Events with Concurrent Latent Structure (Zhiyue Zhang et al., 2024)</a></li><li><a href=#22--201237-countarfactuals----generating-plausible-model-agnostic-counterfactual-explanations-with-adversarial-random-forests-susanne-dandl-et-al-2024>(2/2 | 201/237) CountARFactuals &ndash; Generating plausible model-agnostic counterfactual explanations with adversarial random forests (Susanne Dandl et al., 2024)</a></li></ul></li><li><a href=#cshc-4>cs.HC (4)</a><ul><li><a href=#14--202237-fakes-of-varying-shades-how-warning-affects-human-perception-and-engagement-regarding-llm-hallucinations-mahjabin-nahar-et-al-2024>(1/4 | 202/237) Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations (Mahjabin Nahar et al., 2024)</a></li><li><a href=#24--203237-biodegradable-interactive-materials-zhihan-zhang-et-al-2024>(2/4 | 203/237) Biodegradable Interactive Materials (Zhihan Zhang et al., 2024)</a></li><li><a href=#34--204237-exploring-emotions-in-multi-componential-space-using-interactive-vr-games-rukshani-somarathna-et-al-2024>(3/4 | 204/237) Exploring Emotions in Multi-componential Space using Interactive VR Games (Rukshani Somarathna et al., 2024)</a></li><li><a href=#44--205237-nlp4gov-a-comprehensive-library-for-computational-policy-analysis-mahasweta-chakraborti-et-al-2024>(4/4 | 205/237) NLP4Gov: A Comprehensive Library for Computational Policy Analysis (Mahasweta Chakraborti et al., 2024)</a></li></ul></li><li><a href=#quant-ph-4>quant-ph (4)</a><ul><li><a href=#14--206237-direct-interpolative-construction-of-the-discrete-fourier-transform-as-a-matrix-product-operator-jielun-chen-et-al-2024>(1/4 | 206/237) Direct interpolative construction of the discrete Fourier transform as a matrix product operator (Jielun Chen et al., 2024)</a></li><li><a href=#24--207237-faster-and-shorter-synthesis-of-hamiltonian-simulation-circuits-timothée-goubault-de-brugière-et-al-2024>(2/4 | 207/237) Faster and shorter synthesis of Hamiltonian simulation circuits (Timothée Goubault de Brugière et al., 2024)</a></li><li><a href=#34--208237-the-power-of-a-single-haar-random-state-constructing-and-separating-quantum-pseudorandomness-boyang-chen-et-al-2024>(3/4 | 208/237) The power of a single Haar random state: constructing and separating quantum pseudorandomness (Boyang Chen et al., 2024)</a></li><li><a href=#44--209237-circuit-knitting-faces-exponential-sampling-overhead-scaling-bounded-by-entanglement-cost-mingrui-jing-et-al-2024>(4/4 | 209/237) Circuit Knitting Faces Exponential Sampling Overhead Scaling Bounded by Entanglement Cost (Mingrui Jing et al., 2024)</a></li></ul></li><li><a href=#cslo-3>cs.LO (3)</a><ul><li><a href=#13--210237-a-reinforcement-learning-based-reset-policy-for-cdcl-sat-solvers-chunxiao-li-et-al-2024>(1/3 | 210/237) A Reinforcement Learning based Reset Policy for CDCL SAT Solvers (Chunxiao Li et al., 2024)</a></li><li><a href=#23--211237-parametricity-via-cohesion-c-b-aberlé-2024>(2/3 | 211/237) Parametricity via Cohesion (C. B. Aberlé, 2024)</a></li><li><a href=#33--212237-model-checking-recursive-probabilistic-programs-with-conditioning-francesco-pontiggia-et-al-2024>(3/3 | 212/237) Model Checking Recursive Probabilistic Programs with Conditioning (Francesco Pontiggia et al., 2024)</a></li></ul></li><li><a href=#csdc-3>cs.DC (3)</a><ul><li><a href=#13--213237-wilkins-hpc-in-situ-workflows-made-easy-orcun-yildiz-et-al-2024>(1/3 | 213/237) Wilkins: HPC In Situ Workflows Made Easy (Orcun Yildiz et al., 2024)</a></li><li><a href=#23--214237-use-cases-for-high-performance-research-desktops-robert-henschel-et-al-2024>(2/3 | 214/237) Use Cases for High Performance Research Desktops (Robert Henschel et al., 2024)</a></li><li><a href=#33--215237-inspirit-optimizing-heterogeneous-task-scheduling-through-adaptive-priority-in-task-based-runtime-systems-yiqing-wang-et-al-2024>(3/3 | 215/237) INSPIRIT: Optimizing Heterogeneous Task Scheduling through Adaptive Priority in Task-based Runtime Systems (Yiqing Wang et al., 2024)</a></li></ul></li><li><a href=#statme-1>stat.ME (1)</a><ul><li><a href=#11--216237-multi-task-learning-via-robust-regularized-clustering-with-non-convex-group-penalties-akira-okazaki-et-al-2024>(1/1 | 216/237) Multi-task learning via robust regularized clustering with non-convex group penalties (Akira Okazaki et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--217237-towards-trustworthy-automated-program-verifiers-formally-validating-translations-into-an-intermediate-verification-language-extended-version-gaurav-parthasarathy-et-al-2024>(1/1 | 217/237) Towards Trustworthy Automated Program Verifiers: Formally Validating Translations into an Intermediate Verification Language (extended version) (Gaurav Parthasarathy et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--218237-integrating-generative-ai-into-financial-market-prediction-for-improved-decision-making-chang-che-et-al-2024>(1/1 | 218/237) Integrating Generative AI into Financial Market Prediction for Improved Decision Making (Chang Che et al., 2024)</a></li></ul></li><li><a href=#csit-4>cs.IT (4)</a><ul><li><a href=#14--219237-movable-antennas-assisted-secure-transmission-without-eavesdroppers-instantaneous-csi-guojie-hu-et-al-2024>(1/4 | 219/237) Movable Antennas-Assisted Secure Transmission Without Eavesdroppers&rsquo; Instantaneous CSI (Guojie Hu et al., 2024)</a></li><li><a href=#24--220237-optimization-of-resources-for-digital-radio-transmission-over-iboc-fm-through-max-min-fairness-mónica-rico-martínez-et-al-2024>(2/4 | 220/237) Optimization of resources for digital radio transmission over IBOC FM through max-min fairness (Mónica Rico Martínez et al., 2024)</a></li><li><a href=#34--221237-design-and-optimization-of-cooperative-sensing-with-limited-backhaul-capacity-wenrui-li-et-al-2024>(3/4 | 221/237) Design and Optimization of Cooperative Sensing With Limited Backhaul Capacity (Wenrui Li et al., 2024)</a></li><li><a href=#44--222237-mmse-channel-estimation-in-large-scale-mimo-improved-robustness-with-reduced-complexity-giacomo-bacci-et-al-2024>(4/4 | 222/237) MMSE Channel Estimation in Large-Scale MIMO: Improved Robustness with Reduced Complexity (Giacomo Bacci et al., 2024)</a></li></ul></li><li><a href=#eesssy-2>eess.SY (2)</a><ul><li><a href=#12--223237-implementation-of-complex-valued-sliding-mode-controllers-in-three-phase-power-converters-arnau-dòria-cerezo-et-al-2024>(1/2 | 223/237) Implementation of complex-valued sliding mode controllers in three-phase power converters (Arnau Dòria-Cerezo et al., 2024)</a></li><li><a href=#22--224237-a-rolling-horizon-restoration-framework-for-post-disaster-restoration-of-electrical-distribution-networks-ran-wei-et-al-2024>(2/2 | 224/237) A Rolling Horizon Restoration Framework for Post-disaster Restoration of Electrical Distribution Networks (Ran Wei et al., 2024)</a></li></ul></li><li><a href=#econem-1>econ.EM (1)</a><ul><li><a href=#11--225237-early-warning-systems-for-financial-markets-of-emerging-economies-artem-kraevskiy-et-al-2024>(1/1 | 225/237) Early warning systems for financial markets of emerging economies (Artem Kraevskiy et al., 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--226237-a-classification-of-well-behaved-graph-clustering-schemes-vilhelm-agdur-2024>(1/1 | 226/237) A classification of well-behaved graph clustering schemes (Vilhelm Agdur, 2024)</a></li></ul></li><li><a href=#csne-2>cs.NE (2)</a><ul><li><a href=#12--227237-spikeexplorer-hardware-oriented-design-space-exploration-for-spiking-neural-networks-on-fpga-dario-padovano-et-al-2024>(1/2 | 227/237) SpikeExplorer: hardware-oriented Design Space Exploration for Spiking Neural Networks on FPGA (Dario Padovano et al., 2024)</a></li><li><a href=#22--228237-benchmarking-parameter-control-methods-in-differential-evolution-for-mixed-integer-black-box-optimization-ryoji-tanabe-2024>(2/2 | 228/237) Benchmarking Parameter Control Methods in Differential Evolution for Mixed-Integer Black-Box Optimization (Ryoji Tanabe, 2024)</a></li></ul></li><li><a href=#physicsao-ph-1>physics.ao-ph (1)</a><ul><li><a href=#11--229237-site-specific-deterministic-temperature-and-humidity-forecasts-with-explainable-and-reliable-machine-learning-mengmeng-han-et-al-2024>(1/1 | 229/237) Site-specific Deterministic Temperature and Humidity Forecasts with Explainable and Reliable Machine Learning (MengMeng Han et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--230237-foundation-model-for-advancing-healthcare-challenges-opportunities-and-future-directions-yuting-he-et-al-2024>(1/1 | 230/237) Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions (Yuting He et al., 2024)</a></li></ul></li><li><a href=#mathna-1>math.NA (1)</a><ul><li><a href=#11--231237-computational-analysis-of-a-contraction-rheometer-for-the-grade-two-fluid-model-sara-pollock-et-al-2024>(1/1 | 231/237) Computational analysis of a contraction rheometer for the grade-two fluid model (Sara Pollock et al., 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--232237-additive-approximation-algorithm-for-geodesic-centers-in-δ-hyperbolic-graphs-dibyayan-chakraborty-et-al-2024>(1/2 | 232/237) Additive approximation algorithm for geodesic centers in $δ$-hyperbolic graphs (Dibyayan Chakraborty et al., 2024)</a></li><li><a href=#22--233237-lower-bounds-for-graph-reconstruction-with-maximal-independent-set-queries-lukas-michel-et-al-2024>(2/2 | 233/237) Lower bounds for graph reconstruction with maximal independent set queries (Lukas Michel et al., 2024)</a></li></ul></li><li><a href=#cscg-2>cs.CG (2)</a><ul><li><a href=#12--234237-the-maximum-clique-problem-in-a-disk-graph-made-easy-j-mark-keil-et-al-2024>(1/2 | 234/237) The Maximum Clique Problem in a Disk Graph Made Easy (J. Mark Keil et al., 2024)</a></li><li><a href=#22--235237-improved-total-domination-and-total-roman-domination-in-unit-disk-graphs-sasmita-rout-et-al-2024>(2/2 | 235/237) Improved Total Domination and Total Roman Domination in Unit Disk Graphs (Sasmita Rout et al., 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--236237-reservoir-sampling-over-joins-binyang-dai-et-al-2024>(1/1 | 236/237) Reservoir Sampling over Joins (Binyang Dai et al., 2024)</a></li></ul></li><li><a href=#cset-1>cs.ET (1)</a><ul><li><a href=#11--237237-tegra----scaling-up-terascale-graph-processing-with-disaggregated-computing-william-shaddix-et-al-2024>(1/1 | 237/237) TEGRA &ndash; Scaling Up Terascale Graph Processing with Disaggregated Computing (William Shaddix et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>