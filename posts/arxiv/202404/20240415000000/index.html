<!doctype html><html><head><title>arXiv @ 2024.04.15</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202404/20240415000000/"><meta property="og:site_name" content="Akitenkrad's Blog"><meta property="og:title" content="arXiv @ 2024.04.15"><meta property="og:description" content="Primary Categories cs.AR (1) cs.CC (1) cs.CL (14) cs.CR (5) cs.CV (26) cs.CY (3) cs.DB (1) cs.DS (1) cs.GR (1) cs.GT (1) cs.IR (5) cs.IT (1) cs.LG (10) cs.MA (1) cs.NI (2) cs.PL (1) cs.RO (5) cs.SD (1) cs.SE (3) cs.SI (2) eess.IV (1) eess.SP (1) eess.SY (6) math.NA (2) q-fin.PM (1) Keywords keyword cs.CL cs.CV cs.LG Active Learning 1 Adversarial Learning 1 Anomaly Detection 1 Autoencoder 2 Benchmarking 2 8 Black Box 2 3 ChatGPT 1 1 Clustering 1 2 Constrained Learning 1 Contrastive Learning 1 1 Convolution 2 Convolutional Neural Network 1 Coreference Resolution 1 Data Augmentation 1 2 Deep Neural Network 4 1 Dialogue System 1 Diffusion Model 2 3 1 Distribution Shift 2 Domain Adaptation 1 1 Emotion Recognition 1 1 Fairness 1 Federated Learning 1 Few-shot 4 Few-shot Learning 2 Fine-tuning 5 3 2 Foundation Model 1 Graph 1 Graph Attention Networks 1 In-context Learning 3 Information Retrieval 1 Knowledge Distillation 1 1 Knowledge Graph 1 Knowledge Transfer 1 LSTM 1 Large Language Model 10 1 4 Low-Resource 3 Markov Decision Process 1 Masked Language Model 1 Meta Learning 1 Multi-modal 4 11 2 Natural Language Generation 1 Natural Language Understanding 1 Neural Machine Translation 1 Object Detection 2 Prompt 3 2 Question Answering 2 Reasoning 1 Recurrent Neural Network 1 Representation Learning 1 Self-supervised Learning 4 1 Sentiment Analysis 1 Simulation 1 Simulator 1 Stemming 1 Summarization 1 Supervised Learning 2 3 Tensor Decomposition 1 Text Generation 1 Transfer Learning 1 Transformer 1 1 Unsupervised Learning 1 2 Vision Transformer 2 Vision-and-Language 1 3 Zero-shot 1 cs."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-15T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-15T00:00:00+00:00"><meta property="article:tag" content="ArXiv"><meta property="article:tag" content="Published:2024"><meta name=description content="arXiv @ 2024.04.15"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1,dnt;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE")}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240408000000/ title="arXiv @ 2024.04.08">arXiv @ 2024.04.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/ title="arXiv @ 2024.04.09">arXiv @ 2024.04.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240410000000/ title="arXiv @ 2024.04.10">arXiv @ 2024.04.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/ title="arXiv @ 2024.04.11">arXiv @ 2024.04.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/ title="arXiv @ 2024.04.12">arXiv @ 2024.04.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/ title="arXiv @ 2024.04.13">arXiv @ 2024.04.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240414000000/ title="arXiv @ 2024.04.14">arXiv @ 2024.04.14</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/ title="arXiv @ 2024.04.15">arXiv @ 2024.04.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/ title="arXiv @ 2024.04.16">arXiv @ 2024.04.16</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202404/20240415000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Monday, Apr 15, 2024</p></div><div class=title><h1>arXiv @ 2024.04.15</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#cscc-1>cs.CC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#cscl-14>cs.CL (14)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#cscr-5>cs.CR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#cscv-26>cs.CV (26)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#cscy-3>cs.CY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#csds-1>cs.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#csir-5>cs.IR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#csit-1>cs.IT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#cslg-10>cs.LG (10)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#csni-2>cs.NI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#csro-5>cs.RO (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#cssd-1>cs.SD (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#csse-3>cs.SE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#cssi-2>cs.SI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#eessiv-1>eess.IV (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#eesssy-6>eess.SY (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/#q-finpm-1>q-fin.PM (1)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td>1</td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>1</td></tr><tr><td>Anomaly Detection</td><td></td><td>1</td><td></td></tr><tr><td>Autoencoder</td><td></td><td>2</td><td></td></tr><tr><td>Benchmarking</td><td>2</td><td>8</td><td></td></tr><tr><td>Black Box</td><td></td><td>2</td><td>3</td></tr><tr><td>ChatGPT</td><td>1</td><td></td><td>1</td></tr><tr><td>Clustering</td><td></td><td>1</td><td>2</td></tr><tr><td>Constrained Learning</td><td></td><td>1</td><td></td></tr><tr><td>Contrastive Learning</td><td>1</td><td></td><td>1</td></tr><tr><td>Convolution</td><td></td><td>2</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>1</td><td></td></tr><tr><td>Coreference Resolution</td><td>1</td><td></td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>1</td><td>2</td></tr><tr><td>Deep Neural Network</td><td></td><td>4</td><td>1</td></tr><tr><td>Dialogue System</td><td>1</td><td></td><td></td></tr><tr><td>Diffusion Model</td><td>2</td><td>3</td><td>1</td></tr><tr><td>Distribution Shift</td><td></td><td>2</td><td></td></tr><tr><td>Domain Adaptation</td><td>1</td><td>1</td><td></td></tr><tr><td>Emotion Recognition</td><td></td><td>1</td><td>1</td></tr><tr><td>Fairness</td><td></td><td></td><td>1</td></tr><tr><td>Federated Learning</td><td></td><td></td><td>1</td></tr><tr><td>Few-shot</td><td></td><td>4</td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td>2</td><td></td></tr><tr><td>Fine-tuning</td><td>5</td><td>3</td><td>2</td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td></td></tr><tr><td>Graph</td><td>1</td><td></td><td></td></tr><tr><td>Graph Attention Networks</td><td></td><td>1</td><td></td></tr><tr><td>In-context Learning</td><td>3</td><td></td><td></td></tr><tr><td>Information Retrieval</td><td>1</td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>1</td><td>1</td></tr><tr><td>Knowledge Graph</td><td>1</td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td></td><td>1</td><td></td></tr><tr><td>LSTM</td><td>1</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>10</td><td>1</td><td>4</td></tr><tr><td>Low-Resource</td><td>3</td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td>1</td><td></td><td></td></tr><tr><td>Masked Language Model</td><td></td><td>1</td><td></td></tr><tr><td>Meta Learning</td><td>1</td><td></td><td></td></tr><tr><td>Multi-modal</td><td>4</td><td>11</td><td>2</td></tr><tr><td>Natural Language Generation</td><td>1</td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td>1</td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>1</td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>2</td><td></td></tr><tr><td>Prompt</td><td>3</td><td>2</td><td></td></tr><tr><td>Question Answering</td><td>2</td><td></td><td></td></tr><tr><td>Reasoning</td><td>1</td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td>1</td><td></td></tr><tr><td>Representation Learning</td><td>1</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>4</td><td>1</td></tr><tr><td>Sentiment Analysis</td><td></td><td>1</td><td></td></tr><tr><td>Simulation</td><td>1</td><td></td><td></td></tr><tr><td>Simulator</td><td>1</td><td></td><td></td></tr><tr><td>Stemming</td><td>1</td><td></td><td></td></tr><tr><td>Summarization</td><td></td><td>1</td><td></td></tr><tr><td>Supervised Learning</td><td>2</td><td>3</td><td></td></tr><tr><td>Tensor Decomposition</td><td></td><td>1</td><td></td></tr><tr><td>Text Generation</td><td>1</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td>1</td><td></td></tr><tr><td>Transformer</td><td>1</td><td>1</td><td></td></tr><tr><td>Unsupervised Learning</td><td>1</td><td>2</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>2</td><td></td></tr><tr><td>Vision-and-Language</td><td>1</td><td>3</td><td></td></tr><tr><td>Zero-shot</td><td></td><td>1</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=csse-3>cs.SE (3)</h2><h3 id=13--196-zero-shot-code-representation-learning-via-prompt-tuning-nan-cui-et-al-2024>(1/3 | 1/96) Zero-Shot Code Representation Learning via Prompt Tuning (Nan Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nan Cui, Xiaodong Gu, Beijun Shen. (2024)<br><strong>Zero-Shot Code Representation Learning via Prompt Tuning</strong><br><button class=copy-to-clipboard title="Zero-Shot Code Representation Learning via Prompt Tuning" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 95<br>Keywords: Fine-tuning, Fine-tuning, Representation Learning, Zero-shot, Transformer, Code Generation, Pre-trained Language Model, Pre-trained Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08947v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08947v1.pdf filename=2404.08947v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning <b>code</b> <b>representations</b> <b>has</b> been the core prerequisite of many software engineering tasks such as <b>code</b> <b>clone</b> detection and <b>code</b> <b>generation.</b> State-of-the-art program <b>representation</b> <b>techniques</b> mainly utilize <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> such as CodeBERT. A <b>Transformer</b> encoder is firstly <b>pre-trained</b> <b>on</b> <b>a</b> large-scale <b>code</b> <b>corpus</b> to acquire general knowledge about source <b>code.</b> <b>The</b> <b>pre-trained</b> <b>model</b> <b>is</b> then <b>fine-tuned</b> on specific tasks using an amount of labeled data. However, gathering training samples for the downstream tasks can be prohibitively expensive and impractical for domain-specific languages or project-specific tasks. Besides, pre-training and downstream tasks are usually heterogeneous, which makes it difficult to fully explore the knowledge learned during pre-training. In this paper, we propose Zecoler, a <b>zero-shot</b> approach for learning <b>code</b> <b>representations.</b> <b>Zecoler</b> is built upon a <b>pre-trained</b> <b>programming</b> <b>language</b> model. In order to elicit knowledge from the <b>PLMs</b> efficiently, Zecoler casts the downstream tasks to the same form of pre-training objectives by inserting train-able <b>prompts</b> into the original input. These <b>prompts</b> can guide <b>PLMs</b> on how to generate better results. Subsequently, we employ the <b>prompt</b> tuning technique to search for the optimal <b>prompts</b> for <b>PLMs</b> automatically. This enables the <b>representation</b> <b>model</b> to efficiently fit the downstream tasks through <b>fine-tuning</b> on the dataset in source language domain and then reuse the <b>pre-trained</b> <b>knowledge</b> <b>for</b> the target domain in a <b>zero-shot</b> style. We evaluate Zecoler in five <b>code</b> <b>intelligence</b> tasks including <b>code</b> <b>clone</b> detection, <b>code</b> <b>search,</b> method name prediction, <b>code</b> <b>summarization,</b> and <b>code</b> <b>generation.</b> The results show that our approach significantly outperforms baseline models under the <b>zero-shot</b> setting.</p></p class="citation"></blockquote><h3 id=23--296-large-language-models-for-mobile-gui-text-input-generation-an-empirical-study-chenhui-cui-et-al-2024>(2/3 | 2/96) Large Language Models for Mobile GUI Text Input Generation: An Empirical Study (Chenhui Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenhui Cui, Tao Li, Junjie Wang, Chunyang Chen, Dave Towey, Rubing Huang. (2024)<br><strong>Large Language Models for Mobile GUI Text Input Generation: An Empirical Study</strong><br><button class=copy-to-clipboard title="Large Language Models for Mobile GUI Text Input Generation: An Empirical Study" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 90<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, Text Generation, Large Language Model, Large Language Model, Prompt, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08948v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08948v1.pdf filename=2404.08948v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mobile applications (apps) have become an essential part of our daily lives, making ensuring their quality an important activity. GUI testing, a quality assurance method, has frequently been used for mobile apps. When conducting GUI testing, it is important to generate effective <b>text</b> <b>inputs</b> for the <b>text-input</b> <b>components.</b> Some GUIs require these <b>text</b> <b>inputs</b> to move from one page to the next, which remains a challenge to achieving complete UI exploration. Recently, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown excellent <b>text-generation</b> <b>capabilities.</b> Among the <b>LLMs,</b> OpenAI&rsquo;s <b>GPT</b> series has been widely discussed and used. However, it may not be possible to use these <b>LLMs</b> for GUI testing actual mobile apps, due to the <b>security</b> and privacy issues related to the production data. Therefore, it is necessary to explore the potential of different <b>LLMs</b> to guide <b>text-input</b> <b>generation</b> in mobile GUI testing. This paper reports on a <b>large-scale</b> <b>empirical</b> <b>study</b> that extensively investigates the effectiveness of nine state-of-the-art <b>LLMs</b> in Android <b>text-input</b> <b>generation</b> for UI pages. We collected 114 UI pages from 62 open-source Android apps and extracted contextual information from the UI pages to construct <b>prompts</b> for <b>LLMs</b> to generate <b>text</b> <b>inputs.</b> The experimental results show that some <b>LLMs</b> can generate relatively more effective and higher-quality <b>text</b> <b>inputs,</b> achieving a 50.58% to 66.67% page-pass-through rate, and even detecting some real bugs in open-source apps. Compared with the <b>GPT-3.5</b> and <b>GPT-4</b> <b>LLMs,</b> other <b>LLMs</b> reduce the page-pass-through rates by 17.97% to 84.79% and 21.93% to 85.53%, respectively. We also found that using more complete UI contextual information can increase the page-pass-through rates of <b>LLMs</b> for generating <b>text</b> <b>inputs.</b> In addition, we also describe six insights gained regarding the use of <b>LLMs</b> for Android testing: These insights will benefit the Android testing community.</p></p class="citation"></blockquote><h3 id=33--396-aligning-llms-for-fl-free-program-repair-junjielong-xu-et-al-2024>(3/3 | 3/96) Aligning LLMs for FL-free Program Repair (Junjielong Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junjielong Xu, Ying Fu, Shin Hwei Tan, Pinjia He. (2024)<br><strong>Aligning LLMs for FL-free Program Repair</strong><br><button class=copy-to-clipboard title="Aligning LLMs for FL-free Program Repair" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-LG, cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08877v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08877v1.pdf filename=2404.08877v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved decent results on automated program repair (APR). However, the next token prediction training objective of decoder-only <b>LLMs</b> (e.g., <b>GPT-4)</b> is misaligned with the masked span prediction objective of current infilling-style methods, which impedes <b>LLMs</b> from fully leveraging pre-trained knowledge for program repair. In addition, while some <b>LLMs</b> are capable of locating and repairing bugs end-to-end when using the related artifacts (e.g., test cases) as input, existing methods regard them as separate tasks and ask <b>LLMs</b> to generate patches at fixed locations. This restriction hinders <b>LLMs</b> from exploring potential patches beyond the given locations. In this paper, we investigate a new approach to adapt <b>LLMs</b> to program repair. Our core insight is that <b>LLM&rsquo;s</b> APR capability can be greatly improved by simply aligning the output to their training objective and allowing them to refine the whole program without first performing fault localization. Based on this insight, we designed D4C, a straightforward <b>prompting</b> framework for APR. D4C can repair 180 bugs correctly in Defects4J, with each patch being sampled only 10 times. This surpasses the SOTA APR methods with perfect fault localization by 10% and reduces the patch sampling number by 90%. Our findings reveal that (1) objective alignment is crucial for fully exploiting <b>LLM&rsquo;s</b> pre-trained capability, and (2) replacing the traditional localize-then-repair workflow with direct debugging is more effective for <b>LLM-based</b> APR methods. Thus, we believe this paper introduces a new mindset for harnessing <b>LLMs</b> in APR.</p></p class="citation"></blockquote><h2 id=cscl-14>cs.CL (14)</h2><h3 id=114--496-adapting-mental-health-prediction-tasks-for-cross-lingual-learning-via-meta-training-and-in-context-learning-with-large-language-model-zita-lifelo-et-al-2024>(1/14 | 4/96) Adapting Mental Health Prediction Tasks for Cross-lingual Learning via Meta-Training and In-context Learning with Large Language Model (Zita Lifelo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zita Lifelo, Huansheng Ning, Sahraoui Dhelim. (2024)<br><strong>Adapting Mental Health Prediction Tasks for Cross-lingual Learning via Meta-Training and In-context Learning with Large Language Model</strong><br><button class=copy-to-clipboard title="Adapting Mental Health Prediction Tasks for Cross-lingual Learning via Meta-Training and In-context Learning with Large Language Model" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs-LG, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Low-Resource, Meta Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09045v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09045v1.pdf filename=2404.09045v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Timely identification is essential for the efficient handling of mental health illnesses such as depression. However, the current research fails to adequately address the prediction of mental health conditions from social media data in <b>low-resource</b> African languages like Swahili. This study introduces two distinct approaches utilising model-agnostic <b>meta-learning</b> <b>and</b> leveraging <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to address this gap. Experiments are conducted on three datasets translated to <b>low-resource</b> language and applied to four mental health tasks, which include stress, depression, depression severity and suicidal ideation prediction. we first apply a <b>meta-learning</b> <b>model</b> with self-supervision, which results in improved model initialisation for rapid adaptation and cross-lingual transfer. The results show that our <b>meta-trained</b> <b>model</b> performs significantly better than standard <b>fine-tuning</b> methods, outperforming the baseline <b>fine-tuning</b> in macro F1 score with 18% and 0.8% over XLM-R and mBERT. In parallel, we use <b>LLMs&rsquo;</b> <b>in-context</b> <b>learning</b> capabilities to assess their performance accuracy across the Swahili mental health prediction tasks by analysing different cross-lingual <b>prompting</b> approaches. Our analysis showed that Swahili <b>prompts</b> performed better than cross-lingual <b>prompts</b> but less than English <b>prompts.</b> Our findings show that <b>in-context</b> <b>learning</b> can be achieved through cross-lingual transfer through carefully crafted <b>prompt</b> templates with examples and instructions.</p></p class="citation"></blockquote><h3 id=214--596-curiousllm-elevating-multi-document-qa-with-reasoning-infused-knowledge-graph-prompting-zukang-yang-et-al-2024>(2/14 | 5/96) CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge Graph Prompting (Zukang Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zukang Yang, Zixuan Zhu. (2024)<br><strong>CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge Graph Prompting</strong><br><button class=copy-to-clipboard title="CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge Graph Prompting" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 78<br>Keywords: Graph, Fine-tuning, Knowledge Graph, Question Answering, Question Answering, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09077v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09077v1.pdf filename=2404.09077v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of <b>Question</b> <b>Answering</b> <b>(QA),</b> unifying <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with external databases has shown great success. However, these methods often fall short in providing the advanced <b>reasoning</b> needed for complex <b>QA</b> tasks. To address these issues, we improve over a novel approach called <b>Knowledge</b> <b>Graph</b> <b>Prompting</b> (KGP), which combines <b>knowledge</b> <b>graphs</b> with a <b>LLM-based</b> agent to improve <b>reasoning</b> and search accuracy. Nevertheless, the original KGP framework necessitates costly <b>fine-tuning</b> with <b>large</b> <b>datasets</b> <b>yet</b> still suffers from <b>LLM</b> hallucination. Therefore, we propose a <b>reasoning-infused</b> <b>LLM</b> agent to enhance this framework. This agent mimics human curiosity to ask follow-up <b>questions</b> <b>to</b> more efficiently navigate the search. This simple modification significantly boosts the <b>LLM</b> performance in <b>QA</b> tasks without the high costs and latency associated with the initial KGP framework. Our ultimate goal is to further develop this approach, leading to more accurate, faster, and cost-effective solutions in the <b>QA</b> domain.</p></p class="citation"></blockquote><h3 id=314--696-multilingual-evaluation-of-semantic-textual-relatedness-sharvi-endait-et-al-2024>(3/14 | 6/96) Multilingual Evaluation of Semantic Textual Relatedness (Sharvi Endait et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sharvi Endait, Srushti Sonavane, Ridhima Sinare, Pritika Rohera, Advait Naik, Dipali Kadam. (2024)<br><strong>Multilingual Evaluation of Semantic Textual Relatedness</strong><br><button class=copy-to-clipboard title="Multilingual Evaluation of Semantic Textual Relatedness" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Low-Resource, Supervised Learning, Unsupervised Learning, Information Retrieval, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09047v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09047v1.pdf filename=2404.09047v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The explosive growth of online content demands robust Natural Language Processing (NLP) techniques that can capture nuanced meanings and cultural context across diverse languages. Semantic Textual Relatedness (STR) goes beyond superficial word overlap, considering linguistic elements and non-linguistic factors like topic, sentiment, and perspective. Despite its pivotal role, prior NLP research has predominantly focused on English, limiting its applicability across languages. Addressing this gap, our paper dives into capturing deeper connections between sentences beyond simple word overlap. Going beyond English-centric NLP research, we explore STR in Marathi, Hindi, Spanish, and English, unlocking the potential for <b>information</b> <b>retrieval,</b> <b>machine</b> <b>translation,</b> and more. Leveraging the SemEval-2024 shared task, we explore various language models across three learning paradigms: <b>supervised,</b> <b>unsupervised,</b> and cross-lingual. Our comprehensive methodology gains promising results, demonstrating the effectiveness of our approach. This work aims to not only showcase our achievements but also inspire further research in multilingual STR, particularly for <b>low-resourced</b> languages.</p></p class="citation"></blockquote><h3 id=414--796-do-llms-play-dice-exploring-probability-distribution-sampling-in-large-language-models-for-behavioral-simulation-jia-gu-et-al-2024>(4/14 | 7/96) Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation (Jia Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jia Gu, Liang Pang, Huawei Shen, Xueqi Cheng. (2024)<br><strong>Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation</strong><br><button class=copy-to-clipboard title="Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Markov Decision Process, Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09043v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09043v1.pdf filename=2404.09043v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid advancement of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and their remarkable capabilities in handling complex language tasks, an increasing number of studies are employing <b>LLMs</b> as agents to emulate the sequential decision-making processes of humans often represented as Markov decision-making processes <b>(MDPs).</b> The actions within this decision-making framework adhere to specific probability distributions and require iterative sampling. This arouses our curiosity regarding the capacity of <b>LLM</b> agents to comprehend probability distributions, thereby guiding the agent&rsquo;s behavioral decision-making through probabilistic sampling and generating behavioral sequences. To answer the above question, we divide the problem into two main aspects: <b>simulation</b> where the exact probability distribution is known, and generation of sequences where the probability distribution is ambiguous. In the first case, the agent is required to give the type and parameters of the probability distribution through the problem description, and then give the sampling sequence. However, our analysis shows that <b>LLM</b> agents perform poorly in this case, but the sampling success rate can be improved through programming tools. Real-world scenarios often entail unknown probability distributions. Thus, in the second case, we ask the agents to change the activity level in online social networks and analyze the frequency of actions. Ultimately, our analysis shows that <b>LLM</b> agents cannot sample probability distributions even using programming tools. Therefore, careful consideration is still required before directly applying <b>LLM</b> agents as agents to simulate human behavior.</p></p class="citation"></blockquote><h3 id=514--896-llm-in-context-recall-is-prompt-dependent-daniel-machlab-et-al-2024>(5/14 | 8/96) LLM In-Context Recall is Prompt Dependent (Daniel Machlab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Machlab, Rick Battle. (2024)<br><strong>LLM In-Context Recall is Prompt Dependent</strong><br><button class=copy-to-clipboard title="LLM In-Context Recall is Prompt Dependent" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08865v1.pdf filename=2404.08865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> highlights the critical importance of conducting thorough evaluations to discern their comparative advantages, limitations, and optimal use cases. Particularly important is assessing their capacity to accurately retrieve information included in a given <b>prompt.</b> A model&rsquo;s ability to do this significantly influences how effectively it can utilize contextual details, thus impacting its practical efficacy and dependability in real-world applications. Our research analyzes the <b>in-context</b> recall performance of various <b>LLMs</b> using the needle-in-a-haystack method. In this approach, a factoid (the &ldquo;needle&rdquo;) is embedded within a block of filler text (the &ldquo;haystack&rdquo;), which the model is asked to retrieve. We assess the recall performance of each model across various haystack lengths and with varying needle placements to identify performance patterns. This study demonstrates that an <b>LLM&rsquo;s</b> recall capability is not only contingent upon the <b>prompt&rsquo;s</b> content but also may be compromised by biases in its training data. Conversely, adjustments to model architecture, training strategy, or <b>fine-tuning</b> can improve performance. Our analysis provides insight into <b>LLM</b> behavior, offering direction for the development of more effective applications of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=614--996-multimodal-cross-document-event-coreference-resolution-using-linear-semantic-transfer-and-mixed-modality-ensembles-abhijnan-nath-et-al-2024>(6/14 | 9/96) Multimodal Cross-Document Event Coreference Resolution Using Linear Semantic Transfer and Mixed-Modality Ensembles (Abhijnan Nath et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhijnan Nath, Huma Jamil, Shafiuddin Rehan Ahmed, George Baker, Rahul Ghosh, James H. Martin, Nathaniel Blanchard, Nikhil Krishnaswamy. (2024)<br><strong>Multimodal Cross-Document Event Coreference Resolution Using Linear Semantic Transfer and Mixed-Modality Ensembles</strong><br><button class=copy-to-clipboard title="Multimodal Cross-Document Event Coreference Resolution Using Linear Semantic Transfer and Mixed-Modality Ensembles" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 49<br>Keywords: Diffusion Model, Benchmarking, Fine-tuning, Multi-modal, Multi-modal, Coreference Resolution, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08949v1.pdf filename=2404.08949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Event <b>coreference</b> <b>resolution</b> (ECR) is the task of determining whether distinct mentions of events within a multi-document corpus are actually linked to the same underlying occurrence. Images of the events can help facilitate resolution when language is ambiguous. Here, we propose a <b>multimodal</b> cross-document event <b>coreference</b> <b>resolution</b> method that integrates visual and textual cues with a simple linear map between vision and language models. As existing ECR <b>benchmark</b> datasets rarely provide images for all event mentions, we augment the popular ECB+ dataset with event-centric images scraped from the internet and generated using image <b>diffusion</b> <b>models.</b> We establish three methods that incorporate images and text for <b>coreference:</b> <b>1)</b> a standard fused model with <b>finetuning,</b> 2) a novel linear mapping method without <b>finetuning</b> and 3) an ensembling approach based on splitting mention pairs by semantic and discourse-level difficulty. We evaluate on 2 datasets: the augmented ECB+, and AIDA Phase 1. Our ensemble systems using cross-modal linear mapping establish an upper limit (91.9 CoNLL F1) on ECB+ ECR performance given the preprocessing assumptions used, and establish a novel baseline on AIDA Phase 1. Our results demonstrate the utility of <b>multimodal</b> information in ECR for certain challenging <b>coreference</b> <b>problems,</b> and highlight a need for more <b>multimodal</b> resources in the <b>coreference</b> <b>resolution</b> space.</p></p class="citation"></blockquote><h3 id=714--1096-ming-moe-enhancing-medical-multi-task-learning-in-large-language-models-with-sparse-mixture-of-low-rank-adapter-experts-yusheng-liao-et-al-2024>(7/14 | 10/96) MING-MOE: Enhancing Medical Multi-Task Learning in Large Language Models with Sparse Mixture of Low-Rank Adapter Experts (Yusheng Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yusheng Liao, Shuyang Jiang, Yu Wang, Yanfeng Wang. (2024)<br><strong>MING-MOE: Enhancing Medical Multi-Task Learning in Large Language Models with Sparse Mixture of Low-Rank Adapter Experts</strong><br><button class=copy-to-clipboard title="MING-MOE: Enhancing Medical Multi-Task Learning in Large Language Models with Sparse Mixture of Low-Rank Adapter Experts" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: ChatGPT, Natural Language Understanding, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09027v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09027v1.pdf filename=2404.09027v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> like <b>ChatGPT</b> have shown substantial progress in <b>natural</b> <b>language</b> <b>understanding</b> and generation, proving valuable across various disciplines, including the medical field. Despite advancements, challenges persist due to the complexity and diversity inherent in medical tasks which often require multi-task learning capabilities. Previous approaches, although beneficial, fall short in real-world applications because they necessitate task-specific annotations at inference time, limiting broader generalization. This paper introduces MING-MOE, a novel Mixture-of-Expert~(MOE)-based medical <b>large</b> <b>language</b> <b>model</b> designed to manage diverse and complex medical tasks without requiring task-specific annotations, thus enhancing its usability across extensive datasets. MING-MOE employs a Mixture of Low-Rank Adaptation (MoLoRA) technique, allowing for efficient parameter usage by maintaining base model parameters static while adapting through a minimal set of trainable parameters. We demonstrate that MING-MOE achieves state-of-the-art (SOTA) performance on over 20 medical tasks, illustrating a significant improvement over existing models. This approach not only extends the capabilities of medical language models but also improves inference efficiency.</p></p class="citation"></blockquote><h3 id=814--1196-enforcing-paraphrase-generation-via-controllable-latent-diffusion-wei-zou-et-al-2024>(8/14 | 11/96) Enforcing Paraphrase Generation via Controllable Latent Diffusion (Wei Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Zou, Ziyuan Zhuang, Shujian Huang, Jia Liu, Jiajun Chen. (2024)<br><strong>Enforcing Paraphrase Generation via Controllable Latent Diffusion</strong><br><button class=copy-to-clipboard title="Enforcing Paraphrase Generation via Controllable Latent Diffusion" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Diffusion Model, Text Generation, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08938v1.pdf filename=2404.08938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Paraphrase generation aims to produce high-quality and diverse utterances of a given <b>text.</b> <b>Though</b> state-of-the-art generation via the <b>diffusion</b> <b>model</b> reconciles generation quality and diversity, textual <b>diffusion</b> <b>suffers</b> from a truncation issue that hinders efficiency and quality control. In this work, we propose \textit{L}atent \textit{D}iffusion \textit{P}araphraser~(LDP), a novel paraphrase generation by modeling a controllable <b>diffusion</b> <b>process</b> given a learned latent space. LDP achieves superior generation efficiency compared to its <b>diffusion</b> <b>counterparts.</b> It facilitates only input segments to enforce paraphrase semantics, which further improves the results without external features. Experiments show that LDP achieves improved and diverse paraphrase generation compared to baselines. Further analysis shows that our method is also helpful to other similar <b>text</b> <b>generations</b> and <b>domain</b> <b>adaptations.</b> Our code and data are available at <a href=https://github.com/NIL-zhuang/ld4pg>https://github.com/NIL-zhuang/ld4pg</a>.</p></p class="citation"></blockquote><h3 id=914--1296-towards-enhancing-health-coaching-dialogue-in-low-resource-settings-yue-zhou-et-al-2024>(9/14 | 12/96) Towards Enhancing Health Coaching Dialogue in Low-Resource Settings (Yue Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yue Zhou, Barbara Di Eugenio, Brian Ziebart, Lisa Sharp, Bing Liu, Ben Gerber, Nikolaos Agadakos, Shweta Yadav. (2024)<br><strong>Towards Enhancing Health Coaching Dialogue in Low-Resource Settings</strong><br><button class=copy-to-clipboard title="Towards Enhancing Health Coaching Dialogue in Low-Resource Settings" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Low-Resource, Dialogue System, Natural Language Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08888v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08888v1.pdf filename=2404.08888v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Health coaching helps patients identify and accomplish lifestyle-related goals, effectively improving the control of chronic diseases and mitigating mental health conditions. However, health coaching is cost-prohibitive due to its highly personalized and labor-intensive nature. In this paper, we propose to build a <b>dialogue</b> <b>system</b> that converses with the patients, helps them create and accomplish specific goals, and can address their emotions with empathy. However, building such a system is challenging since real-world health coaching datasets are limited and empathy is subtle. Thus, we propose a modularized health coaching <b>dialogue</b> <b>system</b> with simplified NLU and <b>NLG</b> frameworks combined with mechanism-conditioned empathetic response generation. Through automatic and human evaluation, we show that our system generates more empathetic, fluent, and coherent responses and outperforms the state-of-the-art in NLU tasks while requiring less annotation. We view our approach as a key step towards building automated and more accessible health coaching systems.</p></p class="citation"></blockquote><h3 id=1014--1396-ronid-new-intent-discovery-with-generated-reliable-labels-and-cluster-friendly-representations-shun-zhang-et-al-2024>(10/14 | 13/96) RoNID: New Intent Discovery with Generated-Reliable Labels and Cluster-friendly Representations (Shun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shun Zhang, Chaoran Yan, Jian Yang, Changyu Ren, Jiaqi Bai, Tongliang Li, Zhoujun Li. (2024)<br><strong>RoNID: New Intent Discovery with Generated-Reliable Labels and Cluster-friendly Representations</strong><br><button class=copy-to-clipboard title="RoNID: New Intent Discovery with Generated-Reliable Labels and Cluster-friendly Representations" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 28<br>Keywords: Benchmarking, Contrastive Learning, Representation Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08977v1.pdf filename=2404.08977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>New Intent Discovery (NID) strives to identify known and reasonably deduce novel intent groups in the open-world scenario. But current methods face issues with inaccurate pseudo-labels and poor <b>representation</b> <b>learning,</b> creating a negative feedback loop that degrades overall model performance, including accuracy and the adjusted rand index. To address the aforementioned challenges, we propose a Robust New Intent Discovery (RoNID) framework optimized by an EM-style method, which focuses on constructing reliable pseudo-labels and obtaining cluster-friendly discriminative <b>representations.</b> <b>RoNID</b> comprises two main modules: reliable pseudo-label generation module and cluster-friendly <b>representation</b> <b>learning</b> module. Specifically, the pseudo-label generation module assigns reliable synthetic labels by solving an optimal transport problem in the E-step, which effectively provides high-quality <b>supervised</b> signals for the input of the cluster-friendly <b>representation</b> <b>learning</b> module. To learn cluster-friendly <b>representation</b> <b>with</b> strong intra-cluster compactness and large inter-cluster separation, the <b>representation</b> <b>learning</b> module combines intra-cluster and inter-cluster <b>contrastive</b> <b>learning</b> in the M-step to feed more discriminative features into the generation module. RoNID can be performed iteratively to ultimately yield a robust model with reliable pseudo-labels and cluster-friendly <b>representations.</b> <b>Experimental</b> results on multiple <b>benchmarks</b> demonstrate our method brings substantial improvements over previous state-of-the-art methods by a large margin of +1~+4 points.</p></p class="citation"></blockquote><h3 id=1114--1496-oovs-in-the-spotlight-how-to-inflect-them-tomáš-sourada-et-al-2024>(11/14 | 14/96) OOVs in the Spotlight: How to Inflect them? (Tomáš Sourada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tomáš Sourada, Jana Straková, Rudolf Rosa. (2024)<br><strong>OOVs in the Spotlight: How to Inflect them?</strong><br><button class=copy-to-clipboard title="OOVs in the Spotlight: How to Inflect them?" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: LSTM, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08974v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08974v1.pdf filename=2404.08974v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We focus on morphological inflection in out-of-vocabulary (OOV) conditions, an under-researched subtask in which state-of-the-art systems usually are less effective. We developed three systems: a retrograde model and two sequence-to-sequence (seq2seq) models based on <b>LSTM</b> and <b>Transformer.</b> For testing in OOV conditions, we automatically extracted a large dataset of nouns in the morphologically rich Czech language, with lemma-disjoint data splits, and we further manually annotated a real-world OOV dataset of neologisms. In the standard OOV conditions, <b>Transformer</b> achieves the best results, with increasing performance in ensemble with <b>LSTM,</b> the retrograde model and SIGMORPHON baselines. On the real-world OOV dataset of neologisms, the retrograde model outperforms all neural models. Finally, our seq2seq models achieve state-of-the-art results in 9 out of 16 languages from SIGMORPHON 2022 shared task data in the OOV evaluation (feature overlap) in the large data condition. We release the Czech OOV Inflection Dataset for rigorous evaluation in OOV conditions. Further, we release the inflection system with the seq2seq models as a ready-to-use Python library.</p></p class="citation"></blockquote><h3 id=1214--1596-on-speculative-decoding-for-multimodal-large-language-models-mukul-gagrani-et-al-2024>(12/14 | 15/96) On Speculative Decoding for Multimodal Large Language Models (Mukul Gagrani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mukul Gagrani, Raghavv Goel, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher Lott. (2024)<br><strong>On Speculative Decoding for Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="On Speculative Decoding for Multimodal Large Language Models" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08856v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08856v1.pdf filename=2404.08856v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inference with <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) is slow due to their <b>large-language-model</b> <b>backbone</b> <b>which</b> suffers from memory bandwidth bottleneck and generates tokens auto-regressively. In this paper, we explore the application of speculative decoding to enhance the inference efficiency of MLLMs, specifically the LLaVA 7B model. We show that a language-only model can serve as a good draft model for speculative decoding with LLaVA 7B, bypassing the need for image tokens and their associated processing components from the draft model. Our experiments across three different tasks show that speculative decoding can achieve a memory-bound speedup of up to 2.37$\times$ using a 115M parameter language model that we trained from scratch. Additionally, we introduce a compact LLaVA draft model incorporating an image adapter, which shows marginal performance gains in image captioning while maintaining comparable results in other tasks.</p></p class="citation"></blockquote><h3 id=1314--1696-wikisplit-easy-data-refinement-for-split-and-rephrase-hayato-tsukagoshi-et-al-2024>(13/14 | 16/96) WikiSplit++: Easy Data Refinement for Split and Rephrase (Hayato Tsukagoshi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hayato Tsukagoshi, Tsutomu Hirao, Makoto Morishita, Katsuki Chousa, Ryohei Sasano, Koichi Takeda. (2024)<br><strong>WikiSplit++: Easy Data Refinement for Split and Rephrase</strong><br><button class=copy-to-clipboard title="WikiSplit++: Easy Data Refinement for Split and Rephrase" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09002v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09002v1.pdf filename=2404.09002v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task of Split and Rephrase, which splits a complex sentence into multiple simple sentences with the same meaning, improves readability and enhances the performance of downstream tasks in natural language processing (NLP). However, while Split and Rephrase can be improved using a text-to-text generation approach that applies encoder-decoder models <b>fine-tuned</b> with a large-scale dataset, it still suffers from hallucinations and under-splitting. To address these issues, this paper presents a simple and strong data refinement approach. Here, we create WikiSplit++ by removing instances in WikiSplit where complex sentences do not entail at least one of the simpler sentences and reversing the order of reference simple sentences. Experimental results show that training with WikiSplit++ leads to better performance than training with WikiSplit, even with fewer training instances. In particular, our approach yields significant gains in the number of splits and the entailment ratio, a proxy for measuring hallucinations.</p></p class="citation"></blockquote><h3 id=1414--1796-labeled-morphological-segmentation-with-semi-markov-models-ryan-cotterell-et-al-2024>(14/14 | 17/96) Labeled Morphological Segmentation with Semi-Markov Models (Ryan Cotterell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryan Cotterell, Thomas Müller, Alexander Fraser, Hinrich Schütze. (2024)<br><strong>Labeled Morphological Segmentation with Semi-Markov Models</strong><br><button class=copy-to-clipboard title="Labeled Morphological Segmentation with Semi-Markov Models" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08997v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08997v1.pdf filename=2404.08997v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present labeled morphological segmentation, an alternative view of morphological processing that unifies several tasks. From an annotation standpoint, we additionally introduce a new hierarchy of morphotactic tagsets. Finally, we develop \modelname, a discriminative morphological segmentation system that, contrary to previous work, explicitly models morphotactics. We show that \textsc{chipmunk} yields improved performance on three tasks for all six languages: (i) morphological segmentation, (ii) <b>stemming</b> and (iii) morphological tag classification. On morphological segmentation, our method shows absolute improvements of 2&ndash;6 points $F_1$ over the baseline.</p></p class="citation"></blockquote><h2 id=csir-5>cs.IR (5)</h2><h3 id=15--1896-introducing-super-rags-in-mistral-8x7b-v1-ayush-thakur-et-al-2024>(1/5 | 18/96) Introducing Super RAGs in Mistral 8x7B-v1 (Ayush Thakur et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayush Thakur, Raghav Gupta. (2024)<br><strong>Introducing Super RAGs in Mistral 8x7B-v1</strong><br><button class=copy-to-clipboard title="Introducing Super RAGs in Mistral 8x7B-v1" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs-LG, cs.IR<br>Keyword Score: 70<br>Keywords: Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Mistral, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08940v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08940v1.pdf filename=2404.08940v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The relentless pursuit of enhancing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has led to the advent of Super <b>Retrieval-Augmented</b> <b>Generation</b> <b>(Super</b> <b>RAGs),</b> a novel approach designed to elevate the performance of <b>LLMs</b> by integrating external knowledge sources with minimal structural modifications. This paper presents the integration of Super <b>RAGs</b> into the <b>Mistral</b> 8x7B v1, a state-of-the-art <b>LLM,</b> and examines the resultant improvements in accuracy, speed, and user satisfaction. Our methodology uses a <b>fine-tuned</b> instruct model setup and a cache tuning fork system, ensuring efficient and relevant data <b>retrieval.</b> <b>The</b> <b>evaluation,</b> conducted over several epochs, demonstrates significant enhancements across all metrics. The findings suggest that Super <b>RAGs</b> can effectively augment <b>LLMs,</b> paving the way for more sophisticated and reliable AI systems. This research contributes to the field by providing empirical evidence of the benefits of Super <b>RAGs</b> and offering insights into their potential applications.</p></p class="citation"></blockquote><h3 id=25--1996-approximate-cluster-based-sparse-document-retrieval-with-segmented-maximum-term-weights-yifan-qiao-et-al-2024>(2/5 | 19/96) Approximate Cluster-Based Sparse Document Retrieval with Segmented Maximum Term Weights (Yifan Qiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Qiao, Shanxiu He, Yingrui Yang, Parker Carlson, Tao Yang. (2024)<br><strong>Approximate Cluster-Based Sparse Document Retrieval with Segmented Maximum Term Weights</strong><br><button class=copy-to-clipboard title="Approximate Cluster-Based Sparse Document Retrieval with Segmented Maximum Term Weights" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 13<br>Keywords: Clustering, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08896v1.pdf filename=2404.08896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper revisits cluster-based retrieval that partitions the inverted index into multiple groups and skips the index partially at cluster and document levels during online inference using a learned sparse representation. It proposes an approximate search scheme with two parameters to control the rank-safeness competitiveness of <b>pruning</b> with segmented maximum term weights within each cluster. Cluster-level maximum weight segmentation allows an improvement in the rank score bound estimation and threshold-based <b>pruning</b> to be approximately adaptive to bound estimation tightness, resulting in better relevance and efficiency. The experiments with MS MARCO passage ranking and BEIR datasets demonstrate the usefulness of the proposed scheme with a comparison to the baselines. This paper presents the design of this approximate retrieval scheme with rank-safeness analysis, compares <b>clustering</b> and segmentation options, and reports evaluation results.</p></p class="citation"></blockquote><h3 id=35--2096-countering-mainstream-bias-via-end-to-end-adaptive-local-learning-jinhao-pan-et-al-2024>(3/5 | 20/96) Countering Mainstream Bias via End-to-End Adaptive Local Learning (Jinhao Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinhao Pan, Ziwei Zhu, Jianling Wang, Allen Lin, James Caverlee. (2024)<br><strong>Countering Mainstream Bias via End-to-End Adaptive Local Learning</strong><br><button class=copy-to-clipboard title="Countering Mainstream Bias via End-to-End Adaptive Local Learning" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08887v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08887v1.pdf filename=2404.08887v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collaborative filtering (CF) based <b>recommendations</b> suffer from mainstream bias &ndash; where mainstream users are favored over niche users, leading to poor <b>recommendation</b> quality for many long-tail users. In this paper, we identify two root causes of this mainstream bias: (i) discrepancy modeling, whereby CF algorithms focus on modeling mainstream users while neglecting niche users with unique preferences; and (ii) unsynchronized learning, where niche users require more training epochs than mainstream users to reach peak performance. Targeting these causes, we propose a novel end-To-end Adaptive Local Learning (TALL) framework to provide high-quality <b>recommendations</b> to both mainstream and niche users. TALL uses a loss-driven Mixture-of-Experts module to adaptively ensemble experts to provide customized local models for different users. Further, it contains an adaptive weight module to synchronize the learning paces of different users by dynamically adjusting weights in the loss. Extensive experiments demonstrate the state-of-the-art performance of the proposed model. Code and data are provided at \url{https://github.com/JP-25/end-To-end-Adaptive-Local-Leanring-TALL-}</p></p class="citation"></blockquote><h3 id=45--2196-misinformation-resilient-search-rankings-with-webgraph-based-interventions-peter-carragher-et-al-2024>(4/5 | 21/96) Misinformation Resilient Search Rankings with Webgraph-based Interventions (Peter Carragher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Carragher, Evan M. Williams, Kathleen M. Carley. (2024)<br><strong>Misinformation Resilient Search Rankings with Webgraph-based Interventions</strong><br><button class=copy-to-clipboard title="Misinformation Resilient Search Rankings with Webgraph-based Interventions" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-SI, cs.IR<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08869v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08869v1.pdf filename=2404.08869v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of unreliable news domains on the internet has had wide-reaching negative impacts on society. We introduce and evaluate interventions aimed at reducing traffic to unreliable news domains from search engines while maintaining traffic to reliable domains. We build these interventions on the principles of <b>fairness</b> (penalize sites for what is in their control), generality (label/fact-check agnostic), targeted (increase the cost of adversarial behavior), and scalability (works at webscale). We refine our methods on small-scale webdata as a testbed and then generalize the interventions to a large-scale webgraph containing 93.9M domains and 1.6B edges. We demonstrate that our methods penalize unreliable domains far more than reliable domains in both settings and we explore multiple avenues to mitigate unintended effects on both the small-scale and large-scale webgraph experiments. These results indicate the potential of our approach to reduce the spread of misinformation and foster a more reliable online information ecosystem. This research contributes to the development of targeted strategies to enhance the trustworthiness and quality of search engine results, ultimately benefiting users and the broader digital community.</p></p class="citation"></blockquote><h3 id=55--2296-improving-technical-how-to-query-accuracy-with-automated-search-results-verification-and-reranking-lei-ding-et-al-2024>(5/5 | 22/96) Improving Technical &lsquo;How-to&rsquo; Query Accuracy with Automated Search Results Verification and Reranking (Lei Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Ding, Jeshwanth Bheemanpally, Yi Zhang. (2024)<br><strong>Improving Technical &lsquo;How-to&rsquo; Query Accuracy with Automated Search Results Verification and Reranking</strong><br><button class=copy-to-clipboard title="Improving Technical 'How-to' Query Accuracy with Automated Search Results Verification and Reranking" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 10<br>Keywords: Rerank<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08860v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08860v1.pdf filename=2404.08860v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many people use search engines to find online guidance to solve computer or mobile device problems. Users frequently encounter challenges in identifying effective solutions from search results, often wasting time trying ineffective solutions that seem relevant yet fail to solve the real problems. This paper introduces a novel approach to improving the accuracy and relevance of online technical support search results through automated search results verification and <b>reranking.</b> Taking &ldquo;How-to&rdquo; queries specific to on-device execution as a starting point, we first developed a solution that allows an AI agent to interpret and execute step-by-step instructions in the search results in a controlled Android environment. We further integrated the agent&rsquo;s findings into a <b>reranking</b> mechanism that orders search results based on the success indicators of the tested solutions. The paper details the architecture of our solution and a comprehensive evaluation of the system through a series of tests across various application domains. The results demonstrate a significant improvement in the quality and reliability of the top-ranked results. Our findings suggest a paradigm shift in how search engine ranking for online technical support help can be optimized, offering a scalable and automated solution to the pervasive challenge of finding effective and reliable online help.</p></p class="citation"></blockquote><h2 id=cscv-26>cs.CV (26)</h2><h3 id=126--2396-label-free-anomaly-detection-in-aerial-agricultural-images-with-masked-image-modeling-sambal-shikhar-et-al-2024>(1/26 | 23/96) Label-free Anomaly Detection in Aerial Agricultural Images with Masked Image Modeling (Sambal Shikhar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sambal Shikhar, Anupam Sobti. (2024)<br><strong>Label-free Anomaly Detection in Aerial Agricultural Images with Masked Image Modeling</strong><br><button class=copy-to-clipboard title="Label-free Anomaly Detection in Aerial Agricultural Images with Masked Image Modeling" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 70<br>Keywords: Anomaly Detection, Autoencoder, Self-supervised Learning, Self-supervised Learning, Supervised Learning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08931v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08931v1.pdf filename=2404.08931v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting various types of stresses (nutritional, water, nitrogen, etc.) in agricultural fields is critical for farmers to ensure maximum productivity. However, stresses show up in different shapes and sizes across different crop types and varieties. Hence, this is posed as an <b>anomaly</b> <b>detection</b> task in agricultural images. Accurate <b>anomaly</b> <b>detection</b> in agricultural UAV images is vital for early identification of field irregularities. Traditional <b>supervised</b> <b>learning</b> faces challenges in adapting to diverse anomalies, necessitating extensive annotated data. In this work, we overcome this limitation with <b>self-supervised</b> <b>learning</b> using a masked image modeling approach. Masked <b>Autoencoders</b> (MAE) extract meaningful normal features from unlabeled image samples which produces high reconstruction error for the abnormal pixels during reconstruction. To remove the need of using only <code>normal" data while training, we use an &lt;b>anomaly&lt;/b> &lt;b>suppression&lt;/b> loss mechanism that effectively minimizes the reconstruction of anomalous pixels and allows the model to learn anomalous areas without explicitly separating </code>normal" images for training. Evaluation on the Agriculture-Vision data challenge shows a mIOU score improvement in comparison to prior state of the art in <b>unsupervised</b> and <b>self-supervised</b> <b>methods.</b> A single model generalizes across all the <b>anomaly</b> <b>categories</b> in the Agri-Vision Challenge Dataset</p></p class="citation"></blockquote><h3 id=226--2496-heat-head-level-parameter-efficient-adaptation-of-vision-transformers-with-taylor-expansion-importance-scores-yibo-zhong-et-al-2024>(2/26 | 24/96) HEAT: Head-level Parameter Efficient Adaptation of Vision Transformers with Taylor-expansion Importance Scores (Yibo Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yibo Zhong, Yao Zhou. (2024)<br><strong>HEAT: Head-level Parameter Efficient Adaptation of Vision Transformers with Taylor-expansion Importance Scores</strong><br><button class=copy-to-clipboard title="HEAT: Head-level Parameter Efficient Adaptation of Vision Transformers with Taylor-expansion Importance Scores" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 53<br>Keywords: Vision Transformer, Benchmarking, Fine-tuning, Transfer Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08894v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08894v1.pdf filename=2404.08894v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prior computer <b>vision</b> <b>research</b> extensively explores adapting pre-trained <b>vision</b> <b>transformers</b> (ViT) to downstream tasks. However, the substantial number of parameters requiring adaptation has led to a focus on Parameter Efficient <b>Transfer</b> <b>Learning</b> (PETL) as an approach to efficiently adapt large pre-trained models by training only a subset of parameters, achieving both parameter and storage efficiency. Although the significantly reduced parameters have shown promising performance under <b>transfer</b> <b>learning</b> scenarios, the structural redundancy inherent in the model still leaves room for improvement, which warrants further investigation. In this paper, we propose Head-level Efficient Adaptation with Taylor-expansion importance score (HEAT): a simple method that efficiently <b>fine-tuning</b> ViTs at head levels. In particular, the first-order Taylor expansion is employed to calculate each head&rsquo;s importance score, termed Taylor-expansion Importance Score (TIS), indicating its contribution to specific tasks. Additionally, three strategies for calculating TIS have been employed to maximize the effectiveness of TIS. These strategies calculate TIS from different perspectives, reflecting varying contributions of parameters. Besides ViT, HEAT has also been applied to hierarchical <b>transformers</b> such as Swin <b>Transformer,</b> demonstrating its versatility across different <b>transformer</b> architectures. Through extensive experiments, HEAT has demonstrated superior performance over state-of-the-art PETL methods on the VTAB-1K <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=326--2596-practicaldg-perturbation-distillation-on-vision-language-models-for-hybrid-domain-generalization-zining-chen-et-al-2024>(3/26 | 25/96) PracticalDG: Perturbation Distillation on Vision-Language Models for Hybrid Domain Generalization (Zining Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zining Chen, Weiqiu Wang, Zhicheng Zhao, Fei Su, Aidong Men, Hongying Meng. (2024)<br><strong>PracticalDG: Perturbation Distillation on Vision-Language Models for Hybrid Domain Generalization</strong><br><button class=copy-to-clipboard title="PracticalDG: Perturbation Distillation on Vision-Language Models for Hybrid Domain Generalization" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Distribution Shift, Distribution Shift, Fine-tuning, Knowledge Distillation, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09011v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09011v1.pdf filename=2404.09011v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Domain Generalization (DG) aims to resolve <b>distribution</b> <b>shifts</b> between source and target domains, and current DG methods are default to the setting that data from source and target domains share identical categories. Nevertheless, there exists unseen classes from target domains in practical scenarios. To address this issue, Open Set Domain Generalization (OSDG) has emerged and several methods have been exclusively proposed. However, most existing methods adopt complex architectures with slight improvement compared with DG methods. Recently, <b>vision-language</b> models (VLMs) have been introduced in DG following the <b>fine-tuning</b> paradigm, but consume huge training overhead with large vision models. Therefore, in this paper, we innovate to transfer knowledge from VLMs to lightweight vision models and improve the robustness by introducing Perturbation <b>Distillation</b> (PD) from three perspectives, including Score, Class and Instance (SCI), named SCI-PD. Moreover, previous methods are oriented by the <b>benchmarks</b> with identical and fixed splits, ignoring the divergence between source domains. These methods are revealed to suffer from sharp performance decay with our proposed new <b>benchmark</b> Hybrid Domain Generalization (HDG) and a novel metric $H^{2}$-CV, which construct various splits to comprehensively assess the robustness of algorithms. Extensive experiments demonstrate that our method outperforms state-of-the-art algorithms on multiple datasets, especially improving the robustness when confronting data scarcity.</p></p class="citation"></blockquote><h3 id=426--2696-pm2-a-new-prompting-multi-modal-model-paradigm-for-few-shot-medical-image-classification-zhenwei-wang-et-al-2024>(4/26 | 26/96) PM2: A New Prompting Multi-modal Model Paradigm for Few-shot Medical Image Classification (Zhenwei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenwei Wang, Qiule Sun, Bingbing Zhang, Pengfei Wang, Jianxin Zhang, Qiang Zhang. (2024)<br><strong>PM2: A New Prompting Multi-modal Model Paradigm for Few-shot Medical Image Classification</strong><br><button class=copy-to-clipboard title="PM2: A New Prompting Multi-modal Model Paradigm for Few-shot Medical Image Classification" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 43<br>Keywords: Few-shot, Few-shot Learning, Foundation Model, Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08915v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08915v1.pdf filename=2404.08915v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> <b>learning</b> has been successfully applied to medical image classification as only very few medical examples are available for training. Due to the challenging problem of limited number of annotated medical images, image representations should not be solely derived from a single image modality which is insufficient for characterizing concept classes. In this paper, we propose a new <b>prompting</b> <b>multi-modal</b> model paradigm on medical image classification based on <b>multi-modal</b> <b>foundation</b> <b>models,</b> called PM2. Besides image modality,PM2 introduces another supplementary text input, known as <b>prompt,</b> to further describe corresponding image or concept classes and facilitate <b>few-shot</b> <b>learning</b> across diverse modalities. To better explore the potential of <b>prompt</b> engineering, we empirically investigate five distinct <b>prompt</b> schemes under the new paradigm. Furthermore, linear probing in <b>multi-modal</b> models acts as a linear classification head taking as input only class token, which ignores completely merits of rich statistics inherent in high-level visual tokens. Thus, we alternatively perform a linear classification on feature distribution of visual tokens and class token simultaneously. To effectively mine such rich statistics, a global covariance pooling with efficient matrix power normalization is used to aggregate visual tokens. Then we study and combine two classification heads. One is shared for class token of image from vision encoder and <b>prompt</b> representation encoded by text encoder. The other is to classification on feature distribution of visual tokens from vision encoder. Extensive experiments on three medical datasets show that our PM2 significantly outperforms counterparts regardless of <b>prompt</b> schemes and achieves state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=526--2796-changeanywhere-sample-generation-for-remote-sensing-change-detection-via-semantic-latent-diffusion-model-kai-tang-et-al-2024>(5/26 | 27/96) ChangeAnywhere: Sample Generation for Remote Sensing Change Detection via Semantic Latent Diffusion Model (Kai Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Tang, Jin Chen. (2024)<br><strong>ChangeAnywhere: Sample Generation for Remote Sensing Change Detection via Semantic Latent Diffusion Model</strong><br><button class=copy-to-clipboard title="ChangeAnywhere: Sample Generation for Remote Sensing Change Detection via Semantic Latent Diffusion Model" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 43<br>Keywords: Diffusion Model, Benchmarking, Few-shot, Supervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08892v1.pdf filename=2404.08892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remote sensing change detection (CD) is a pivotal technique that pinpoints changes on a global scale based on multi-temporal images. With the recent expansion of deep learning, <b>supervised</b> deep learning-based CD models have shown satisfactory performance. However, CD sample labeling is very time-consuming as it is densely labeled and requires expert knowledge. To alleviate this problem, we introduce ChangeAnywhere, a novel CD sample generation method using the semantic latent <b>diffusion</b> <b>model</b> and single-temporal images. Specifically, ChangeAnywhere leverages the relative ease of acquiring large single-temporal semantic datasets to generate large-scale, diverse, and semantically annotated bi-temporal CD datasets. ChangeAnywhere captures the two essentials of CD samples, i.e., change implies semantically different, and non-change implies reasonable change under the same semantic constraints. We generated ChangeAnywhere-100K, the largest synthesis CD dataset with 100,000 pairs of CD samples based on the proposed method. The ChangeAnywhere-100K significantly improved both <b>zero-shot</b> and <b>few-shot</b> performance on two CD <b>benchmark</b> datasets for various deep learning-based CD models, as demonstrated by transfer experiments. This paper delineates the enormous potential of ChangeAnywhere for CD sample generation and demonstrates the subsequent enhancement of model performance. Therefore, ChangeAnywhere offers a potent tool for remote sensing CD. All codes and pre-trained models will be available at <a href=https://github.com/tangkai-RS/ChangeAnywhere>https://github.com/tangkai-RS/ChangeAnywhere</a>.</p></p class="citation"></blockquote><h3 id=626--2896-mma-dfer-multimodal-adaptation-of-unimodal-models-for-dynamic-facial-expression-recognition-in-the-wild-kateryna-chumachenko-et-al-2024>(6/26 | 28/96) MMA-DFER: MultiModal Adaptation of unimodal models for Dynamic Facial Expression Recognition in-the-wild (Kateryna Chumachenko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kateryna Chumachenko, Alexandros Iosifidis, Moncef Gabbouj. (2024)<br><strong>MMA-DFER: MultiModal Adaptation of unimodal models for Dynamic Facial Expression Recognition in-the-wild</strong><br><button class=copy-to-clipboard title="MMA-DFER: MultiModal Adaptation of unimodal models for Dynamic Facial Expression Recognition in-the-wild" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Self-supervised Learning, Self-supervised Learning, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09010v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09010v1.pdf filename=2404.09010v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic Facial Expression Recognition (DFER) has received significant interest in the recent years dictated by its pivotal role in enabling empathic and human-compatible technologies. Achieving robustness towards in-the-wild data in DFER is particularly important for real-world applications. One of the directions aimed at improving such models is <b>multimodal</b> <b>emotion</b> <b>recognition</b> based on audio and video data. <b>Multimodal</b> learning in DFER increases the model capabilities by leveraging richer, complementary data representations. Within the field of <b>multimodal</b> DFER, recent methods have focused on exploiting advances of <b>self-supervised</b> <b>learning</b> (SSL) for pre-training of strong <b>multimodal</b> encoders. Another line of research has focused on adapting pre-trained static models for DFER. In this work, we propose a different perspective on the problem and investigate the advancement of <b>multimodal</b> DFER performance by adapting SSL-pre-trained disjoint unimodal encoders. We identify main challenges associated with this task, namely, intra-modality adaptation, cross-modal alignment, and temporal adaptation, and propose solutions to each of them. As a result, we demonstrate improvement over current state-of-the-art on two popular DFER <b>benchmarks,</b> namely DFEW and MFAW.</p></p class="citation"></blockquote><h3 id=726--2996-rethinking-iterative-stereo-matching-from-diffusion-bridge-model-perspective-yuguang-shi-2024>(7/26 | 29/96) Rethinking Iterative Stereo Matching from Diffusion Bridge Model Perspective (Yuguang Shi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuguang Shi. (2024)<br><strong>Rethinking Iterative Stereo Matching from Diffusion Bridge Model Perspective</strong><br><button class=copy-to-clipboard title="Rethinking Iterative Stereo Matching from Diffusion Bridge Model Perspective" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Diffusion Model, Graph Attention Networks, Benchmarking, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09051v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09051v1.pdf filename=2404.09051v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, iteration-based stereo matching has shown great potential. However, these models optimize the disparity map using <b>RNN</b> variants. The discrete optimization process poses a challenge of information loss, which restricts the level of detail that can be expressed in the generated disparity map. In order to address these issues, we propose a novel training approach that incorporates <b>diffusion</b> <b>models</b> into the iterative optimization process. We designed a Time-based <b>Gated</b> Recurrent Unit (T-GRU) to correlate temporal and disparity outputs. Unlike standard recurrent units, we employ Agent Attention to generate more expressive features. We also designed an attention-based context network to capture a large amount of contextual information. Experiments on several public <b>benchmarks</b> show that we have achieved competitive stereo matching performance. Our model ranks first in the Scene Flow dataset, achieving over a 7% improvement compared to competing methods, and requires only 8 iterations to achieve state-of-the-art results.</p></p class="citation"></blockquote><h3 id=826--3096-amu-tuning-effective-logit-bias-for-clip-based-few-shot-learning-yuwei-tang-et-al-2024>(8/26 | 30/96) AMU-Tuning: Effective Logit Bias for CLIP-based Few-shot Learning (Yuwei Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuwei Tang, Zhenyi Lin, Qilong Wang, Pengfei Zhu, Qinghua Hu. (2024)<br><strong>AMU-Tuning: Effective Logit Bias for CLIP-based Few-shot Learning</strong><br><button class=copy-to-clipboard title="AMU-Tuning: Effective Logit Bias for CLIP-based Few-shot Learning" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Few-shot, Few-shot Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08958v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08958v1.pdf filename=2404.08958v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, pre-trained <b>vision-language</b> models (e.g., CLIP) have shown great potential in <b>few-shot</b> <b>learning</b> and attracted a lot of research interest. Although efforts have been made to improve <b>few-shot</b> <b>ability</b> of CLIP, key factors on the effectiveness of existing methods have not been well studied, limiting further exploration of CLIP&rsquo;s potential in <b>few-shot</b> <b>learning.</b> In this paper, we first introduce a unified formulation to analyze CLIP-based <b>few-shot</b> <b>learning</b> methods from a perspective of logit bias, which encourages us to learn an effective logit bias for further improving performance of CLIP-based <b>few-shot</b> <b>learning</b> methods. To this end, we disassemble three key components involved in computation of logit bias (i.e., logit features, logit predictor, and logit fusion) and empirically analyze the effect on performance of <b>few-shot</b> <b>classification.</b> Based on analysis of key components, this paper proposes a novel AMU-Tuning method to learn effective logit bias for CLIP-based <b>few-shot</b> <b>classification.</b> Specifically, our AMU-Tuning predicts logit bias by exploiting the appropriate $\underline{\textbf{A}}$uxiliary features, which are fed into an efficient feature-initialized linear classifier with $\underline{\textbf{M}}$ulti-branch training. Finally, an $\underline{\textbf{U}}$ncertainty-based fusion is developed to incorporate logit bias into CLIP for <b>few-shot</b> <b>classification.</b> The experiments are conducted on several widely used <b>benchmarks,</b> and the results show AMU-Tuning clearly outperforms its counterparts while achieving state-of-the-art performance of CLIP-based <b>few-shot</b> <b>learning</b> without bells and whistles.</p></p class="citation"></blockquote><h3 id=926--3196-chimpvlm-ethogram-enhanced-chimpanzee-behaviour-recognition-otto-brookes-et-al-2024>(9/26 | 31/96) ChimpVLM: Ethogram-Enhanced Chimpanzee Behaviour Recognition (Otto Brookes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Otto Brookes, Majid Mirmehdi, Hjalmar Kuhl, Tilo Burghardt. (2024)<br><strong>ChimpVLM: Ethogram-Enhanced Chimpanzee Behaviour Recognition</strong><br><button class=copy-to-clipboard title="ChimpVLM: Ethogram-Enhanced Chimpanzee Behaviour Recognition" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Fine-tuning, Multi-modal, Masked Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08937v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08937v1.pdf filename=2404.08937v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show that chimpanzee behaviour understanding from camera traps can be enhanced by providing visual architectures with access to an embedding of text descriptions that detail species behaviours. In particular, we present a <b>vision-language</b> model which employs <b>multi-modal</b> decoding of visual features extracted directly from camera trap videos to process query tokens representing behaviours and output class predictions. Query tokens are initialised using a standardised ethogram of chimpanzee behaviour, rather than using random or name-based initialisations. In addition, the effect of initialising query tokens using a <b>masked</b> <b>language</b> <b>model</b> <b>fine-tuned</b> on a text corpus of known behavioural patterns is explored. We evaluate our system on the PanAf500 and PanAf20K datasets and demonstrate the performance benefits of our <b>multi-modal</b> decoding approach and query initialisation strategy on multi-class and multi-label recognition tasks, respectively. Results and ablations corroborate performance improvements. We achieve state-of-the-art performance over vision and <b>vision-language</b> models in top-1 accuracy (+6.34%) on PanAf500 and overall (+1.1%) and tail-class (+2.26%) mean average precision on PanAf20K. We share complete source code and network weights for full reproducibility of results and easy utilisation.</p></p class="citation"></blockquote><h3 id=1026--3296-bg-yolo-a-bidirectional-guided-method-for-underwater-object-detection-jian-zhang-et-al-2024>(10/26 | 32/96) BG-YOLO: A Bidirectional-Guided Method for Underwater Object Detection (Jian Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jian Zhang, Ruiteng Zhang, Xinyue Yan, Xiting Zhuang, Ruicheng Cao. (2024)<br><strong>BG-YOLO: A Bidirectional-Guided Method for Underwater Object Detection</strong><br><button class=copy-to-clipboard title="BG-YOLO: A Bidirectional-Guided Method for Underwater Object Detection" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68T07, 68T45, I-4-3; I-4-8; I-4-9; I-4-10; I-2-10, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Convolution, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08979v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08979v1.pdf filename=2404.08979v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Degraded underwater images decrease the accuracy of underwater <b>object</b> <b>detection.</b> However, existing methods for underwater image enhancement mainly focus on improving the indicators in visual aspects, which may not benefit the tasks of underwater image detection, and may lead to serious degradation in performance. To alleviate this problem, we proposed a bidirectional-guided method for underwater <b>object</b> <b>detection,</b> referred to as BG-YOLO. In the proposed method, network is organized by constructing an enhancement branch and a detection branch in a parallel way. The enhancement branch consists of a cascade of an image enhancement subnet and an <b>object</b> <b>detection</b> subnet. And the detection branch only consists of a detection subnet. A feature guided module connects the shallow <b>convolution</b> layer of the two branches. When training the enhancement branch, the <b>object</b> <b>detection</b> subnet in the enhancement branch guides the image enhancement subnet to be optimized towards the direction that is most conducive to the detection task. The shallow feature map of the trained enhancement branch will be output to the feature guided module, constraining the optimization of detection branch through consistency loss and <b>prompting</b> detection branch to learn more detailed information of the <b>objects.</b> <b>And</b> hence the detection performance will be refined. During the detection tasks, only detection branch will be reserved so that no additional cost of computation will be introduced. Extensive experiments demonstrate that the proposed method shows significant improvement in performance of the detector in severely degraded underwater scenes while maintaining a remarkable detection speed.</p></p class="citation"></blockquote><h3 id=1126--3396-constructing-and-exploring-intermediate-domains-in-mixed-domain-semi-supervised-medical-image-segmentation-qinghe-ma-et-al-2024>(11/26 | 33/96) Constructing and Exploring Intermediate Domains in Mixed Domain Semi-supervised Medical Image Segmentation (Qinghe Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qinghe Ma, Jian Zhang, Lei Qi, Qian Yu, Yinghuan Shi, Yang Gao. (2024)<br><strong>Constructing and Exploring Intermediate Domains in Mixed Domain Semi-supervised Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="Constructing and Exploring Intermediate Domains in Mixed Domain Semi-supervised Medical Image Segmentation" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Knowledge Transfer, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08951v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08951v1.pdf filename=2404.08951v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Both limited annotation and <b>domain</b> <b>shift</b> are prevalent challenges in medical image segmentation. Traditional semi-supervised segmentation and <b>unsupervised</b> <b>domain</b> <b>adaptation</b> methods address one of these issues separately. However, the coexistence of limited annotation and <b>domain</b> <b>shift</b> is quite common, which motivates us to introduce a novel and challenging scenario: Mixed <b>Domain</b> <b>Semi-supervised</b> medical image Segmentation (MiDSS). In this scenario, we handle data from multiple medical centers, with limited annotations available for a single <b>domain</b> <b>and</b> a large amount of unlabeled data from multiple <b>domains.</b> <b>We</b> found that the key to solving the problem lies in how to generate reliable pseudo labels for the unlabeled data in the presence of <b>domain</b> <b>shift</b> with labeled data. To tackle this issue, we employ Unified Copy-Paste (UCP) between images to construct intermediate <b>domains,</b> <b>facilitating</b> the <b>knowledge</b> <b>transfer</b> from the <b>domain</b> <b>of</b> labeled data to the <b>domains</b> <b>of</b> unlabeled data. To fully utilize the information within the intermediate <b>domain,</b> <b>we</b> propose a symmetric Guidance training strategy (SymGD), which additionally offers direct guidance to unlabeled data by merging pseudo labels from intermediate samples. Subsequently, we introduce a Training Process aware Random Amplitude MixUp (TP-RAM) to progressively incorporate style-transition components into intermediate samples. Compared with existing state-of-the-art approaches, our method achieves a notable 13.57% improvement in Dice score on Prostate dataset, as demonstrated on three public datasets. Our code is available at <a href=https://github.com/MQinghe/MiDSS>https://github.com/MQinghe/MiDSS</a> .</p></p class="citation"></blockquote><h3 id=1226--3496-diffusion-models-meet-remote-sensing-principles-methods-and-perspectives-yidan-liu-et-al-2024>(12/26 | 34/96) Diffusion Models Meet Remote Sensing: Principles, Methods, and Perspectives (Yidan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yidan Liu, Jun Yue, Shaobo Xia, Pedram Ghamisi, Weiying Xie, Leyuan Fang. (2024)<br><strong>Diffusion Models Meet Remote Sensing: Principles, Methods, and Perspectives</strong><br><button class=copy-to-clipboard title="Diffusion Models Meet Remote Sensing: Principles, Methods, and Perspectives" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08926v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08926v1.pdf filename=2404.08926v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a newly emerging advance in deep generative models, <b>diffusion</b> <b>models</b> have achieved state-of-the-art results in many fields, including computer vision, natural language processing, and molecule design. The remote sensing community has also noticed the powerful ability of <b>diffusion</b> <b>models</b> and quickly applied them to a variety of tasks for image processing. Given the rapid increase in research on <b>diffusion</b> <b>models</b> in the field of remote sensing, it is necessary to conduct a comprehensive review of existing <b>diffusion</b> <b>model-based</b> remote sensing papers, to help researchers recognize the potential of <b>diffusion</b> <b>models</b> and provide some directions for further exploration. Specifically, this paper first introduces the theoretical background of <b>diffusion</b> <b>models,</b> and then systematically reviews the applications of <b>diffusion</b> <b>models</b> in remote sensing, including image generation, enhancement, and interpretation. Finally, the limitations of existing remote sensing <b>diffusion</b> <b>models</b> and worthy research directions for further exploration are discussed and <b>summarized.</b></p></p class="citation"></blockquote><h3 id=1326--3596-a-lightweight-spatiotemporal-network-for-online-eye-tracking-with-event-camera-yan-ru-pei-et-al-2024>(13/26 | 35/96) A Lightweight Spatiotemporal Network for Online Eye Tracking with Event Camera (Yan Ru Pei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Ru Pei, Sasskia Brüers, Sébastien Crouzet, Douglas McLelland, Olivier Coenen. (2024)<br><strong>A Lightweight Spatiotemporal Network for Online Eye Tracking with Event Camera</strong><br><button class=copy-to-clipboard title="A Lightweight Spatiotemporal Network for Online Eye Tracking with Event Camera" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08858v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08858v1.pdf filename=2404.08858v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Event-based data are commonly encountered in edge computing environments where efficiency and low latency are critical. To interface with such data and leverage their rich temporal features, we propose a causal spatiotemporal <b>convolutional</b> <b>network.</b> This solution targets efficient implementation on edge-appropriate hardware with limited resources in three ways: 1) deliberately targets a simple architecture and set of operations <b>(convolutions,</b> ReLU activations) 2) can be configured to perform online inference efficiently via buffering of layer outputs 3) can achieve more than 90% activation sparsity through regularization during training, enabling very significant efficiency gains on event-based processors. In addition, we propose a general affine augmentation strategy acting directly on the events, which alleviates the problem of dataset scarcity for event-based systems. We apply our model on the AIS 2024 event-based eye tracking challenge, reaching a score of 0.9916 p10 accuracy on the Kaggle private testset.</p></p class="citation"></blockquote><h3 id=1426--3696-understanding-multimodal-deep-neural-networks-a-concept-selection-view-chenming-shang-et-al-2024>(14/26 | 36/96) Understanding Multimodal Deep Neural Networks: A Concept Selection View (Chenming Shang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenming Shang, Hengyuan Zhang, Hao Wen, Yujiu Yang. (2024)<br><strong>Understanding Multimodal Deep Neural Networks: A Concept Selection View</strong><br><button class=copy-to-clipboard title="Understanding Multimodal Deep Neural Networks: A Concept Selection View" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 16<br>Keywords: Black Box, Deep Neural Network, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08964v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08964v1.pdf filename=2404.08964v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>multimodal</b> <b>deep</b> <b>neural</b> <b>networks,</b> represented by CLIP, have generated rich downstream applications owing to their excellent performance, thus making understanding the decision-making process of CLIP an essential research topic. Due to the complex structure and the massive pre-training data, it is often regarded as a <b>black-box</b> <b>model</b> that is too difficult to understand and interpret. Concept-based models map the <b>black-box</b> <b>visual</b> representations extracted by <b>deep</b> <b>neural</b> <b>networks</b> onto a set of human-understandable concepts and use the concepts to make predictions, enhancing the transparency of the decision-making process. However, these methods involve the datasets labeled with fine-grained attributes by expert knowledge, which incur high costs and introduce excessive human prior knowledge and bias. In this paper, we observe the long-tail distribution of concepts, based on which we propose a two-stage Concept Selection Model (CSM) to mine core concepts without introducing any human priors. The concept greedy rough selection algorithm is applied to extract head concepts, and then the concept mask fine selection method performs the extraction of core concepts. Experiments show that our approach achieves comparable performance to end-to-end <b>black-box</b> <b>models,</b> and human evaluation demonstrates that the concepts discovered by our method are interpretable and comprehensible for humans.</p></p class="citation"></blockquote><h3 id=1526--3796-trustworthy-multimodal-fusion-for-sentiment-analysis-in-ordinal-sentiment-space-zhuyang-xie-et-al-2024>(15/26 | 37/96) Trustworthy Multimodal Fusion for Sentiment Analysis in Ordinal Sentiment Space (Zhuyang Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuyang Xie, Yan Yang, Jie Wang, Xiaorong Liu, Xiaofan Li. (2024)<br><strong>Trustworthy Multimodal Fusion for Sentiment Analysis in Ordinal Sentiment Space</strong><br><button class=copy-to-clipboard title="Trustworthy Multimodal Fusion for Sentiment Analysis in Ordinal Sentiment Space" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08923v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08923v1.pdf filename=2404.08923v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> video <b>sentiment</b> <b>analysis</b> aims to integrate multiple modal information to analyze the opinions and attitudes of speakers. Most previous work focuses on exploring the semantic interactions of intra- and inter-modality. However, these works ignore the reliability of multimodality, i.e., modalities tend to contain noise, semantic ambiguity, missing modalities, etc. In addition, previous <b>multimodal</b> approaches treat different modalities equally, largely ignoring their different contributions. Furthermore, existing <b>multimodal</b> <b>sentiment</b> <b>analysis</b> methods directly regress <b>sentiment</b> <b>scores</b> without considering ordinal relationships within <b>sentiment</b> <b>categories,</b> with limited performance. To address the aforementioned problems, we propose a trustworthy <b>multimodal</b> <b>sentiment</b> <b>ordinal</b> network (TMSON) to improve performance in <b>sentiment</b> <b>analysis.</b> Specifically, we first devise a unimodal feature extractor for each modality to obtain modality-specific features. Then, an uncertainty distribution estimation network is customized, which estimates the unimodal uncertainty distributions. Next, Bayesian fusion is performed on the learned unimodal distributions to obtain <b>multimodal</b> distributions for <b>sentiment</b> <b>prediction.</b> Finally, an ordinal-aware <b>sentiment</b> <b>space</b> is constructed, where ordinal regression is used to constrain the <b>multimodal</b> distributions. Our proposed TMSON outperforms baselines on <b>multimodal</b> <b>sentiment</b> <b>analysis</b> tasks, and empirical results demonstrate that TMSON is capable of reducing uncertainty to obtain more robust predictions.</p></p class="citation"></blockquote><h3 id=1626--3896-eiven-efficient-implicit-attribute-value-extraction-using-multimodal-llm-henry-peng-zou-et-al-2024>(16/26 | 38/96) EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal LLM (Henry Peng Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Henry Peng Zou, Gavin Heqing Yu, Ziwei Fan, Dan Bu, Han Liu, Peng Dai, Dongmei Jia, Cornelia Caragea. (2024)<br><strong>EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal LLM</strong><br><button class=copy-to-clipboard title="EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal LLM" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-IR, cs-LG, cs.CV<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08886v1.pdf filename=2404.08886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In e-commerce, accurately extracting product attribute values from <b>multimodal</b> data is crucial for improving user experience and operational efficiency of retailers. However, previous approaches to <b>multimodal</b> attribute value extraction often struggle with implicit attribute values embedded in images or text, rely heavily on extensive labeled data, and can easily confuse similar attribute values. To address these issues, we introduce EIVEN, a data- and parameter-efficient generative framework that pioneers the use of <b>multimodal</b> <b>LLM</b> for implicit attribute value extraction. EIVEN leverages the rich inherent knowledge of a pre-trained <b>LLM</b> and vision encoder to reduce reliance on labeled data. We also introduce a novel Learning-by-Comparison technique to reduce model confusion by enforcing attribute value comparison and difference identification. Additionally, we construct initial open-source datasets for <b>multimodal</b> implicit attribute value extraction. Our extensive experiments reveal that EIVEN significantly outperforms existing methods in extracting implicit attribute values while requiring less labeled data.</p></p class="citation"></blockquote><h3 id=1726--3996-fast-fishing-approximating-bait-for-efficient-and-scalable-deep-active-image-classification-denis-huseljic-et-al-2024>(17/26 | 39/96) Fast Fishing: Approximating BAIT for Efficient and Scalable Deep Active Image Classification (Denis Huseljic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Denis Huseljic, Paul Hahn, Marek Herde, Lukas Rauch, Bernhard Sick. (2024)<br><strong>Fast Fishing: Approximating BAIT for Efficient and Scalable Deep Active Image Classification</strong><br><button class=copy-to-clipboard title="Fast Fishing: Approximating BAIT for Efficient and Scalable Deep Active Image Classification" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 15<br>Keywords: Active Learning, Deep Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08981v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08981v1.pdf filename=2404.08981v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Deep</b> <b>active</b> <b>learning</b> (AL) seeks to minimize the annotation costs for training <b>deep</b> <b>neural</b> <b>networks.</b> BAIT, a recently proposed AL strategy based on the Fisher Information, has demonstrated impressive performance across various datasets. However, BAIT&rsquo;s high computational and memory requirements hinder its applicability on large-scale classification tasks, resulting in current research neglecting BAIT in their evaluation. This paper introduces two methods to enhance BAIT&rsquo;s computational efficiency and scalability. Notably, we significantly reduce its time complexity by approximating the Fisher Information. In particular, we adapt the original formulation by i) taking the expectation over the most probable classes, and ii) constructing a binary classification task, leading to an alternative likelihood for gradient computations. Consequently, this allows the efficient use of BAIT on large-scale datasets, including ImageNet. Our unified and comprehensive evaluation across a variety of datasets demonstrates that our approximations achieve strong performance with considerably reduced time complexity. Furthermore, we provide an extensive open-source toolbox that implements recent state-of-the-art AL strategies, available at <a href=https://github.com/dhuseljic/dal-toolbox>https://github.com/dhuseljic/dal-toolbox</a>.</p></p class="citation"></blockquote><h3 id=1826--4096-mcpnet-an-interpretable-classifier-via-multi-level-concept-prototypes-bor-shiun-wang-et-al-2024>(18/26 | 40/96) MCPNet: An Interpretable Classifier via Multi-Level Concept Prototypes (Bor-Shiun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bor-Shiun Wang, Chien-Yi Wang, Wei-Chen Chiu. (2024)<br><strong>MCPNet: An Interpretable Classifier via Multi-Level Concept Prototypes</strong><br><button class=copy-to-clipboard title="MCPNet: An Interpretable Classifier via Multi-Level Concept Prototypes" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 15<br>Keywords: Black Box, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08968v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08968v1.pdf filename=2404.08968v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in post-hoc and inherently interpretable methods have markedly enhanced the explanations of <b>black</b> <b>box</b> classifier models. These methods operate either through post-analysis or by integrating concept learning during model training. Although being effective in bridging the semantic gap between a model&rsquo;s latent space and human interpretation, these explanation methods only partially reveal the model&rsquo;s decision-making process. The outcome is typically limited to high-level semantics derived from the last feature map. We argue that the explanations lacking insights into the decision processes at low and mid-level features are neither fully faithful nor useful. Addressing this gap, we introduce the Multi-Level Concept Prototypes Classifier (MCPNet), an inherently interpretable model. MCPNet autonomously learns meaningful concept prototypes across multiple feature map levels using Centered Kernel Alignment (CKA) loss and an energy-based weighted PCA mechanism, and it does so without reliance on predefined concept labels. Further, we propose a novel classifier paradigm that learns and aligns multi-level concept prototype distributions for classification purposes via Class-aware Concept Distribution (CCD) loss. Our experiments reveal that our proposed MCPNet while being adaptable to various model architectures, offers comprehensive multi-level explanations while maintaining classification accuracy. Additionally, its concept distribution-based classification approach shows improved generalization capabilities in <b>few-shot</b> classification scenarios.</p></p class="citation"></blockquote><h3 id=1926--4196-loopgaussian-creating-3d-cinemagraph-with-multi-view-images-via-eulerian-motion-field-jiyang-li-et-al-2024>(19/26 | 41/96) LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via Eulerian Motion Field (Jiyang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiyang Li, Lechao Cheng, Zhangye Wang, Tingting Mu, Jingxuan He. (2024)<br><strong>LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via Eulerian Motion Field</strong><br><button class=copy-to-clipboard title="LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via Eulerian Motion Field" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Autoencoder, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08966v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08966v2.pdf filename=2404.08966v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cinemagraph is a unique form of visual media that combines elements of still photography and subtle motion to create a captivating experience. However, the majority of videos generated by recent works lack depth information and are confined to the constraints of 2D image space. In this paper, inspired by significant progress in the field of novel view synthesis (NVS) achieved by 3D Gaussian Splatting (3D-GS), we propose LoopGaussian to elevate cinemagraph from 2D image space to 3D space using 3D Gaussian modeling. To achieve this, we first employ the 3D-GS method to reconstruct 3D Gaussian point clouds from multi-view images of static scenes,incorporating shape regularization terms to prevent blurring or artifacts caused by object deformation. We then adopt an <b>autoencoder</b> tailored for 3D Gaussian to project it into feature space. To maintain the local continuity of the scene, we devise SuperGaussian for <b>clustering</b> based on the acquired features. By calculating the similarity between clusters and employing a two-stage estimation method, we derive an Eulerian motion field to describe velocities across the entire scene. The 3D Gaussian points then move within the estimated Eulerian motion field. Through bidirectional animation techniques, we ultimately generate a 3D Cinemagraph that exhibits natural and seamlessly loopable dynamics. Experiment results validate the effectiveness of our approach, demonstrating high-quality and visually appealing scene generation. The project is available at <a href=https://pokerlishao.github.io/LoopGaussian/>https://pokerlishao.github.io/LoopGaussian/</a>.</p></p class="citation"></blockquote><h3 id=2026--4296-seeing-text-in-the-dark-algorithm-and-benchmark-chengpei-xu-et-al-2024>(20/26 | 42/96) Seeing Text in the Dark: Algorithm and Benchmark (Chengpei Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengpei Xu, Hao Fu, Long Ma, Wenjing Jia, Chengqi Zhang, Feng Xia, Xiaoyu Ai, Binghao Li, Wenjie Zhang. (2024)<br><strong>Seeing Text in the Dark: Algorithm and Benchmark</strong><br><button class=copy-to-clipboard title="Seeing Text in the Dark: Algorithm and Benchmark" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Constrained Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08965v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08965v1.pdf filename=2404.08965v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Localizing text in low-light environments is challenging due to visual degradations. Although a straightforward solution involves a two-stage pipeline with low-light image enhancement (LLE) as the initial step followed by detector, LLE is primarily designed for human vision instead of machine and can accumulate errors. In this work, we propose an efficient and effective single-stage approach for localizing text in dark that circumvents the need for LLE. We introduce a <b>constrained</b> <b>learning</b> module as an auxiliary mechanism during the training stage of the text detector. This module is designed to guide the text detector in preserving textual spatial features amidst feature map resizing, thus minimizing the loss of spatial information in texts under low-light visual degradations. Specifically, we incorporate spatial reconstruction and spatial semantic constraints within this module to ensure the text detector acquires essential positional and contextual range knowledge. Our approach enhances the original text detector&rsquo;s ability to identify text&rsquo;s local topological features using a dynamic snake feature pyramid network and adopts a bottom-up contour shaping strategy with a novel rectangular accumulation technique for accurate delineation of streamlined text features. In addition, we present a comprehensive low-light dataset for arbitrary-shaped text, encompassing diverse scenes and languages. Notably, our method achieves state-of-the-art results on this low-light dataset and exhibits comparable performance on standard normal light datasets. The code and dataset will be released.</p></p class="citation"></blockquote><h3 id=2126--4396-dedode-v2-analyzing-and-improving-the-dedode-keypoint-detector-johan-edstedt-et-al-2024>(21/26 | 43/96) DeDoDe v2: Analyzing and Improving the DeDoDe Keypoint Detector (Johan Edstedt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johan Edstedt, Georg Bökman, Zhenjun Zhao. (2024)<br><strong>DeDoDe v2: Analyzing and Improving the DeDoDe Keypoint Detector</strong><br><button class=copy-to-clipboard title="DeDoDe v2: Analyzing and Improving the DeDoDe Keypoint Detector" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08928v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08928v1.pdf filename=2404.08928v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we analyze and improve into the recently proposed DeDoDe keypoint detector. We focus our analysis on some key issues. First, we find that DeDoDe keypoints tend to cluster together, which we fix by performing non-max suppression on the target distribution of the detector during training. Second, we address issues related to <b>data</b> <b>augmentation.</b> In particular, the DeDoDe detector is sensitive to large rotations. We fix this by including 90-degree rotations as well as horizontal flips. Finally, the decoupled nature of the DeDoDe detector makes evaluation of downstream usefulness problematic. We fix this by matching the keypoints with a pretrained dense matcher (RoMa) and evaluating two-view pose estimates. We find that the original long training is detrimental to performance, and therefore propose a much shorter training schedule. We integrate all these improvements into our proposed detector DeDoDe v2 and evaluate it with the original DeDoDe descriptor on the MegaDepth-1500 and IMC2022 <b>benchmarks.</b> Our proposed detector significantly increases pose estimation results, notably from 75.9 to 78.3 mAA on the IMC2022 challenge. Code and weights are available at <a href=https://github.com/Parskatt/DeDoDe>https://github.com/Parskatt/DeDoDe</a></p></p class="citation"></blockquote><h3 id=2226--4496-shifting-spotlight-for-co-supervision-a-simple-yet-efficient-single-branch-network-to-see-through-camouflage-yang-hu-et-al-2024>(22/26 | 44/96) Shifting Spotlight for Co-supervision: A Simple yet Efficient Single-branch Network to See Through Camouflage (Yang Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Hu, Jinxia Zhang, Kaihua Zhang, Yin Yuan. (2024)<br><strong>Shifting Spotlight for Co-supervision: A Simple yet Efficient Single-branch Network to See Through Camouflage</strong><br><button class=copy-to-clipboard title="Shifting Spotlight for Co-supervision: A Simple yet Efficient Single-branch Network to See Through Camouflage" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08936v1.pdf filename=2404.08936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient and accurate camouflaged <b>object</b> <b>detection</b> (COD) poses a challenge in the field of computer vision. Recent approaches explored the utility of edge information for network co-supervision, achieving notable advancements. However, these approaches introduce an extra branch for complex edge extraction, complicate the model architecture and increases computational demands. Addressing this issue, our work replicates the effect that animal&rsquo;s camouflage can be easily revealed under a shifting spotlight, and leverages it for network co-supervision to form a compact yet efficient single-branch network, the Co-Supervised Spotlight Shifting Network (CS$^3$Net). The spotlight shifting strategy allows CS$^3$Net to learn additional prior within a single-branch framework, obviating the need for resource demanding multi-branch design. To leverage the prior of spotlight shifting co-supervision, we propose Shadow Refinement Module (SRM) and Projection Aware Attention (PAA) for feature refinement and enhancement. To ensure the continuity of multi-scale features aggregation, we utilize the Extended Neighbor Connection Decoder (ENCD) for generating the final predictions. Empirical evaluations on public datasets confirm that our CS$^3$Net offers an optimal balance between efficiency and performance: it accomplishes a 32.13% reduction in Multiply-Accumulate (MACs) operations compared to leading efficient COD models, while also delivering superior performance.</p></p class="citation"></blockquote><h3 id=2326--4596-pnerv-enhancing-spatial-consistency-via-pyramidal-neural-representation-for-videos-qi-zhao-et-al-2024>(23/26 | 45/96) PNeRV: Enhancing Spatial Consistency via Pyramidal Neural Representation for Videos (Qi Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Zhao, M. Salman Asif, Zhan Ma. (2024)<br><strong>PNeRV: Enhancing Spatial Consistency via Pyramidal Neural Representation for Videos</strong><br><button class=copy-to-clipboard title="PNeRV: Enhancing Spatial Consistency via Pyramidal Neural Representation for Videos" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Tensor Decomposition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08921v1.pdf filename=2404.08921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The primary focus of Neural Representation for Videos (NeRV) is to effectively model its spatiotemporal consistency. However, current NeRV systems often face a significant issue of spatial inconsistency, leading to decreased perceptual quality. To address this issue, we introduce the Pyramidal Neural Representation for Videos (PNeRV), which is built on a multi-scale information connection and comprises a lightweight rescaling operator, Kronecker Fully-connected layer (KFc), and a Benign Selective Memory (BSM) mechanism. The KFc, inspired by the <b>tensor</b> <b>decomposition</b> of the vanilla Fully-connected layer, facilitates low-cost rescaling and global correlation modeling. BSM merges high-level features with granular ones adaptively. Furthermore, we provide an analysis based on the Universal Approximation Theory of the NeRV system and validate the effectiveness of the proposed PNeRV.We conducted comprehensive experiments to demonstrate that PNeRV surpasses the performance of contemporary NeRV models, achieving the best results in video regression on UVG and DAVIS under various metrics (PSNR, SSIM, LPIPS, and FVD). Compared to vanilla NeRV, PNeRV achieves a +4.49 dB gain in PSNR and a 231% increase in FVD on UVG, along with a +3.28 dB PSNR and 634% FVD increase on DAVIS.</p></p class="citation"></blockquote><h3 id=2426--4696-exploring-explainability-in-video-action-recognition-avinab-saha-et-al-2024>(24/26 | 46/96) Exploring Explainability in Video Action Recognition (Avinab Saha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Avinab Saha, Shashank Gupta, Sravan Kumar Ankireddy, Karl Chahine, Joydeep Ghosh. (2024)<br><strong>Exploring Explainability in Video Action Recognition</strong><br><button class=copy-to-clipboard title="Exploring Explainability in Video Action Recognition" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Deep Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09067v1.pdf filename=2404.09067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image Classification and Video Action Recognition are perhaps the two most foundational tasks in computer vision. Consequently, explaining the inner workings of trained <b>deep</b> <b>neural</b> <b>networks</b> is of prime importance. While numerous efforts focus on explaining the decisions of trained <b>deep</b> <b>neural</b> <b>networks</b> in image classification, exploration in the domain of its temporal version, video action recognition, has been scant. In this work, we take a deeper look at this problem. We begin by revisiting Grad-CAM, one of the popular feature attribution methods for Image Classification, and its extension to Video Action Recognition tasks and examine the method&rsquo;s limitations. To address these, we introduce Video-TCAV, by building on TCAV for Image Classification tasks, which aims to quantify the importance of specific concepts in the decision-making process of Video Action Recognition models. As the scalable generation of concepts is still an open problem, we propose a machine-assisted approach to generate spatial and spatiotemporal concepts relevant to Video Action Recognition for testing Video-TCAV. We then establish the importance of temporally-varying concepts by demonstrating the superiority of dynamic spatiotemporal concepts over trivial spatial concepts. In conclusion, we introduce a framework for investigating hypotheses in action recognition and quantitatively testing them, thus advancing research in the explainability of <b>deep</b> <b>neural</b> <b>networks</b> used in video action recognition.</p></p class="citation"></blockquote><h3 id=2526--4796-maprotonet-a-multi-scale-attentive-interpretable-prototypical-part-network-for-3d-magnetic-resonance-imaging-brain-tumor-classification-binghua-li-et-al-2024>(25/26 | 47/96) MAProtoNet: A Multi-scale Attentive Interpretable Prototypical Part Network for 3D Magnetic Resonance Imaging Brain Tumor Classification (Binghua Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Binghua Li, Jie Mao, Zhe Sun, Chao Li, Qibin Zhao, Toshihisa Tanaka. (2024)<br><strong>MAProtoNet: A Multi-scale Attentive Interpretable Prototypical Part Network for 3D Magnetic Resonance Imaging Brain Tumor Classification</strong><br><button class=copy-to-clipboard title="MAProtoNet: A Multi-scale Attentive Interpretable Prototypical Part Network for 3D Magnetic Resonance Imaging Brain Tumor Classification" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Deep Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08917v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08917v1.pdf filename=2404.08917v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated diagnosis with artificial intelligence has emerged as a promising area in the realm of medical imaging, while the interpretability of the introduced <b>deep</b> <b>neural</b> <b>networks</b> still remains an urgent concern. Although contemporary works, such as XProtoNet and MProtoNet, has sought to design interpretable prediction models for the issue, the localization precision of their resulting attribution maps can be further improved. To this end, we propose a Multi-scale Attentive Prototypical part Network, termed MAProtoNet, to provide more precise maps for attribution. Specifically, we introduce a concise multi-scale module to merge attentive features from quadruplet attention layers, and produces attribution maps. The proposed quadruplet attention layers can enhance the existing online class activation mapping loss via capturing interactions between the spatial and channel dimension, while the multi-scale module then fuses both fine-grained and coarse-grained information for precise maps generation. We also apply a novel multi-scale mapping loss for supervision on the proposed multi-scale module. Compared to existing interpretable prototypical part networks in medical imaging, MAProtoNet can achieve state-of-the-art performance in localization on brain tumor segmentation (BraTS) datasets, resulting in approximately 4% overall improvement on activation precision score (with a best score of 85.8%), without using additional annotated labels of segmentation. Our code will be released in <a href=https://github.com/TUAT-Novice/maprotonet>https://github.com/TUAT-Novice/maprotonet</a>.</p></p class="citation"></blockquote><h3 id=2626--4896-a-fourier-enhanced-multi-modal-3d-small-object-optical-mark-recognition-and-positioning-method-for-percutaneous-abdominal-puncture-surgical-navigation-zezhao-guo-et-al-2024>(26/26 | 48/96) A Fourier-enhanced multi-modal 3D small object optical mark recognition and positioning method for percutaneous abdominal puncture surgical navigation (Zezhao Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zezhao Guo, Yanzhong Guo, Zhanfang Zhao. (2024)<br><strong>A Fourier-enhanced multi-modal 3D small object optical mark recognition and positioning method for percutaneous abdominal puncture surgical navigation</strong><br><button class=copy-to-clipboard title="A Fourier-enhanced multi-modal 3D small object optical mark recognition and positioning method for percutaneous abdominal puncture surgical navigation" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08990v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08990v1.pdf filename=2404.08990v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Navigation for thoracoabdominal puncture surgery is used to locate the needle entry point on the patient&rsquo;s body surface. The traditional reflective ball navigation method is difficult to position the needle entry point on the soft, irregular, smooth chest and abdomen. Due to the lack of clear characteristic points on the body surface using structured light technology, it is difficult to identify and locate arbitrary needle insertion points. Based on the high stability and high accuracy requirements of surgical navigation, this paper proposed a novel method, a muti-modal 3D small object medical marker detection method, which identifies the center of a small single ring as the needle insertion point. Moreover, this novel method leverages Fourier transform enhancement technology to augment the dataset, enrich image details, and enhance the network&rsquo;s capability. The method extracts the Region of Interest (ROI) of the feature image from both enhanced and original images, followed by generating a mask map. Subsequently, the point cloud of the ROI from the depth map is obtained through the registration of ROI point cloud contour fitting. In addition, this method employs Tukey loss for optimal precision. The experimental results show this novel method proposed in this paper not only achieves high-precision and high-stability positioning, but also enables the positioning of any needle insertion point.</p></p class="citation"></blockquote><h2 id=csni-2>cs.NI (2)</h2><h3 id=12--4996-generative-ai-agent-for-next-generation-mimo-design-fundamentals-challenges-and-vision-zhe-wang-et-al-2024>(1/2 | 49/96) Generative AI Agent for Next-Generation MIMO Design: Fundamentals, Challenges, and Vision (Zhe Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhe Wang, Jiayi Zhang, Hongyang Du, Ruichen Zhang, Dusit Niyato, Bo Ai, Khaled B. Letaief. (2024)<br><strong>Generative AI Agent for Next-Generation MIMO Design: Fundamentals, Challenges, and Vision</strong><br><button class=copy-to-clipboard title="Generative AI Agent for Next-Generation MIMO Design: Fundamentals, Challenges, and Vision" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-IT, cs-LG, cs-NI, cs.NI, eess-SP, math-IT<br>Keyword Score: 60<br>Keywords: Generative AI, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08878v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08878v1.pdf filename=2404.08878v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Next-generation multiple input multiple output (MIMO) is expected to be intelligent and scalable. In this paper, we study <b>generative</b> <b>artificial</b> intelligence (AI) agent-enabled next-generation MIMO design. Firstly, we provide an overview of the development, fundamentals, and challenges of the next-generation MIMO. Then, we propose the concept of the <b>generative</b> <b>AI</b> agent, which is capable of generating tailored and specialized contents with the aid of <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> and <b>retrieval</b> <b>augmented</b> <b>generation</b> <b>(RAG).</b> Next, we comprehensively discuss the features and advantages of the <b>generative</b> <b>AI</b> agent framework. More importantly, to tackle existing challenges of next-generation MIMO, we discuss <b>generative</b> <b>AI</b> agent-enabled next-generation MIMO design, from the perspective of performance analysis, signal processing, and resource allocation. Furthermore, we present two compelling case studies that demonstrate the effectiveness of leveraging the <b>generative</b> <b>AI</b> agent for performance analysis in complex configuration scenarios. These examples highlight how the integration of <b>generative</b> <b>AI</b> agents can significantly enhance the analysis and design of next-generation MIMO systems. Finally, we discuss important potential research future directions.</p></p class="citation"></blockquote><h3 id=22--5096-prosecutor-protecting-mobile-aigc-services-on-two-layer-blockchain-via-reputation-and-contract-theoretic-approaches-yinqiu-liu-et-al-2024>(2/2 | 50/96) ProSecutor: Protecting Mobile AIGC Services on Two-Layer Blockchain via Reputation and Contract Theoretic Approaches (Yinqiu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinqiu Liu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Abbas Jamalipour, Xuemin, Shen. (2024)<br><strong>ProSecutor: Protecting Mobile AIGC Services on Two-Layer Blockchain via Reputation and Contract Theoretic Approaches</strong><br><button class=copy-to-clipboard title="ProSecutor: Protecting Mobile AIGC Services on Two-Layer Blockchain via Reputation and Contract Theoretic Approaches" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Generative AI, High-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08899v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08899v1.pdf filename=2404.08899v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mobile AI-Generated Content (AIGC) has achieved great attention in unleashing the power of <b>generative</b> <b>AI</b> and scaling the AIGC services. By employing numerous Mobile AIGC Service Providers (MASPs), ubiquitous and low-latency AIGC services for clients can be realized. Nonetheless, the interactions between clients and MASPs in public mobile networks, pertaining to three key mechanisms, namely MASP selection, payment scheme, and fee-ownership transfer, are unprotected. In this paper, we design the above mechanisms using a systematic approach and present the first blockchain to protect mobile AIGC, called ProSecutor. Specifically, by roll-up and layer-2 channels, ProSecutor forms a two-layer architecture, realizing tamper-proof data recording and atomic fee-ownership transfer with high resource efficiency. Then, we present the Objective-Subjective Service Assessment (OS^{2}A) framework, which effectively evaluates the AIGC services by fusing the objective service quality with the reputation-based subjective experience of the service outcome (i.e., AIGC outputs). Deploying OS^{2}A on ProSecutor, firstly, the MASP selection can be realized by sorting the reputation. Afterward, the contract theory is adopted to optimize the payment scheme and help clients avoid moral hazards in mobile networks. We implement the prototype of ProSecutor on BlockEmulator.Extensive experiments demonstrate that ProSecutor achieves 12.5x throughput and saves 67.5% storage resources compared with BlockEmulator. Moreover, the effectiveness and efficiency of the proposed mechanisms are validated.</p></p class="citation"></blockquote><h2 id=csro-5>cs.RO (5)</h2><h3 id=15--5196-safe-reinforcement-learning-on-the-constraint-manifold-theory-and-applications-puze-liu-et-al-2024>(1/5 | 51/96) Safe Reinforcement Learning on the Constraint Manifold: Theory and Applications (Puze Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Puze Liu, Haitham Bou-Ammar, Jan Peters, Davide Tateo. (2024)<br><strong>Safe Reinforcement Learning on the Constraint Manifold: Theory and Applications</strong><br><button class=copy-to-clipboard title="Safe Reinforcement Learning on the Constraint Manifold: Theory and Applications" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 55<br>Keywords: Fine-tuning, Fine-tuning, Geometry, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09080v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09080v1.pdf filename=2404.09080v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integrating learning-based techniques, especially <b>reinforcement</b> <b>learning,</b> into robotics is promising for solving complex problems in unstructured environments. However, most existing approaches are trained in well-tuned simulators and subsequently deployed on real robots without online <b>fine-tuning.</b> In this setting, the <b>simulation&rsquo;s</b> realism seriously impacts the deployment&rsquo;s success rate. Instead, learning with real-world interaction data offers a promising alternative: not only eliminates the need for a <b>fine-tuned</b> simulator but also applies to a broader range of tasks where accurate modeling is unfeasible. One major problem for on-robot <b>reinforcement</b> <b>learning</b> is ensuring safety, as uncontrolled exploration can cause catastrophic damage to the robot or the environment. Indeed, safety specifications, often represented as constraints, can be complex and non-linear, making safety challenging to guarantee in learning systems. In this paper, we show how we can impose complex safety constraints on learning-based robotics systems in a principled manner, both from theoretical and practical points of view. Our approach is based on the concept of the Constraint Manifold, representing the set of safe robot configurations. Exploiting differential <b>geometry</b> techniques, i.e., the tangent space, we can construct a safe action space, allowing learning agents to sample arbitrary actions while ensuring safety. We demonstrate the method&rsquo;s effectiveness in a real-world Robot Air Hockey task, showing that our method can handle high-dimensional tasks with complex constraints. Videos of the real robot experiments are available on the project website (<a href=https://puzeliu.github.io/TRO-ATACOM)>https://puzeliu.github.io/TRO-ATACOM)</a>.</p></p class="citation"></blockquote><h3 id=25--5296-neurit-pushing-the-limit-of-neural-inertial-tracking-for-indoor-robotic-iot-xinzhe-zheng-et-al-2024>(2/5 | 52/96) NeurIT: Pushing the Limit of Neural Inertial Tracking for Indoor Robotic IoT (Xinzhe Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinzhe Zheng, Sijie Ji, Yipeng Pan, Kaiwen Zhang, Chenshu Wu. (2024)<br><strong>NeurIT: Pushing the Limit of Neural Inertial Tracking for Indoor Robotic IoT</strong><br><button class=copy-to-clipboard title="NeurIT: Pushing the Limit of Neural Inertial Tracking for Indoor Robotic IoT" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-HC, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Recurrent Neural Network, Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08939v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08939v1.pdf filename=2404.08939v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inertial tracking is vital for robotic IoT and has gained popularity thanks to the ubiquity of low-cost Inertial Measurement Units (IMUs) and deep learning-powered tracking algorithms. Existing works, however, have not fully utilized IMU measurements, particularly magnetometers, nor maximized the potential of deep learning to achieve the desired accuracy. To enhance the tracking accuracy for indoor robotic applications, we introduce NeurIT, a sequence-to-sequence framework that elevates tracking accuracy to a new level. NeurIT employs a Time-Frequency Block-recurrent <b>Transformer</b> (TF-BRT) at its core, combining the power of <b>recurrent</b> <b>neural</b> <b>network</b> <b>(RNN)</b> and <b>Transformer</b> to learn representative features in both time and frequency domains. To fully utilize IMU information, we strategically employ body-frame differentiation of the magnetometer, which considerably reduces the tracking error. NeurIT is implemented on a customized robotic platform and evaluated in various indoor environments. Experimental results demonstrate that NeurIT achieves a mere 1-meter tracking error over a 300-meter distance. Notably, it significantly outperforms state-of-the-art baselines by 48.21% on unseen data. NeurIT also performs comparably to the visual-inertial approach (Tango Phone) in vision-favored conditions and surpasses it in plain environments. We believe NeurIT takes an important step forward toward practical neural inertial tracking for ubiquitous and scalable tracking of robotic things. NeurIT, including the source code and the dataset, is open-sourced here: <a href=https://github.com/NeurIT-Project/NeurIT>https://github.com/NeurIT-Project/NeurIT</a>.</p></p class="citation"></blockquote><h3 id=35--5396-airship-formations-for-animal-motion-capture-and-behavior-analysis-eric-price-et-al-2024>(3/5 | 53/96) Airship Formations for Animal Motion Capture and Behavior Analysis (Eric Price et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric Price, Aamir Ahmad. (2024)<br><strong>Airship Formations for Animal Motion Capture and Behavior Analysis</strong><br><button class=copy-to-clipboard title="Airship Formations for Animal Motion Capture and Behavior Analysis" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08986v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08986v1.pdf filename=2404.08986v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Using UAVs for wildlife observation and motion capture offers manifold advantages for studying animals in the wild, especially grazing herds in open terrain. The aerial perspective allows observation at a scale and depth that is not possible on the ground, offering new insights into group behavior. However, the very nature of wildlife field-studies puts traditional fixed wing and multi-copter systems to their limits: limited flight time, noise and safety aspects affect their efficacy, where lighter than air systems can remain on station for many hours. Nevertheless, airships are challenging from a ground handling perspective as well as from a control point of view, being voluminous and highly affected by wind. In this work, we showcase a system designed to use airship formations to track, follow, and visually record wild horses from multiple angles, including airship design, <b>simulation,</b> control, on board computer vision, autonomous operation and practical aspects of field experiments.</p></p class="citation"></blockquote><h3 id=45--5496-physics-aware-iterative-learning-and-prediction-of-saliency-map-for-bimanual-grasp-planning-shiyao-wang-et-al-2024>(4/5 | 54/96) Physics-Aware Iterative Learning and Prediction of Saliency Map for Bimanual Grasp Planning (Shiyao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiyao Wang, Xiuping Liu, Charlie C. L. Wang, Jian Liu. (2024)<br><strong>Physics-Aware Iterative Learning and Prediction of Saliency Map for Bimanual Grasp Planning</strong><br><button class=copy-to-clipboard title="Physics-Aware Iterative Learning and Prediction of Saliency Map for Bimanual Grasp Planning" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08944v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08944v1.pdf filename=2404.08944v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning the skill of human bimanual grasping can extend the capabilities of robotic systems when grasping large or heavy objects. However, it requires a much larger search space for grasp points than single-hand grasping and numerous bimanual grasping annotations for network learning, making both data-driven or analytical grasping methods inefficient and insufficient. We propose a framework for bimanual grasp saliency learning that aims to predict the contact points for bimanual grasping based on existing human single-handed grasping data. We learn saliency corresponding vectors through minimal bimanual contact annotations that establishes correspondences between grasp positions of both hands, capable of eliminating the need for training a large-scale bimanual grasp dataset. The existing single-handed grasp saliency value serves as the initial value for bimanual grasp saliency, and we learn a saliency adjusted score that adds the initial value to obtain the final bimanual grasp saliency value, capable of predicting preferred bimanual grasp positions from single-handed grasp saliency. We also introduce a physics-balance loss function and a physics-aware refinement module that enables physical grasp balance, capable of enhancing the generalization of unknown objects. Comprehensive experiments in <b>simulation</b> and comparisons on dexterous grippers have demonstrated that our method can achieve balanced bimanual grasping effectively.</p></p class="citation"></blockquote><h3 id=55--5596-learning-surface-terrain-classifications-from-ground-penetrating-radar-anja-sheppard-et-al-2024>(5/5 | 55/96) Learning Surface Terrain Classifications from Ground Penetrating Radar (Anja Sheppard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anja Sheppard, Jason Brown, Nilton Renno, Katherine A. Skinner. (2024)<br><strong>Learning Surface Terrain Classifications from Ground Penetrating Radar</strong><br><button class=copy-to-clipboard title="Learning Surface Terrain Classifications from Ground Penetrating Radar" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09094v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09094v1.pdf filename=2404.09094v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Terrain classification is an important problem for mobile robots operating in extreme environments as it can aid downstream tasks such as autonomous navigation and planning. While RGB cameras are widely used for terrain identification, vision-based methods can suffer due to poor lighting conditions and occlusions. In this paper, we propose the novel use of Ground Penetrating Radar (GPR) for terrain characterization for mobile robot platforms. Our approach leverages machine learning for surface terrain classification from GPR data. We collect a new dataset consisting of four different terrain types, and present qualitative and quantitative results. Our results demonstrate that classification networks can learn terrain categories from GPR signals. Additionally, we integrate our GPR-based classification approach into a <b>multimodal</b> semantic mapping framework to demonstrate a practical use case of GPR for surface terrain classification on mobile robots. Overall, this work extends the usability of GPR sensors deployed on robots to enable terrain classification in addition to GPR&rsquo;s existing scientific use cases.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--5696-is-next-token-prediction-sufficient-for-gpt-exploration-on-code-logic-comprehension-mengnan-qi-et-al-2024>(1/1 | 56/96) Is Next Token Prediction Sufficient for GPT? Exploration on Code Logic Comprehension (Mengnan Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengnan Qi, Yufan Huang, Yongqiang Yao, Maoquan Wang, Bin Gu, Neel Sundaresan. (2024)<br><strong>Is Next Token Prediction Sufficient for GPT? Exploration on Code Logic Comprehension</strong><br><button class=copy-to-clipboard title="Is Next Token Prediction Sufficient for GPT? Exploration on Code Logic Comprehension" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-CL, cs-LG, cs-PL, cs.PL<br>Keyword Score: 50<br>Keywords: LLaMA, Transformer, Sentence Embedding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08885v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08885v1.pdf filename=2404.08885v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has experienced exponential growth, they demonstrate remarkable performance across various tasks. Notwithstanding, contemporary research primarily centers on enhancing the size and quality of pretraining data, still utilizing the next token prediction task on autoregressive <b>transformer</b> model structure. The efficacy of this task in truly facilitating the model&rsquo;s comprehension of code logic remains questionable, we speculate that it still interprets code as mere text, while human emphasizes the underlying logical knowledge. In order to prove it, we introduce a new task, &ldquo;Logically Equivalent Code Selection,&rdquo; which necessitates the selection of logically equivalent code from a candidate set, given a query code. Our experimental findings indicate that current <b>LLMs</b> underperform in this task, since they understand code by unordered bag of keywords. To ameliorate their performance, we propose an advanced pretraining task, &ldquo;Next Token Prediction+&rdquo;. This task aims to modify the <b>sentence</b> <b>embedding</b> distribution of the <b>LLM</b> without sacrificing its generative capabilities. Our experimental results reveal that following this pretraining, both Code <b>Llama</b> and StarCoder, the prevalent code domain pretraining models, display significant improvements on our logically equivalent code selection task and the code completion task.</p></p class="citation"></blockquote><h2 id=eesssy-6>eess.SY (6)</h2><h3 id=16--5796-active-learning-for-control-oriented-identification-of-nonlinear-systems-bruce-d-lee-et-al-2024>(1/6 | 57/96) Active Learning for Control-Oriented Identification of Nonlinear Systems (Bruce D. Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bruce D. Lee, Ingvar Ziemann, George J. Pappas, Nikolai Matni. (2024)<br><strong>Active Learning for Control-Oriented Identification of Nonlinear Systems</strong><br><button class=copy-to-clipboard title="Active Learning for Control-Oriented Identification of Nonlinear Systems" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 40<br>Keywords: Active Learning, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09030v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09030v1.pdf filename=2404.09030v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model-based <b>reinforcement</b> <b>learning</b> is an effective approach for controlling an unknown system. It is based on a longstanding pipeline familiar to the control community in which one performs experiments on the environment to collect a dataset, uses the resulting dataset to identify a model of the system, and finally performs control synthesis using the identified model. As interacting with the system may be costly and time consuming, targeted exploration is crucial for developing an effective control-oriented model with minimal experimentation. Motivated by this challenge, recent work has begun to study finite sample data requirements and sample efficient algorithms for the problem of optimal exploration in model-based <b>reinforcement</b> <b>learning.</b> However, existing theory and algorithms are limited to model classes which are linear in the parameters. Our work instead focuses on models with nonlinear parameter dependencies, and presents the first finite sample analysis of an <b>active</b> <b>learning</b> algorithm suitable for a general class of nonlinear dynamics. In certain settings, the excess control cost of our algorithm achieves the optimal rate, up to logarithmic factors. We validate our approach in <b>simulation,</b> showcasing the advantage of <b>active,</b> <b>control-oriented</b> exploration for controlling nonlinear systems.</p></p class="citation"></blockquote><h3 id=26--5896-consistency-analysis-of-refined-instrumental-variable-methods-for-continuous-time-system-identification-in-closed-loop-rodrigo-a-gonzález-et-al-2024>(2/6 | 58/96) Consistency analysis of refined instrumental variable methods for continuous-time system identification in closed-loop (Rodrigo A. González et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rodrigo A. González, Siqi Pan, Cristian R. Rojas, James S. Welsh. (2024)<br><strong>Consistency analysis of refined instrumental variable methods for continuous-time system identification in closed-loop</strong><br><button class=copy-to-clipboard title="Consistency analysis of refined instrumental variable methods for continuous-time system identification in closed-loop" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 40<br>Keywords: Continuous Time, Continuous Time, Discrete Time, Discrete Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08955v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08955v1.pdf filename=2404.08955v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Refined instrumental variable methods have been broadly used for identification of <b>continuous-time</b> <b>systems</b> in both open and closed-loop settings. However, the theoretical properties of these methods are still yet to be fully understood when operating in closed-loop. In this paper, we address the consistency of the simplified refined instrumental variable method for <b>continuous-time</b> <b>systems</b> (SRIVC) and its closed-loop variant CLSRIVC when they are applied on data that is generated from a feedback loop. In particular, we consider feedback loops consisting of <b>continuous-time</b> <b>controllers,</b> as well as the <b>discrete-time</b> <b>control</b> case. This paper proves that the SRIVC and CLSRIVC estimators are not generically consistent when there is a <b>continuous-time</b> <b>controller</b> in the loop, and that generic consistency can be achieved when the controller is implemented in <b>discrete-time.</b> <b>Numerical</b> <b>simulations</b> are presented to support the theoretical results.</p></p class="citation"></blockquote><h3 id=36--5996-statistical-analysis-of-block-coordinate-descent-algorithms-for-linear-continuous-time-system-identification-rodrigo-a-gonzález-et-al-2024>(3/6 | 59/96) Statistical Analysis of Block Coordinate Descent Algorithms for Linear Continuous-time System Identification (Rodrigo A. González et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rodrigo A. González, Koen Classens, Cristian R. Rojas, James S. Welsh, Tom Oomen. (2024)<br><strong>Statistical Analysis of Block Coordinate Descent Algorithms for Linear Continuous-time System Identification</strong><br><button class=copy-to-clipboard title="Statistical Analysis of Block Coordinate Descent Algorithms for Linear Continuous-time System Identification" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Continuous Time, Continuous Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09071v1.pdf filename=2404.09071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Block coordinate descent is an optimization technique that is used for estimating multi-input single-output (MISO) <b>continuous-time</b> <b>models,</b> as well as single-input single output (SISO) models in additive form. Despite its widespread use in various optimization contexts, the statistical properties of block coordinate descent in <b>continuous-time</b> <b>system</b> identification have not been covered in the literature. The aim of this paper is to formally analyze the bias properties of the block coordinate descent approach for the identification of MISO and additive SISO systems. We characterize the asymptotic bias at each iteration, and provide sufficient conditions for the consistency of the estimator for each identification setting. The theoretical results are supported by <b>simulation</b> examples.</p></p class="citation"></blockquote><h3 id=46--6096-a-framework-for-safe-probabilistic-invariance-verification-of-stochastic-dynamical-systems-taoran-wu-et-al-2024>(4/6 | 60/96) A Framework for Safe Probabilistic Invariance Verification of Stochastic Dynamical Systems (Taoran Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taoran Wu, Yiqing Yu, Bican Xia, Ji Wang, Bai Xue. (2024)<br><strong>A Framework for Safe Probabilistic Invariance Verification of Stochastic Dynamical Systems</strong><br><button class=copy-to-clipboard title="A Framework for Safe Probabilistic Invariance Verification of Stochastic Dynamical Systems" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Continuous Time, Continuous Time, Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09007v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09007v1.pdf filename=2404.09007v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensuring safety through set invariance has proven to be a valuable method in various robotics and control applications. This paper introduces a comprehensive framework for the safe probabilistic invariance verification of both discrete- and <b>continuous-time</b> <b>stochastic</b> dynamical systems over an infinite time horizon. The objective is to ascertain the lower and upper bounds of the liveness probability for a given safe set and set of initial states. This probability signifies the likelihood of the system remaining within the safe set indefinitely, starting from the set of initial states. To address this problem, we propose optimizations for verifying safe probabilistic invariance in <b>discrete-time</b> <b>and</b> <b>continuous-time</b> <b>stochastic</b> dynamical systems. These optimizations adapt classical stochastic barrier certificates, which are based on Doob&rsquo;s non-negative supermartingale inequality, and the equations described in [29],[31], which can precisely define the probability of reaching a target set while avoiding unsafe states. Finally, we demonstrate the effectiveness of these optimizations through several examples using semi-definite programming tools.</p></p class="citation"></blockquote><h3 id=56--6196-selection-of-time-headway-in-connected-and-autonomous-vehicle-platoons-under-noisy-v2v-communication-guoqi-ma-et-al-2024>(5/6 | 61/96) Selection of Time Headway in Connected and Autonomous Vehicle Platoons under Noisy V2V Communication (Guoqi Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guoqi Ma, Prabhakar R. Pagilla, Swaroop Darbha. (2024)<br><strong>Selection of Time Headway in Connected and Autonomous Vehicle Platoons under Noisy V2V Communication</strong><br><button class=copy-to-clipboard title="Selection of Time Headway in Connected and Autonomous Vehicle Platoons under Noisy V2V Communication" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08889v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08889v1.pdf filename=2404.08889v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate the selection of time headway to ensure robust string stability in connected and autonomous vehicle platoons in the presence of signal noise in Vehicle-to-Vehicle (V2V) communication. In particular, we consider the effect of noise in communicated vehicle acceleration from the predecessor vehicle to the follower vehicle on the selection of the time headway in predecessor-follower type vehicle platooning with a Constant Time Headway Policy (CTHP). Employing a CTHP based control law for each vehicle that utilizes on-board sensors for measurement of position and velocity of the predecessor vehicle and wireless communication network for obtaining the acceleration of the predecessor vehicle, we investigate how time headway is affected by communicated signal noise. We derive constraints on the CTHP controller gains for predecessor acceleration, velocity error and spacing error and a lower bound on the time headway which will ensure robust string stability of the platoon against signal noise. We provide comparative numerical <b>simulations</b> on an example to illustrate the main result.</p></p class="citation"></blockquote><h3 id=66--6296-benefits-of-v2v-communication-in-connected-and-autonomous-vehicles-in-the-presence-of-delays-in-communicated-signals-guoqi-ma-et-al-2024>(6/6 | 62/96) Benefits of V2V communication in connected and autonomous vehicles in the presence of delays in communicated signals (Guoqi Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guoqi Ma, Prabhakar R. Pagilla, Swaroop Darbha. (2024)<br><strong>Benefits of V2V communication in connected and autonomous vehicles in the presence of delays in communicated signals</strong><br><button class=copy-to-clipboard title="Benefits of V2V communication in connected and autonomous vehicles in the presence of delays in communicated signals" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08879v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08879v1.pdf filename=2404.08879v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate the effect of signal delay in communicated information in connected and autonomous vehicles. In particular, we relate this delay&rsquo;s effect on the selection of the time headway in predecessor-follower type vehicle platooning with a constant time headway policy (CTHP). We employ a CTHP control law for each vehicle in the platoon by considering two cases: cooperative adaptive cruise control (CACC) strategy where information from only one predecessor vehicle is employed and CACC+ where information from multiple predecessor vehicles is employed. We investigate how the lower bound on the time headway is affected by signal transmission delay due to wireless communication. We provide a systematic approach to the derivation of the lower bound of the time headway and selection of the appropriate CTHP controller gains for predecessor acceleration, velocity error and spacing error which will ensure robust string stability of the platoon under the presence of signal delay. We corroborate the main result with numerical <b>simulations.</b></p></p class="citation"></blockquote><h2 id=cslg-10>cs.LG (10)</h2><h3 id=110--6396-navigating-the-landscape-of-large-language-models-a-comprehensive-review-and-analysis-of-paradigms-and-fine-tuning-strategies-benjue-weng-2024>(1/10 | 63/96) Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies (Benjue Weng, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjue Weng. (2024)<br><strong>Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies</strong><br><button class=copy-to-clipboard title="Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Fine-tuning, Knowledge Distillation, ChatGPT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09022v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09022v1.pdf filename=2404.09022v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the surge of <b>ChatGPT,the</b> use of <b>large</b> <b>models</b> <b>has</b> significantly increased,rapidly rising to prominence across the industry and sweeping across the internet. This article is a comprehensive review of <b>fine-tuning</b> methods for <b>large</b> <b>models.</b> <b>This</b> paper investigates the latest technological advancements and the application of advanced methods in aspects such as task-adaptive <b>fine-tuning,domain-adaptive</b> <b>fine-tuning,few-shot</b> learning,knowledge <b>distillation,multi-task</b> learning,parameter-efficient <b>fine-tuning,and</b> dynamic <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=210--6496-intuition-aware-mixture-of-rank-1-experts-for-parameter-efficient-finetuning-yijiang-liu-et-al-2024>(2/10 | 64/96) Intuition-aware Mixture-of-Rank-1-Experts for Parameter Efficient Finetuning (Yijiang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijiang Liu, Rongyu Zhang, Huanrui Yang, Kurt Keutzer, Yuan Du, Li Du, Shanghang Zhang. (2024)<br><strong>Intuition-aware Mixture-of-Rank-1-Experts for Parameter Efficient Finetuning</strong><br><button class=copy-to-clipboard title="Intuition-aware Mixture-of-Rank-1-Experts for Parameter Efficient Finetuning" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Clustering, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08985v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08985v1.pdf filename=2404.08985v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated significant potential in performing multiple tasks in multimedia applications, ranging from content generation to interactive entertainment, and artistic creation. However, the diversity of downstream tasks in multitask scenarios presents substantial adaptation challenges for <b>LLMs.</b> While traditional methods often succumb to knowledge confusion on their monolithic dense models, Mixture-of-Experts (MoE) has been emerged as a promising solution with its sparse architecture for effective task decoupling. Inspired by the principles of human cognitive neuroscience, we design a novel framework \texttt{Intuition-MoR1E} that leverages the inherent semantic <b>clustering</b> of instances to mimic the human brain to deal with multitask, offering implicit guidance to router for optimized feature allocation. Moreover, we introduce cutting-edge Rank-1 Experts formulation designed to manage a spectrum of intuitions, demonstrating enhanced parameter efficiency and effectiveness in multitask <b>LLM</b> <b>finetuning.</b> Extensive experiments demonstrate that Intuition-MoR1E achieves superior efficiency and 2.15% overall accuracy improvement across 14 public datasets against other state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=310--6596-incremental-residual-concept-bottleneck-models-chenming-shang-et-al-2024>(3/10 | 65/96) Incremental Residual Concept Bottleneck Models (Chenming Shang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenming Shang, Shiji Zhou, Yujiu Yang, Hengyuan Zhang, Xinzhe Ni, Yuwang Wang. (2024)<br><strong>Incremental Residual Concept Bottleneck Models</strong><br><button class=copy-to-clipboard title="Incremental Residual Concept Bottleneck Models" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Black Box, Deep Neural Network, Multi-modal, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08978v1.pdf filename=2404.08978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Concept Bottleneck Models (CBMs) map the <b>black-box</b> <b>visual</b> representations extracted by <b>deep</b> <b>neural</b> <b>networks</b> onto a set of interpretable concepts and use the concepts to make predictions, enhancing the transparency of the decision-making process. <b>Multimodal</b> pre-trained models can match visual representations with textual concept embeddings, allowing for obtaining the interpretable concept bottleneck without the expertise concept annotations. Recent research has focused on the concept bank establishment and the high-quality concept selection. However, it is challenging to construct a comprehensive concept bank through humans or <b>large</b> <b>language</b> <b>models,</b> which severely limits the performance of CBMs. In this work, we propose the Incremental Residual Concept Bottleneck Model (Res-CBM) to address the challenge of concept completeness. Specifically, the residual concept bottleneck model employs a set of optimizable vectors to complete missing concepts, then the incremental concept discovery module converts the complemented vectors with unclear meanings into potential concepts in the candidate concept bank. Our approach can be applied to any user-defined concept bank, as a post-hoc processing method to enhance the performance of any CBMs. Furthermore, to measure the descriptive efficiency of CBMs, the Concept Utilization Efficiency (CUE) metric is proposed. Experiments show that the Res-CBM outperforms the current state-of-the-art methods in terms of both accuracy and efficiency and achieves comparable performance to <b>black-box</b> <b>models</b> across multiple datasets.</p></p class="citation"></blockquote><h3 id=410--6696-beyond-known-clusters-probe-new-prototypes-for-efficient-generalized-class-discovery-ye-wang-et-al-2024>(4/10 | 66/96) Beyond Known Clusters: Probe New Prototypes for Efficient Generalized Class Discovery (Ye Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ye Wang, Yaxiong Wang, Yujiao Wu, Bingchen Zhao, Xueming Qian. (2024)<br><strong>Beyond Known Clusters: Probe New Prototypes for Efficient Generalized Class Discovery</strong><br><button class=copy-to-clipboard title="Beyond Known Clusters: Probe New Prototypes for Efficient Generalized Class Discovery" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Clustering, Contrastive Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08995v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08995v1.pdf filename=2404.08995v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generalized Class Discovery (GCD) aims to dynamically assign labels to unlabelled data partially based on knowledge learned from labelled data, where the unlabelled data may come from known or novel classes. The prevailing approach generally involves <b>clustering</b> across all data and learning conceptions by prototypical <b>contrastive</b> <b>learning.</b> However, existing methods largely hinge on the performance of <b>clustering</b> algorithms and are thus subject to their inherent limitations. Firstly, the estimated cluster number is often smaller than the ground truth, making the existing methods suffer from the lack of prototypes for comprehensive conception learning. To address this issue, we propose an adaptive probing mechanism that introduces learnable potential prototypes to expand cluster prototypes (centers). As there is no ground truth for the potential prototype, we develop a <b>self-supervised</b> prototype learning framework to optimize the potential prototype in an end-to-end fashion. Secondly, <b>clustering</b> is computationally intensive, and the conventional strategy of <b>clustering</b> both labelled and unlabelled instances exacerbates this issue. To counteract this inefficiency, we opt to cluster only the unlabelled instances and subsequently expand the cluster prototypes with our introduced potential prototypes to fast explore novel classes. Despite the simplicity of our proposed method, extensive empirical analysis on a wide range of datasets confirms that our method consistently delivers state-of-the-art results. Specifically, our method surpasses the nearest competitor by a significant margin of \textbf{9.7}$%$ within the Stanford Cars dataset and \textbf{12$\times$} <b>clustering</b> efficiency within the Herbarium 19 dataset. We will make the code and checkpoints publicly available at \url{https://github.com/xjtuYW/PNP.git}.</p></p class="citation"></blockquote><h3 id=510--6796-improving-personalisation-in-valence-and-arousal-prediction-using-data-augmentation-munachiso-nwadike-et-al-2024>(5/10 | 67/96) Improving Personalisation in Valence and Arousal Prediction using Data Augmentation (Munachiso Nwadike et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Munachiso Nwadike, Jialin Li, Hanan Salam. (2024)<br><strong>Improving Personalisation in Valence and Arousal Prediction using Data Augmentation</strong><br><button class=copy-to-clipboard title="Improving Personalisation in Valence and Arousal Prediction using Data Augmentation" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Data Augmentation, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09042v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09042v1.pdf filename=2404.09042v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of <b>emotion</b> <b>recognition</b> and Human-Machine Interaction (HMI), personalised approaches have exhibited their efficacy in capturing individual-specific characteristics and enhancing affective prediction accuracy. However, personalisation techniques often face the challenge of limited <b>data</b> <b>for</b> target individuals. This paper presents our work on an enhanced personalisation strategy, that leverages <b>data</b> <b>augmentation</b> to develop tailored models for continuous valence and arousal prediction. Our proposed approach, Distance Weighting Augmentation (DWA), employs a weighting-based augmentation method that expands a target individual&rsquo;s dataset, leveraging distance metrics to identify similar samples at the segment-level. Experimental results on the MuSe-Personalisation 2023 Challenge dataset demonstrate that our method significantly improves the performance of features sets which have low baseline performance, on the test set. This improvement in poor-performing features comes without sacrificing performance on high-performing features. In particular, our method achieves a maximum combined testing CCC of 0.78, compared to the reported baseline score of 0.76 (reproduced at 0.72). It also achieved a peak arousal and valence scores of 0.81 and 0.76, compared to reproduced baseline scores of 0.76 and 0.67 respectively. Through this work, we make significant contributions to the advancement of personalised affective computing models, enhancing the practicality and adaptability of <b>data-level</b> <b>personalisation</b> in real world contexts.</p></p class="citation"></blockquote><h3 id=610--6896-praffl-a-preference-aware-scheme-in-fair-federated-learning-rongguang-ye-et-al-2024>(6/10 | 68/96) PraFFL: A Preference-Aware Scheme in Fair Federated Learning (Rongguang Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rongguang Ye, Ming Tang. (2024)<br><strong>PraFFL: A Preference-Aware Scheme in Fair Federated Learning</strong><br><button class=copy-to-clipboard title="PraFFL: A Preference-Aware Scheme in Fair Federated Learning" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-DC, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fairness, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08973v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08973v1.pdf filename=2404.08973v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fairness</b> in <b>federated</b> <b>learning</b> has emerged as a critical concern, aiming to develop an unbiased model for any special group (e.g., male or female) of sensitive features. However, there is a trade-off between model performance and <b>fairness,</b> i.e., improving <b>fairness</b> will decrease model performance. Existing approaches have characterized such a trade-off by introducing hyperparameters to quantify client&rsquo;s preferences for <b>fairness</b> and model performance. Nevertheless, these methods are limited to scenarios where each client has only a single pre-defined preference. In practical systems, each client may simultaneously have multiple preferences for the model performance and <b>fairness.</b> The key challenge is to design a method that allows the model to adapt to diverse preferences of each client in real time. To this end, we propose a Preference-aware scheme in Fair <b>Federated</b> <b>Learning</b> paradigm (called PraFFL). PraFFL can adaptively adjust the model based on each client&rsquo;s preferences to meet their needs. We theoretically prove that PraFFL can provide the optimal model for client&rsquo;s arbitrary preferences. Experimental results show that our proposed PraFFL outperforms five existing fair <b>federated</b> <b>learning</b> algorithms in terms of the model&rsquo;s capability in adapting to clients&rsquo; different preferences.</p></p class="citation"></blockquote><h3 id=710--6996-stability-and-generalization-in-free-adversarial-training-xiwei-cheng-et-al-2024>(7/10 | 69/96) Stability and Generalization in Free Adversarial Training (Xiwei Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiwei Cheng, Kexin Fu, Farzan Farnia. (2024)<br><strong>Stability and Generalization in Free Adversarial Training</strong><br><button class=copy-to-clipboard title="Stability and Generalization in Free Adversarial Training" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 15<br>Keywords: Adversarial Learning, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08980v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08980v1.pdf filename=2404.08980v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>adversarial</b> <b>training</b> methods have resulted in significant improvements in the deep neural nets&rsquo; robustness against norm-bounded <b>adversarial</b> <b>perturbations,</b> their generalization performance from training samples to test data has been shown to be considerably worse than standard empirical risk minimization methods. Several recent studies seek to connect the generalization behavior of adversarially trained classifiers to various gradient-based min-max optimization algorithms used for their training. In this work, we study the generalization performance of <b>adversarial</b> <b>training</b> methods using the algorithmic stability framework. Specifically, our goal is to compare the generalization performance of the vanilla <b>adversarial</b> <b>training</b> scheme fully optimizing the perturbations at every iteration vs. the free <b>adversarial</b> <b>training</b> simultaneously optimizing the norm-bounded perturbations and classifier parameters. Our proven generalization bounds indicate that the free <b>adversarial</b> <b>training</b> method could enjoy a lower generalization gap between training and test samples due to the simultaneous nature of its min-max optimization algorithm. We perform several numerical experiments to evaluate the generalization performance of vanilla, fast, and free <b>adversarial</b> <b>training</b> methods. Our empirical findings also show the improved generalization performance of the free <b>adversarial</b> <b>training</b> method and further demonstrate that the better generalization result could translate to greater robustness against <b>black-box</b> <b>attack</b> schemes. The code is available at <a href=https://github.com/Xiwei-Cheng/Stability_FreeAT>https://github.com/Xiwei-Cheng/Stability_FreeAT</a>.</p></p class="citation"></blockquote><h3 id=810--7096-theoretical-research-on-generative-diffusion-models-an-overview-melike-nur-yeğin-et-al-2024>(8/10 | 70/96) Theoretical research on generative diffusion models: an overview (Melike Nur Yeğin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Melike Nur Yeğin, Mehmet Fatih Amasyalı. (2024)<br><strong>Theoretical research on generative diffusion models: an overview</strong><br><button class=copy-to-clipboard title="Theoretical research on generative diffusion models: an overview" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09016v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09016v1.pdf filename=2404.09016v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative <b>diffusion</b> <b>models</b> showed high success in many fields with a powerful theoretical background. They convert the data distribution to noise and remove the noise back to obtain a similar distribution. Many existing reviews focused on the specific application areas without concentrating on the research about the algorithm. Unlike them we investigated the theoretical developments of the generative <b>diffusion</b> <b>models.</b> These approaches mainly divide into two: training-based and sampling-based. Awakening to this allowed us a clear and understandable categorization for the researchers who will make new developments in the future.</p></p class="citation"></blockquote><h3 id=910--7196-an-evaluation-framework-for-synthetic-data-generation-models-ioannis-e-livieris-et-al-2024>(9/10 | 71/96) An evaluation framework for synthetic data generation models (Ioannis E. Livieris et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ioannis E. Livieris, Nikos Alimpertis, George Domalis, Dimitris Tsakalidis. (2024)<br><strong>An evaluation framework for synthetic data generation models</strong><br><button class=copy-to-clipboard title="An evaluation framework for synthetic data generation models" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08866v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08866v1.pdf filename=2404.08866v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nowadays, the use of synthetic <b>data</b> <b>has</b> gained popularity as a cost-efficient strategy for enhancing <b>data</b> <b>augmentation</b> for improving machine learning models performance as well as addressing concerns related to sensitive <b>data</b> <b>privacy.</b> Therefore, the necessity of ensuring quality of generated synthetic <b>data,</b> <b>in</b> terms of accurate representation of real <b>data,</b> <b>consists</b> of primary importance. In this work, we present a new framework for evaluating synthetic <b>data</b> <b>generation</b> models&rsquo; ability for developing high-quality synthetic <b>data.</b> <b>The</b> proposed approach is able to provide strong statistical and theoretical information about the evaluation framework and the compared models&rsquo; ranking. Two use case scenarios demonstrate the applicability of the proposed framework for evaluating the ability of synthetic <b>data</b> <b>generation</b> models to generated high quality <b>data.</b> <b>The</b> implementation code can be found in <a href=https://github.com/novelcore/synthetic_data_evaluation_framework>https://github.com/novelcore/synthetic_data_evaluation_framework</a>.</p></p class="citation"></blockquote><h3 id=1010--7296-alice-combining-feature-selection-and-inter-rater-agreeability-for-machine-learning-insights-bachana-anasashvili-et-al-2024>(10/10 | 72/96) ALICE: Combining Feature Selection and Inter-Rater Agreeability for Machine Learning Insights (Bachana Anasashvili et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bachana Anasashvili, Vahidin Jeleskovic. (2024)<br><strong>ALICE: Combining Feature Selection and Inter-Rater Agreeability for Machine Learning Insights</strong><br><button class=copy-to-clipboard title="ALICE: Combining Feature Selection and Inter-Rater Agreeability for Machine Learning Insights" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-HC, cs-LG, cs.LG, stat-AP, stat-ML<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09053v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09053v1.pdf filename=2404.09053v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a new Python library called Automated Learning for Insightful Comparison and Evaluation (ALICE), which merges conventional feature selection and the concept of inter-rater agreeability in a simple, user-friendly manner to seek insights into <b>black</b> <b>box</b> Machine Learning models. The framework is proposed following an overview of the key concepts of interpretability in ML. The entire architecture and intuition of the main methods of the framework are also thoroughly discussed and results from initial experiments on a customer churn predictive modeling task are presented, alongside ideas for possible avenues to explore for the future. The full source code for the framework and the experiment notebooks can be found at: <a href=https://github.com/anasashb/aliceHU>https://github.com/anasashb/aliceHU</a></p></p class="citation"></blockquote><h2 id=cscr-5>cs.CR (5)</h2><h3 id=15--7396-proof-of-learning-with-incentive-security-zishuo-zhao-et-al-2024>(1/5 | 73/96) Proof-of-Learning with Incentive Security (Zishuo Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zishuo Zhao, Zhixuan Fang, Xuechao Wang, Yuan Zhou. (2024)<br><strong>Proof-of-Learning with Incentive Security</strong><br><button class=copy-to-clipboard title="Proof-of-Learning with Incentive Security" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-ET, cs-GT, cs-LG, cs.CR<br>Keyword Score: 40<br>Keywords: Stochastic Gradient Descent, Stemming, Adversarial Attack, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09005v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09005v1.pdf filename=2404.09005v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most concurrent blockchain systems rely heavily on the Proof-of-Work (PoW) or Proof-of-Stake (PoS) mechanisms for decentralized consensus and <b>security</b> assurance. However, the substantial energy expenditure <b>stemming</b> from computationally intensive yet meaningless tasks has raised considerable concerns surrounding traditional PoW approaches, The PoS mechanism, while free of energy consumption, is subject to <b>security</b> and economic issues. Addressing these issues, the paradigm of Proof-of-Useful-Work (PoUW) seeks to employ challenges of practical significance as PoW, thereby imbuing energy consumption with tangible value. While previous efforts in Proof of Learning (PoL) explored the utilization of deep learning model training <b>SGD</b> tasks as PoUW challenges, recent research has revealed its vulnerabilities to <b>adversarial</b> <b>attacks</b> and the theoretical hardness in crafting a byzantine-secure PoL mechanism. In this paper, we introduce the concept of incentive-security that incentivizes rational provers to behave honestly for their best interest, bypassing the existing hardness to design a PoL mechanism with computational efficiency, a provable incentive-security guarantee and controllable difficulty. Particularly, our work is secure against two attacks to the recent work of Jia et al. [2021], and also improves the computational overhead from $\Theta(1)$ to $O(\frac{\log E}{E})$. Furthermore, while most recent research assumes trusted problem providers and verifiers, our design also guarantees frontend incentive-security even when problem providers are untrusted, and verifier incentive-security that bypasses the Verifier&rsquo;s Dilemma. By incorporating ML training into blockchain consensus mechanisms with provable guarantees, our research not only proposes an eco-friendly solution to blockchain systems, but also provides a proposal for a completely decentralized computing power market in the new AI age.</p></p class="citation"></blockquote><h3 id=25--7496-codecloak-a-method-for-evaluating-and-mitigating-code-leakage-by-llm-code-assistants-amit-finkman-et-al-2024>(2/5 | 74/96) CodeCloak: A Method for Evaluating and Mitigating Code Leakage by LLM Code Assistants (Amit Finkman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amit Finkman, Eden Bar-Kochva, Avishag Shapira, Dudu Mimran, Yuval Elovici, Asaf Shabtai. (2024)<br><strong>CodeCloak: A Method for Evaluating and Mitigating Code Leakage by LLM Code Assistants</strong><br><button class=copy-to-clipboard title="CodeCloak: A Method for Evaluating and Mitigating Code Leakage by LLM Code Assistants" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs-LG, cs-PL, cs.CR<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09066v1.pdf filename=2404.09066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>LLM-based</b> code assistants are becoming increasingly popular among developers. These tools help developers improve their coding efficiency and reduce errors by providing real-time suggestions based on the developer&rsquo;s codebase. While beneficial, these tools might inadvertently expose the developer&rsquo;s proprietary code to the code assistant service provider during the development process. In this work, we propose two complementary methods to mitigate the risk of code leakage when using <b>LLM-based</b> code assistants. The first is a technique for reconstructing a developer&rsquo;s original codebase from code segments sent to the code assistant service (i.e., <b>prompts)</b> during the development process, enabling assessment and evaluation of the extent of code leakage to third parties (or adversaries). The second is CodeCloak, a novel deep <b>reinforcement</b> <b>learning</b> agent that manipulates the <b>prompts</b> before sending them to the code assistant service. CodeCloak aims to achieve the following two contradictory goals: (i) minimizing code leakage, while (ii) preserving relevant and useful suggestions for the developer. Our evaluation, employing GitHub Copilot, StarCoder, and CodeLlama <b>LLM-based</b> code assistants models, demonstrates the effectiveness of our CodeCloak approach on a diverse set of code repositories of varying sizes, as well as its transferability across different models. In addition, we generate a realistic simulated coding environment to thoroughly analyze code leakage risks and evaluate the effectiveness of our proposed mitigation techniques under practical development scenarios.</p></p class="citation"></blockquote><h3 id=35--7596-gophy-novel-proof-of-useful-work-blockchain-architecture-for-high-energy-physics-felix-hoffmann-et-al-2024>(3/5 | 75/96) Gophy: Novel Proof-of-Useful-Work blockchain architecture for High Energy Physics (Felix Hoffmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felix Hoffmann, Udo Kebschull. (2024)<br><strong>Gophy: Novel Proof-of-Useful-Work blockchain architecture for High Energy Physics</strong><br><button class=copy-to-clipboard title="Gophy: Novel Proof-of-Useful-Work blockchain architecture for High Energy Physics" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: 94A60, I-m, cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09093v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09093v1.pdf filename=2404.09093v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this publication, a novel architecture for Proof-of-Useful-Work blockchain consensus which aims to replace hash-based block problems with Monte Carlo <b>simulation-based</b> block problems to donate computational power to real-world HEP experiments is described. Design decisions are detailed and challenges are addressed. The architecture is being implemented using Golang and can be run inside the CbmRoot software environment. The goal is to build a bridge between the disciplines HEP and blockchain to build a novel blockchain network in which the network&rsquo;s computational power is not wasted but instead used to support a scientific experiment while at the same time securing the underlying permissioned blockchain. The blockchain features a token-based cryptocurrency that is rewarded to miners that donate computational power and acts as an additional incentive to participate which traditional volunteer computing can not provide. The implementation named gophy is being implemented in Golang and is expected to be open-sourced before the end of 2024.</p></p class="citation"></blockquote><h3 id=45--7696-gview-a-versatile-assistant-for-security-researchers-raul-zaharia-et-al-2024>(4/5 | 76/96) GView: A Versatile Assistant for Security Researchers (Raul Zaharia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raul Zaharia, Dragoş Gavriluţ, Gheorghiţă Mutu, Dorel Lucanu. (2024)<br><strong>GView: A Versatile Assistant for Security Researchers</strong><br><button class=copy-to-clipboard title="GView: A Versatile Assistant for Security Researchers" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-SE, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09058v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09058v1.pdf filename=2404.09058v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cyber <b>security</b> attacks have become increasingly complex over time, with various phases of their kill chain, involving binaries, scripts, documents, executed commands, vulnerabilities, or network traffic. We propose a tool, GView, that is designed to investigate possible attacks by providing guided analysis for various file types using automatic artifact identification, extraction, coherent correlation &,inference, and meaningful & intuitive views at different levels of granularity w.r.t. revealed information. The concept behind GView simplifies navigation through all payloads in a complex attack, streamlining the process for <b>security</b> researchers, and Increasing the quality of analysis. GView is generic in the sense it supports a variety of file types and has multiple visualization modes that can be automatically adjusted for each file type alone. Our evaluation shows that GView significantly improves the analysis time of an attack compared to conventional tools used in forensics.</p></p class="citation"></blockquote><h3 id=55--7796-enhancing-security-awareness-through-gamified-approaches-yussuf-ahmed-et-al-2024>(5/5 | 77/96) Enhancing Security Awareness Through Gamified Approaches (Yussuf Ahmed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yussuf Ahmed, Micheal Ezealor, Haitham Mahmoud, MohamedAjmal Azad, Mohamed BenFarah, Mehdi Yousefi. (2024)<br><strong>Enhancing Security Awareness Through Gamified Approaches</strong><br><button class=copy-to-clipboard title="Enhancing Security Awareness Through Gamified Approaches" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09052v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09052v1.pdf filename=2404.09052v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advent of smart grid (SG) systems, electricity networks have been able to ensure greater efficiency and utility by interconnecting their grids through cloud-based technology. As SGs become increasingly complex, a wide range of <b>security</b> challenges arise, threatening the grid&rsquo;s reliability, safety, efficiency, and stability. The <b>security</b> challenges include the potential exposure of personal data due to hackers intercepting the communications between the SG infrastructure and the smart meters. <b>Security</b> awareness plays a vital role in addressing some of these challenges. However, the traditional training programs are no longer efficient for instilling information <b>security</b> culture in organisations or from an individual user perspective. Gamification is a new concept in the field of information <b>security</b> awareness training (SAT) campaigns that can be introduced to fill in this gap by providing employees with a means of practising and learning about many <b>security</b> flaws and risks that exist within the organisation. Thus, this paper examines the effectiveness of gamification in promoting <b>security</b> awareness among smart meter components for smart grid users/operators. A gaming application is developed as part of the study with the aim of training and evaluating the results through three difficulty levels of questionnaires. Furthermore, the results are evaluated for the three difficulty levels as well as the overall flag captured. It can be demonstrated that the scores of participants in the three levels have improved by 40%, 35% and 29%, respectively. This reflects the awareness of learning within our system.</p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=12--7896-efficient-discretization-of-the-laplacian-on-complex-geometries-gustav-eriksson-2024>(1/2 | 78/96) Efficient discretization of the Laplacian on complex geometries (Gustav Eriksson, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gustav Eriksson. (2024)<br><strong>Efficient discretization of the Laplacian on complex geometries</strong><br><button class=copy-to-clipboard title="Efficient discretization of the Laplacian on complex geometries" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, GLUE<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09050v1.pdf filename=2404.09050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Highly accurate <b>simulations</b> of problems including second derivatives on complex geometries are of primary interest in academia and industry. Consider for example the Navier-Stokes equations or wave propagation problems of acoustic or elastic waves. Current finite difference discretization methods are accurate and efficient on modern hardware, but they lack flexibility when it comes to complex geometries. In this work I extend the continuous summation-by-parts (SBP) framework to second derivatives and combine it with spectral-type SBP operators on Gauss-Lobatto quadrature points to obtain a highly efficient discretization (accurate with respect to runtime) of the Laplacian on complex domains. The resulting Laplace operator is defined on a grid without duplicated points on the interfaces, thus removing unnecessary degrees of freedom in the scheme, and is proven to satisfy a discrete equivalent to Green&rsquo;s first identity. Semi-discrete stability using the new Laplace operator is proven for the acoustic wave equation in 2D. Furthermore, the method can easily be coupled together with traditional finite difference operators using <b>glue-grid</b> interpolation operators, resulting in a method with great practical potential. Two numerical experiments are done on the acoustic wave equation in 2D. First on a problem with an analytical solution, demonstrating the accuracy and efficiency properties of the method. Finally, a more realistic problem is solved, where a complex region of the domain is discretized using the new method and coupled to the rest of the domain discretized using a traditional finite difference method.</p></p class="citation"></blockquote><h3 id=22--7996-numerical-aspects-of-hyperbolic-geometry-dorota-celinska-kopczynska-et-al-2024>(2/2 | 79/96) Numerical Aspects of Hyperbolic Geometry (Dorota Celinska-Kopczynska et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dorota Celinska-Kopczynska, Eryk Kopczynski. (2024)<br><strong>Numerical Aspects of Hyperbolic Geometry</strong><br><button class=copy-to-clipboard title="Numerical Aspects of Hyperbolic Geometry" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09039v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09039v1.pdf filename=2404.09039v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hyperbolic <b>geometry</b> has recently found applications in social networks, machine learning and computational biology. With the increasing popularity, questions about the best representations of hyperbolic spaces arise, as each representation comes with some numerical instability. This paper compares various 2D and 3D hyperbolic <b>geometry</b> representations. To this end, we conduct an extensive <b>simulational</b> scheme based on six tests of numerical precision errors. Our comparisons include the most popular models and less-known mixed and reduced representations. According to our results, polar representation wins, although the halfplane invariant is also very successful. We complete the comparison with a brief discussion of the non-numerical advantages of various representations.</p></p class="citation"></blockquote><h2 id=cscy-3>cs.CY (3)</h2><h3 id=13--8096-three-disclaimers-for-safe-disclosure-a-cardwriter-for-reporting-the-use-of-generative-ai-in-writing-process-won-ik-cho-et-al-2024>(1/3 | 80/96) Three Disclaimers for Safe Disclosure: A Cardwriter for Reporting the Use of Generative AI in Writing Process (Won Ik Cho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Won Ik Cho, Eunjung Cho, Hyeonji Shin. (2024)<br><strong>Three Disclaimers for Safe Disclosure: A Cardwriter for Reporting the Use of Generative AI in Writing Process</strong><br><button class=copy-to-clipboard title="Three Disclaimers for Safe Disclosure: A Cardwriter for Reporting the Use of Generative AI in Writing Process" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 30<br>Keywords: Generative AI, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09041v1.pdf filename=2404.09041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>artificial</b> intelligence (AI) and <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are increasingly being used in the academic writing process. This is despite the current lack of unified framework for reporting the use of machine assistance. In this work, we propose &ldquo;Cardwriter&rdquo;, an intuitive interface that produces a short report for authors to declare their use of <b>generative</b> <b>AI</b> in their writing process. The demo is available online, at <a href=https://cardwriter.vercel.app>https://cardwriter.vercel.app</a></p></p class="citation"></blockquote><h3 id=23--8196-vrpd-dt-vehicle-routing-problem-with-drones-under-dynamically-changing-traffic-conditions-navid-imran-et-al-2024>(2/3 | 81/96) VRPD-DT: Vehicle Routing Problem with Drones Under Dynamically Changing Traffic Conditions (Navid Imran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Navid Imran, Myounggyu Won. (2024)<br><strong>VRPD-DT: Vehicle Routing Problem with Drones Under Dynamically Changing Traffic Conditions</strong><br><button class=copy-to-clipboard title="VRPD-DT: Vehicle Routing Problem with Drones Under Dynamically Changing Traffic Conditions" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09065v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09065v1.pdf filename=2404.09065v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The vehicle routing problem with drones (VRP-D) is to determine the optimal routes of trucks and drones such that the total operational cost is minimized in a scenario where the trucks work in tandem with the drones to deliver parcels to customers. While various heuristic algorithms have been developed to address the problem, existing solutions are built based on simplistic cost models, overlooking the temporal dynamics of the costs, which fluctuate depending on the dynamically changing traffic conditions. In this paper, we present a novel problem called the vehicle routing problem with drones under dynamically changing traffic conditions (VRPD-DT) to address the limitation of existing VRP-D solutions. We design a novel cost model that factors in the actual travel distance and projected travel time, computed using a machine learning-driven travel time prediction algorithm. A variable neighborhood descent (VND) algorithm is developed to find the optimal truck-drone routes under the dynamics of traffic conditions through incorporation of the travel time prediction model. A <b>simulation</b> study was performed to evaluate the performance compared with a state-of-the-art VRP-D heuristic solution. The results demonstrate that the proposed algorithm outperforms the state-of-the-art algorithm in various delivery scenarios.</p></p class="citation"></blockquote><h3 id=33--8296-business-models-for-the-simulation-hypothesis-evangelos-katsamakas-2024>(3/3 | 82/96) Business models for the simulation hypothesis (Evangelos Katsamakas, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Evangelos Katsamakas. (2024)<br><strong>Business models for the simulation hypothesis</strong><br><button class=copy-to-clipboard title="Business models for the simulation hypothesis" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs-ET, cs.CY, econ-GN, q-fin-EC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08991v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08991v1.pdf filename=2404.08991v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>simulation</b> hypothesis suggests that we live in a computer <b>simulation.</b> That notion has attracted significant scholarly and popular interest. This article explores the <b>simulation</b> hypothesis from a business perspective. Due to the lack of a name for a universe consistent with the <b>simulation</b> hypothesis, we propose the term simuverse. We argue that if we live in a <b>simulation,</b> there must be a business justification. Therefore, we ask: If we live in a simuverse, what is its business model? We identify and explore business model scenarios, such as simuverse as a project, service, or platform. We also explore business model pathways and risk management issues. The article contributes to the <b>simulation</b> hypothesis literature and is the first to provide a business model perspective on the <b>simulation</b> hypothesis. The article discusses theoretical and practical implications and identifies opportunities for future research related to sustainability, digital transformation, and Artificial Intelligence (AI).</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--8396-bullion-a-column-store-for-machine-learning-gang-liao-et-al-2024>(1/1 | 83/96) Bullion: A Column Store for Machine Learning (Gang Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gang Liao, Ye Liu, Jianjun Chen, Daniel J. Abadi. (2024)<br><strong>Bullion: A Column Store for Machine Learning</strong><br><button class=copy-to-clipboard title="Bullion: A Column Store for Machine Learning" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs-LG, cs.DB<br>Keyword Score: 30<br>Keywords: Generative AI, Quantization, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08901v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08901v1.pdf filename=2404.08901v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The past two decades have witnessed columnar storage revolutionizing data warehousing and analytics. However, the rapid growth of machine learning poses new challenges to this domain. This paper presents Bullion, a columnar storage system tailored for machine learning workloads. Bullion addresses the complexities of data compliance, optimizes the encoding of long sequence sparse features, efficiently manages wide-table projections, and introduces feature <b>quantization</b> in storage. By aligning with the evolving requirements of ML applications, Bullion extends columnar storage to various scenarios, from advertising and <b>recommendation</b> systems to the expanding realm of <b>Generative</b> <b>AI.</b> Preliminary experimental results and theoretical analysis demonstrate Bullion&rsquo;s superior performance in handling the unique demands of machine learning workloads compared to existing columnar storage solutions. Bullion significantly reduces I/O costs for deletion compliance, achieves substantial storage savings with its optimized encoding scheme for sparse features, and drastically improves metadata parsing speed for wide-table projections. These advancements position Bullion as a critical component in the future of machine learning infrastructure, enabling organizations to efficiently manage and process the massive volumes of data required for training and inference in modern AI applications.</p></p class="citation"></blockquote><h2 id=cssi-2>cs.SI (2)</h2><h3 id=12--8496-beam-management-in-low-earth-orbit-satellite-communication-with-handover-frequency-control-and-satellite-terrestrial-spectrum-sharing-yaohua-sun-et-al-2024>(1/2 | 84/96) Beam Management in Low Earth Orbit Satellite Communication With Handover Frequency Control and Satellite-Terrestrial Spectrum Sharing (Yaohua Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaohua Sun, Jianfeng Zhu, Mugen Peng. (2024)<br><strong>Beam Management in Low Earth Orbit Satellite Communication With Handover Frequency Control and Satellite-Terrestrial Spectrum Sharing</strong><br><button class=copy-to-clipboard title="Beam Management in Low Earth Orbit Satellite Communication With Handover Frequency Control and Satellite-Terrestrial Spectrum Sharing" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08967v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08967v1.pdf filename=2404.08967v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To achieve ubiquitous wireless connectivity, low earth orbit (LEO) satellite networks have drawn much attention. However, effective beam management is challenging due to time-varying cell load, high dynamic network topology, and complex interference situations. In this paper, under inter-satellite handover frequency and satellite-terrestrial/inter-beam interference constraints, we formulate a practical beam management problem, aiming to maximize the long-term service satisfaction of cells. Particularly, Lyapunov framework is leveraged to equivalently transform the primal problem into multiple single epoch optimization problems, where virtual queue stability constraints replace inter-satellite handover frequency constraints. Since each single epoch problem is NP-hard, we further decompose it into three subproblems, including inter-satellite handover decision, beam hopping design and satellite-terrestrial spectrum sharing. First, a proactive inter-satellite handover mechanism is developed to balance handover frequency and satellite loads. Subsequently, a beam hopping design algorithm is presented based on conflict <b>graphs</b> to achieve interference mitigation among beams, and then a flexible satellite-terrestrial spectrum sharing algorithm is designed to satisfy the demands of beam cells and improve spectral efficiency. <b>Simulation</b> results show that our proposal significantly improves service satisfaction compared with baselines, where the average data queue length of beam cells is reduced by over 50% with affordable handover frequency.</p></p class="citation"></blockquote><h3 id=22--8596-beam-management-in-low-earth-orbit-satellite-networks-with-random-traffic-arrival-and-time-varying-topology-jianfeng-zhu-et-al-2024>(2/2 | 85/96) Beam Management in Low Earth Orbit Satellite Networks with Random Traffic Arrival and Time-varying Topology (Jianfeng Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianfeng Zhu, Yaohua Sun, Mugen Peng. (2024)<br><strong>Beam Management in Low Earth Orbit Satellite Networks with Random Traffic Arrival and Time-varying Topology</strong><br><button class=copy-to-clipboard title="Beam Management in Low Earth Orbit Satellite Networks with Random Traffic Arrival and Time-varying Topology" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08959v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08959v1.pdf filename=2404.08959v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low earth orbit (LEO) satellite communication networks have been considered as promising solutions to providing high data rate and seamless coverage, where satellite beam management plays a key role. However, due to the limitation of beam resource, dynamic network topology, beam spectrum reuse, time-varying traffic arrival and service continuity requirement, it is challenging to effectively allocate time-frequency resource of satellite beams to multiple cells. In this paper, aiming at reducing time-averaged beam revisit time and mitigate inter-satellite handover, a beam management problem is formulated for dynamic LEO satellite communication networks, under inter-cell interference and network stability constraints. Particularly, inter-cell interference constraints are further simplified into off-axis angle based constraints, which provide tractable rules for spectrum sharing between two beam cells. To deal with the long-term performance optimization, the primal problem is transformed into a series of single epoch problems by adopting Lyapunov optimization framework. Since the transformed problem is NP-hard, it is further divided into three subproblems, including serving beam allocation, beam service time allocation and serving satellite allocation. With the help of conflict <b>graphs</b> built with off-axis angle based constraints, serving beam allocation and beam service time allocation algorithms are developed to reduce beam revisit time and cell packet queue length. Then, we further develop a satellite-cell service relationship optimization algorithm to better adapt to dynamic network topology. Compared with baselines, numerical results show that our proposal can reduce average beam revisit time by 20.8% and keep strong network stability with similar inter-satellite handover frequency.</p></p class="citation"></blockquote><h2 id=csit-1>cs.IT (1)</h2><h3 id=11--8696-towards-efficient-device-identification-in-massive-random-access-a-multi-stage-approach-jyotish-robin-et-al-2024>(1/1 | 86/96) Towards Efficient Device Identification in Massive Random Access: A Multi-stage Approach (Jyotish Robin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jyotish Robin, Elza Erkip. (2024)<br><strong>Towards Efficient Device Identification in Massive Random Access: A Multi-stage Approach</strong><br><button class=copy-to-clipboard title="Towards Efficient Device Identification in Massive Random Access: A Multi-stage Approach" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09062v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09062v1.pdf filename=2404.09062v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient and low-latency wireless connectivity between the base station (BS) and a sparse set of sporadically active devices from a massive number of devices is crucial for random access in emerging massive machine-type communications (mMTC). This paper addresses the challenge of identifying active devices while meeting stringent access delay and reliability constraints in mMTC environments. A novel multi-stage active device identification framework is proposed where we aim to refine a partial estimate of the active device set using feedback and hypothesis testing across multiple stages eventually leading to an exact recovery of active devices after the final stage of processing. In our proposed approach, active devices independently transmit binary preambles during each stage, leveraging feedback signals from the BS, whereas the BS employs a non-coherent binary energy detection. The minimum user identification cost associated with our multi-stage non-coherent active device identification framework with feedback, in terms of the required number of channel-uses, is quantified using information-theoretic techniques in the asymptotic regime of total number of devices $\ell$ when the number of active devices $k$ scales as $k = {\Theta}(1)$. Practical implementations of our multi-stage active device identification schemes, leveraging Belief Propagation (BP) techniques, are also presented and evaluated. <b>Simulation</b> results show that our multi-stage BP strategies exhibit superior performance over single-stage strategies, even when considering overhead costs associated with feedback and hypothesis testing.</p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=11--8796-a-biologically-inspired-computational-trust-model-for-open-multi-agent-systems-which-is-resilient-to-trustor-population-changes-zoi-lygizou-et-al-2024>(1/1 | 87/96) A biologically inspired computational trust model for open multi-agent systems which is resilient to trustor population changes (Zoi Lygizou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zoi Lygizou, Dimitris Kalles. (2024)<br><strong>A biologically inspired computational trust model for open multi-agent systems which is resilient to trustor population changes</strong><br><button class=copy-to-clipboard title="A biologically inspired computational trust model for open multi-agent systems which is resilient to trustor population changes" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-MA, cs.MA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.10014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.10014v1.pdf filename=2404.10014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current trust and reputation models continue to have significant limitations, such as the inability to deal with agents constantly entering or exiting open multi-agent systems (open MAS), as well as continuously changing behaviors. Our study is based on CA, a previously proposed decentralized computational trust model from the trustee&rsquo;s point of view, inspired by synaptic plasticity and the formation of assemblies in the human brain. It is designed to meet the requirements of highly dynamic and open MAS, and its main difference with most conventional trust and reputation models is that the trustor does not select a trustee to delegate a task; instead, the trustee determines whether it is qualified to successfully execute it. We ran a series of <b>simulations</b> to compare CA model to FIRE, a well-established, decentralized trust and reputation model for open MAS under conditions of continuous trustee and trustor population replacement, as well as continuous change of trustees&rsquo; abilities to perform tasks. The main finding is that FIRE is superior to changes in the trustee population, whereas CA is resilient to the trustor population changes. When the trustees switch performance profiles FIRE clearly outperforms despite the fact that both models&rsquo; performances are significantly impacted by this environmental change. Findings lead us to conclude that learning to use the appropriate trust model, according to the dynamic conditions in effect could maximize the trustor&rsquo;s benefits.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--8896-deep-reinforcement-learning-based-online-scheduling-policy-for-deep-neural-network-multi-tenant-multi-accelerator-systems-francesco-g-blanco-et-al-2024>(1/1 | 88/96) Deep Reinforcement Learning based Online Scheduling Policy for Deep Neural Network Multi-Tenant Multi-Accelerator Systems (Francesco G. Blanco et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco G. Blanco, Enrico Russo, Maurizio Palesi, Davide Patti, Giuseppe Ascia, Vincenzo Catania. (2024)<br><strong>Deep Reinforcement Learning based Online Scheduling Policy for Deep Neural Network Multi-Tenant Multi-Accelerator Systems</strong><br><button class=copy-to-clipboard title="Deep Reinforcement Learning based Online Scheduling Policy for Deep Neural Network Multi-Tenant Multi-Accelerator Systems" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-DC, cs-LG, cs.AR<br>Keyword Score: 20<br>Keywords: Deep Neural Network, Deep Neural Network, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08950v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08950v1.pdf filename=2404.08950v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Currently, there is a growing trend of outsourcing the execution of <b>DNNs</b> to cloud services. For service providers, managing multi-tenancy and ensuring high-quality service delivery, particularly in meeting stringent execution time constraints, assumes paramount importance, all while endeavoring to maintain cost-effectiveness. In this context, the utilization of heterogeneous multi-accelerator systems becomes increasingly relevant. This paper presents RELMAS, a low-overhead <b>deep</b> <b>reinforcement</b> <b>learning</b> algorithm designed for the online scheduling of <b>DNNs</b> in multi-tenant environments, taking into account the dataflow heterogeneity of accelerators and memory bandwidths contentions. By doing so, service providers can employ the most efficient scheduling policy for user requests, optimizing Service-Level-Agreement (SLA) satisfaction rates and enhancing hardware utilization. The application of RELMAS to a heterogeneous multi-accelerator system composed of various instances of Simba and Eyeriss sub-accelerators resulted in up to a 173% improvement in SLA satisfaction rate compared to state-of-the-art scheduling techniques across different workload scenarios, with less than a 1.5% energy overhead.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--8996-faster-game-solving-via-hyperparameter-schedules-naifeng-zhang-et-al-2024>(1/1 | 89/96) Faster Game Solving via Hyperparameter Schedules (Naifeng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naifeng Zhang, Stephen McAleer, Tuomas Sandholm. (2024)<br><strong>Faster Game Solving via Hyperparameter Schedules</strong><br><button class=copy-to-clipboard title="Faster Game Solving via Hyperparameter Schedules" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09097v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09097v1.pdf filename=2404.09097v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>counterfactual</b> regret minimization (CFR) family of algorithms consists of iterative algorithms for imperfect-information games. In two-player zero-sum games, the time average of the iterates converges to a Nash equilibrium. The state-of-the-art prior variants, Discounted CFR (DCFR) and Predictive CFR$^+$ (PCFR$^+$) are the fastest known algorithms for solving two-player zero-sum games in practice, both in the extensive-form setting and the normal-form setting. They enhance the convergence rate compared to vanilla CFR by applying discounted weights to early iterations in various ways, leveraging fixed weighting schemes. We introduce Hyperparameter Schedules (HSs), which are remarkably simple yet highly effective in expediting the rate of convergence. HS dynamically adjusts the hyperparameter governing the discounting scheme of CFR variants. HSs on top of DCFR or PCFR$^+$ is now the new state of the art in solving zero-sum games and yields orders-of-magnitude speed improvements. The new algorithms are also easy to implement because 1) they are small modifications to the existing ones in terms of code and 2) they require no game-specific tuning.</p></p class="citation"></blockquote><h2 id=eessiv-1>eess.IV (1)</h2><h3 id=11--9096-maskel-a-model-for-human-whole-body-x-rays-generation-from-human-masking-images-yingjie-xi-et-al-2024>(1/1 | 90/96) MaSkel: A Model for Human Whole-body X-rays Generation from Human Masking Images (Yingjie Xi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingjie Xi, Boyuan Cheng, Jingyao Cai, Jian Jun Zhang, Xiaosong Yang. (2024)<br><strong>MaSkel: A Model for Human Whole-body X-rays Generation from Human Masking Images</strong><br><button class=copy-to-clipboard title="MaSkel: A Model for Human Whole-body X-rays Generation from Human Masking Images" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09000v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09000v1.pdf filename=2404.09000v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The human whole-body X-rays could offer a valuable reference for various applications, including medical diagnostics, digital animation modeling, and ergonomic design. The traditional method of obtaining X-ray information requires the use of CT (Computed Tomography) scan machines, which emit potentially harmful radiation. Thus it faces a significant limitation for realistic applications because it lacks adaptability and safety. In our work, We proposed a new method to directly generate the 2D human whole-body X-rays from the human masking images. The predicted images will be similar to the real ones with the same image style and anatomic structure. We employed a <b>data-driven</b> <b>strategy.</b> By leveraging advanced generative techniques, our model MaSkel(Masking image to Skeleton X-rays) could generate a high-quality X-ray image from a human masking image without the need for invasive and harmful radiation exposure, which not only provides a new path to generate highly anatomic and customized <b>data</b> <b>but</b> also reduces health risks. To our knowledge, our model MaSkel is the first work for predicting whole-body X-rays. In this paper, we did two parts of the work. The first one is to solve the <b>data</b> <b>limitation</b> problem, the diffusion-based techniques are utilized to make a <b>data</b> <b>augmentation,</b> which provides two synthetic datasets for preliminary pretraining. Then we designed a two-stage training strategy to train MaSkel. At last, we make qualitative and quantitative evaluations of the generated X-rays. In addition, we invite some professional doctors to assess our predicted <b>data.</b> <b>These</b> evaluations demonstrate the MaSkel&rsquo;s superior ability to generate anatomic X-rays from human masking images. The related code and links of the dataset are available at <a href=https://github.com/2022yingjie/MaSkel>https://github.com/2022yingjie/MaSkel</a>.</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--9196-degrees-of-freedom-for-radiating-systems-mats-gustafsson-2024>(1/1 | 91/96) Degrees of Freedom for Radiating Systems (Mats Gustafsson, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mats Gustafsson. (2024)<br><strong>Degrees of Freedom for Radiating Systems</strong><br><button class=copy-to-clipboard title="Degrees of Freedom for Radiating Systems" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-CE, eess-SP, eess.SP, physics-class-ph<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08976v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08976v1.pdf filename=2404.08976v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Electromagnetic degrees of freedom are instrumental in antenna design, wireless communications, imaging, and scattering. Larger number of degrees of freedom enhances control in antenna design, influencing radiation patterns and directivity, while in communication systems, it links to spatial channels for increased data rates and reliability, and resolution in imaging. The correlation between computed degrees of freedom and physical quantities is not fully understood, <b>prompting</b> a comparison between classical estimates, Weyl&rsquo;s law, modal expansions, and optimization techniques. In this paper, it is shown that NDoF for arbitrary shaped radiating structures approaches the shadow area measured in squared wavelengths.</p></p class="citation"></blockquote><h2 id=q-finpm-1>q-fin.PM (1)</h2><h3 id=11--9296-developing-an-attention-based-ensemble-learning-framework-for-financial-portfolio-optimisation-zhenglong-li-et-al-2024>(1/1 | 92/96) Developing An Attention-Based Ensemble Learning Framework for Financial Portfolio Optimisation (Zhenglong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenglong Li, Vincent Tam. (2024)<br><strong>Developing An Attention-Based Ensemble Learning Framework for Financial Portfolio Optimisation</strong><br><button class=copy-to-clipboard title="Developing An Attention-Based Ensemble Learning Framework for Financial Portfolio Optimisation" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.PM<br>Categories: cs-CE, cs-LG, q-fin-PM, q-fin.PM<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08935v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08935v1.pdf filename=2404.08935v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, deep or <b>reinforcement</b> <b>learning</b> approaches have been applied to optimise investment portfolios through learning the spatial and temporal information under the dynamic financial market. Yet in most cases, the existing approaches may produce biased trading signals based on the conventional price data due to a lot of market noises, which possibly fails to balance the investment returns and risks. Accordingly, a multi-agent and self-adaptive portfolio optimisation framework integrated with attention mechanisms and time series, namely the MASAAT, is proposed in this work in which multiple trading agents are created to observe and analyse the price series and directional change data that recognises the significant changes of asset prices at different levels of granularity for enhancing the signal-to-noise ratio of price series. Afterwards, by reconstructing the tokens of financial data in a sequence, the attention-based cross-sectional analysis module and temporal analysis module of each agent can effectively capture the correlations between assets and the dependencies between time points. Besides, a portfolio generator is integrated into the proposed framework to fuse the spatial-temporal information and then summarise the portfolios suggested by all trading agents to produce a newly ensemble portfolio for reducing biased trading actions and balancing the overall returns and risks. The experimental results clearly demonstrate that the MASAAT framework achieves impressive enhancement when compared with many well-known portfolio optimsation approaches on three challenging data sets of DJIA, S&amp;P 500 and CSI 300. More importantly, our proposal has potential strengths in many possible applications for future study.</p></p class="citation"></blockquote><h2 id=cssd-1>cs.SD (1)</h2><h3 id=11--9396-voice-attribute-editing-with-text-prompt-zhengyan-sheng-et-al-2024>(1/1 | 93/96) Voice Attribute Editing with Text Prompt (Zhengyan Sheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengyan Sheng, Yang Ai, Li-Juan Liu, Jia Pan, Zhen-Hua Ling. (2024)<br><strong>Voice Attribute Editing with Text Prompt</strong><br><button class=copy-to-clipboard title="Voice Attribute Editing with Text Prompt" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08857v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08857v1.pdf filename=2404.08857v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite recent advancements in speech generation with text <b>prompt</b> providing control over speech style, voice attributes in synthesized speech remain elusive and challenging to control. This paper introduces a novel task: voice attribute editing with text <b>prompt,</b> with the goal of making relative modifications to voice attributes according to the actions described in the text <b>prompt.</b> To solve this task, VoxEditor, an end-to-end generative model, is proposed. In VoxEditor, addressing the insufficiency of text <b>prompt,</b> a Residual Memory (ResMem) block is designed, that efficiently maps voice attributes and these descriptors into the shared feature space. Additionally, the ResMem block is enhanced with a voice attribute degree prediction (VADP) block to align voice attributes with corresponding descriptors, addressing the imprecision of text <b>prompt</b> caused by non-quantitative descriptions of voice attributes. We also establish the open-source VCTK-RVA dataset, which leads the way in manual annotations detailing voice characteristic differences among different speakers. Extensive experiments demonstrate the effectiveness and generalizability of our proposed method in terms of both objective and subjective metrics. The dataset and audio samples are available on the website.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--9496-inversevis-revealing-the-hidden-with-curved-sphere-tracing-kai-lawonn-et-al-2024>(1/1 | 94/96) InverseVis: Revealing the Hidden with Curved Sphere Tracing (Kai Lawonn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Lawonn, Monique Meuschke, Tobias Günther. (2024)<br><strong>InverseVis: Revealing the Hidden with Curved Sphere Tracing</strong><br><button class=copy-to-clipboard title="InverseVis: Revealing the Hidden with Curved Sphere Tracing" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: I-3-3, cs-GR, cs.GR<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.09092v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.09092v1.pdf filename=2404.09092v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exploratory analysis of scalar fields on surface meshes presents significant challenges in identifying and visualizing important regions, particularly on the surface&rsquo;s backside. Previous visualization methods achieved only a limited visibility of significant features, i.e., regions with high or low scalar values, during interactive exploration. In response to this, we propose a novel technique, InverseVis, which leverages curved sphere tracing and uses the otherwise unused space to enhance visibility. Our approach combines direct and indirect rendering, allowing camera rays to wrap around the surface and reveal information from the backside. To achieve this, we formulate an energy term that guides the image synthesis in previously unused space, highlighting the most important regions of the backside. By quantifying the amount of visible important features, we optimize the camera position to maximize the visibility of the scalar field on both the front and backsides. InverseVis is <b>benchmarked</b> against state-of-the-art methods and a derived technique, showcasing its effectiveness in revealing essential features and outperforming existing approaches.</p></p class="citation"></blockquote><h2 id=csds-1>cs.DS (1)</h2><h3 id=11--9596-improved-approximations-for-flexible-network-design-dylan-hyatt-denesik-et-al-2024>(1/1 | 95/96) Improved Approximations for Flexible Network Design (Dylan Hyatt-Denesik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dylan Hyatt-Denesik, Afrouz Jabal Ameli, Laura Sanita. (2024)<br><strong>Improved Approximations for Flexible Network Design</strong><br><button class=copy-to-clipboard title="Improved Approximations for Flexible Network Design" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08972v1.pdf filename=2404.08972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Flexible network design deals with building a network that guarantees some connectivity requirements between its vertices, even when some of its elements (like vertices or edges) fail. In particular, the set of edges (resp. vertices) of a given <b>graph</b> are here partitioned into safe and unsafe. The goal is to identify a minimum size subgraph that is 2-edge-connected (resp. 2-vertex-connected), and stay so whenever any of the unsafe elements gets removed. In this paper, we provide improved approximation algorithms for flexible network design problems, considering both edge-connectivity and vertex-connectivity, as well as connectivity values higher than 2. For the vertex-connectivity variant, in particular, our algorithm is the first with approximation factor strictly better than 2.</p></p class="citation"></blockquote><h2 id=cscc-1>cs.CC (1)</h2><h3 id=11--9696-almost-optimal-time-lower-bound-for-approximating-parameterized-clique-csp-and-more-under-eth-venkatesan-guruswami-et-al-2024>(1/1 | 96/96) Almost Optimal Time Lower Bound for Approximating Parameterized Clique, CSP, and More, under ETH (Venkatesan Guruswami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Venkatesan Guruswami, Bingkai Lin, Xuandi Ren, Yican Sun, Kewen Wu. (2024)<br><strong>Almost Optimal Time Lower Bound for Approximating Parameterized Clique, CSP, and More, under ETH</strong><br><button class=copy-to-clipboard title="Almost Optimal Time Lower Bound for Approximating Parameterized Clique, CSP, and More, under ETH" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs.CC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08870v1.pdf filename=2404.08870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Parameterized Inapproximability Hypothesis (PIH), which is an analog of the PCP theorem in parameterized complexity, asserts that, there is a constant $\varepsilon> 0$ such that for any computable function $f:\mathbb{N}\to\mathbb{N}$, no $f(k)\cdot n^{O(1)}$-time algorithm can, on input a $k$-variable CSP instance with domain size $n$, find an assignment satisfying $1-\varepsilon$ fraction of the constraints. A recent work by Guruswami, Lin, Ren, Sun, and Wu (STOC'24) established PIH under the Exponential Time Hypothesis (ETH). In this work, we improve the quantitative aspects of PIH and prove (under ETH) that approximating sparse parameterized CSPs within a constant factor requires $n^{k^{1-o(1)}}$ time. This immediately implies that, assuming ETH, finding a $(k/2)$-clique in an $n$-vertex <b>graph</b> with a $k$-clique requires $n^{k^{1-o(1)}}$ time. We also prove almost optimal time lower bounds for approximating $k$-ExactCover and Max $k$-Coverage. Our proof follows the blueprint of the previous work to identify a &ldquo;vector-structured&rdquo; ETH-hard CSP whose satisfiability can be checked via an appropriate form of &ldquo;parallel&rdquo; PCP. Using further ideas in the reduction, we guarantee additional structures for constraints in the CSP. We then leverage this to design a parallel PCP of almost linear size based on Reed-Muller codes and derandomized low degree testing.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240414000000/ title="arXiv @ 2024.04.14" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.04.14</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/ title="arXiv @ 2024.04.16" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.04.16</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#csse-3>cs.SE (3)</a><ul><li><a href=#13--196-zero-shot-code-representation-learning-via-prompt-tuning-nan-cui-et-al-2024>(1/3 | 1/96) Zero-Shot Code Representation Learning via Prompt Tuning (Nan Cui et al., 2024)</a></li><li><a href=#23--296-large-language-models-for-mobile-gui-text-input-generation-an-empirical-study-chenhui-cui-et-al-2024>(2/3 | 2/96) Large Language Models for Mobile GUI Text Input Generation: An Empirical Study (Chenhui Cui et al., 2024)</a></li><li><a href=#33--396-aligning-llms-for-fl-free-program-repair-junjielong-xu-et-al-2024>(3/3 | 3/96) Aligning LLMs for FL-free Program Repair (Junjielong Xu et al., 2024)</a></li></ul></li><li><a href=#cscl-14>cs.CL (14)</a><ul><li><a href=#114--496-adapting-mental-health-prediction-tasks-for-cross-lingual-learning-via-meta-training-and-in-context-learning-with-large-language-model-zita-lifelo-et-al-2024>(1/14 | 4/96) Adapting Mental Health Prediction Tasks for Cross-lingual Learning via Meta-Training and In-context Learning with Large Language Model (Zita Lifelo et al., 2024)</a></li><li><a href=#214--596-curiousllm-elevating-multi-document-qa-with-reasoning-infused-knowledge-graph-prompting-zukang-yang-et-al-2024>(2/14 | 5/96) CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge Graph Prompting (Zukang Yang et al., 2024)</a></li><li><a href=#314--696-multilingual-evaluation-of-semantic-textual-relatedness-sharvi-endait-et-al-2024>(3/14 | 6/96) Multilingual Evaluation of Semantic Textual Relatedness (Sharvi Endait et al., 2024)</a></li><li><a href=#414--796-do-llms-play-dice-exploring-probability-distribution-sampling-in-large-language-models-for-behavioral-simulation-jia-gu-et-al-2024>(4/14 | 7/96) Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation (Jia Gu et al., 2024)</a></li><li><a href=#514--896-llm-in-context-recall-is-prompt-dependent-daniel-machlab-et-al-2024>(5/14 | 8/96) LLM In-Context Recall is Prompt Dependent (Daniel Machlab et al., 2024)</a></li><li><a href=#614--996-multimodal-cross-document-event-coreference-resolution-using-linear-semantic-transfer-and-mixed-modality-ensembles-abhijnan-nath-et-al-2024>(6/14 | 9/96) Multimodal Cross-Document Event Coreference Resolution Using Linear Semantic Transfer and Mixed-Modality Ensembles (Abhijnan Nath et al., 2024)</a></li><li><a href=#714--1096-ming-moe-enhancing-medical-multi-task-learning-in-large-language-models-with-sparse-mixture-of-low-rank-adapter-experts-yusheng-liao-et-al-2024>(7/14 | 10/96) MING-MOE: Enhancing Medical Multi-Task Learning in Large Language Models with Sparse Mixture of Low-Rank Adapter Experts (Yusheng Liao et al., 2024)</a></li><li><a href=#814--1196-enforcing-paraphrase-generation-via-controllable-latent-diffusion-wei-zou-et-al-2024>(8/14 | 11/96) Enforcing Paraphrase Generation via Controllable Latent Diffusion (Wei Zou et al., 2024)</a></li><li><a href=#914--1296-towards-enhancing-health-coaching-dialogue-in-low-resource-settings-yue-zhou-et-al-2024>(9/14 | 12/96) Towards Enhancing Health Coaching Dialogue in Low-Resource Settings (Yue Zhou et al., 2024)</a></li><li><a href=#1014--1396-ronid-new-intent-discovery-with-generated-reliable-labels-and-cluster-friendly-representations-shun-zhang-et-al-2024>(10/14 | 13/96) RoNID: New Intent Discovery with Generated-Reliable Labels and Cluster-friendly Representations (Shun Zhang et al., 2024)</a></li><li><a href=#1114--1496-oovs-in-the-spotlight-how-to-inflect-them-tomáš-sourada-et-al-2024>(11/14 | 14/96) OOVs in the Spotlight: How to Inflect them? (Tomáš Sourada et al., 2024)</a></li><li><a href=#1214--1596-on-speculative-decoding-for-multimodal-large-language-models-mukul-gagrani-et-al-2024>(12/14 | 15/96) On Speculative Decoding for Multimodal Large Language Models (Mukul Gagrani et al., 2024)</a></li><li><a href=#1314--1696-wikisplit-easy-data-refinement-for-split-and-rephrase-hayato-tsukagoshi-et-al-2024>(13/14 | 16/96) WikiSplit++: Easy Data Refinement for Split and Rephrase (Hayato Tsukagoshi et al., 2024)</a></li><li><a href=#1414--1796-labeled-morphological-segmentation-with-semi-markov-models-ryan-cotterell-et-al-2024>(14/14 | 17/96) Labeled Morphological Segmentation with Semi-Markov Models (Ryan Cotterell et al., 2024)</a></li></ul></li><li><a href=#csir-5>cs.IR (5)</a><ul><li><a href=#15--1896-introducing-super-rags-in-mistral-8x7b-v1-ayush-thakur-et-al-2024>(1/5 | 18/96) Introducing Super RAGs in Mistral 8x7B-v1 (Ayush Thakur et al., 2024)</a></li><li><a href=#25--1996-approximate-cluster-based-sparse-document-retrieval-with-segmented-maximum-term-weights-yifan-qiao-et-al-2024>(2/5 | 19/96) Approximate Cluster-Based Sparse Document Retrieval with Segmented Maximum Term Weights (Yifan Qiao et al., 2024)</a></li><li><a href=#35--2096-countering-mainstream-bias-via-end-to-end-adaptive-local-learning-jinhao-pan-et-al-2024>(3/5 | 20/96) Countering Mainstream Bias via End-to-End Adaptive Local Learning (Jinhao Pan et al., 2024)</a></li><li><a href=#45--2196-misinformation-resilient-search-rankings-with-webgraph-based-interventions-peter-carragher-et-al-2024>(4/5 | 21/96) Misinformation Resilient Search Rankings with Webgraph-based Interventions (Peter Carragher et al., 2024)</a></li><li><a href=#55--2296-improving-technical-how-to-query-accuracy-with-automated-search-results-verification-and-reranking-lei-ding-et-al-2024>(5/5 | 22/96) Improving Technical &lsquo;How-to&rsquo; Query Accuracy with Automated Search Results Verification and Reranking (Lei Ding et al., 2024)</a></li></ul></li><li><a href=#cscv-26>cs.CV (26)</a><ul><li><a href=#126--2396-label-free-anomaly-detection-in-aerial-agricultural-images-with-masked-image-modeling-sambal-shikhar-et-al-2024>(1/26 | 23/96) Label-free Anomaly Detection in Aerial Agricultural Images with Masked Image Modeling (Sambal Shikhar et al., 2024)</a></li><li><a href=#226--2496-heat-head-level-parameter-efficient-adaptation-of-vision-transformers-with-taylor-expansion-importance-scores-yibo-zhong-et-al-2024>(2/26 | 24/96) HEAT: Head-level Parameter Efficient Adaptation of Vision Transformers with Taylor-expansion Importance Scores (Yibo Zhong et al., 2024)</a></li><li><a href=#326--2596-practicaldg-perturbation-distillation-on-vision-language-models-for-hybrid-domain-generalization-zining-chen-et-al-2024>(3/26 | 25/96) PracticalDG: Perturbation Distillation on Vision-Language Models for Hybrid Domain Generalization (Zining Chen et al., 2024)</a></li><li><a href=#426--2696-pm2-a-new-prompting-multi-modal-model-paradigm-for-few-shot-medical-image-classification-zhenwei-wang-et-al-2024>(4/26 | 26/96) PM2: A New Prompting Multi-modal Model Paradigm for Few-shot Medical Image Classification (Zhenwei Wang et al., 2024)</a></li><li><a href=#526--2796-changeanywhere-sample-generation-for-remote-sensing-change-detection-via-semantic-latent-diffusion-model-kai-tang-et-al-2024>(5/26 | 27/96) ChangeAnywhere: Sample Generation for Remote Sensing Change Detection via Semantic Latent Diffusion Model (Kai Tang et al., 2024)</a></li><li><a href=#626--2896-mma-dfer-multimodal-adaptation-of-unimodal-models-for-dynamic-facial-expression-recognition-in-the-wild-kateryna-chumachenko-et-al-2024>(6/26 | 28/96) MMA-DFER: MultiModal Adaptation of unimodal models for Dynamic Facial Expression Recognition in-the-wild (Kateryna Chumachenko et al., 2024)</a></li><li><a href=#726--2996-rethinking-iterative-stereo-matching-from-diffusion-bridge-model-perspective-yuguang-shi-2024>(7/26 | 29/96) Rethinking Iterative Stereo Matching from Diffusion Bridge Model Perspective (Yuguang Shi, 2024)</a></li><li><a href=#826--3096-amu-tuning-effective-logit-bias-for-clip-based-few-shot-learning-yuwei-tang-et-al-2024>(8/26 | 30/96) AMU-Tuning: Effective Logit Bias for CLIP-based Few-shot Learning (Yuwei Tang et al., 2024)</a></li><li><a href=#926--3196-chimpvlm-ethogram-enhanced-chimpanzee-behaviour-recognition-otto-brookes-et-al-2024>(9/26 | 31/96) ChimpVLM: Ethogram-Enhanced Chimpanzee Behaviour Recognition (Otto Brookes et al., 2024)</a></li><li><a href=#1026--3296-bg-yolo-a-bidirectional-guided-method-for-underwater-object-detection-jian-zhang-et-al-2024>(10/26 | 32/96) BG-YOLO: A Bidirectional-Guided Method for Underwater Object Detection (Jian Zhang et al., 2024)</a></li><li><a href=#1126--3396-constructing-and-exploring-intermediate-domains-in-mixed-domain-semi-supervised-medical-image-segmentation-qinghe-ma-et-al-2024>(11/26 | 33/96) Constructing and Exploring Intermediate Domains in Mixed Domain Semi-supervised Medical Image Segmentation (Qinghe Ma et al., 2024)</a></li><li><a href=#1226--3496-diffusion-models-meet-remote-sensing-principles-methods-and-perspectives-yidan-liu-et-al-2024>(12/26 | 34/96) Diffusion Models Meet Remote Sensing: Principles, Methods, and Perspectives (Yidan Liu et al., 2024)</a></li><li><a href=#1326--3596-a-lightweight-spatiotemporal-network-for-online-eye-tracking-with-event-camera-yan-ru-pei-et-al-2024>(13/26 | 35/96) A Lightweight Spatiotemporal Network for Online Eye Tracking with Event Camera (Yan Ru Pei et al., 2024)</a></li><li><a href=#1426--3696-understanding-multimodal-deep-neural-networks-a-concept-selection-view-chenming-shang-et-al-2024>(14/26 | 36/96) Understanding Multimodal Deep Neural Networks: A Concept Selection View (Chenming Shang et al., 2024)</a></li><li><a href=#1526--3796-trustworthy-multimodal-fusion-for-sentiment-analysis-in-ordinal-sentiment-space-zhuyang-xie-et-al-2024>(15/26 | 37/96) Trustworthy Multimodal Fusion for Sentiment Analysis in Ordinal Sentiment Space (Zhuyang Xie et al., 2024)</a></li><li><a href=#1626--3896-eiven-efficient-implicit-attribute-value-extraction-using-multimodal-llm-henry-peng-zou-et-al-2024>(16/26 | 38/96) EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal LLM (Henry Peng Zou et al., 2024)</a></li><li><a href=#1726--3996-fast-fishing-approximating-bait-for-efficient-and-scalable-deep-active-image-classification-denis-huseljic-et-al-2024>(17/26 | 39/96) Fast Fishing: Approximating BAIT for Efficient and Scalable Deep Active Image Classification (Denis Huseljic et al., 2024)</a></li><li><a href=#1826--4096-mcpnet-an-interpretable-classifier-via-multi-level-concept-prototypes-bor-shiun-wang-et-al-2024>(18/26 | 40/96) MCPNet: An Interpretable Classifier via Multi-Level Concept Prototypes (Bor-Shiun Wang et al., 2024)</a></li><li><a href=#1926--4196-loopgaussian-creating-3d-cinemagraph-with-multi-view-images-via-eulerian-motion-field-jiyang-li-et-al-2024>(19/26 | 41/96) LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via Eulerian Motion Field (Jiyang Li et al., 2024)</a></li><li><a href=#2026--4296-seeing-text-in-the-dark-algorithm-and-benchmark-chengpei-xu-et-al-2024>(20/26 | 42/96) Seeing Text in the Dark: Algorithm and Benchmark (Chengpei Xu et al., 2024)</a></li><li><a href=#2126--4396-dedode-v2-analyzing-and-improving-the-dedode-keypoint-detector-johan-edstedt-et-al-2024>(21/26 | 43/96) DeDoDe v2: Analyzing and Improving the DeDoDe Keypoint Detector (Johan Edstedt et al., 2024)</a></li><li><a href=#2226--4496-shifting-spotlight-for-co-supervision-a-simple-yet-efficient-single-branch-network-to-see-through-camouflage-yang-hu-et-al-2024>(22/26 | 44/96) Shifting Spotlight for Co-supervision: A Simple yet Efficient Single-branch Network to See Through Camouflage (Yang Hu et al., 2024)</a></li><li><a href=#2326--4596-pnerv-enhancing-spatial-consistency-via-pyramidal-neural-representation-for-videos-qi-zhao-et-al-2024>(23/26 | 45/96) PNeRV: Enhancing Spatial Consistency via Pyramidal Neural Representation for Videos (Qi Zhao et al., 2024)</a></li><li><a href=#2426--4696-exploring-explainability-in-video-action-recognition-avinab-saha-et-al-2024>(24/26 | 46/96) Exploring Explainability in Video Action Recognition (Avinab Saha et al., 2024)</a></li><li><a href=#2526--4796-maprotonet-a-multi-scale-attentive-interpretable-prototypical-part-network-for-3d-magnetic-resonance-imaging-brain-tumor-classification-binghua-li-et-al-2024>(25/26 | 47/96) MAProtoNet: A Multi-scale Attentive Interpretable Prototypical Part Network for 3D Magnetic Resonance Imaging Brain Tumor Classification (Binghua Li et al., 2024)</a></li><li><a href=#2626--4896-a-fourier-enhanced-multi-modal-3d-small-object-optical-mark-recognition-and-positioning-method-for-percutaneous-abdominal-puncture-surgical-navigation-zezhao-guo-et-al-2024>(26/26 | 48/96) A Fourier-enhanced multi-modal 3D small object optical mark recognition and positioning method for percutaneous abdominal puncture surgical navigation (Zezhao Guo et al., 2024)</a></li></ul></li><li><a href=#csni-2>cs.NI (2)</a><ul><li><a href=#12--4996-generative-ai-agent-for-next-generation-mimo-design-fundamentals-challenges-and-vision-zhe-wang-et-al-2024>(1/2 | 49/96) Generative AI Agent for Next-Generation MIMO Design: Fundamentals, Challenges, and Vision (Zhe Wang et al., 2024)</a></li><li><a href=#22--5096-prosecutor-protecting-mobile-aigc-services-on-two-layer-blockchain-via-reputation-and-contract-theoretic-approaches-yinqiu-liu-et-al-2024>(2/2 | 50/96) ProSecutor: Protecting Mobile AIGC Services on Two-Layer Blockchain via Reputation and Contract Theoretic Approaches (Yinqiu Liu et al., 2024)</a></li></ul></li><li><a href=#csro-5>cs.RO (5)</a><ul><li><a href=#15--5196-safe-reinforcement-learning-on-the-constraint-manifold-theory-and-applications-puze-liu-et-al-2024>(1/5 | 51/96) Safe Reinforcement Learning on the Constraint Manifold: Theory and Applications (Puze Liu et al., 2024)</a></li><li><a href=#25--5296-neurit-pushing-the-limit-of-neural-inertial-tracking-for-indoor-robotic-iot-xinzhe-zheng-et-al-2024>(2/5 | 52/96) NeurIT: Pushing the Limit of Neural Inertial Tracking for Indoor Robotic IoT (Xinzhe Zheng et al., 2024)</a></li><li><a href=#35--5396-airship-formations-for-animal-motion-capture-and-behavior-analysis-eric-price-et-al-2024>(3/5 | 53/96) Airship Formations for Animal Motion Capture and Behavior Analysis (Eric Price et al., 2024)</a></li><li><a href=#45--5496-physics-aware-iterative-learning-and-prediction-of-saliency-map-for-bimanual-grasp-planning-shiyao-wang-et-al-2024>(4/5 | 54/96) Physics-Aware Iterative Learning and Prediction of Saliency Map for Bimanual Grasp Planning (Shiyao Wang et al., 2024)</a></li><li><a href=#55--5596-learning-surface-terrain-classifications-from-ground-penetrating-radar-anja-sheppard-et-al-2024>(5/5 | 55/96) Learning Surface Terrain Classifications from Ground Penetrating Radar (Anja Sheppard et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--5696-is-next-token-prediction-sufficient-for-gpt-exploration-on-code-logic-comprehension-mengnan-qi-et-al-2024>(1/1 | 56/96) Is Next Token Prediction Sufficient for GPT? Exploration on Code Logic Comprehension (Mengnan Qi et al., 2024)</a></li></ul></li><li><a href=#eesssy-6>eess.SY (6)</a><ul><li><a href=#16--5796-active-learning-for-control-oriented-identification-of-nonlinear-systems-bruce-d-lee-et-al-2024>(1/6 | 57/96) Active Learning for Control-Oriented Identification of Nonlinear Systems (Bruce D. Lee et al., 2024)</a></li><li><a href=#26--5896-consistency-analysis-of-refined-instrumental-variable-methods-for-continuous-time-system-identification-in-closed-loop-rodrigo-a-gonzález-et-al-2024>(2/6 | 58/96) Consistency analysis of refined instrumental variable methods for continuous-time system identification in closed-loop (Rodrigo A. González et al., 2024)</a></li><li><a href=#36--5996-statistical-analysis-of-block-coordinate-descent-algorithms-for-linear-continuous-time-system-identification-rodrigo-a-gonzález-et-al-2024>(3/6 | 59/96) Statistical Analysis of Block Coordinate Descent Algorithms for Linear Continuous-time System Identification (Rodrigo A. González et al., 2024)</a></li><li><a href=#46--6096-a-framework-for-safe-probabilistic-invariance-verification-of-stochastic-dynamical-systems-taoran-wu-et-al-2024>(4/6 | 60/96) A Framework for Safe Probabilistic Invariance Verification of Stochastic Dynamical Systems (Taoran Wu et al., 2024)</a></li><li><a href=#56--6196-selection-of-time-headway-in-connected-and-autonomous-vehicle-platoons-under-noisy-v2v-communication-guoqi-ma-et-al-2024>(5/6 | 61/96) Selection of Time Headway in Connected and Autonomous Vehicle Platoons under Noisy V2V Communication (Guoqi Ma et al., 2024)</a></li><li><a href=#66--6296-benefits-of-v2v-communication-in-connected-and-autonomous-vehicles-in-the-presence-of-delays-in-communicated-signals-guoqi-ma-et-al-2024>(6/6 | 62/96) Benefits of V2V communication in connected and autonomous vehicles in the presence of delays in communicated signals (Guoqi Ma et al., 2024)</a></li></ul></li><li><a href=#cslg-10>cs.LG (10)</a><ul><li><a href=#110--6396-navigating-the-landscape-of-large-language-models-a-comprehensive-review-and-analysis-of-paradigms-and-fine-tuning-strategies-benjue-weng-2024>(1/10 | 63/96) Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies (Benjue Weng, 2024)</a></li><li><a href=#210--6496-intuition-aware-mixture-of-rank-1-experts-for-parameter-efficient-finetuning-yijiang-liu-et-al-2024>(2/10 | 64/96) Intuition-aware Mixture-of-Rank-1-Experts for Parameter Efficient Finetuning (Yijiang Liu et al., 2024)</a></li><li><a href=#310--6596-incremental-residual-concept-bottleneck-models-chenming-shang-et-al-2024>(3/10 | 65/96) Incremental Residual Concept Bottleneck Models (Chenming Shang et al., 2024)</a></li><li><a href=#410--6696-beyond-known-clusters-probe-new-prototypes-for-efficient-generalized-class-discovery-ye-wang-et-al-2024>(4/10 | 66/96) Beyond Known Clusters: Probe New Prototypes for Efficient Generalized Class Discovery (Ye Wang et al., 2024)</a></li><li><a href=#510--6796-improving-personalisation-in-valence-and-arousal-prediction-using-data-augmentation-munachiso-nwadike-et-al-2024>(5/10 | 67/96) Improving Personalisation in Valence and Arousal Prediction using Data Augmentation (Munachiso Nwadike et al., 2024)</a></li><li><a href=#610--6896-praffl-a-preference-aware-scheme-in-fair-federated-learning-rongguang-ye-et-al-2024>(6/10 | 68/96) PraFFL: A Preference-Aware Scheme in Fair Federated Learning (Rongguang Ye et al., 2024)</a></li><li><a href=#710--6996-stability-and-generalization-in-free-adversarial-training-xiwei-cheng-et-al-2024>(7/10 | 69/96) Stability and Generalization in Free Adversarial Training (Xiwei Cheng et al., 2024)</a></li><li><a href=#810--7096-theoretical-research-on-generative-diffusion-models-an-overview-melike-nur-yeğin-et-al-2024>(8/10 | 70/96) Theoretical research on generative diffusion models: an overview (Melike Nur Yeğin et al., 2024)</a></li><li><a href=#910--7196-an-evaluation-framework-for-synthetic-data-generation-models-ioannis-e-livieris-et-al-2024>(9/10 | 71/96) An evaluation framework for synthetic data generation models (Ioannis E. Livieris et al., 2024)</a></li><li><a href=#1010--7296-alice-combining-feature-selection-and-inter-rater-agreeability-for-machine-learning-insights-bachana-anasashvili-et-al-2024>(10/10 | 72/96) ALICE: Combining Feature Selection and Inter-Rater Agreeability for Machine Learning Insights (Bachana Anasashvili et al., 2024)</a></li></ul></li><li><a href=#cscr-5>cs.CR (5)</a><ul><li><a href=#15--7396-proof-of-learning-with-incentive-security-zishuo-zhao-et-al-2024>(1/5 | 73/96) Proof-of-Learning with Incentive Security (Zishuo Zhao et al., 2024)</a></li><li><a href=#25--7496-codecloak-a-method-for-evaluating-and-mitigating-code-leakage-by-llm-code-assistants-amit-finkman-et-al-2024>(2/5 | 74/96) CodeCloak: A Method for Evaluating and Mitigating Code Leakage by LLM Code Assistants (Amit Finkman et al., 2024)</a></li><li><a href=#35--7596-gophy-novel-proof-of-useful-work-blockchain-architecture-for-high-energy-physics-felix-hoffmann-et-al-2024>(3/5 | 75/96) Gophy: Novel Proof-of-Useful-Work blockchain architecture for High Energy Physics (Felix Hoffmann et al., 2024)</a></li><li><a href=#45--7696-gview-a-versatile-assistant-for-security-researchers-raul-zaharia-et-al-2024>(4/5 | 76/96) GView: A Versatile Assistant for Security Researchers (Raul Zaharia et al., 2024)</a></li><li><a href=#55--7796-enhancing-security-awareness-through-gamified-approaches-yussuf-ahmed-et-al-2024>(5/5 | 77/96) Enhancing Security Awareness Through Gamified Approaches (Yussuf Ahmed et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#12--7896-efficient-discretization-of-the-laplacian-on-complex-geometries-gustav-eriksson-2024>(1/2 | 78/96) Efficient discretization of the Laplacian on complex geometries (Gustav Eriksson, 2024)</a></li><li><a href=#22--7996-numerical-aspects-of-hyperbolic-geometry-dorota-celinska-kopczynska-et-al-2024>(2/2 | 79/96) Numerical Aspects of Hyperbolic Geometry (Dorota Celinska-Kopczynska et al., 2024)</a></li></ul></li><li><a href=#cscy-3>cs.CY (3)</a><ul><li><a href=#13--8096-three-disclaimers-for-safe-disclosure-a-cardwriter-for-reporting-the-use-of-generative-ai-in-writing-process-won-ik-cho-et-al-2024>(1/3 | 80/96) Three Disclaimers for Safe Disclosure: A Cardwriter for Reporting the Use of Generative AI in Writing Process (Won Ik Cho et al., 2024)</a></li><li><a href=#23--8196-vrpd-dt-vehicle-routing-problem-with-drones-under-dynamically-changing-traffic-conditions-navid-imran-et-al-2024>(2/3 | 81/96) VRPD-DT: Vehicle Routing Problem with Drones Under Dynamically Changing Traffic Conditions (Navid Imran et al., 2024)</a></li><li><a href=#33--8296-business-models-for-the-simulation-hypothesis-evangelos-katsamakas-2024>(3/3 | 82/96) Business models for the simulation hypothesis (Evangelos Katsamakas, 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--8396-bullion-a-column-store-for-machine-learning-gang-liao-et-al-2024>(1/1 | 83/96) Bullion: A Column Store for Machine Learning (Gang Liao et al., 2024)</a></li></ul></li><li><a href=#cssi-2>cs.SI (2)</a><ul><li><a href=#12--8496-beam-management-in-low-earth-orbit-satellite-communication-with-handover-frequency-control-and-satellite-terrestrial-spectrum-sharing-yaohua-sun-et-al-2024>(1/2 | 84/96) Beam Management in Low Earth Orbit Satellite Communication With Handover Frequency Control and Satellite-Terrestrial Spectrum Sharing (Yaohua Sun et al., 2024)</a></li><li><a href=#22--8596-beam-management-in-low-earth-orbit-satellite-networks-with-random-traffic-arrival-and-time-varying-topology-jianfeng-zhu-et-al-2024>(2/2 | 85/96) Beam Management in Low Earth Orbit Satellite Networks with Random Traffic Arrival and Time-varying Topology (Jianfeng Zhu et al., 2024)</a></li></ul></li><li><a href=#csit-1>cs.IT (1)</a><ul><li><a href=#11--8696-towards-efficient-device-identification-in-massive-random-access-a-multi-stage-approach-jyotish-robin-et-al-2024>(1/1 | 86/96) Towards Efficient Device Identification in Massive Random Access: A Multi-stage Approach (Jyotish Robin et al., 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#11--8796-a-biologically-inspired-computational-trust-model-for-open-multi-agent-systems-which-is-resilient-to-trustor-population-changes-zoi-lygizou-et-al-2024>(1/1 | 87/96) A biologically inspired computational trust model for open multi-agent systems which is resilient to trustor population changes (Zoi Lygizou et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--8896-deep-reinforcement-learning-based-online-scheduling-policy-for-deep-neural-network-multi-tenant-multi-accelerator-systems-francesco-g-blanco-et-al-2024>(1/1 | 88/96) Deep Reinforcement Learning based Online Scheduling Policy for Deep Neural Network Multi-Tenant Multi-Accelerator Systems (Francesco G. Blanco et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--8996-faster-game-solving-via-hyperparameter-schedules-naifeng-zhang-et-al-2024>(1/1 | 89/96) Faster Game Solving via Hyperparameter Schedules (Naifeng Zhang et al., 2024)</a></li></ul></li><li><a href=#eessiv-1>eess.IV (1)</a><ul><li><a href=#11--9096-maskel-a-model-for-human-whole-body-x-rays-generation-from-human-masking-images-yingjie-xi-et-al-2024>(1/1 | 90/96) MaSkel: A Model for Human Whole-body X-rays Generation from Human Masking Images (Yingjie Xi et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--9196-degrees-of-freedom-for-radiating-systems-mats-gustafsson-2024>(1/1 | 91/96) Degrees of Freedom for Radiating Systems (Mats Gustafsson, 2024)</a></li></ul></li><li><a href=#q-finpm-1>q-fin.PM (1)</a><ul><li><a href=#11--9296-developing-an-attention-based-ensemble-learning-framework-for-financial-portfolio-optimisation-zhenglong-li-et-al-2024>(1/1 | 92/96) Developing An Attention-Based Ensemble Learning Framework for Financial Portfolio Optimisation (Zhenglong Li et al., 2024)</a></li></ul></li><li><a href=#cssd-1>cs.SD (1)</a><ul><li><a href=#11--9396-voice-attribute-editing-with-text-prompt-zhengyan-sheng-et-al-2024>(1/1 | 93/96) Voice Attribute Editing with Text Prompt (Zhengyan Sheng et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--9496-inversevis-revealing-the-hidden-with-curved-sphere-tracing-kai-lawonn-et-al-2024>(1/1 | 94/96) InverseVis: Revealing the Hidden with Curved Sphere Tracing (Kai Lawonn et al., 2024)</a></li></ul></li><li><a href=#csds-1>cs.DS (1)</a><ul><li><a href=#11--9596-improved-approximations-for-flexible-network-design-dylan-hyatt-denesik-et-al-2024>(1/1 | 95/96) Improved Approximations for Flexible Network Design (Dylan Hyatt-Denesik et al., 2024)</a></li></ul></li><li><a href=#cscc-1>cs.CC (1)</a><ul><li><a href=#11--9696-almost-optimal-time-lower-bound-for-approximating-parameterized-clique-csp-and-more-under-eth-venkatesan-guruswami-et-al-2024>(1/1 | 96/96) Almost Optimal Time Lower Bound for Approximating Parameterized Clique, CSP, and More, under ETH (Venkatesan Guruswami et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>