<!doctype html><html><head><title>arXiv @ 2024.04.03</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.04.03"><meta property="og:description" content="Primary Categories astro-ph.IM (1) cs.AI (3) cs.CE (1) cs.CL (57) cs.CR (6) cs.CV (89) cs.DC (1) cs.DS (1) cs.ET (1) cs.HC (5) cs.IR (7) cs.IT (7) cs.LG (36) cs.MA (2) cs.NE (1) cs.NI (1) cs.PL (1) cs.RO (12) cs.SD (2) cs.SE (5) econ.TH (1) eess.IV (4) eess.SP (2) eess.SY (7) math.NA (3) math.OC (2) math.ST (2) nlin.AO (1) physics.flu-dyn (1) physics.med-ph (1) physics.soc-ph (2) q-bio.QM (1) quant-ph (4) stat.ML (3) Keywords keyword cs."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202404/20240403000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-03T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-03T00:00:00+00:00"><meta name=description content="arXiv @ 2024.04.03"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240408000000/ title="arXiv @ 2024.04.08">arXiv @ 2024.04.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/ title="arXiv @ 2024.04.09">arXiv @ 2024.04.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240410000000/ title="arXiv @ 2024.04.10">arXiv @ 2024.04.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/ title="arXiv @ 2024.04.11">arXiv @ 2024.04.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/ title="arXiv @ 2024.04.12">arXiv @ 2024.04.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/ title="arXiv @ 2024.04.13">arXiv @ 2024.04.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240414000000/ title="arXiv @ 2024.04.14">arXiv @ 2024.04.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/ title="arXiv @ 2024.04.15">arXiv @ 2024.04.15</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202404/20240403000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Wednesday, Apr 3, 2024</p></div><div class=title><h1>arXiv @ 2024.04.03</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#astro-phim-1>astro-ph.IM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#csai-3>cs.AI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#cscl-57>cs.CL (57)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#cscr-6>cs.CR (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#cscv-89>cs.CV (89)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#csds-1>cs.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#cset-1>cs.ET (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#cshc-5>cs.HC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#csir-7>cs.IR (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#csit-7>cs.IT (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#cslg-36>cs.LG (36)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#csma-2>cs.MA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#csne-1>cs.NE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#csro-12>cs.RO (12)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#csse-5>cs.SE (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#econth-1>econ.TH (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#eessiv-4>eess.IV (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#eesssp-2>eess.SP (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#eesssy-7>eess.SY (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#mathna-3>math.NA (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#mathoc-2>math.OC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#mathst-2>math.ST (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#nlinao-1>nlin.AO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#physicsflu-dyn-1>physics.flu-dyn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#physicsmed-ph-1>physics.med-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#physicssoc-ph-2>physics.soc-ph (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#quant-ph-4>quant-ph (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/#statml-3>stat.ML (3)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Adversarial Attack</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Autoencoder</td><td></td><td>1</td><td>3</td><td></td></tr><tr><td>BERT</td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>Bag-of-Words</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Benchmarking</td><td>8</td><td>22</td><td>2</td><td></td></tr><tr><td>Black Box</td><td></td><td>1</td><td></td><td></td></tr><tr><td>ChatGPT</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Chatbot</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Claude</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Cohere</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td>7</td><td>3</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>11</td><td>3</td><td></td></tr><tr><td>Coreference Resolution</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Curriculum Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Data Augmentation</td><td>4</td><td>1</td><td>1</td><td></td></tr><tr><td>Dialogue System</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>15</td><td>1</td><td>1</td></tr><tr><td>Direct Preference Optimization</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Disambiguation</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Distribution Shift</td><td>2</td><td>2</td><td>4</td><td></td></tr><tr><td>Document Embedding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Domain Adaptation</td><td>1</td><td>2</td><td>1</td><td></td></tr><tr><td>Explainable AI</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Fairness</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Federated Learning</td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Few-shot</td><td>1</td><td>4</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>22</td><td>15</td><td>5</td><td></td></tr><tr><td>Foundation Model</td><td></td><td>5</td><td></td><td></td></tr><tr><td>GPT</td><td>5</td><td>3</td><td></td><td></td></tr><tr><td>GPT-3</td><td>3</td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>3</td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>3</td><td>1</td><td></td><td></td></tr><tr><td>Gemini</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Generative AI</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td>9</td><td></td><td>2</td></tr><tr><td>Graph</td><td>5</td><td>3</td><td>5</td><td></td></tr><tr><td>Graph Attention Networks</td><td>1</td><td>2</td><td>2</td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Grounding</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Hallucination Detection</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Hate Speech Detection</td><td></td><td></td><td>1</td><td></td></tr><tr><td>High-Resource</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td>7</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>7</td><td></td><td></td><td></td></tr><tr><td>Information Retrieval</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Instruction Following</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>3</td><td>12</td><td>1</td><td></td></tr><tr><td>Knowledge Graph</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td>1</td><td></td><td></td><td></td></tr><tr><td>LLaMA</td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>Language Generation</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>69</td><td>9</td><td>12</td><td></td></tr><tr><td>Low-Resource</td><td>6</td><td></td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Machine Unlearning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Masked Language Model</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Massive Multitask Language Understanding (MMLU)</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Mathematical Reasoning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Metaphor Detection</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Mistral</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Model Compression</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Multi-modal</td><td>4</td><td>13</td><td>1</td><td></td></tr><tr><td>Multiple Instance Learning</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Mutual Information</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Natural Language Generation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>7</td><td></td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>7</td><td></td><td>1</td></tr><tr><td>Open-Domain Dialogue</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Out-of-domain</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>4</td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Prompt</td><td>10</td><td>15</td><td>1</td><td></td></tr><tr><td>Prompt Learning</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Pruning</td><td></td><td></td><td>3</td><td></td></tr><tr><td>Quantization</td><td>2</td><td>4</td><td></td><td></td></tr><tr><td>Question Answering</td><td>4</td><td>8</td><td></td><td></td></tr><tr><td>Reasoning</td><td>6</td><td>5</td><td>1</td><td></td></tr><tr><td>Recommendation</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Reinforcement Learning</td><td>2</td><td>1</td><td></td><td>1</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td>1</td><td>2</td><td>1</td><td></td></tr><tr><td>Rerank</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Retrieval-Augmented Generation</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Scaling Law</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>4</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Self-supervised Pre-training</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Sentence Embedding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td>2</td><td>1</td><td>5</td></tr><tr><td>Simulator</td><td></td><td>2</td><td>1</td><td>5</td></tr><tr><td>Stemming</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td>3</td><td></td></tr><tr><td>Summarization</td><td>4</td><td></td><td>1</td><td></td></tr><tr><td>Supervised Learning</td><td>4</td><td>7</td><td></td><td></td></tr><tr><td>Text Classification</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Text Generation</td><td>3</td><td>1</td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>11</td><td></td><td></td></tr><tr><td>Textual Entailment</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td>4</td><td>1</td><td></td></tr><tr><td>Transformer</td><td>4</td><td>15</td><td>4</td><td>1</td></tr><tr><td>Unsupervised Learning</td><td>1</td><td>8</td><td></td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Video-and-Language</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>10</td><td></td><td>2</td></tr><tr><td>Vision-and-Language</td><td>1</td><td>12</td><td>1</td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td>5</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>3</td><td>4</td><td>1</td><td>1</td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscv-89>cs.CV (89)</h2><h3 id=189--1273-llama-excitor-general-instruction-tuning-via-indirect-feature-interaction-bo-zou-et-al-2024>(1/89 | 1/273) LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction (Bo Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Zou, Chao Yang, Yu Qiao, Chengbin Quan, Youjian Zhao. (2024)<br><strong>LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction</strong><br><button class=copy-to-clipboard title="LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 116<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Multi-modal, LLaMA, Transformer, Instruction Following, Massive Multitask Language Understanding (MMLU), Instruction Tuning, Large Language Model, Prompt, Self-Attention, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00913v1.pdf filename=2404.00913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing methods to <b>fine-tune</b> <b>LLMs,</b> like Adapter, Prefix-tuning, and LoRA, which introduce extra modules or additional input sequences to inject new skills or knowledge, may compromise the innate abilities of <b>LLMs.</b> In this paper, we propose <b>LLaMA-Excitor,</b> a lightweight method that stimulates the <b>LLMs&rsquo;</b> potential to better follow <b>instructions</b> <b>by</b> gradually paying more attention to worthwhile information. Specifically, the <b>LLaMA-Excitor</b> does not directly change the intermediate hidden state during the <b>self-attention</b> calculation of the <b>transformer</b> structure. We designed the Excitor block as a bypass module for the similarity score computation in <b>LLMs&rsquo;</b> <b>self-attention</b> to reconstruct keys and change the importance of values by learnable <b>prompts.</b> <b>LLaMA-Excitor</b> ensures a self-adaptive allocation of additional attention to input <b>instructions,</b> <b>thus</b> effectively preserving <b>LLMs&rsquo;</b> pre-trained knowledge when <b>fine-tuning</b> <b>LLMs</b> on low-quality <b>instruction-following</b> <b>datasets.</b> Furthermore, we unify the modeling of <b>multi-modal</b> tuning and language-only tuning, extending <b>LLaMA-Excitor</b> to a powerful visual <b>instruction</b> <b>follower</b> without the need for complex <b>multi-modal</b> alignment. Our proposed approach is evaluated in language-only and <b>multi-modal</b> tuning experimental scenarios. Notably, <b>LLaMA-Excitor</b> is the only method that maintains basic capabilities while achieving a significant improvement (+6%) on the <b>MMLU</b> <b>benchmark.</b> In the visual <b>instruction</b> <b>tuning,</b> we achieve a new state-of-the-art image captioning performance of 157.5 CIDEr on MSCOCO, and a comparable performance (88.39%) on ScienceQA to cutting-edge models with more parameters and extensive <b>vision-language</b> pertaining.</p></p class="citation"></blockquote><h3 id=289--2273-nerf-mae--masked-autoencoders-for-self-supervised-3d-representation-learning-for-neural-radiance-fields-muhammad-zubair-irshad-et-al-2024>(2/89 | 2/273) NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields (Muhammad Zubair Irshad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Zubair Irshad, Sergey Zakahrov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, Rares Ambrus. (2024)<br><strong>NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields</strong><br><button class=copy-to-clipboard title="NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 100<br>Keywords: Vision Transformer, Object Detection, Autoencoder, Geometry, Representation Learning, Self-supervised Learning, Self-supervised Pre-training, Supervised Learning, Transfer Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01300v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01300v1.pdf filename=2404.01300v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural fields excel in computer <b>vision</b> <b>and</b> robotics due to their ability to understand the 3D visual world such as inferring semantics, <b>geometry,</b> and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their <b>self-supervised</b> <b>pretraining,</b> specifically using masked <b>autoencoders,</b> to generate effective 3D <b>representations</b> <b>from</b> posed RGB images. Owing to the astounding success of extending <b>transformers</b> to novel data modalities, we employ standard 3D <b>Vision</b> <b>Transformers</b> to suit the unique formulation of NeRFs. We leverage NeRF&rsquo;s volumetric grid as a dense input to the <b>transformer,</b> contrasting it with other 3D <b>representations</b> <b>such</b> as pointclouds where the information density can be uneven, and the <b>representation</b> <b>is</b> irregular. Due to the difficulty of applying masked <b>autoencoders</b> to an implicit <b>representation,</b> <b>such</b> as NeRF, we opt for extracting an explicit <b>representation</b> <b>that</b> canonicalizes scenes across domains by employing the camera trajectory for sampling. Our goal is made possible by masking random patches from NeRF&rsquo;s radiance and density grid and employing a standard 3D Swin <b>Transformer</b> to reconstruct the masked patches. In doing so, the model can learn the semantic and spatial structure of complete scenes. We pretrain this <b>representation</b> <b>at</b> scale on our proposed curated posed-RGB data, totaling over 1.6 million images. Once pretrained, the encoder is used for effective 3D <b>transfer</b> <b>learning.</b> Our novel <b>self-supervised</b> <b>pretraining</b> for NeRFs, NeRF-MAE, scales remarkably well and improves performance on various challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining, NeRF-MAE significantly outperforms <b>self-supervised</b> <b>3D</b> pretraining and NeRF scene understanding baselines on Front3D and ScanNet datasets with an absolute performance improvement of over 20% AP50 and 8% AP25 for 3D <b>object</b> <b>detection.</b></p></p class="citation"></blockquote><h3 id=389--3273-instance-aware-group-quantization-for-vision-transformers-jaehyeon-moon-et-al-2024>(3/89 | 3/273) Instance-Aware Group Quantization for Vision Transformers (Jaehyeon Moon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaehyeon Moon, Dohyung Kim, Junyong Cheon, Bumsub Ham. (2024)<br><strong>Instance-Aware Group Quantization for Vision Transformers</strong><br><button class=copy-to-clipboard title="Instance-Aware Group Quantization for Vision Transformers" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 100<br>Keywords: Vision Transformer, Object Detection, Convolution, Convolutional Neural Network, Convolutional Neural Network, Model Compression, Quantization, Quantization, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00928v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00928v1.pdf filename=2404.00928v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Post-training <b>quantization</b> (PTQ) is an efficient <b>model</b> <b>compression</b> technique that <b>quantizes</b> a pretrained full-precision <b>model</b> <b>using</b> only a small calibration set of unlabeled samples without retraining. PTQ methods for <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> provide <b>quantization</b> results comparable to full-precision counterparts. Directly applying them to <b>vision</b> <b>transformers</b> (ViTs), however, incurs severe performance degradation, mainly due to the differences in architectures between <b>CNNs</b> and ViTs. In particular, the distribution of activations for each channel vary drastically according to input instances, making PTQ methods for <b>CNNs</b> inappropriate for ViTs. To address this, we introduce instance-aware group <b>quantization</b> for ViTs (IGQ-ViT). To this end, we propose to split the channels of activation maps into multiple groups dynamically for each input instance, such that activations within each group share similar statistical properties. We also extend our scheme to <b>quantize</b> softmax attentions across tokens. In addition, the number of groups for each layer is adjusted to minimize the discrepancies between predictions from <b>quantized</b> and full-precision <b>models,</b> <b>under</b> a bit-operation (BOP) constraint. We show extensive experimental results on image classification, <b>object</b> <b>detection,</b> and instance segmentation, with various <b>transformer</b> architectures, demonstrating the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=489--4273-learning-by-correction-efficient-tuning-task-for-zero-shot-generative-vision-language-reasoning-rongjie-li-et-al-2024>(4/89 | 4/273) Learning by Correction: Efficient Tuning Task for Zero-Shot Generative Vision-Language Reasoning (Rongjie Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rongjie Li, Yu Wu, Xuming He. (2024)<br><strong>Learning by Correction: Efficient Tuning Task for Zero-Shot Generative Vision-Language Reasoning</strong><br><button class=copy-to-clipboard title="Learning by Correction: Efficient Tuning Task for Zero-Shot Generative Vision-Language Reasoning" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 100<br>Keywords: Zero-shot, Image2text, Instruction Following, Question Answering, Reasoning, Text Generation, Visual Question Answering, Instruction Tuning, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00909v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00909v1.pdf filename=2404.00909v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative <b>vision-language</b> models (VLMs) have shown impressive performance in <b>zero-shot</b> <b>vision-language</b> tasks like image captioning and <b>visual</b> <b>question</b> <b>answering.</b> However, improving their <b>zero-shot</b> <b>reasoning</b> typically requires second-stage <b>instruction</b> <b>tuning,</b> which relies heavily on human-labeled or <b>large</b> <b>language</b> <b>model-generated</b> annotation, incurring high labeling costs. To tackle this challenge, we introduce Image-Conditioned Caption Correction (ICCC), a novel pre-training task designed to enhance VLMs&rsquo; <b>zero-shot</b> performance without the need for labeled task-aware data. The ICCC task compels VLMs to rectify mismatches between <b>visual</b> <b>and</b> <b>language</b> concepts, thereby enhancing <b>instruction</b> <b>following</b> and <b>text</b> <b>generation</b> conditioned on <b>visual</b> <b>inputs.</b> <b>Leveraging</b> language structure and a lightweight dependency parser, we construct data samples of ICCC task from <b>image-text</b> datasets with low labeling and computation costs. Experimental results on BLIP-2 and InstructBLIP demonstrate significant improvements in <b>zero-shot</b> <b>image-text</b> generation-based VL tasks through ICCC <b>instruction</b> <b>tuning.</b></p></p class="citation"></blockquote><h3 id=589--5273-evaluating-text-to-visual-generation-with-image-to-text-generation-zhiqiu-lin-et-al-2024>(5/89 | 5/273) Evaluating Text-to-Visual Generation with Image-to-Text Generation (Zhiqiu Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, Deva Ramanan. (2024)<br><strong>Evaluating Text-to-Visual Generation with Image-to-Text Generation</strong><br><button class=copy-to-clipboard title="Evaluating Text-to-Visual Generation with Image-to-Text Generation" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs-MM, cs.CV<br>Keyword Score: 93<br>Keywords: Benchmarking, Generative AI, Bag-of-Words, GPT, Image2text, Image2text, Reasoning, Visual Question Answering, Visual Question Answering, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01291v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01291v1.pdf filename=2404.01291v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite significant progress in <b>generative</b> <b>AI,</b> comprehensive evaluation remains challenging because of the lack of effective metrics and standardized <b>benchmarks.</b> For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text <b>prompt,</b> but it fails to produce reliable scores for complex <b>prompts</b> involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a &ldquo;bag of words&rdquo;, conflating <b>prompts</b> such as &ldquo;the horse is eating the grass&rdquo; with &ldquo;the grass is eating the horse&rdquo;. To address this, we introduce the VQAScore, which uses a <b>visual-question-answering</b> <b>(VQA)</b> <b>model</b> to produce an alignment score by computing the probability of a &ldquo;Yes&rdquo; answer to a simple &ldquo;Does this figure show &lsquo;{text}&rsquo;?&rdquo; question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) <b>image-text</b> alignment <b>benchmarks.</b> We also compute VQAScore with an in-house model that follows best practices in the literature. For example, we use a bidirectional image-question encoder that allows image embeddings to depend on the question being asked (and vice versa). Our in-house model, CLIP-FlanT5, outperforms even the strongest baselines that make use of the proprietary <b>GPT-4V.</b> Interestingly, although we train with only images, VQAScore can also align text with video and 3D models. VQAScore allows researchers to <b>benchmark</b> text-to-visual generation using complex texts that capture the compositional structure of real-world <b>prompts.</b> We introduce GenAI-Bench, a more challenging <b>benchmark</b> with 1,600 compositional text <b>prompts</b> that require parsing scenes, objects, attributes, relationships, and high-order <b>reasoning</b> like comparison and logic. GenAI-Bench also offers over 15,000 human ratings for leading image and video generation models such as Stable Diffusion, DALL-E 3, and Gen2.</p></p class="citation"></blockquote><h3 id=689--6273-sugar-pre-training-3d-visual-representations-for-robotics-shizhe-chen-et-al-2024>(6/89 | 6/273) SUGAR: Pre-training 3D Visual Representations for Robotics (Shizhe Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shizhe Chen, Ricardo Garcia, Ivan Laptev, Cordelia Schmid. (2024)<br><strong>SUGAR: Pre-training 3D Visual Representations for Robotics</strong><br><button class=copy-to-clipboard title="SUGAR: Pre-training 3D Visual Representations for Robotics" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 80<br>Keywords: Geometry, Knowledge Distillation, Knowledge Distillation, Representation Learning, Simulation, Simulator, Zero-shot, Transformer, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01491v1.pdf filename=2404.01491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning generalizable visual <b>representations</b> <b>from</b> Internet data has yielded promising results for robotics. Yet, prevailing approaches focus on pre-training 2D <b>representations,</b> <b>being</b> sub-optimal to deal with occlusions and accurately localize objects in complex 3D scenes. Meanwhile, 3D <b>representation</b> <b>learning</b> has been limited to single-object understanding. To address these limitations, we introduce a novel 3D pre-training framework for robotics named SUGAR that captures semantic, geometric and affordance properties of objects through 3D point clouds. We underscore the importance of cluttered scenes in 3D <b>representation</b> <b>learning,</b> and automatically construct a multi-object dataset benefiting from cost-free supervision in <b>simulation.</b> SUGAR employs a versatile <b>transformer-based</b> model to jointly address five pre-training tasks, namely cross-modal <b>knowledge</b> <b>distillation</b> for semantic learning, masked point modeling to understand <b>geometry</b> structures, grasping pose synthesis for object affordance, 3D instance segmentation and referring expression <b>grounding</b> to analyze cluttered scenes. We evaluate our learned <b>representation</b> <b>on</b> three robotic-related tasks, namely, <b>zero-shot</b> 3D object recognition, referring expression <b>grounding,</b> and language-driven robotic manipulation. Experimental results show that SUGAR&rsquo;s 3D <b>representation</b> <b>outperforms</b> state-of-the-art 2D and 3D representations.</p></p class="citation"></blockquote><h3 id=789--7273-direct-preference-optimization-of-video-large-multimodal-models-from-language-model-reward-ruohong-zhang-et-al-2024>(7/89 | 7/273) Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward (Ruohong Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, Yiming Yang. (2024)<br><strong>Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward</strong><br><button class=copy-to-clipboard title="Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 76<br>Keywords: Direct Preference Optimization, Multi-modal, Multi-modal, GPT, Instruction Following, Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01258v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01258v2.pdf filename=2404.01258v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Preference modeling techniques, such as <b>direct</b> <b>preference</b> <b>optimization</b> (DPO), has shown effective in enhancing the generalization abilities of <b>large</b> <b>language</b> <b>model</b> <b>(LLM).</b> However, in tasks involving video <b>instruction-following,</b> <b>providing</b> informative feedback, especially for detecting hallucinations in generated responses, remains a significant challenge. Previous studies have explored using <b>large</b> <b>large</b> <b>multimodal</b> <b>models</b> (LMMs) as reward models to guide preference modeling, but their ability to accurately assess the factuality of generated responses compared to corresponding videos has not been conclusively established. This paper introduces a novel framework that utilizes detailed video captions as a proxy of video content, enabling language models to incorporate this information as supporting evidence for scoring video <b>Question</b> <b>Answering</b> <b>(QA)</b> predictions. Our approach demonstrates robust alignment with OpenAI <b>GPT-4V</b> model&rsquo;s reward mechanism, which directly takes video frames as input. Furthermore, we show that applying this tailored reward through DPO significantly improves the performance of video LMMs on video <b>QA</b> tasks.</p></p class="citation"></blockquote><h3 id=889--8273-vision-language-models-for-decoding-provider-attention-during-neonatal-resuscitation-felipe-parodi-et-al-2024>(8/89 | 8/273) Vision-language models for decoding provider attention during neonatal resuscitation (Felipe Parodi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felipe Parodi, Jordan Matelsky, Alejandra Regla-Vargas, Elizabeth Foglia, Charis Lim, Danielle Weinberg, Konrad Kording, Heidi Herrick, Michael Platt. (2024)<br><strong>Vision-language models for decoding provider attention during neonatal resuscitation</strong><br><button class=copy-to-clipboard title="Vision-language models for decoding provider attention during neonatal resuscitation" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Vision Transformer, Fine-tuning, Simulation, Simulator, Transformer, Vision Transformer, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01207v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01207v1.pdf filename=2404.01207v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neonatal resuscitations demand an exceptional level of attentiveness from providers, who must process multiple streams of information simultaneously. Gaze strongly influences decision making; thus, understanding where a provider is looking during neonatal resuscitations could inform provider training, enhance real-time decision support, and improve the design of delivery rooms and neonatal intensive care units (NICUs). Current approaches to quantifying neonatal providers&rsquo; gaze rely on manual coding or <b>simulations,</b> which limit scalability and utility. Here, we introduce an automated, real-time, deep learning approach capable of decoding provider gaze into semantic classes directly from first-person point-of-view videos recorded during live resuscitations. Combining state-of-the-art, real-time segmentation with <b>vision-language</b> <b>models</b> (CLIP), our low-shot pipeline attains 91% classification accuracy in identifying gaze targets without training. Upon <b>fine-tuning,</b> the performance of our gaze-guided <b>vision</b> <b>transformer</b> exceeds 98% accuracy in gaze classification, approaching human-level precision. This system, capable of real-time inference, enables objective quantification of provider attention dynamics during live neonatal resuscitation. Our approach offers a scalable solution that seamlessly integrates with existing infrastructure for data-scarce gaze analysis, thereby offering new opportunities for understanding and refining clinical decision making.</p></p class="citation"></blockquote><h3 id=989--9273-detect2interact-localizing-object-key-field-in-visual-question-answering-vqa-with-llms-jialou-wang-et-al-2024>(9/89 | 9/273) Detect2Interact: Localizing Object Key Field in Visual Question Answering (VQA) with LLMs (Jialou Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialou Wang, Manli Zhu, Yulei Li, Honglei Li, Longzhi Yang, Wai Lok Woo. (2024)<br><strong>Detect2Interact: Localizing Object Key Field in Visual Question Answering (VQA) with LLMs</strong><br><button class=copy-to-clipboard title="Detect2Interact: Localizing Object Key Field in Visual Question Answering (VQA) with LLMs" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Object Detection, GPT, GPT-4, Question Answering, Visual Question Answering, Visual Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01151v1.pdf filename=2404.01151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Localization plays a crucial role in enhancing the practicality and precision of <b>VQA</b> systems. By enabling fine-grained identification and interaction with specific parts of an <b>object,</b> <b>it</b> significantly improves the system&rsquo;s ability to provide contextually relevant and spatially accurate responses, crucial for applications in dynamic environments like robotics and augmented reality. However, traditional systems face challenges in accurately mapping <b>objects</b> <b>within</b> images to generate nuanced and spatially aware responses. In this work, we introduce &ldquo;Detect2Interact&rdquo;, which addresses these challenges by introducing an advanced approach for fine-grained <b>object</b> <b>visual</b> <b>key</b> <b>field</b> detection. First, we use the segment anything model (SAM) to generate detailed spatial maps of <b>objects</b> <b>in</b> images. Next, we use Vision Studio to extract semantic <b>object</b> <b>descriptions.</b> Third, we employ <b>GPT-4&rsquo;s</b> common sense knowledge, bridging the gap between an <b>object&rsquo;s</b> <b>semantics</b> and its spatial map. As a result, Detect2Interact achieves consistent qualitative results on <b>object</b> <b>key</b> field detection across extensive test cases and outperforms the existing <b>VQA</b> system with <b>object</b> <b>detection</b> by providing a more reasonable and finer visual representation.</p></p class="citation"></blockquote><h3 id=1089--10273-structured-initialization-for-attention-in-vision-transformers-jianqiao-zheng-et-al-2024>(10/89 | 10/273) Structured Initialization for Attention in Vision Transformers (Jianqiao Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianqiao Zheng, Xueqian Li, Simon Lucey. (2024)<br><strong>Structured Initialization for Attention in Vision Transformers</strong><br><button class=copy-to-clipboard title="Structured Initialization for Attention in Vision Transformers" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Vision Transformer, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01139v1.pdf filename=2404.01139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The training of <b>vision</b> <b>transformer</b> (ViT) networks on small-scale datasets poses a significant challenge. By contrast, <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> have an architectural inductive bias enabling them to perform well on such problems. In this paper, we argue that the architectural bias inherent to <b>CNNs</b> can be reinterpreted as an initialization bias within ViT. This insight is significant as it empowers ViTs to perform equally well on small-scale problems while maintaining their flexibility for large-scale applications. Our inspiration for this ``structured&rsquo;&rsquo; initialization stems from our empirical observation that random impulse filters can achieve comparable performance to learned filters within <b>CNNs.</b> Our approach achieves state-of-the-art performance for data-efficient ViT learning across numerous <b>benchmarks</b> including CIFAR-10, CIFAR-100, and SVHN.</p></p class="citation"></blockquote><h3 id=1189--11273-harnessing-large-language-models-for-training-free-video-anomaly-detection-luca-zanella-et-al-2024>(11/89 | 11/273) Harnessing Large Language Models for Training-free Video Anomaly Detection (Luca Zanella et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Zanella, Willi Menapace, Massimiliano Mancini, Yiming Wang, Elisa Ricci. (2024)<br><strong>Harnessing Large Language Models for Training-free Video Anomaly Detection</strong><br><button class=copy-to-clipboard title="Harnessing Large Language Models for Training-free Video Anomaly Detection" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Anomaly Detection, Unsupervised Learning, Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01014v1.pdf filename=2404.01014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video <b>anomaly</b> <b>detection</b> (VAD) aims to temporally locate abnormal events in a video. Existing works mostly rely on training deep models to learn the distribution of normality with either video-level supervision, one-class supervision, or in an <b>unsupervised</b> setting. Training-based methods are prone to be domain-specific, thus being costly for practical deployment as any domain change will involve data collection and model training. In this paper, we radically depart from previous efforts and propose LAnguage-based VAD (LAVAD), a method tackling VAD in a novel, training-free paradigm, exploiting the capabilities of pre-trained <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and existing <b>vision-language</b> models (VLMs). We leverage VLM-based captioning models to generate textual descriptions for each frame of any test video. With the textual scene description, we then devise a <b>prompting</b> mechanism to unlock the capability of <b>LLMs</b> in terms of temporal aggregation and <b>anomaly</b> <b>score</b> estimation, turning <b>LLMs</b> into an effective video <b>anomaly</b> <b>detector.</b> We further leverage modality-aligned VLMs and propose effective techniques based on cross-modal similarity for cleaning noisy captions and refining the <b>LLM-based</b> <b>anomaly</b> <b>scores.</b> We evaluate LAVAD on two <b>large</b> <b>datasets</b> <b>featuring</b> real-world surveillance scenarios (UCF-Crime and XD-Violence), showing that it outperforms both <b>unsupervised</b> and one-class methods without requiring any training or data collection.</p></p class="citation"></blockquote><h3 id=1289--12273-transfer-learning-with-point-transformers-kartik-gupta-et-al-2024>(12/89 | 12/273) Transfer Learning with Point Transformers (Kartik Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kartik Gupta, Rahul Vippala, Sahima Srivastava. (2024)<br><strong>Transfer Learning with Point Transformers</strong><br><button class=copy-to-clipboard title="Transfer Learning with Point Transformers" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 60<br>Keywords: MNIST, Fine-tuning, Fine-tuning, Transfer Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00846v1.pdf filename=2404.00846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Point <b>Transformers</b> are near state-of-the-art models for classification, segmentation, and detection tasks on Point Cloud data. They utilize a self attention based mechanism to model large range spatial dependencies between multiple point sets. In this project we explore two things: classification performance of these attention based networks on ModelNet10 dataset and then, we use the trained model to classify 3D <b>MNIST</b> dataset after <b>finetuning.</b> We also train the model from scratch on 3D <b>MNIST</b> dataset to compare the performance of <b>finetuned</b> and from-scratch model on the <b>MNIST</b> dataset. We observe that since the two datasets have a large difference in the degree of the distributions, <b>transfer</b> <b>learned</b> models do not outperform the from-scratch models in this case. Although we do expect <b>transfer</b> <b>learned</b> models to converge faster since they already know the lower level edges, corners, etc features from the ModelNet10 dataset.</p></p class="citation"></blockquote><h3 id=1389--13273-s2rc-gcn-a-spatial-spectral-reliable-contrastive-graph-convolutional-network-for-complex-land-cover-classification-using-hyperspectral-images-renxiang-guan-et-al-2024>(13/89 | 13/273) S2RC-GCN: A Spatial-Spectral Reliable Contrastive Graph Convolutional Network for Complex Land Cover Classification Using Hyperspectral Images (Renxiang Guan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renxiang Guan, Zihao Li, Chujia Song, Guo Yu, Xianju Li, Ruyi Feng. (2024)<br><strong>S2RC-GCN: A Spatial-Spectral Reliable Contrastive Graph Convolutional Network for Complex Land Cover Classification Using Hyperspectral Images</strong><br><button class=copy-to-clipboard title="S2RC-GCN: A Spatial-Spectral Reliable Contrastive Graph Convolutional Network for Complex Land Cover Classification Using Hyperspectral Images" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Contrastive Learning, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00964v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00964v1.pdf filename=2404.00964v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spatial correlations between different ground objects are an important feature of mining land cover research. <b>Graph</b> <b>Convolutional</b> <b>Networks</b> <b>(GCNs)</b> can effectively capture such spatial feature representations and have demonstrated promising results in performing hyperspectral imagery (HSI) classification tasks of complex land. However, the existing <b>GCN-based</b> HSI classification methods are prone to interference from redundant information when extracting complex features. To classify complex scenes more effectively, this study proposes a novel spatial-spectral reliable <b>contrastive</b> <b>graph</b> <b>convolutional</b> <b>classification</b> framework named S2RC-GCN. Specifically, we fused the spectral and spatial features extracted by the 1D- and 2D-encoder, and the 2D-encoder includes an attention model to automatically extract important information. We then leveraged the fused high-level features to construct <b>graphs</b> <b>and</b> <b>fed</b> the resulting <b>graphs</b> <b>into</b> <b>the</b> <b>GCNs</b> to determine more effective <b>graph</b> <b>representations.</b> <b>Furthermore,</b> a novel reliable <b>contrastive</b> <b>graph</b> <b>convolution</b> <b>was</b> proposed for reliable <b>contrastive</b> <b>learning</b> to learn and fuse robust features. Finally, to test the performance of the model on complex object classification, we used imagery taken by Gaofen-5 in the Jiang Xia area to construct complex land cover datasets. The test results show that compared with other models, our model achieved the best results and effectively improved the classification performance of complex remote sensing imagery.</p></p class="citation"></blockquote><h3 id=1489--14273-prompt-learning-for-oriented-power-transmission-tower-detection-in-high-resolution-sar-images-tianyang-li-et-al-2024>(14/89 | 14/273) Prompt Learning for Oriented Power Transmission Tower Detection in High-Resolution SAR Images (Tianyang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyang Li, Chao Wang, Hong Zhang. (2024)<br><strong>Prompt Learning for Oriented Power Transmission Tower Detection in High-Resolution SAR Images</strong><br><button class=copy-to-clipboard title="Prompt Learning for Oriented Power Transmission Tower Detection in High-Resolution SAR Images" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 51<br>Keywords: Object Detection, Geometry, Multi-modal, Multi-modal, Transformer, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01074v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01074v1.pdf filename=2404.01074v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting transmission towers from synthetic aperture radar (SAR) images remains a challenging task due to the comparatively small size and side-looking <b>geometry,</b> with background clutter interference frequently hindering tower identification. A large number of interfering signals superimposes the return signal from the tower. We found that localizing or <b>prompting</b> <b>positions</b> of power transmission towers is beneficial to address this obstacle. Based on this revelation, this paper introduces <b>prompt</b> <b>learning</b> into the oriented <b>object</b> <b>detector</b> (P2Det) for <b>multimodal</b> information learning. P2Det contains the sparse <b>prompt</b> <b>coding</b> and cross-attention between the <b>multimodal</b> data. Specifically, the sparse <b>prompt</b> <b>encoder</b> (SPE) is proposed to represent point locations, converting <b>prompts</b> <b>into</b> sparse embeddings. The image embeddings are generated through the <b>Transformer</b> layers. Then a two-way fusion module (TWFM) is proposed to calculate the cross-attention of the two different embeddings. The interaction of image-level and <b>prompt-level</b> <b>features</b> is utilized to address the clutter interference. A shape-adaptive refinement module (SARM) is proposed to reduce the effect of aspect ratio. Extensive experiments demonstrated the effectiveness of the proposed model on high-resolution SAR images. P2Det provides a novel insight for <b>multimodal</b> <b>object</b> <b>detection</b> due to its competitive performance.</p></p class="citation"></blockquote><h3 id=1589--15273-t-mamba-frequency-enhanced-gated-long-range-dependency-for-tooth-3d-cbct-segmentation-jing-hao-et-al-2024>(15/89 | 15/273) T-Mamba: Frequency-Enhanced Gated Long-Range Dependency for Tooth 3D CBCT Segmentation (Jing Hao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Hao, Lei He, Kuo Feng Hung. (2024)<br><strong>T-Mamba: Frequency-Enhanced Gated Long-Range Dependency for Tooth 3D CBCT Segmentation</strong><br><button class=copy-to-clipboard title="T-Mamba: Frequency-Enhanced Gated Long-Range Dependency for Tooth 3D CBCT Segmentation" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Graph Attention Networks, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01065v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01065v1.pdf filename=2404.01065v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient tooth segmentation in three-dimensional (3D) imaging, critical for orthodontic diagnosis, remains challenging due to noise, low contrast, and artifacts in CBCT images. Both <b>convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> and <b>transformers</b> have emerged as popular architectures for image segmentation. However, their efficacy in handling long-range dependencies is limited due to inherent locality or computational complexity. To address this issue, we propose T-Mamba, integrating shared positional encoding and frequency-based features into vision mamba, to address limitations in spatial position preservation and feature enhancement in frequency domain. Besides, we also design a gate selection unit to integrate two features in spatial domain and one feature in frequency domain adaptively. T-Mamba is the first work to introduce frequency-based features into vision mamba. Extensive experiments demonstrate that T-Mamba achieves new SOTA results on the public Tooth CBCT dataset and outperforms previous SOTA methods by a large margin, i.e., IoU + 3.63%, SO + 2.43%, DSC +2.30%, HD -4.39mm, and ASSD -0.37mm. The code and models are publicly available at <a href=https://github.com/isbrycee/T-Mamba>https://github.com/isbrycee/T-Mamba</a>.</p></p class="citation"></blockquote><h3 id=1689--16273-harnessing-the-power-of-attention-for-patch-based-biomedical-image-classification-gousia-habib-et-al-2024>(16/89 | 16/273) Harnessing The Power of Attention For Patch-Based Biomedical Image Classification (Gousia Habib et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gousia Habib, Shaima Qureshi, Malik ishfaq. (2024)<br><strong>Harnessing The Power of Attention For Patch-Based Biomedical Image Classification</strong><br><button class=copy-to-clipboard title="Harnessing The Power of Attention For Patch-Based Biomedical Image Classification" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Data Augmentation, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00949v1.pdf filename=2404.00949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Biomedical image analysis can be facilitated by an innovative architecture rooted in <b>self-attention</b> mechanisms. The traditional <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN),</b> characterized by fixed-sized windows, needs help capturing intricate spatial and temporal relations at the pixel level. The immutability of <b>CNN</b> filter weights post-training further restricts input fluctuations. Recognizing these limitations, we propose a new paradigm of attention-based models instead of <b>convolutions.</b> As an alternative to traditional <b>CNNs,</b> these models demonstrate robust modelling capabilities and the ability to grasp comprehensive long-range contextual information efficiently. Providing a solution to critical challenges faced by attention-based vision models such as inductive bias, weight sharing, receptive field limitations, and <b>data</b> <b>handling</b> in high resolution, our work combines non-overlapping (vanilla patching) with novel overlapped Shifted Patching Techniques (S.P.T.s) to induce local context that enhances model generalization. Moreover, we examine the novel Lancoz5 interpolation technique, which adapts variable image sizes to higher resolutions. Experimental evidence validates our model&rsquo;s generalization effectiveness, comparing favourably with existing approaches. Attention-based methods are particularly effective with ample <b>data,</b> <b>especially</b> when advanced <b>data</b> <b>augmentation</b> methodologies are integrated to strengthen their robustness.</p></p class="citation"></blockquote><h3 id=1789--17273-model-agnostic-human-preference-inversion-in-diffusion-models-jeeyung-kim-et-al-2024>(17/89 | 17/273) Model-Agnostic Human Preference Inversion in Diffusion Models (Jeeyung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeeyung Kim, Ze Wang, Qiang Qiu. (2024)<br><strong>Model-Agnostic Human Preference Inversion in Diffusion Models</strong><br><button class=copy-to-clipboard title="Model-Agnostic Human Preference Inversion in Diffusion Models" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Fine-tuning, Knowledge Distillation, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00879v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00879v1.pdf filename=2404.00879v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient <b>text-to-image</b> generation remains a challenging task due to the high computational costs associated with the multi-step sampling in <b>diffusion</b> <b>models.</b> Although <b>distillation</b> of pre-trained <b>diffusion</b> <b>models</b> has been successful in reducing sampling steps, low-step image generation often falls short in terms of quality. In this study, we propose a novel sampling design to achieve high-quality one-step image generation aligning with human preferences, particularly focusing on exploring the impact of the prior noise distribution. Our approach, <b>Prompt</b> Adaptive Human Preference Inversion (PAHI), optimizes the noise distributions for each <b>prompt</b> based on human preferences without the need for <b>fine-tuning</b> <b>diffusion</b> <b>models.</b> Our experiments showcase that the tailored noise distributions significantly improve image quality with only a marginal increase in computational cost. Our findings underscore the importance of noise optimization and pave the way for efficient and high-quality <b>text-to-image</b> synthesis.</p></p class="citation"></blockquote><h3 id=1889--18273-prompt-learning-via-meta-regularization-jinyoung-park-et-al-2024>(18/89 | 18/273) Prompt Learning via Meta-Regularization (Jinyoung Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinyoung Park, Juyeon Ko, Hyunwoo J. Kim. (2024)<br><strong>Prompt Learning via Meta-Regularization</strong><br><button class=copy-to-clipboard title="Prompt Learning via Meta-Regularization" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Fine-tuning, Zero-shot, Prompt, Prompt Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00851v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00851v1.pdf filename=2404.00851v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-trained <b>vision-language</b> models have shown impressive success on various computer vision tasks with their <b>zero-shot</b> generalizability. Recently, <b>prompt</b> <b>learning</b> approaches have been explored to efficiently and effectively adapt the <b>vision-language</b> models to a variety of downstream tasks. However, most existing <b>prompt</b> <b>learning</b> methods suffer from task overfitting since the general knowledge of the pre-trained vision language models is forgotten while the <b>prompts</b> <b>are</b> <b>finetuned</b> on a small data set from a specific target task. To address this issue, we propose a <b>Prompt</b> <b>Meta-Regularization</b> (ProMetaR) to improve the generalizability of <b>prompt</b> <b>learning</b> for <b>vision-language</b> models. Specifically, ProMetaR meta-learns both the regularizer and the soft <b>prompts</b> <b>to</b> harness the task-specific knowledge from the downstream tasks and task-agnostic general knowledge from the <b>vision-language</b> models. Further, ProMetaR augments the task to generate multiple virtual tasks to alleviate the meta-overfitting. In addition, we provide the analysis to comprehend how ProMetaR improves the generalizability of <b>prompt</b> <b>tuning</b> in the perspective of the gradient alignment. Our extensive experiments demonstrate that our ProMetaR improves the generalizability of conventional <b>prompt</b> <b>learning</b> methods under base-to-base/base-to-new and domain generalization settings. The code of ProMetaR is available at <a href=https://github.com/mlvlab/ProMetaR>https://github.com/mlvlab/ProMetaR</a>.</p></p class="citation"></blockquote><h3 id=1989--19273-equivariant-local-reference-frames-for-unsupervised-non-rigid-point-cloud-shape-correspondence-ling-wang-et-al-2024>(19/89 | 19/273) Equivariant Local Reference Frames for Unsupervised Non-rigid Point Cloud Shape Correspondence (Ling Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ling Wang, Runfa Chen, Yikai Wang, Fuchun Sun, Xinzhou Wang, Sun Kai, Guangyuan Fu, Jianwei Zhang, Wenbing Huang. (2024)<br><strong>Equivariant Local Reference Frames for Unsupervised Non-rigid Point Cloud Shape Correspondence</strong><br><button class=copy-to-clipboard title="Equivariant Local Reference Frames for Unsupervised Non-rigid Point Cloud Shape Correspondence" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Graph, Graph Neural Network, Benchmarking, Out-of-distribution, Unsupervised Learning, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00959v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00959v1.pdf filename=2404.00959v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> non-rigid point cloud shape correspondence underpins a multitude of 3D vision tasks, yet itself is non-trivial given the exponential complexity <b>stemming</b> from inter-point degree-of-freedom, i.e., pose transformations. Based on the assumption of local rigidity, one solution for reducing complexity is to decompose the overall shape into independent local regions using Local Reference Frames (LRFs) that are invariant to SE(3) transformations. However, the focus solely on local structure neglects global geometric contexts, resulting in less distinctive LRFs that lack crucial semantic information necessary for effective matching. Furthermore, such complexity introduces <b>out-of-distribution</b> geometric contexts during inference, thus complicating generalization. To this end, we introduce 1) EquiShape, a novel structure tailored to learn pair-wise LRFs with global structural cues for both spatial and semantic consistency, and 2) LRF-Refine, an optimization strategy generally applicable to LRF-based methods, aimed at addressing the generalization challenges. Specifically, for EquiShape, we employ cross-talk within separate equivariant <b>graph</b> <b>neural</b> <b>networks</b> (Cross-GVP) to build long-range dependencies to compensate for the lack of semantic information in local structure modeling, deducing pair-wise independent SE(3)-equivariant LRF vectors for each point. For LRF-Refine, the optimization adjusts LRFs within specific contexts and knowledge, enhancing the geometric and semantic generalizability of point features. Our overall framework surpasses the state-of-the-art methods by a large margin on three <b>benchmarks.</b> Code and models will be publicly available.</p></p class="citation"></blockquote><h3 id=2089--20273-magicmirror-fast-and-high-quality-avatar-generation-with-a-constrained-search-space-armand-comas-massagué-et-al-2024>(20/89 | 20/273) MagicMirror: Fast and High-Quality Avatar Generation with a Constrained Search Space (Armand Comas-Massagué et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler. (2024)<br><strong>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained Search Space</strong><br><button class=copy-to-clipboard title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained Search Space" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 45<br>Keywords: Diffusion Model, Geometry, Knowledge Distillation, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01296v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01296v1.pdf filename=2404.01296v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text <b>prompts</b> to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of <b>Text-to-Image</b> <b>Diffusion</b> <b>Models,</b> to ensure superior view invariance and enable direct optimization of avatar <b>geometry.</b> These foundational ideas are complemented by our optimization pipeline built on Variational Score <b>Distillation</b> (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text <b>prompts.</b> You can find more results and videos in our website: <a href=https://syntec-research.github.io/MagicMirror>https://syntec-research.github.io/MagicMirror</a></p></p class="citation"></blockquote><h3 id=2189--21273-videodistill-language-aware-vision-distillation-for-video-question-answering-bo-zou-et-al-2024>(21/89 | 21/273) VideoDistill: Language-aware Vision Distillation for Video Question Answering (Bo Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Zou, Chao Yang, Yu Qiao, Chengbin Quan, Youjian Zhao. (2024)<br><strong>VideoDistill: Language-aware Vision Distillation for Video Question Answering</strong><br><button class=copy-to-clipboard title="VideoDistill: Language-aware Vision Distillation for Video Question Answering" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Graph Attention Networks, Benchmarking, Knowledge Distillation, Question Answering, Video-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00973v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00973v1.pdf filename=2404.00973v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Significant advancements in video <b>question</b> <b>answering</b> (VideoQA) have been made thanks to thriving large image-language pretraining frameworks. Although these image-language models can efficiently represent both video and language branches, they typically employ a goal-free vision perception process and do not interact vision with language well during the answer generation, thus omitting crucial visual cues. In this paper, we are inspired by the human recognition and learning pattern and propose VideoDistill, a framework with language-aware (i.e., goal-driven) behavior in both vision perception and answer generation process. VideoDistill generates answers only from <b>question-related</b> <b>visual</b> embeddings and follows a thinking-observing-answering approach that closely resembles human behavior, distinguishing it from previous research. Specifically, we develop a language-aware <b>gating</b> mechanism to replace the standard cross-attention, avoiding language&rsquo;s direct fusion into visual representations. We incorporate this mechanism into two key components of the entire framework. The first component is a differentiable sparse sampling module, which selects frames containing the necessary dynamics and semantics relevant to the <b>questions.</b> <b>The</b> second component is a vision refinement module that merges existing spatial-temporal attention layers to ensure the extraction of multi-grained visual semantics associated with the <b>questions.</b> <b>We</b> conduct experimental evaluations on various challenging video <b>question-answering</b> <b>benchmarks,</b> and VideoDistill achieves state-of-the-art performance in both general and long-form VideoQA datasets. In Addition, we verify that VideoDistill can effectively alleviate the utilization of language shortcut solutions in the EgoTaskQA dataset.</p></p class="citation"></blockquote><h3 id=2289--22273-causalchaos-dataset-for-comprehensive-causal-action-question-answering-over-longer-causal-chains-grounded-in-dynamic-visual-scenes-ting-en-lam-et-al-2024>(22/89 | 22/273) CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes (Ting En Lam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ting En Lam, Yuhan Chen, Elston Tan, Eric Peh, Ruirui Chen, Paritosh Parmar, Basura Fernando. (2024)<br><strong>CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes</strong><br><button class=copy-to-clipboard title="CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Question Answering, Question Answering, Reasoning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01299v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01299v1.pdf filename=2404.01299v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Causal video <b>question</b> <b>answering</b> <b>(QA)</b> has garnered increasing interest, yet existing datasets often lack depth in causal <b>reasoning</b> analysis. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic &ldquo;Tom and Jerry&rdquo; cartoon series. With thoughtful <b>questions</b> <b>and</b> multi-level answers, our dataset contains much longer causal chains embedded in dynamic interactions and visuals, at the same time principles of animation allows animators to create well-defined, unambiguous causal relationships. These factors allow models to solve more challenging, yet well-defined causal relationships. We also introduce hard negative mining, including CausalConfusion version. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling and joint modeling of vision and language as the immediate areas for future efforts to focus upon. Along with the other complementary datasets, our new challenging dataset will pave the way for these developments in the field. We will release our dataset, codes, and models to help future efforts in this domain.</p></p class="citation"></blockquote><h3 id=2389--23273-cosmicman-a-text-to-image-foundation-model-for-humans-shikai-li-et-al-2024>(23/89 | 23/273) CosmicMan: A Text-to-Image Foundation Model for Humans (Shikai Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shikai Li, Jianglin Fu, Kaiyuan Liu, Wentao Wang, Kwan-Yee Lin, Wayne Wu. (2024)<br><strong>CosmicMan: A Text-to-Image Foundation Model for Humans</strong><br><button class=copy-to-clipboard title="CosmicMan: A Text-to-Image Foundation Model for Humans" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Foundation Model, Text2image, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01294v1.pdf filename=2404.01294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present CosmicMan, a <b>text-to-image</b> <b>foundation</b> <b>model</b> specialized for generating high-fidelity human images. Unlike current general-purpose <b>foundation</b> <b>models</b> that are stuck in the dilemma of inferior quality and <b>text-image</b> misalignment for humans, CosmicMan enables generating photo-realistic human images with meticulous appearance, reasonable structure, and precise <b>text-image</b> alignment with detailed dense descriptions. At the heart of CosmicMan&rsquo;s success are the new reflections and perspectives on data and models: (1) We found that data quality and a scalable data production flow are essential for the final results from trained models. Hence, we propose a new data production paradigm, Annotate Anyone, which serves as a perpetual data flywheel to produce high-quality data with accurate yet cost-effective annotations over time. Based on this, we constructed a large-scale dataset, CosmicMan-HQ 1.0, with 6 Million high-quality real-world human images in a mean resolution of 1488x1255, and attached with precise text annotations deriving from 115 Million attributes in diverse granularities. (2) We argue that a <b>text-to-image</b> <b>foundation</b> <b>model</b> specialized for humans must be pragmatic &ndash; easy to integrate into down-streaming tasks while effective in producing high-quality human images. Hence, we propose to model the relationship between dense text descriptions and image pixels in a decomposed manner, and present Decomposed-Attention-Refocusing (Daring) training framework. It seamlessly decomposes the cross-attention features in existing <b>text-to-image</b> <b>diffusion</b> <b>model,</b> and enforces attention refocusing without adding extra modules. Through Daring, we show that explicitly discretizing continuous text space into several basic groups that align with human body structure is the key to tackling the misalignment problem in a breeze.</p></p class="citation"></blockquote><h3 id=2489--24273-getting-it-right-improving-spatial-consistency-in-text-to-image-models-agneet-chatterjee-et-al-2024>(24/89 | 24/273) Getting it Right: Improving Spatial Consistency in Text-to-Image Models (Agneet Chatterjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Agneet Chatterjee, Gabriela Ben Melech Stan, Estelle Aflalo, Sayak Paul, Dhruba Ghosh, Tejas Gokhale, Ludwig Schmidt, Hannaneh Hajishirzi, Vasudev Lal, Chitta Baral, Yezhou Yang. (2024)<br><strong>Getting it Right: Improving Spatial Consistency in Text-to-Image Models</strong><br><button class=copy-to-clipboard title="Getting it Right: Improving Spatial Consistency in Text-to-Image Models" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Text2image, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01197v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01197v1.pdf filename=2404.01197v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One of the key shortcomings in current <b>text-to-image</b> (T2I) models is their inability to consistently generate images which faithfully follow the spatial relationships specified in the text <b>prompt.</b> In this paper, we offer a comprehensive investigation of this limitation, while also developing datasets and methods that achieve state-of-the-art performance. First, we find that current <b>vision-language</b> datasets do not represent spatial relationships well enough; to alleviate this bottleneck, we create SPRIGHT, the first spatially-focused, large scale dataset, by re-captioning 6 million images from 4 widely used vision datasets. Through a 3-fold evaluation and analysis pipeline, we find that SPRIGHT largely improves upon existing datasets in capturing spatial relationships. To demonstrate its efficacy, we leverage only ~0.25% of SPRIGHT and achieve a 22% improvement in generating spatially accurate images while also improving the FID and CMMD scores. Secondly, we find that training on images containing a large number of objects results in substantial improvements in spatial consistency. Notably, we attain state-of-the-art on T2I-CompBench with a spatial score of 0.2133, by <b>fine-tuning</b> on &lt;500 images. Finally, through a set of controlled experiments and ablations, we document multiple findings that we believe will enhance the understanding of factors that affect spatial consistency in <b>text-to-image</b> models. We publicly release our dataset and model to foster further research in this area.</p></p class="citation"></blockquote><h3 id=2589--25273-diagnosis-of-skin-cancer-using-vgg16-and-vgg19-based-transfer-learning-models-amir-faghihi-et-al-2024>(25/89 | 25/273) Diagnosis of Skin Cancer Using VGG16 and VGG19 Based Transfer Learning Models (Amir Faghihi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Faghihi, Mohammadreza Fathollahi, Roozbeh Rajabi. (2024)<br><strong>Diagnosis of Skin Cancer Using VGG16 and VGG19 Based Transfer Learning Models</strong><br><button class=copy-to-clipboard title="Diagnosis of Skin Cancer Using VGG16 and VGG19 Based Transfer Learning Models" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Fine-tuning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01160v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01160v1.pdf filename=2404.01160v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Today, skin cancer is considered as one of the most dangerous and common cancers in the world which demands special attention. Skin cancer may be developed in different types; including melanoma, actinic keratosis, basal cell carcinoma, squamous cell carcinoma, and Merkel cell carcinoma. Among them, melanoma is more unpredictable. Melanoma cancer can be diagnosed at early stages increasing the possibility of disease treatment. Automatic classification of skin lesions is a challenging task due to diverse forms and grades of the disease, demanding the requirement of novel methods implementation. Deep <b>convolution</b> neural networks <b>(CNN)</b> have shown an excellent potential for data and image classification. In this article, we inspect skin lesion classification problem using <b>CNN</b> techniques. Remarkably, we present that prominent classification accuracy of lesion detection can be obtained by proper designing and applying of <b>transfer</b> <b>learning</b> framework on pre-trained neural networks, without any requirement for data enlargement procedures i.e. merging VGG16 and VGG19 architectures pre-trained by a generic dataset with modified AlexNet network, and then, <b>fine-tuned</b> by a subject-specific dataset containing dermatology images. The <b>convolution</b> neural network was trained using 2541 images and, in particular, dropout was used to prevent the network from overfitting. Finally, the validity of the model was checked by applying the K-fold cross validation method. The proposed model increased classification accuracy by 3% (from 94.2% to 98.18%) in comparison with other methods.</p></p class="citation"></blockquote><h3 id=2689--26273-syncmask-synchronized-attentional-masking-for-fashion-centric-vision-language-pretraining-chull-hwan-song-et-al-2024>(26/89 | 26/273) SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining (Chull Hwan Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chull Hwan Song, Taebaek Hwang, Jooyoung Yoon, Shunghyun Choi, Yeong Hyeon Gu. (2024)<br><strong>SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining</strong><br><button class=copy-to-clipboard title="SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Contrastive Learning, Image2text, Masked Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01156v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01156v1.pdf filename=2404.01156v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-language</b> models (VLMs) have made significant strides in cross-modal understanding through large-scale paired datasets. However, in fashion domain, datasets often exhibit a disparity between the information conveyed in image and text. This issue stems from datasets containing multiple images of a single fashion item all paired with one text, leading to cases where some textual details are not visible in individual images. This mismatch, particularly when non-co-occurring elements are <b>masked,</b> <b>undermines</b> <b>the</b> training of conventional VLM objectives like <b>Masked</b> <b>Language</b> <b>Modeling</b> and <b>Masked</b> <b>Image</b> <b>Modeling,</b> thereby hindering the model&rsquo;s ability to accurately align fine-grained visual and textual features. Addressing this problem, we propose Synchronized attentional Masking (SyncMask), which generate masks that pinpoint the image patches and word tokens where the information co-occur in both image and text. This synchronization is accomplished by harnessing cross-attentional features obtained from a momentum model, ensuring a precise alignment between the two modalities. Additionally, we enhance grouped batch sampling with semi-hard negatives, effectively mitigating false negative issues in <b>Image-Text</b> Matching and <b>Image-Text</b> <b>Contrastive</b> <b>learning</b> objectives within fashion datasets. Our experiments demonstrate the effectiveness of the proposed approach, outperforming existing methods in three downstream tasks.</p></p class="citation"></blockquote><h3 id=2789--27273-cliptone-unsupervised-learning-for-text-based-image-tone-adjustment-hyeongmin-lee-et-al-2024>(27/89 | 27/273) CLIPtone: Unsupervised Learning for Text-based Image Tone Adjustment (Hyeongmin Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyeongmin Lee, Kyoungkook Kang, Jungseul Ok, Sunghyun Cho. (2024)<br><strong>CLIPtone: Unsupervised Learning for Text-based Image Tone Adjustment</strong><br><button class=copy-to-clipboard title="CLIPtone: Unsupervised Learning for Text-based Image Tone Adjustment" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV, eess-IV<br>Keyword Score: 40<br>Keywords: Supervised Learning, Supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01123v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01123v1.pdf filename=2404.01123v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent image tone adjustment (or enhancement) approaches have predominantly adopted <b>supervised</b> <b>learning</b> for learning human-centric perceptual assessment. However, these approaches are constrained by intrinsic challenges of <b>supervised</b> <b>learning.</b> Primarily, the requirement for expertly-curated or retouched images escalates the data acquisition expenses. Moreover, their coverage of target style is confined to stylistic variants inferred from the training data. To surmount the above challenges, we propose an <b>unsupervised</b> <b>learning-based</b> approach for text-based image tone adjustment method, CLIPtone, that extends an existing image enhancement method to accommodate natural language descriptions. Specifically, we design a hyper-network to adaptively modulate the pretrained parameters of the backbone model based on text description. To assess whether the adjusted image aligns with the text description without ground truth image, we utilize CLIP, which is trained on a vast set of language-image pairs and thus encompasses knowledge of human perception. The major advantages of our approach are three fold: (i) minimal data collection expenses, (ii) support for a range of adjustments, and (iii) the ability to handle novel text descriptions unseen in training. Our approach&rsquo;s efficacy is demonstrated through comprehensive experiments, including a user study.</p></p class="citation"></blockquote><h3 id=2889--28273-traveler-a-multi-lmm-agent-framework-for-video-question-answering-chuyi-shang-et-al-2024>(28/89 | 28/273) TraveLER: A Multi-LMM Agent Framework for Video Question-Answering (Chuyi Shang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuyi Shang, Amos You, Sanjay Subramanian, Trevor Darrell, Roei Herzig. (2024)<br><strong>TraveLER: A Multi-LMM Agent Framework for Video Question-Answering</strong><br><button class=copy-to-clipboard title="TraveLER: A Multi-LMM Agent Framework for Video Question-Answering" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 39<br>Keywords: Benchmarking, Fine-tuning, Multi-modal, Multi-modal, Zero-shot, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01476v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01476v1.pdf filename=2404.01476v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, Large <b>Multimodal</b> Models (LMMs) have made significant progress in video <b>question-answering</b> <b>using</b> a frame-wise approach by leveraging large-scale, image-based pretraining in a <b>zero-shot</b> manner. While image-based methods for videos have shown impressive performance, a current limitation is that they often overlook how key timestamps are selected and cannot adjust when incorrect timestamps are identified. Moreover, they are unable to extract details relevant to the <b>question,</b> <b>instead</b> providing general descriptions of the frame. To overcome this, we design a multi-LMM agent framework that travels along the video, iteratively collecting relevant information from keyframes through interactive <b>question-asking</b> <b>until</b> there is sufficient information to answer the <b>question.</b> <b>Specifically,</b> we propose TraveLER, a model that can create a plan to &ldquo;Traverse&rdquo; through the video, ask <b>questions</b> <b>about</b> individual frames to &ldquo;Locate&rdquo; and store key information, and then &ldquo;Evaluate&rdquo; if there is enough information to answer the <b>question.</b> <b>Finally,</b> if there is not enough information, our method is able to &ldquo;Replan&rdquo; based on its collected knowledge. Through extensive experiments, we find that the proposed TraveLER approach improves performance on several video <b>question-answering</b> <b>benchmarks,</b> such as NExT-QA, STAR, and Perception Test, without the need to <b>fine-tune</b> on specific datasets.</p></p class="citation"></blockquote><h3 id=2989--29273-feature-splatting-language-driven-physics-based-scene-synthesis-and-editing-ri-zhao-qiu-et-al-2024>(29/89 | 29/273) Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing (Ri-Zhao Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ri-Zhao Qiu, Ge Yang, Weijia Zeng, Xiaolong Wang. (2024)<br><strong>Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing</strong><br><button class=copy-to-clipboard title="Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 35<br>Keywords: Foundation Model, Geometry, Knowledge Distillation, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01223v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01223v1.pdf filename=2404.01223v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language <b>foundation</b> <b>models</b> that are grounded by natural language. Our first contribution is a way to <b>distill</b> high-quality, object-centric <b>vision-language</b> features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, <b>geometry,</b> material properties and semantics grounded on natural language. Project website: <a href=https://feature-splatting.github.io/>https://feature-splatting.github.io/</a></p></p class="citation"></blockquote><h3 id=3089--30273-gov-nesf-generalizable-open-vocabulary-neural-semantic-fields-yunsong-wang-et-al-2024>(30/89 | 30/273) GOV-NeSF: Generalizable Open-Vocabulary Neural Semantic Fields (Yunsong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunsong Wang, Hanlin Chen, Gim Hee Lee. (2024)<br><strong>GOV-NeSF: Generalizable Open-Vocabulary Neural Semantic Fields</strong><br><button class=copy-to-clipboard title="GOV-NeSF: Generalizable Open-Vocabulary Neural Semantic Fields" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: Fine-tuning, Foundation Model, Geometry, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00931v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00931v1.pdf filename=2404.00931v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>vision-language</b> <b>foundation</b> <b>models</b> have significantly enhanced open-vocabulary 3D scene understanding. However, the generalizability of existing methods is constrained due to their framework designs and their reliance on 3D data. We address this limitation by introducing Generalizable Open-Vocabulary Neural Semantic Fields (GOV-NeSF), a novel approach offering a generalizable implicit representation of 3D scenes with open-vocabulary semantics. We aggregate the <b>geometry-aware</b> features using a cost volume, and propose a Multi-view Joint Fusion module to aggregate multi-view features through a cross-view attention mechanism, which effectively predicts view-specific blending weights for both colors and open-vocabulary features. Remarkably, our GOV-NeSF exhibits state-of-the-art performance in both 2D and 3D open-vocabulary semantic segmentation, eliminating the need for ground truth semantic labels or depth priors, and effectively generalize across scenes and datasets without <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=3189--31273-from-pixels-to-graphs-open-vocabulary-scene-graph-generation-with-vision-language-models-rongjie-li-et-al-2024>(31/89 | 31/273) From Pixels to Graphs: Open-Vocabulary Scene Graph Generation with Vision-Language Models (Rongjie Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rongjie Li, Songyang Zhang, Dahua Lin, Kai Chen, Xuming He. (2024)<br><strong>From Pixels to Graphs: Open-Vocabulary Scene Graph Generation with Vision-Language Models</strong><br><button class=copy-to-clipboard title="From Pixels to Graphs: Open-Vocabulary Scene Graph Generation with Vision-Language Models" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Graph, Image2text, Reasoning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00906v1.pdf filename=2404.00906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene <b>graph</b> generation (SGG) aims to parse a visual scene into an intermediate <b>graph</b> representation for downstream <b>reasoning</b> tasks. Despite recent advancements, existing methods struggle to generate scene <b>graphs</b> with novel visual relation concepts. To address this challenge, we introduce a new open-vocabulary SGG framework based on sequence generation. Our framework leverages <b>vision-language</b> pre-trained models (VLM) by incorporating an image-to-graph generation paradigm. Specifically, we generate scene <b>graph</b> sequences via <b>image-to-text</b> generation with VLM and then construct scene <b>graphs</b> from these sequences. By doing so, we harness the strong capabilities of VLM for open-vocabulary SGG and seamlessly integrate explicit relational modeling for enhancing the VL tasks. Experimental results demonstrate that our design not only achieves superior performance with an open vocabulary but also enhances downstream <b>vision-language</b> task performance through explicit relation modeling knowledge.</p></p class="citation"></blockquote><h3 id=3289--32273-modality-translation-for-object-detection-adaptation-without-forgetting-prior-knowledge-heitor-rapela-medeiros-et-al-2024>(32/89 | 32/273) Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge (Heitor Rapela Medeiros et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Heitor Rapela Medeiros, Masih Aminbeidokhti, Fidel Guerrero Pena, David Latortue, Eric Granger, Marco Pedersoli. (2024)<br><strong>Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge</strong><br><button class=copy-to-clipboard title="Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Distribution Shift, Distribution Shift, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01492v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01492v1.pdf filename=2404.01492v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A common practice in deep learning consists of training large neural networks on massive datasets to perform accurately for different domains and tasks. While this methodology may work well in numerous application areas, it only applies across modalities due to a larger <b>distribution</b> <b>shift</b> in data captured using different sensors. This paper focuses on the problem of adapting a large <b>object</b> <b>detection</b> model to one or multiple modalities while being efficient. To do so, we propose ModTr as an alternative to the common approach of <b>fine-tuning</b> large models. ModTr consists of adapting the input with a small transformation network trained to minimize the detection loss directly. The original model can therefore work on the translated inputs without any further change or <b>fine-tuning</b> to its parameters. Experimental results on translating from IR to RGB images on two well-known datasets show that this simple ModTr approach provides detectors that can perform comparably or better than the standard <b>fine-tuning</b> without forgetting the original knowledge. This opens the doors to a more flexible and efficient service-based detection pipeline in which, instead of using a different detector for each modality, a unique and unaltered server is constantly running, where multiple modalities with the corresponding translations can query it. Code: <a href=https://github.com/heitorrapela/ModTr>https://github.com/heitorrapela/ModTr</a>.</p></p class="citation"></blockquote><h3 id=3389--33273-finding-regions-of-interest-in-whole-slide-images-using-multiple-instance-learning-martim-afonso-et-al-2024>(33/89 | 33/273) Finding Regions of Interest in Whole Slide Images Using Multiple Instance Learning (Martim Afonso et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martim Afonso, Praphulla M. S. Bhawsar, Monjoy Saha, Jonas S. Almeida, Arlindo L. Oliveira. (2024)<br><strong>Finding Regions of Interest in Whole Slide Images Using Multiple Instance Learning</strong><br><button class=copy-to-clipboard title="Finding Regions of Interest in Whole Slide Images Using Multiple Instance Learning" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Multiple Instance Learning, Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01446v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01446v1.pdf filename=2404.01446v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Whole Slide Images (WSI), obtained by high-resolution digital scanning of microscope slides at <b>multiple</b> <b>scales,</b> <b>are</b> the cornerstone of modern Digital Pathology. However, they represent a particular challenge to AI-based/AI-mediated analysis because pathology labeling is typically done at slide-level, instead of tile-level. It is not just that medical diagnostics is recorded at the specimen level, the detection of oncogene mutation is also experimentally obtained, and recorded by initiatives like The Cancer Genome Atlas (TCGA), at the slide level. This configures a dual challenge: a) accurately predicting the overall cancer phenotype and b) finding out what cellular morphologies are associated with it at the tile level. To address these challenges, a weakly <b>supervised</b> <b>Multiple</b> <b>Instance</b> <b>Learning</b> (MIL) approach was explored for two prevalent cancer types, Invasive Breast Carcinoma (TCGA-BRCA) and Lung Squamous Cell Carcinoma (TCGA-LUSC). This approach was explored for tumor detection at low magnification levels and TP53 mutations at various levels. Our results show that a novel additive implementation of MIL matched the performance of reference implementation (AUC 0.96), and was only slightly outperformed by Attention MIL (AUC 0.97). More interestingly from the perspective of the molecular pathologist, these different AI architectures identify distinct sensitivities to morphological features (through the detection of Regions of Interest, RoI) at different amplification levels. Tellingly, TP53 mutation was most sensitive to features at the higher applications where cellular morphology is resolved.</p></p class="citation"></blockquote><h3 id=3489--34273-dpmesh-exploiting-diffusion-prior-for-occluded-human-mesh-recovery-yixuan-zhu-et-al-2024>(34/89 | 34/273) DPMesh: Exploiting Diffusion Prior for Occluded Human Mesh Recovery (Yixuan Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixuan Zhu, Ao Li, Yansong Tang, Wenliang Zhao, Jie Zhou, Jiwen Lu. (2024)<br><strong>DPMesh: Exploiting Diffusion Prior for Occluded Human Mesh Recovery</strong><br><button class=copy-to-clipboard title="DPMesh: Exploiting Diffusion Prior for Occluded Human Mesh Recovery" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Reasoning, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01424v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01424v1.pdf filename=2404.01424v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recovery of occluded human meshes presents challenges for current methods due to the difficulty in extracting effective image features under severe occlusion. In this paper, we introduce DPMesh, an innovative framework for occluded human mesh recovery that capitalizes on the profound <b>diffusion</b> <b>prior</b> about object structure and spatial relationships embedded in a pre-trained <b>text-to-image</b> <b>diffusion</b> <b>model.</b> Unlike previous methods reliant on conventional backbones for vanilla feature extraction, DPMesh seamlessly integrates the pre-trained denoising U-Net with potent knowledge as its image backbone and performs a single-step inference to provide occlusion-aware information. To enhance the perception capability for occluded poses, DPMesh incorporates well-designed guidance via condition injection, which produces effective controls from 2D observations for the denoising U-Net. Furthermore, we explore a dedicated noisy key-point <b>reasoning</b> approach to mitigate disturbances arising from occlusion and crowded scenarios. This strategy fully unleashes the perceptual capability of the <b>diffusion</b> <b>prior,</b> thereby enhancing accuracy. Extensive experiments affirm the efficacy of our framework, as we outperform state-of-the-art methods on both occlusion-specific and standard datasets. The persuasive results underscore its ability to achieve precise and robust 3D human mesh recovery, particularly in challenging scenarios involving occlusion and crowded scenes.</p></p class="citation"></blockquote><h3 id=3589--35273-on-the-faithfulness-of-vision-transformer-explanations-junyi-wu-et-al-2024>(35/89 | 35/273) On the Faithfulness of Vision Transformer Explanations (Junyi Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyi Wu, Weitai Kang, Hao Tang, Yuan Hong, Yan Yan. (2024)<br><strong>On the Faithfulness of Vision Transformer Explanations</strong><br><button class=copy-to-clipboard title="On the Faithfulness of Vision Transformer Explanations" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01415v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01415v1.pdf filename=2404.01415v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To interpret <b>Vision</b> <b>Transformers,</b> post-hoc explanations assign salience scores to input pixels, providing human-understandable heatmaps. However, whether these interpretations reflect true rationales behind the model&rsquo;s output is still underexplored. To address this gap, we study the faithfulness criterion of explanations: the assigned salience scores should represent the influence of the corresponding input pixels on the model&rsquo;s predictions. To evaluate faithfulness, we introduce Salience-guided Faithfulness Coefficient (SaCo), a novel evaluation metric leveraging essential information of salience distribution. Specifically, we conduct pair-wise comparisons among distinct pixel groups and then aggregate the differences in their salience scores, resulting in a coefficient that indicates the explanation&rsquo;s degree of faithfulness. Our explorations reveal that current metrics struggle to differentiate between advanced explanation methods and Random Attribution, thereby failing to capture the faithfulness property. In contrast, our proposed SaCo offers a reliable faithfulness measurement, establishing a robust metric for interpretations. Furthermore, our SaCo demonstrates that the use of gradient and multi-layer aggregation can markedly enhance the faithfulness of attention-based explanation, shedding light on potential paths for advancing <b>Vision</b> <b>Transformer</b> explainability.</p></p class="citation"></blockquote><h3 id=3689--36273-ovfoodseg-elevating-open-vocabulary-food-image-segmentation-via-image-informed-textual-representation-xiongwei-wu-et-al-2024>(36/89 | 36/273) OVFoodSeg: Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation (Xiongwei Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiongwei Wu, Sicheng Yu, Ee-Peng Lim, Chong-Wah Ngo. (2024)<br><strong>OVFoodSeg: Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation</strong><br><button class=copy-to-clipboard title="OVFoodSeg: Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-MM, cs.CV<br>Keyword Score: 30<br>Keywords: Image2text, Text Embedding, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01409v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01409v1.pdf filename=2404.01409v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of food computing, segmenting ingredients from images poses substantial challenges due to the large intra-class variance among the same ingredients, the emergence of new ingredients, and the high annotation costs associated with large food segmentation datasets. Existing approaches primarily utilize a closed-vocabulary and static <b>text</b> <b>embeddings</b> setting. These methods often fall short in effectively handling the ingredients, particularly new and diverse ones. In response to these limitations, we introduce OVFoodSeg, a framework that adopts an open-vocabulary setting and enhances <b>text</b> <b>embeddings</b> with visual context. By integrating <b>vision-language</b> models (VLMs), our approach enriches <b>text</b> <b>embedding</b> with image-specific information through two innovative modules, eg, an <b>image-to-text</b> learner FoodLearner and an Image-Informed <b>Text</b> <b>Encoder.</b> The training process of OVFoodSeg is divided into two stages: the pre-training of FoodLearner and the subsequent learning phase for segmentation. The pre-training phase equips FoodLearner with the capability to align visual information with corresponding textual representations that are specifically related to food, while the second phase adapts both the FoodLearner and the Image-Informed <b>Text</b> <b>Encoder</b> for the segmentation task. By addressing the deficiencies of previous models, OVFoodSeg demonstrates a significant improvement, achieving an 4.9% increase in mean Intersection over Union (mIoU) on the FoodSeg103 dataset, setting a new milestone for food image segmentation.</p></p class="citation"></blockquote><h3 id=3789--37273-uncovering-the-text-embedding-in-text-to-image-diffusion-models-hu-yu-et-al-2024>(37/89 | 37/273) Uncovering the Text Embedding in Text-to-Image Diffusion Models (Hu Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hu Yu, Hao Luo, Fan Wang, Feng Zhao. (2024)<br><strong>Uncovering the Text Embedding in Text-to-Image Diffusion Models</strong><br><button class=copy-to-clipboard title="Uncovering the Text Embedding in Text-to-Image Diffusion Models" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Text2image, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01154v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01154v1.pdf filename=2404.01154v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The correspondence between input <b>text</b> <b>and</b> the generated image exhibits opacity, wherein minor textual modifications can induce substantial deviations in the generated image. While, <b>text</b> <b>embedding,</b> as the pivotal intermediary between <b>text</b> <b>and</b> images, remains relatively underexplored. In this paper, we address this research gap by delving into the <b>text</b> <b>embedding</b> space, unleashing its capacity for controllable image editing and explicable semantic direction attributes within a learning-free framework. Specifically, we identify two critical insights regarding the importance of per-word embedding and their contextual correlations within <b>text</b> <b>embedding,</b> providing instructive principles for learning-free image editing. Additionally, we find that <b>text</b> <b>embedding</b> inherently possesses diverse semantic potentials, and further reveal this property through the lens of singular value decomposition (SVD). These uncovered properties offer practical utility for image editing and semantic discovery. More importantly, we expect the in-depth analyses and findings of the <b>text</b> <b>embedding</b> can enhance the understanding of <b>text-to-image</b> <b>diffusion</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=3889--38273-medical-visual-prompting-mvp-a-unified-framework-for-versatile-and-high-quality-medical-image-segmentation-yulin-chen-et-al-2024>(38/89 | 38/273) Medical Visual Prompting (MVP): A Unified Framework for Versatile and High-Quality Medical Image Segmentation (Yulin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yulin Chen, Guoheng Huang, Kai Huang, Zijin Lin, Guo Zhong, Shenghong Luo, Jie Deng, Jian Zhou. (2024)<br><strong>Medical Visual Prompting (MVP): A Unified Framework for Versatile and High-Quality Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="Medical Visual Prompting (MVP): A Unified Framework for Versatile and High-Quality Medical Image Segmentation" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01127v1.pdf filename=2404.01127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate segmentation of lesion regions is crucial for clinical diagnosis and treatment across various diseases. While deep <b>convolutional</b> <b>networks</b> have achieved satisfactory results in medical image segmentation, they face challenges such as loss of lesion shape information due to continuous <b>convolution</b> and downsampling, as well as the high cost of manually labeling lesions with varying shapes and sizes. To address these issues, we propose a novel medical visual <b>prompting</b> (MVP) framework that leverages pre-training and <b>prompting</b> concepts from natural language processing (NLP). The framework utilizes three key components: Super-Pixel Guided <b>Prompting</b> (SPGP) for superpixelating the input image, Image Embedding Guided <b>Prompting</b> (IEGP) for freezing patch embedding and merging with superpixels to provide visual <b>prompts,</b> and Adaptive Attention Mechanism Guided <b>Prompting</b> (AAGP) for pinpointing <b>prompt</b> content and efficiently adapting all layers. By integrating SPGP, IEGP, and AAGP, the MVP enables the segmentation network to better learn shape <b>prompting</b> information and facilitates mutual learning across different tasks. Extensive experiments conducted on five datasets demonstrate superior performance of this method in various challenging medical image tasks, while simplifying single-task medical segmentation models. This novel framework offers improved performance with fewer parameters and holds significant potential for accurate segmentation of lesion regions in various medical tasks, making it clinically valuable.</p></p class="citation"></blockquote><h3 id=3989--39273-few-shot-point-cloud-reconstruction-and-denoising-via-learned-guassian-splats-renderings-and-fine-tuned-diffusion-features-pietro-bonazzi-2024>(39/89 | 39/273) Few-shot point cloud reconstruction and denoising via learned Guassian splats renderings and fine-tuned diffusion features (Pietro Bonazzi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pietro Bonazzi. (2024)<br><strong>Few-shot point cloud reconstruction and denoising via learned Guassian splats renderings and fine-tuned diffusion features</strong><br><button class=copy-to-clipboard title="Few-shot point cloud reconstruction and denoising via learned Guassian splats renderings and fine-tuned diffusion features" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CG, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Few-shot, Fine-tuning, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01112v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01112v3.pdf filename=2404.01112v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing deep learning methods for the reconstruction and denoising of point clouds rely on small datasets of 3D shapes. We circumvent the problem by leveraging deep learning methods trained on billions of images. We propose a method to reconstruct point clouds from few images and to denoise point clouds from their rendering by exploiting prior knowledge <b>distilled</b> from image-based deep learning models. To improve reconstruction in constraint settings, we regularize the training of a differentiable renderer with hybrid surface and appearance by introducing semantic consistency supervision. In addition, we propose a pipeline to <b>finetune</b> Stable Diffusion to denoise renderings of noisy point clouds and we demonstrate how these learned filters can be used to remove point cloud noise coming without 3D supervision. We compare our method with DSS and PointRadiance and achieved higher quality 3D reconstruction on the Sketchfab Testset and SCUT Dataset.</p></p class="citation"></blockquote><h3 id=4089--40273-survey-of-bias-in-text-to-image-generation-definition-evaluation-and-mitigation-yixin-wan-et-al-2024>(40/89 | 40/273) Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and Mitigation (Yixin Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixin Wan, Arjun Subramonian, Anaelia Ovalle, Zongyu Lin, Ashima Suvarna, Christina Chance, Hritik Bansal, Rebecca Pattichis, Kai-Wei Chang. (2024)<br><strong>Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and Mitigation</strong><br><button class=copy-to-clipboard title="Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and Mitigation" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-CY, cs.CV<br>Keyword Score: 30<br>Keywords: Gemini, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01030v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01030v2.pdf filename=2404.01030v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent advancement of large and powerful models with <b>Text-to-Image</b> (T2I) generation abilities &ndash; such as OpenAI&rsquo;s DALLE-3 and Google&rsquo;s <b>Gemini</b> &ndash; enables users to generate high-quality images from textual <b>prompts.</b> However, it has become increasingly evident that even simple <b>prompts</b> could cause T2I models to exhibit conspicuous social bias in generated images. Such bias might lead to both allocational and representational harms in society, further marginalizing minority groups. Noting this problem, a large body of recent works has been dedicated to investigating different dimensions of bias in T2I systems. However, an extensive review of these studies is lacking, hindering a systematic understanding of current progress and research gaps. We present the first extensive survey on bias in T2I generative models. In this survey, we review prior studies on dimensions of bias: Gender, Skintone, and Geo-Culture. Specifically, we discuss how these works define, evaluate, and mitigate different aspects of bias. We found that: (1) while gender and skintone biases are widely studied, geo-cultural bias remains under-explored; (2) most works on gender and skintone bias investigated occupational association, while other aspects are less frequently studied; (3) almost all gender bias works overlook non-binary identities in their studies; (4) evaluation datasets and metrics are scattered, with no unified framework for measuring biases; and (5) current mitigation methods fail to resolve biases comprehensively. Based on current limitations, we point out future research directions that contribute to human-centric definitions, evaluations, and mitigation of biases. We hope to highlight the importance of studying biases in T2I systems, as well as encourage future efforts to holistically understand and tackle biases, building fair and trustworthy T2I technologies for everyone.</p></p class="citation"></blockquote><h3 id=4189--41273-towards-memorization-free-diffusion-models-chen-chen-et-al-2024>(41/89 | 41/273) Towards Memorization-Free Diffusion Models (Chen Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Chen, Daochang Liu, Chang Xu. (2024)<br><strong>Towards Memorization-Free Diffusion Models</strong><br><button class=copy-to-clipboard title="Towards Memorization-Free Diffusion Models" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Probabilistic Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00922v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00922v1.pdf filename=2404.00922v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pretrained <b>diffusion</b> <b>models</b> and their outputs are widely accessible due to their exceptional capacity for synthesizing high-quality images and their open-source nature. The users, however, may face litigation risks owing to the models&rsquo; tendency to memorize and regurgitate training data during inference. To address this, we introduce Anti-Memorization Guidance (AMG), a novel framework employing three targeted guidance strategies for the main causes of memorization: image and caption duplication, and highly specific user <b>prompts.</b> Consequently, AMG ensures memorization-free outputs while maintaining high image quality and text alignment, leveraging the synergy of its guidance methods, each indispensable in its own right. AMG also features an innovative automatic detection system for potential memorization during each step of inference process, allows selective application of guidance strategies, minimally interfering with the original sampling process to preserve output utility. We applied AMG to pretrained Denoising <b>Diffusion</b> <b>Probabilistic</b> <b>Models</b> (DDPM) and Stable <b>Diffusion</b> <b>across</b> various generation tasks. The results demonstrate that AMG is the first approach to successfully eradicates all instances of memorization with no or marginal impacts on image quality and text-alignment, as evidenced by FID and CLIP scores.</p></p class="citation"></blockquote><h3 id=4289--42273-rethinking-saliency-guided-weakly-supervised-semantic-segmentation-beomyoung-kim-et-al-2024>(42/89 | 42/273) Rethinking Saliency-Guided Weakly-Supervised Semantic Segmentation (Beomyoung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Beomyoung Kim, Donghyun Kim, Sung Ju Hwang. (2024)<br><strong>Rethinking Saliency-Guided Weakly-Supervised Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Rethinking Saliency-Guided Weakly-Supervised Semantic Segmentation" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Unsupervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00918v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00918v2.pdf filename=2404.00918v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a fresh perspective on the role of saliency maps in <b>weakly-supervised</b> semantic segmentation (WSSS) and offers new insights and research directions based on our empirical findings. We conduct comprehensive experiments and observe that the quality of the saliency map is a critical factor in saliency-guided WSSS approaches. Nonetheless, we find that the saliency maps used in previous works are often arbitrarily chosen, despite their significant impact on WSSS. Additionally, we observe that the choice of the threshold, which has received less attention before, is non-trivial in WSSS. To facilitate more meaningful and rigorous research for saliency-guided WSSS, we introduce \texttt{WSSS-BED}, a standardized framework for conducting research under unified conditions. \texttt{WSSS-BED} provides various saliency maps and activation maps for seven WSSS methods, as well as saliency maps from <b>unsupervised</b> salient <b>object</b> <b>detection</b> models.</p></p class="citation"></blockquote><h3 id=4389--43273-meta-episodic-learning-with-dynamic-task-sampling-for-clip-based-point-cloud-classification-shuvozit-ghose-et-al-2024>(43/89 | 43/273) Meta Episodic learning with Dynamic Task Sampling for CLIP-based Point Cloud Classification (Shuvozit Ghose et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuvozit Ghose, Yang Wang. (2024)<br><strong>Meta Episodic learning with Dynamic Task Sampling for CLIP-based Point Cloud Classification</strong><br><button class=copy-to-clipboard title="Meta Episodic learning with Dynamic Task Sampling for CLIP-based Point Cloud Classification" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Few-shot, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00857v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00857v1.pdf filename=2404.00857v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Point cloud classification refers to the process of assigning semantic labels or categories to individual points within a point cloud data structure. Recent works have explored the extension of pre-trained CLIP to 3D recognition. In this direction, CLIP-based point cloud models like PointCLIP, CLIP2Point have become state-of-the-art methods in the <b>few-shot</b> setup. Although these methods show promising performance for some classes like airplanes, desks, guitars, etc, the performance for some classes like the cup, flower pot, sink, nightstand, etc is still far from satisfactory. This is due to the fact that the adapter of CLIP-based models is trained using randomly sampled N-way K-shot data in the standard <b>supervised</b> <b>learning</b> setup. In this paper, we propose a novel meta-episodic learning framework for CLIP-based point cloud classification, addressing the challenges of limited training examples and sampling unknown classes. Additionally, we introduce dynamic task sampling within the episode based on performance memory. This sampling strategy effectively addresses the challenge of sampling unknown classes, ensuring that the model learns from a diverse range of classes and promotes the exploration of underrepresented categories. By dynamically updating the performance memory, we adaptively prioritize the sampling of classes based on their performance, enhancing the model&rsquo;s ability to handle challenging and real-world scenarios. Experiments show an average performance gain of 3-6% on ModelNet40 and ScanobjectNN datasets in a <b>few-shot</b> setup.</p></p class="citation"></blockquote><h3 id=4489--44273-pdf-a-probability-driven-framework-for-open-world-3d-point-cloud-semantic-segmentation-jinfeng-xu-et-al-2024>(44/89 | 44/273) PDF: A Probability-Driven Framework for Open World 3D Point Cloud Semantic Segmentation (Jinfeng Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinfeng Xu, Siyuan Yang, Xianzhi Li, Yuan Tang, Yixue Hao, Long Hu, Min Chen. (2024)<br><strong>PDF: A Probability-Driven Framework for Open World 3D Point Cloud Semantic Segmentation</strong><br><button class=copy-to-clipboard title="PDF: A Probability-Driven Framework for Open World 3D Point Cloud Semantic Segmentation" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Geometry, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00979v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00979v1.pdf filename=2404.00979v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing point cloud semantic segmentation networks cannot identify unknown classes and update their <b>knowledge,</b> <b>due</b> to a closed-set and static perspective of the real world, which would induce the intelligent agent to make bad decisions. To address this problem, we propose a Probability-Driven Framework (PDF) for open world semantic segmentation that includes (i) a lightweight U-decoder branch to identify unknown classes by estimating the uncertainties, (ii) a flexible pseudo-labeling scheme to supply <b>geometry</b> features along with probability distribution features of unknown classes by generating pseudo labels, and (iii) an incremental <b>knowledge</b> <b>distillation</b> strategy to incorporate novel classes into the existing <b>knowledge</b> <b>base</b> gradually. Our framework enables the model to behave like human beings, which could recognize unknown objects and incrementally learn them with the corresponding <b>knowledge.</b> <b>Experimental</b> results on the S3DIS and ScanNetv2 datasets demonstrate that the proposed PDF outperforms other methods by a large margin in both important tasks of open world semantic segmentation.</p></p class="citation"></blockquote><h3 id=4589--45273-object-conditioned-bag-of-instances-for-few-shot-personalized-instance-recognition-umberto-michieli-et-al-2024>(45/89 | 45/273) Object-conditioned Bag of Instances for Few-Shot Personalized Instance Recognition (Umberto Michieli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Umberto Michieli, Jijoong Moon, Daehyun Kim, Mete Ozay. (2024)<br><strong>Object-conditioned Bag of Instances for Few-Shot Personalized Instance Recognition</strong><br><button class=copy-to-clipboard title="Object-conditioned Bag of Instances for Few-Shot Personalized Instance Recognition" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 23<br>Keywords: Object Detection, Benchmarking, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01397v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01397v1.pdf filename=2404.01397v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nowadays, users demand for increased personalization of vision systems to localize and identify personal instances of <b>objects</b> <b>(e.g.,</b> my dog rather than dog) from a <b>few-shot</b> dataset only. Despite outstanding results of deep networks on classical label-abundant <b>benchmarks</b> (e.g., those of the latest YOLOv8 model for standard <b>object</b> <b>detection),</b> they struggle to maintain within-class variability to represent different instances rather than <b>object</b> <b>categories</b> only. We construct an <b>Object-conditioned</b> <b>Bag</b> of Instances (OBoI) based on multi-order statistics of extracted features, where generic <b>object</b> <b>detection</b> models are extended to search and identify personal instances from the OBoI&rsquo;s metric space, without need for backpropagation. By relying on multi-order statistics, OBoI achieves consistent superior accuracy in distinguishing different instances. In the results, we achieve 77.1% personal <b>object</b> <b>recognition</b> accuracy in case of 18 personal instances, showing about 12% relative gain over the state of the art.</p></p class="citation"></blockquote><h3 id=4689--46273-losa-long-short-range-adapter-for-scaling-end-to-end-temporal-action-localization-akshita-gupta-et-al-2024>(46/89 | 46/273) LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action Localization (Akshita Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akshita Gupta, Gaurav Mittal, Ahmed Magooda, Ye Yu, Graham W. Taylor, Mei Chen. (2024)<br><strong>LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action Localization</strong><br><button class=copy-to-clipboard title="LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action Localization" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Foundation Model, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01282v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01282v1.pdf filename=2404.01282v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Temporal Action Localization (TAL) involves localizing and classifying action snippets in an untrimmed video. The emergence of large video <b>foundation</b> <b>models</b> has led RGB-only video backbones to outperform previous methods needing both RGB and optical flow modalities. Leveraging these large models is often limited to training only the TAL head due to the prohibitively large GPU memory required to adapt the video backbone for TAL. To overcome this limitation, we introduce LoSA, the first memory-and-parameter-efficient backbone adapter designed specifically for TAL to handle untrimmed videos. LoSA specializes for TAL by introducing Long-Short-range Adapters that adapt the intermediate layers of the video backbone over different temporal ranges. These adapters run parallel to the video backbone to significantly reduce memory footprint. LoSA also includes Long-Short-range Fusion that strategically combines the output of these adapters from the video backbone layers to enhance the video features provided to the TAL head. Experiments show that LoSA significantly outperforms all existing methods on standard TAL <b>benchmarks,</b> THUMOS-14 and ActivityNet-v1.3, by scaling end-to-end backbone adaptation to billion-parameter-plus models like VideoMAEv2~(ViT-g) and leveraging them beyond head-only <b>transfer</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=4789--47273-biper-binary-neural-networks-using-a-periodic-function-edwin-vargas-et-al-2024>(47/89 | 47/273) BiPer: Binary Neural Networks using a Periodic Function (Edwin Vargas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Edwin Vargas, Claudia Correa, Carlos Hinojosa, Henry Arguello. (2024)<br><strong>BiPer: Binary Neural Networks using a Periodic Function</strong><br><button class=copy-to-clipboard title="BiPer: Binary Neural Networks using a Periodic Function" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01278v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01278v1.pdf filename=2404.01278v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Quantized</b> neural networks employ reduced precision representations for both weights and activations. This <b>quantization</b> process significantly reduces the memory requirements and computational complexity of the network. Binary Neural Networks (BNNs) are the extreme <b>quantization</b> case, representing values with just one bit. Since the sign function is typically used to map real values to binary values, smooth approximations are introduced to mimic the gradients during error backpropagation. Thus, the mismatch between the forward and backward models corrupts the direction of the gradient, causing training inconsistency problems and performance degradation. In contrast to current BNN approaches, we propose to employ a binary periodic (BiPer) function during binarization. Specifically, we use a square wave for the forward pass to obtain the binary values and employ the trigonometric sine function with the same period of the square wave as a differentiable surrogate during the backward pass. We demonstrate that this approach can control the <b>quantization</b> error by using the frequency of the periodic function and improves network performance. Extensive experiments validate the effectiveness of BiPer in <b>benchmark</b> datasets and network architectures, with improvements of up to 1% and 0.69% with respect to state-of-the-art methods in the classification task over CIFAR-10 and ImageNet, respectively. Our code is publicly available at <a href=https://github.com/edmav4/BiPer>https://github.com/edmav4/BiPer</a>.</p></p class="citation"></blockquote><h3 id=4889--48273-llms-are-good-sign-language-translators-jia-gong-et-al-2024>(48/89 | 48/273) LLMs are Good Sign Language Translators (Jia Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jia Gong, Lin Geng Foo, Yixuan He, Hossein Rahmani, Jun Liu. (2024)<br><strong>LLMs are Good Sign Language Translators</strong><br><button class=copy-to-clipboard title="LLMs are Good Sign Language Translators" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00925v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00925v1.pdf filename=2404.00925v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sign Language Translation (SLT) is a challenging task that aims to translate sign videos into spoken language. Inspired by the strong translation capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> that are trained on extensive multilingual text corpora, we aim to harness off-the-shelf <b>LLMs</b> to handle SLT. In this paper, we regularize the sign videos to embody linguistic characteristics of spoken language, and propose a novel SignLLM framework to transform sign videos into a language-like representation for improved readability by off-the-shelf <b>LLMs.</b> SignLLM comprises two key modules: (1) The Vector-Quantized Visual Sign module converts sign videos into a sequence of discrete character-level sign tokens, and (2) the Codebook Reconstruction and Alignment module converts these character-level tokens into word-level sign representations using an optimal transport formulation. A sign-text alignment loss further bridges the gap between sign and text tokens, enhancing semantic compatibility. We achieve state-of-the-art gloss-free results on two widely-used SLT <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=4989--49273-tryon-adapter-efficient-fine-grained-clothing-identity-adaptation-for-high-fidelity-virtual-try-on-jiazheng-xing-et-al-2024>(49/89 | 49/273) TryOn-Adapter: Efficient Fine-Grained Clothing Identity Adaptation for High-Fidelity Virtual Try-On (Jiazheng Xing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiazheng Xing, Chao Xu, Yijie Qian, Yang Liu, Guang Dai, Baigui Sun, Yong Liu, Jingdong Wang. (2024)<br><strong>TryOn-Adapter: Efficient Fine-Grained Clothing Identity Adaptation for High-Fidelity Virtual Try-On</strong><br><button class=copy-to-clipboard title="TryOn-Adapter: Efficient Fine-Grained Clothing Identity Adaptation for High-Fidelity Virtual Try-On" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Diffusion Model, Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00878v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00878v1.pdf filename=2404.00878v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Virtual try-on focuses on adjusting the given clothes to fit a specific person seamlessly while avoiding any distortion of the patterns and textures of the garment. However, the clothing identity uncontrollability and training inefficiency of existing <b>diffusion-based</b> <b>methods,</b> which struggle to maintain the identity even with full parameter training, are significant limitations that hinder the widespread applications. In this work, we propose an effective and efficient framework, termed TryOn-Adapter. Specifically, we first decouple clothing identity into fine-grained factors: style for color and category information, texture for high-frequency details, and structure for smooth spatial adaptive transformation. Our approach utilizes a pre-trained exemplar-based <b>diffusion</b> <b>model</b> as the fundamental network, whose parameters are frozen except for the attention layers. We then customize three lightweight modules (Style Preserving, Texture Highlighting, and Structure Adapting) incorporated with <b>fine-tuning</b> techniques to enable precise and efficient identity control. Meanwhile, we introduce the training-free T-RePaint strategy to further enhance clothing identity preservation while maintaining the realistic try-on effect during the inference. Our experiments demonstrate that our approach achieves state-of-the-art performance on two widely-used <b>benchmarks.</b> Additionally, compared with recent full-tuning <b>diffusion-based</b> <b>methods,</b> we only use about half of their tunable parameters during training. The code will be made publicly available at <a href=https://github.com/jiazheng-xing/TryOn-Adapter>https://github.com/jiazheng-xing/TryOn-Adapter</a>.</p></p class="citation"></blockquote><h3 id=5089--50273-collaborative-learning-of-anomalies-with-privacy-clap-for-unsupervised-video-anomaly-detection-a-new-baseline-anas-al-lahham-et-al-2024>(50/89 | 50/273) Collaborative Learning of Anomalies with Privacy (CLAP) for Unsupervised Video Anomaly Detection: A New Baseline (Anas Al-lahham et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anas Al-lahham, Muhammad Zaigham Zaheer, Nurbek Tastan, Karthik Nandakumar. (2024)<br><strong>Collaborative Learning of Anomalies with Privacy (CLAP) for Unsupervised Video Anomaly Detection: A New Baseline</strong><br><button class=copy-to-clipboard title="Collaborative Learning of Anomalies with Privacy (CLAP) for Unsupervised Video Anomaly Detection: A New Baseline" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Anomaly Detection, Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00847v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00847v1.pdf filename=2404.00847v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> (US) video <b>anomaly</b> <b>detection</b> (VAD) in surveillance applications is gaining more popularity recently due to its practical real-world applications. As surveillance videos are privacy sensitive and the availability of large-scale video data may enable better US-VAD systems, collaborative learning can be highly rewarding in this setting. However, due to the extremely challenging nature of the US-VAD task, where learning is carried out without any annotations, privacy-preserving collaborative learning of US-VAD systems has not been studied yet. In this paper, we propose a new baseline for <b>anomaly</b> <b>detection</b> capable of localizing anomalous events in complex surveillance videos in a fully <b>unsupervised</b> fashion without any labels on a privacy-preserving participant-based distributed training configuration. Additionally, we propose three new evaluation protocols to <b>benchmark</b> <b>anomaly</b> <b>detection</b> approaches on various scenarios of collaborations and data availability. Based on these protocols, we modify existing VAD datasets to extensively evaluate our approach as well as existing US SOTA methods on two large-scale datasets including UCF-Crime and XD-Violence. All proposed evaluation protocols, dataset splits, and codes are available here: <a href=https://github.com/AnasEmad11/CLAP>https://github.com/AnasEmad11/CLAP</a></p></p class="citation"></blockquote><h3 id=5189--51273-temporally-consistent-unbalanced-optimal-transport-for-unsupervised-action-segmentation-ming-xu-et-al-2024>(51/89 | 51/273) Temporally Consistent Unbalanced Optimal Transport for Unsupervised Action Segmentation (Ming Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Xu, Stephen Gould. (2024)<br><strong>Temporally Consistent Unbalanced Optimal Transport for Unsupervised Action Segmentation</strong><br><button class=copy-to-clipboard title="Temporally Consistent Unbalanced Optimal Transport for Unsupervised Action Segmentation" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01518v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01518v2.pdf filename=2404.01518v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel approach to the action segmentation task for long, untrimmed videos, based on solving an optimal transport problem. By encoding a temporal consistency prior into a Gromov-Wasserstein problem, we are able to decode a temporally consistent segmentation from a noisy affinity/matching cost matrix between video frames and action classes. Unlike previous approaches, our method does not require knowing the action order for a video to attain temporal consistency. Furthermore, our resulting (fused) Gromov-Wasserstein problem can be efficiently solved on GPUs using a few iterations of projected mirror descent. We demonstrate the effectiveness of our method in an <b>unsupervised</b> <b>learning</b> setting, where our method is used to generate pseudo-labels for self-training. We evaluate our segmentation approach and <b>unsupervised</b> <b>learning</b> pipeline on the Breakfast, 50-Salads, YouTube Instructions and Desktop Assembly datasets, yielding state-of-the-art results for the <b>unsupervised</b> <b>video</b> action segmentation task.</p></p class="citation"></blockquote><h3 id=5289--52273-bigger-is-not-always-better-scaling-properties-of-latent-diffusion-models-kangfu-mei-et-al-2024>(52/89 | 52/273) Bigger is not Always Better: Scaling Properties of Latent Diffusion Models (Kangfu Mei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kangfu Mei, Zhengzhong Tu, Mauricio Delbracio, Hossein Talebi, Vishal M. Patel, Peyman Milanfar. (2024)<br><strong>Bigger is not Always Better: Scaling Properties of Latent Diffusion Models</strong><br><button class=copy-to-clipboard title="Bigger is not Always Better: Scaling Properties of Latent Diffusion Models" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01367v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01367v1.pdf filename=2404.01367v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the scaling properties of latent <b>diffusion</b> <b>models</b> (LDMs) with an emphasis on their sampling efficiency. While improved network architecture and inference algorithms have shown to effectively boost sampling efficiency of <b>diffusion</b> <b>models,</b> the role of model size &ndash; a critical determinant of sampling efficiency &ndash; has not been thoroughly examined. Through empirical analysis of established <b>text-to-image</b> <b>diffusion</b> <b>models,</b> we conduct an in-depth investigation into how model size influences sampling efficiency across varying sampling steps. Our findings unveil a surprising trend: when operating under a given inference budget, smaller models frequently outperform their larger equivalents in generating high-quality results. Moreover, we extend our study to demonstrate the generalizability of the these findings by applying various <b>diffusion</b> <b>samplers,</b> exploring diverse downstream tasks, evaluating post-distilled models, as well as comparing performance relative to training compute. These findings open up new pathways for the development of LDM scaling strategies which can be employed to enhance generative capabilities within limited inference budgets.</p></p class="citation"></blockquote><h3 id=5389--53273-measuring-style-similarity-in-diffusion-models-gowthami-somepalli-et-al-2024>(53/89 | 53/273) Measuring Style Similarity in Diffusion Models (Gowthami Somepalli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gowthami Somepalli, Anubhav Gupta, Kamal Gupta, Shramay Palta, Micah Goldblum, Jonas Geiping, Abhinav Shrivastava, Tom Goldstein. (2024)<br><strong>Measuring Style Similarity in Diffusion Models</strong><br><button class=copy-to-clipboard title="Measuring Style Similarity in Diffusion Models" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01292v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01292v1.pdf filename=2404.01292v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative models are now widely used by graphic designers and artists. Prior works have shown that these models remember and often replicate content from their training data during generation. Hence as their proliferation increases, it has become important to perform a database search to determine whether the properties of the image are attributable to specific training data, every time before a generated image is used for professional purposes. Existing tools for this purpose focus on retrieving images of similar semantic content. Meanwhile, many artists are concerned with style replication in <b>text-to-image</b> models. We present a framework for understanding and extracting style descriptors from images. Our framework comprises a new dataset curated using the insight that style is a subjective property of an image that captures complex yet meaningful interactions of factors including but not limited to colors, textures, shapes, etc. We also propose a method to extract style descriptors that can be used to attribute style of a generated image to the images used in the training dataset of a <b>text-to-image</b> model. We showcase promising results in various style retrieval tasks. We also quantitatively and qualitatively analyze style attribution and matching in the Stable <b>Diffusion</b> <b>model.</b> Code and artifacts are available at <a href=https://github.com/learn2phoenix/CSD>https://github.com/learn2phoenix/CSD</a>.</p></p class="citation"></blockquote><h3 id=5489--54273-language-guided-domain-generalized-medical-image-segmentation-shahina-kunhimon-et-al-2024>(54/89 | 54/273) Language Guided Domain Generalized Medical Image Segmentation (Shahina Kunhimon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shahina Kunhimon, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan. (2024)<br><strong>Language Guided Domain Generalized Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="Language Guided Domain Generalized Medical Image Segmentation" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01272v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01272v2.pdf filename=2404.01272v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Single source <b>domain</b> <b>generalization</b> (SDG) holds promise for more reliable and consistent image segmentation across real-world clinical settings particularly in the medical <b>domain,</b> <b>where</b> data privacy and acquisition cost constraints often limit the availability of diverse datasets. Depending solely on visual features hampers the model&rsquo;s capacity to adapt effectively to various <b>domains,</b> <b>primarily</b> because of the presence of spurious correlations and <b>domain-specific</b> <b>characteristics</b> embedded within the image features. Incorporating text features alongside visual features is a potential solution to enhance the model&rsquo;s understanding of the data, as it goes beyond pixel-level information to provide valuable context. Textual cues describing the anatomical structures, their appearances, and variations across various imaging modalities can guide the model in <b>domain</b> <b>adaptation,</b> ultimately contributing to more robust and consistent segmentation. In this paper, we propose an approach that explicitly leverages textual information by incorporating a <b>contrastive</b> <b>learning</b> mechanism guided by the text encoder features to learn a more robust feature representation. We assess the effectiveness of our text-guided <b>contrastive</b> <b>feature</b> alignment technique in various scenarios, including cross-modality, cross-sequence, and cross-site settings for different segmentation tasks. Our approach achieves favorable performance against existing methods in literature. Our code and model weights are available at <a href=https://github.com/ShahinaKK/LG_SDG.git>https://github.com/ShahinaKK/LG_SDG.git</a>.</p></p class="citation"></blockquote><h3 id=5589--55273-structldm-structured-latent-diffusion-for-3d-human-generation-tao-hu-et-al-2024>(55/89 | 55/273) StructLDM: Structured Latent Diffusion for 3D Human Generation (Tao Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Hu, Fangzhou Hong, Ziwei Liu. (2024)<br><strong>StructLDM: Structured Latent Diffusion for 3D Human Generation</strong><br><button class=copy-to-clipboard title="StructLDM: Structured Latent Diffusion for 3D Human Generation" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01241v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01241v2.pdf filename=2404.01241v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent 3D human generative models have achieved remarkable progress by learning 3D-aware <b>GANs</b> from 2D images. However, existing 3D human generative methods model humans in a compact 1D latent space, ignoring the articulated structure and semantics of human body topology. In this paper, we explore more expressive and higher-dimensional latent space for 3D human modeling and propose StructLDM, a <b>diffusion-based</b> <b>unconditional</b> 3D human generative model, which is learned from 2D images. StructLDM solves the challenges imposed due to the high-dimensional growth of latent space with three key designs: 1) A semantic structured latent space defined on the dense surface manifold of a statistical human body template. 2) A structured 3D-aware auto-decoder that factorizes the global latent space into several semantic body parts parameterized by a set of conditional structured local NeRFs anchored to the body template, which embeds the properties learned from the 2D training data and can be decoded to render view-consistent humans under different poses and clothing styles. 3) A structured latent <b>diffusion</b> <b>model</b> for generative human appearance sampling. Extensive experiments validate StructLDM&rsquo;s state-of-the-art generation performance and illustrate the expressiveness of the structured latent space over the well-adopted 1D latent space. Notably, StructLDM enables different levels of controllable 3D human generation and editing, including pose/view/shape control, and high-level tasks including compositional generations, part-aware clothing editing, 3D virtual try-on, etc. Our project page is at: <a href=https://taohuumd.github.io/projects/StructLDM/>https://taohuumd.github.io/projects/StructLDM/</a>.</p></p class="citation"></blockquote><h3 id=5689--56273-adaptive-query-prompting-for-multi-domain-landmark-detection-qiusen-wei-et-al-2024>(56/89 | 56/273) Adaptive Query Prompting for Multi-Domain Landmark Detection (Qiusen Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiusen Wei, Guoheng Huang, Xiaochen Yuan, Xuhang Chen, Guo Zhong, Jianwen Huang, Jiajie Huang. (2024)<br><strong>Adaptive Query Prompting for Multi-Domain Landmark Detection</strong><br><button class=copy-to-clipboard title="Adaptive Query Prompting for Multi-Domain Landmark Detection" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01194v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01194v1.pdf filename=2404.01194v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical landmark detection is crucial in various medical imaging modalities and procedures. Although deep learning-based methods have achieve promising performance, they are mostly designed for specific anatomical regions or tasks. In this work, we propose a universal model for multi-domain landmark detection by leveraging <b>transformer</b> architecture and developing a <b>prompting</b> component, named as Adaptive Query <b>Prompting</b> (AQP). Instead of embedding additional modules in the backbone network, we design a separate module to generate <b>prompts</b> that can be effectively extended to any other <b>transformer</b> network. In our proposed AQP, <b>prompts</b> are learnable parameters maintained in a memory space called <b>prompt</b> pool. The central idea is to keep the backbone frozen and then optimize <b>prompts</b> to instruct the model inference process. Furthermore, we employ a lightweight decoder to decode landmarks from the extracted features, namely Light-MLD. Thanks to the lightweight nature of the decoder and AQP, we can handle multiple datasets by sharing the backbone encoder and then only perform partial parameter tuning without incurring much additional cost. It has the potential to be extended to more landmark detection tasks. We conduct experiments on three widely used X-ray datasets for different medical landmark detection tasks. Our proposed Light-MLD coupled with AQP achieves SOTA performance on many metrics even without the use of elaborate structural designs or complex frameworks.</p></p class="citation"></blockquote><h3 id=5789--57273-condition-aware-neural-network-for-controlled-image-generation-han-cai-et-al-2024>(57/89 | 57/273) Condition-Aware Neural Network for Controlled Image Generation (Han Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Cai, Muyang Li, Zhuoyang Zhang, Qinsheng Zhang, Ming-Yu Liu, Song Han. (2024)<br><strong>Condition-Aware Neural Network for Controlled Image Generation</strong><br><button class=copy-to-clipboard title="Condition-Aware Neural Network for Controlled Image Generation" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01143v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01143v1.pdf filename=2404.01143v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Condition-Aware Neural Network (CAN), a new method for adding control to image generative models. In parallel to prior conditional control methods, CAN controls the image generation process by dynamically manipulating the weight of the neural network. This is achieved by introducing a condition-aware weight generation module that generates conditional weight for convolution/linear layers based on the input condition. We test CAN on class-conditional image generation on ImageNet and <b>text-to-image</b> generation on COCO. CAN consistently delivers significant improvements for diffusion <b>transformer</b> models, including DiT and UViT. In particular, CAN combined with EfficientViT (CaT) achieves 2.78 FID on ImageNet 512x512, surpassing DiT-XL/2 while requiring 52x fewer MACs per sampling step.</p></p class="citation"></blockquote><h3 id=5889--58273-texture-preserving-diffusion-models-for-high-fidelity-virtual-try-on-xu-yang-et-al-2024>(58/89 | 58/273) Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On (Xu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Yang, Changxing Ding, Zhibin Hong, Junhao Huang, Jin Tao, Xiangmin Xu. (2024)<br><strong>Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On</strong><br><button class=copy-to-clipboard title="Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01089v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01089v1.pdf filename=2404.01089v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image-based virtual try-on is an increasingly important task for online shopping. It aims to synthesize images of a specific person wearing a specified garment. <b>Diffusion</b> <b>model-based</b> approaches have recently become popular, as they are excellent at image synthesis tasks. However, these approaches usually employ additional image encoders and rely on the cross-attention mechanism for texture transfer from the garment to the person image, which affects the try-on&rsquo;s efficiency and fidelity. To address these issues, we propose an Texture-Preserving <b>Diffusion</b> <b>(TPD)</b> model for virtual try-on, which enhances the fidelity of the results and introduces no additional image encoders. Accordingly, we make contributions from two aspects. First, we propose to concatenate the masked person and reference garment images along the spatial dimension and utilize the resulting image as the input for the <b>diffusion</b> <b>model&rsquo;s</b> denoising UNet. This enables the original <b>self-attention</b> layers contained in the <b>diffusion</b> <b>model</b> to achieve efficient and accurate texture transfer. Second, we propose a novel <b>diffusion-based</b> <b>method</b> that predicts a precise inpainting mask based on the person and reference garment images, further enhancing the reliability of the try-on results. In addition, we integrate mask prediction and image synthesis into a single compact model. The experimental results show that our approach can be applied to various try-on tasks, e.g., garment-to-person and person-to-person try-ons, and significantly outperforms state-of-the-art methods on popular VITON, VITON-HD databases.</p></p class="citation"></blockquote><h3 id=5989--59273-a-comprehensive-review-of-knowledge-distillation-in-computer-vision-sheikh-musa-kaleem-et-al-2024>(59/89 | 59/273) A Comprehensive Review of Knowledge Distillation in Computer Vision (Sheikh Musa Kaleem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sheikh Musa Kaleem, Tufail Rouf, Gousia Habib, Tausifa jan Saleem, Brejesh Lall. (2024)<br><strong>A Comprehensive Review of Knowledge Distillation in Computer Vision</strong><br><button class=copy-to-clipboard title="A Comprehensive Review of Knowledge Distillation in Computer Vision" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00936v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00936v2.pdf filename=2404.00936v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning techniques have been demonstrated to surpass preceding cutting-edge machine learning techniques in recent years, with computer vision being one of the most prominent examples. However, deep learning models suffer from significant drawbacks when deployed in resource-constrained environments due to their large model size and high complexity. <b>Knowledge</b> <b>Distillation</b> is one of the prominent solutions to overcome this challenge. This review paper examines the current state of research on <b>knowledge</b> <b>distillation,</b> a technique for compressing complex models into smaller and simpler ones. The paper provides an overview of the major principles and techniques associated with <b>knowledge</b> <b>distillation</b> and reviews the applications of <b>knowledge</b> <b>distillation</b> in the domain of computer vision. The review focuses on the benefits of <b>knowledge</b> <b>distillation,</b> as well as the problems that must be overcome to improve its effectiveness.</p></p class="citation"></blockquote><h3 id=6089--60273-disr-nerf-diffusion-guided-view-consistent-super-resolution-nerf-jie-long-lee-et-al-2024>(60/89 | 60/273) DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF (Jie Long Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Long Lee, Chen Li, Gim Hee Lee. (2024)<br><strong>DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF</strong><br><button class=copy-to-clipboard title="DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00874v1.pdf filename=2404.00874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present DiSR-NeRF, a <b>diffusion-guided</b> <b>framework</b> for view-consistent super-resolution (SR) NeRF. Unlike prior works, we circumvent the requirement for high-resolution (HR) reference images by leveraging existing powerful 2D super-resolution models. Nonetheless, independent SR 2D images are often inconsistent across different views. We thus propose Iterative 3D Synchronization (I3DS) to mitigate the inconsistency problem via the inherent multi-view consistency property of NeRF. Specifically, our I3DS alternates between upscaling low-resolution (LR) rendered images with <b>diffusion</b> <b>models,</b> and updating the underlying 3D representation with standard NeRF training. We further introduce Renoised Score <b>Distillation</b> (RSD), a novel score-distillation objective for 2D image resolution. Our RSD combines features from ancestral sampling and Score <b>Distillation</b> Sampling (SDS) to generate sharp images that are also LR-consistent. Qualitative and quantitative results on both synthetic and real-world datasets demonstrate that our DiSR-NeRF can achieve better results on NeRF super-resolution compared with existing works. Code and video results available at the project website.</p></p class="citation"></blockquote><h3 id=6189--61273-spikemba-multi-modal-spiking-saliency-mamba-for-temporal-video-grounding-wenrui-li-et-al-2024>(61/89 | 61/273) SpikeMba: Multi-Modal Spiking Saliency Mamba for Temporal Video Grounding (Wenrui Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenrui Li, Xiaopeng Hong, Xiaopeng Fan. (2024)<br><strong>SpikeMba: Multi-Modal Spiking Saliency Mamba for Temporal Video Grounding</strong><br><button class=copy-to-clipboard title="SpikeMba: Multi-Modal Spiking Saliency Mamba for Temporal Video Grounding" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 19<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01174v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01174v1.pdf filename=2404.01174v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Temporal video <b>grounding</b> (TVG) is a critical task in video content understanding. Despite significant advancements, existing methods often limit in capturing the fine-grained relationships between <b>multimodal</b> inputs and the high computational costs with processing long video sequences. To address these limitations, we introduce a novel SpikeMba: <b>multi-modal</b> spiking saliency mamba for temporal video <b>grounding.</b> In our work, we integrate the Spiking Neural Networks (SNNs) and state space models (SSMs) to capture the fine-grained relationships of <b>multimodal</b> features effectively. Specifically, we introduce the relevant slots to enhance the model&rsquo;s memory capabilities, enabling a deeper contextual understanding of video sequences. The contextual moment reasoner leverages these slots to maintain a balance between contextual information preservation and semantic relevance exploration. Simultaneously, the spiking saliency detector capitalizes on the unique properties of SNNs to accurately locate salient proposals. Our experiments demonstrate the effectiveness of SpikeMba, which consistently outperforms state-of-the-art methods across mainstream <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=6289--62273-sgcnerf-few-shot-neural-rendering-via-sparse-geometric-consistency-guidance-yuru-xiao-et-al-2024>(62/89 | 62/273) SGCNeRF: Few-Shot Neural Rendering via Sparse Geometric Consistency Guidance (Yuru Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuru Xiao, Xianming Liu, Deming Zhai, Kui Jiang, Junjun Jiang, Xiangyang Ji. (2024)<br><strong>SGCNeRF: Few-Shot Neural Rendering via Sparse Geometric Consistency Guidance</strong><br><button class=copy-to-clipboard title="SGCNeRF: Few-Shot Neural Rendering via Sparse Geometric Consistency Guidance" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Few-shot, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00992v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00992v1.pdf filename=2404.00992v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Radiance Field (NeRF) technology has made significant strides in creating novel viewpoints. However, its effectiveness is hampered when working with sparsely available views, often leading to performance dips due to overfitting. FreeNeRF attempts to overcome this limitation by integrating implicit <b>geometry</b> regularization, which incrementally improves both <b>geometry</b> and textures. Nonetheless, an initial low positional encoding bandwidth results in the exclusion of high-frequency elements. The quest for a holistic approach that simultaneously addresses overfitting and the preservation of high-frequency details remains ongoing. This study introduces a novel feature matching based sparse <b>geometry</b> regularization module. This module excels in pinpointing high-frequency keypoints, thereby safeguarding the integrity of fine details. Through progressive refinement of <b>geometry</b> and textures across NeRF iterations, we unveil an effective <b>few-shot</b> neural rendering architecture, designated as SGCNeRF, for enhanced novel view synthesis. Our experiments demonstrate that SGCNeRF not only achieves superior <b>geometry-consistent</b> outcomes but also surpasses FreeNeRF, with improvements of 0.7 dB and 0.6 dB in PSNR on the LLFF and DTU datasets, respectively.</p></p class="citation"></blockquote><h3 id=6389--63273-large-motion-model-for-unified-multi-modal-motion-generation-mingyuan-zhang-et-al-2024>(63/89 | 63/273) Large Motion Model for Unified Multi-Modal Motion Generation (Mingyuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyuan Zhang, Daisheng Jin, Chenyang Gu, Fangzhou Hong, Zhongang Cai, Jingfang Huang, Chongzhi Zhang, Xinying Guo, Lei Yang, Ying He, Ziwei Liu. (2024)<br><strong>Large Motion Model for Unified Multi-Modal Motion Generation</strong><br><button class=copy-to-clipboard title="Large Motion Model for Unified Multi-Modal Motion Generation" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01284v1.pdf filename=2404.01284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human motion generation, a cornerstone technique in animation and video production, has widespread applications in various tasks like text-to-motion and music-to-dance. Previous works focus on developing specialist models tailored for each task without scalability. In this work, we present Large Motion Model (LMM), a motion-centric, <b>multi-modal</b> framework that unifies mainstream motion generation tasks into a generalist model. A unified motion model is appealing since it can leverage a wide range of motion data to achieve broad generalization beyond a single task. However, it is also challenging due to the heterogeneous nature of substantially different motion data and tasks. LMM tackles these challenges from three principled aspects: 1) Data: We consolidate datasets with different modalities, formats and tasks into a comprehensive yet unified motion generation dataset, MotionVerse, comprising 10 tasks, 16 datasets, a total of 320k sequences, and 100 million frames. 2) Architecture: We design an articulated attention mechanism ArtAttention that incorporates body part-aware modeling into Diffusion <b>Transformer</b> backbone. 3) Pre-Training: We propose a novel pre-training strategy for LMM, which employs variable frame rates and masking forms, to better exploit knowledge from diverse training data. Extensive experiments demonstrate that our generalist LMM achieves competitive performance across various standard motion generation tasks over state-of-the-art specialist models. Notably, LMM exhibits strong generalization capabilities and emerging properties across many unseen tasks. Additionally, our ablation studies reveal valuable insights about training and scaling up large motion models for future research.</p></p class="citation"></blockquote><h3 id=6489--64273-what-is-point-supervision-worth-in-video-instance-segmentation-shuaiyi-huang-et-al-2024>(64/89 | 64/273) What is Point Supervision Worth in Video Instance Segmentation? (Shuaiyi Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuaiyi Huang, De-An Huang, Zhiding Yu, Shiyi Lan, Subhashree Radhakrishnan, Jose M. Alvarez, Abhinav Shrivastava, Anima Anandkumar. (2024)<br><strong>What is Point Supervision Worth in Video Instance Segmentation?</strong><br><button class=copy-to-clipboard title="What is Point Supervision Worth in Video Instance Segmentation?" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01990v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01990v1.pdf filename=2404.01990v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video instance segmentation (VIS) is a challenging vision task that aims to detect, segment, and track objects in videos. Conventional VIS methods rely on densely-annotated object masks which are expensive. We reduce the human annotations to only one point for each object in a video frame during training, and obtain high-quality mask predictions close to fully <b>supervised</b> models. Our proposed training method consists of a class-agnostic proposal generation module to provide rich negative samples and a spatio-temporal point-based matcher to match the object queries with the provided point annotations. Comprehensive experiments on three VIS <b>benchmarks</b> demonstrate competitive performance of the proposed framework, nearly matching fully <b>supervised</b> methods.</p></p class="citation"></blockquote><h3 id=6589--65273-bem-balanced-and-entropy-based-mix-for-long-tailed-semi-supervised-learning-hongwei-zheng-et-al-2024>(65/89 | 65/273) BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised Learning (Hongwei Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongwei Zheng, Linyuan Zhou, Han Li, Jinming Su, Xiaoming Wei, Xiaoming Xu. (2024)<br><strong>BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised Learning</strong><br><button class=copy-to-clipboard title="BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised Learning" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01179v1.pdf filename=2404.01179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data mixing methods play a crucial role in <b>semi-supervised</b> <b>learning</b> (SSL), but their application is unexplored in long-tailed <b>semi-supervised</b> <b>learning</b> (LTSSL). The primary reason is that the in-batch mixing manner fails to address class imbalance. Furthermore, existing LTSSL methods mainly focus on re-balancing data quantity but ignore class-wise uncertainty, which is also vital for class balance. For instance, some classes with sufficient samples might still exhibit high uncertainty due to indistinguishable features. To this end, this paper introduces the Balanced and Entropy-based Mix (BEM), a pioneering mixing approach to re-balance the class distribution of both data quantity and uncertainty. Specifically, we first propose a class balanced mix bank to store data of each class for mixing. This bank samples data based on the estimated quantity distribution, thus re-balancing data quantity. Then, we present an entropy-based learning approach to re-balance class-wise uncertainty, including entropy-based sampling strategy, entropy-based selection module, and entropy-based class balanced loss. Our BEM first leverages data mixing for improving LTSSL, and it can also serve as a complement to the existing re-balancing methods. Experimental results show that BEM significantly enhances various LTSSL frameworks and achieves state-of-the-art performances across multiple <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=6689--66273-roadside-monocular-3d-detection-via-2d-detection-prompting-yechi-ma-et-al-2024>(66/89 | 66/273) Roadside Monocular 3D Detection via 2D Detection Prompting (Yechi Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yechi Ma, Shuoquan Wei, Churun Zhang, Wei Hua, Yanan Li, Shu Kong. (2024)<br><strong>Roadside Monocular 3D Detection via 2D Detection Prompting</strong><br><button class=copy-to-clipboard title="Roadside Monocular 3D Detection via 2D Detection Prompting" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01064v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01064v2.pdf filename=2404.01064v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The problem of roadside monocular 3D detection requires detecting objects of interested classes in a 2D RGB frame and predicting their 3D information such as locations in bird&rsquo;s-eye-view (BEV). It has broad applications in traffic control, vehicle-vehicle communication, and vehicle-infrastructure cooperative perception. To approach this problem, we present a novel and simple method by <b>prompting</b> the 3D detector using 2D detections. Our method builds on a key insight that, compared with 3D detectors, a 2D detector is much easier to train and performs significantly better w.r.t detections on the 2D image plane. That said, one can exploit 2D detections of a well-trained 2D detector as <b>prompts</b> to a 3D detector, being trained in a way of inflating such 2D detections to 3D towards 3D detection. To construct better <b>prompts</b> using the 2D detector, we explore three techniques: (a) concatenating both 2D and 3D detectors&rsquo; features, (b) attentively fusing 2D and 3D detectors&rsquo; features, and (c) encoding predicted 2D boxes x, y, width, height, label and attentively fusing such with the 3D detector&rsquo;s features. Surprisingly, the third performs the best. Moreover, we present a yaw tuning tactic and a class-grouping strategy that merges classes based on their functionality; these techniques improve 3D detection performance further. Comprehensive ablation studies and extensive experiments demonstrate that our method resoundingly outperforms prior works, achieving the state-of-the-art on two large-scale roadside 3D detection <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=6789--67273-aigcoiqa2024-perceptual-quality-assessment-of-ai-generated-omnidirectional-images-liu-yang-et-al-2024>(67/89 | 67/273) AIGCOIQA2024: Perceptual Quality Assessment of AI Generated Omnidirectional Images (Liu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liu Yang, Huiyu Duan, Long Teng, Yucheng Zhu, Xiaohong Liu, Menghan Hu, Xiongkuo Min, Guangtao Zhai, Patrick Le Callet. (2024)<br><strong>AIGCOIQA2024: Perceptual Quality Assessment of AI Generated Omnidirectional Images</strong><br><button class=copy-to-clipboard title="AIGCOIQA2024: Perceptual Quality Assessment of AI Generated Omnidirectional Images" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 13<br>Keywords: Benchmarking, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01024v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01024v1.pdf filename=2404.01024v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the rapid advancement of Artificial Intelligence Generated Content (AIGC) has attracted widespread attention. Among the AIGC, AI generated omnidirectional images hold significant potential for Virtual Reality (VR) and Augmented Reality (AR) applications, hence omnidirectional AIGC techniques have also been widely studied. AI-generated omnidirectional images exhibit unique distortions compared to natural omnidirectional images, however, there is no dedicated Image Quality Assessment (IQA) criteria for assessing them. This study addresses this gap by establishing a large-scale AI generated omnidirectional image IQA database named AIGCOIQA2024 and constructing a comprehensive <b>benchmark.</b> We first generate 300 omnidirectional images based on 5 AIGC models utilizing 25 text <b>prompts.</b> A subjective IQA experiment is conducted subsequently to assess human visual preferences from three perspectives including quality, comfortability, and correspondence. Finally, we conduct a <b>benchmark</b> experiment to evaluate the performance of state-of-the-art IQA models on our database. The database will be released to facilitate future research.</p></p class="citation"></blockquote><h3 id=6889--68273-generating-content-for-hdr-deghosting-from-frequency-view-tao-hu-et-al-2024>(68/89 | 68/273) Generating Content for HDR Deghosting from Frequency View (Tao Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Hu, Qingsen Yan, Yuankai Qi, Yanning Zhang. (2024)<br><strong>Generating Content for HDR Deghosting from Frequency View</strong><br><button class=copy-to-clipboard title="Generating Content for HDR Deghosting from Frequency View" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00849v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00849v1.pdf filename=2404.00849v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recovering ghost-free High Dynamic Range (HDR) images from multiple Low Dynamic Range (LDR) images becomes challenging when the LDR images exhibit saturation and significant motion. Recent <b>Diffusion</b> <b>Models</b> (DMs) have been introduced in HDR imaging field, demonstrating promising performance, particularly in achieving visually perceptible results compared to previous DNN-based methods. However, DMs require extensive iterations with large models to estimate entire images, resulting in inefficiency that hinders their practical application. To address this challenge, we propose the Low-Frequency aware <b>Diffusion</b> <b>(LF-Diff)</b> model for ghost-free HDR imaging. The key idea of LF-Diff is implementing the DMs in a highly compacted latent space and integrating it into a regression-based model to enhance the details of reconstructed images. Specifically, as low-frequency information is closely related to human visual perception we propose to utilize DMs to create compact low-frequency priors for the reconstruction process. In addition, to take full advantage of the above low-frequency priors, the Dynamic HDR Reconstruction Network (DHRNet) is carried out in a regression-based manner to obtain final HDR images. Extensive experiments conducted on synthetic and real-world <b>benchmark</b> datasets demonstrate that our LF-Diff performs favorably against several state-of-the-art methods and is 10$\times$ faster than previous DM-based methods.</p></p class="citation"></blockquote><h3 id=6989--69273-3mos-multi-sources-multi-resolutions-and-multi-scenes-dataset-for-optical-sar-image-matching-yibin-ye-et-al-2024>(69/89 | 69/273) 3MOS: Multi-sources, Multi-resolutions, and Multi-scenes dataset for Optical-SAR image matching (Yibin Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yibin Ye, Xichao Teng, Shuo Chen, Yijie Bian, Tao Tan, Zhang Li. (2024)<br><strong>3MOS: Multi-sources, Multi-resolutions, and Multi-scenes dataset for Optical-SAR image matching</strong><br><button class=copy-to-clipboard title="3MOS: Multi-sources, Multi-resolutions, and Multi-scenes dataset for Optical-SAR image matching" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Multi-modal, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00838v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00838v1.pdf filename=2404.00838v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optical-SAR image matching is a fundamental task for image fusion and visual navigation. However, all large-scale open SAR dataset for methods development are collected from single platform, resulting in limited satellite types and spatial resolutions. Since images captured by different sensors vary significantly in both geometric and radiometric appearance, existing methods may fail to match corresponding regions containing the same content. Besides, most of existing datasets have not been categorized based on the characteristics of different scenes. To encourage the design of more general <b>multi-modal</b> image matching methods, we introduce a large-scale Multi-sources,Multi-resolutions, and Multi-scenes dataset for Optical-SAR image matching(3MOS). It consists of 155K optical-SAR image pairs, including SAR data from six commercial satellites, with resolutions ranging from 1.25m to 12.5m. The data has been classified into eight scenes including urban, rural, plains, hills, mountains, water, desert, and frozen earth. Extensively experiments show that none of state-of-the-art methods achieve consistently superior performance across different sources, resolutions and scenes. In addition, the distribution of data has a substantial impact on the matching capability of deep learning models, this proposes the <b>domain</b> <b>adaptation</b> challenge in optical-SAR image matching. Our data and code will be available at:https://github.com/3M-OS/3MOS.</p></p class="citation"></blockquote><h3 id=7089--70273-can-biases-in-imagenet-models-explain-generalization-paul-gavrikov-et-al-2024>(70/89 | 70/273) Can Biases in ImageNet Models Explain Generalization? (Paul Gavrikov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Gavrikov, Janis Keuper. (2024)<br><strong>Can Biases in ImageNet Models Explain Generalization?</strong><br><button class=copy-to-clipboard title="Can Biases in ImageNet Models Explain Generalization?" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV, stat-ML<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01509v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01509v1.pdf filename=2404.01509v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The robust generalization of models to rare, in-distribution (ID) samples drawn from the long tail of the training distribution and to out-of-training-distribution (OOD) samples is one of the major challenges of current deep learning methods. For image classification, this manifests in the existence of <b>adversarial</b> <b>attacks,</b> the performance drops on distorted images, and a lack of generalization to concepts such as sketches. The current understanding of generalization in neural networks is very limited, but some biases that differentiate models from human vision have been identified and might be causing these limitations. Consequently, several attempts with varying success have been made to reduce these biases during training to improve generalization. We take a step back and sanity-check these attempts. Fixing the architecture to the well-established ResNet-50, we perform a large-scale study on 48 ImageNet models obtained via different training methods to understand how and if these biases - including shape bias, spectral biases, and critical bands - interact with generalization. Our extensive study results reveal that contrary to previous findings, these biases are insufficient to accurately predict the generalization of a model holistically. We provide access to all checkpoints and evaluation code at <a href=https://github.com/paulgavrikov/biases_vs_generalization>https://github.com/paulgavrikov/biases_vs_generalization</a></p></p class="citation"></blockquote><h3 id=7189--71273-bridging-remote-sensors-with-multisensor-geospatial-foundation-models-boran-han-et-al-2024>(71/89 | 71/273) Bridging Remote Sensors with Multisensor Geospatial Foundation Models (Boran Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boran Han, Shuai Zhang, Xingjian Shi, Markus Reichstein. (2024)<br><strong>Bridging Remote Sensors with Multisensor Geospatial Foundation Models</strong><br><button class=copy-to-clipboard title="Bridging Remote Sensors with Multisensor Geospatial Foundation Models" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01260v1.pdf filename=2404.01260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of geospatial analysis, the diversity of remote sensors, encompassing both optical and microwave technologies, offers a wealth of distinct observational capabilities. Recognizing this, we present msGFM, a multisensor geospatial <b>foundation</b> <b>model</b> that effectively unifies data from four key sensor modalities. This integration spans an expansive dataset of two million multisensor images. msGFM is uniquely adept at handling both paired and unpaired sensor data. For data originating from identical geolocations, our model employs an innovative cross-sensor pretraining approach in masked image modeling, enabling the synthesis of joint representations from diverse sensors. msGFM, incorporating four remote sensors, upholds strong performance, forming a comprehensive model adaptable to various sensor types. msGFM has demonstrated enhanced proficiency in a range of both single-sensor and multisensor downstream tasks. These include scene classification, segmentation, cloud removal, and pan-sharpening. A key discovery of our research is that representations derived from natural images are not always compatible with the distinct characteristics of geospatial remote sensors, underscoring the limitations of existing representations in this field. Our work can serve as a guide for developing multisensor geospatial pretraining models, paving the way for more advanced geospatial capabilities.</p></p class="citation"></blockquote><h3 id=7289--72273-fireants-adaptive-riemannian-optimization-for-multi-scale-diffeomorphic-registration-rohit-jena-et-al-2024>(72/89 | 72/273) FireANTs: Adaptive Riemannian Optimization for Multi-Scale Diffeomorphic Registration (Rohit Jena et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohit Jena, Pratik Chaudhari, James C. Gee. (2024)<br><strong>FireANTs: Adaptive Riemannian Optimization for Multi-Scale Diffeomorphic Registration</strong><br><button class=copy-to-clipboard title="FireANTs: Adaptive Riemannian Optimization for Multi-Scale Diffeomorphic Registration" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01249v1.pdf filename=2404.01249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffeomorphic <b>Image</b> <b>Registration</b> is a critical part of the analysis in various imaging modalities and downstream tasks like <b>image</b> <b>translation,</b> segmentation, and atlas building. Registration algorithms based on optimization have stood the test of time in terms of accuracy, reliability, and robustness across a wide spectrum of modalities and acquisition settings. However, these algorithms converge slowly, are prohibitively expensive to run, and their usage requires a steep learning curve, limiting their scalability to larger clinical and scientific studies. In this paper, we develop multi-scale Adaptive Riemannian Optimization algorithms for diffeomorphic <b>image</b> <b>registration.</b> We demonstrate compelling improvements on <b>image</b> <b>registration</b> across a spectrum of modalities and anatomies by measuring structural and landmark overlap of the registered <b>image</b> <b>volumes.</b> Our proposed framework leads to a consistent improvement in performance, and from 300x up to 2000x speedup over existing algorithms. Our modular library design makes it easy to use and allows customization via user-defined cost functions.</p></p class="citation"></blockquote><h3 id=7389--73273-a-unified-and-interpretable-emotion-representation-and-expression-generation-reni-paskaleva-et-al-2024>(73/89 | 73/273) A Unified and Interpretable Emotion Representation and Expression Generation (Reni Paskaleva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reni Paskaleva, Mykyta Holubakha, Andela Ilic, Saman Motamed, Luc Van Gool, Danda Paudel. (2024)<br><strong>A Unified and Interpretable Emotion Representation and Expression Generation</strong><br><button class=copy-to-clipboard title="A Unified and Interpretable Emotion Representation and Expression Generation" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01243v1.pdf filename=2404.01243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Canonical emotions, such as happy, sad, and fearful, are easy to understand and annotate. However, emotions are often compound, e.g. happily surprised, and can be mapped to the action units (AUs) used for expressing emotions, and trivially to the canonical ones. Intuitively, emotions are continuous as represented by the arousal-valence (AV) model. An interpretable unification of these four modalities - namely, Canonical, Compound, AUs, and AV - is highly desirable, for a better representation and understanding of emotions. However, such unification remains to be unknown in the current literature. In this work, we propose an interpretable and unified emotion model, referred as C2A2. We also develop a method that leverages labels of the non-unified models to annotate the novel unified one. Finally, we modify the text-conditional <b>diffusion</b> <b>models</b> to understand continuous numbers, which are then used to generate continuous expressions using our unified emotion model. Through quantitative and qualitative experiments, we show that our generated images are rich and capture subtle expressions. Our work allows a fine-grained generation of expressions in conjunction with other textual inputs and offers a new label space for emotions at the same time.</p></p class="citation"></blockquote><h3 id=7489--74273-video-interpolation-with-diffusion-models-siddhant-jain-et-al-2024>(74/89 | 74/273) Video Interpolation with Diffusion Models (Siddhant Jain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siddhant Jain, Daniel Watson, Eric Tabellion, Aleksander Hołyński, Ben Poole, Janne Kontkanen. (2024)<br><strong>Video Interpolation with Diffusion Models</strong><br><button class=copy-to-clipboard title="Video Interpolation with Diffusion Models" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01203v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01203v1.pdf filename=2404.01203v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present VIDIM, a generative model for video interpolation, which creates short videos given a start and end frame. In order to achieve high fidelity and generate motions unseen in the input data, VIDIM uses cascaded <b>diffusion</b> <b>models</b> to first generate the target video at low resolution, and then generate the high-resolution video conditioned on the low-resolution generated video. We compare VIDIM to previous state-of-the-art methods on video interpolation, and demonstrate how such works fail in most settings where the underlying motion is complex, nonlinear, or ambiguous while VIDIM can easily handle such cases. We additionally demonstrate how classifier-free guidance on the start and end frame and conditioning the super-resolution model on the original high-resolution frames without additional parameters unlocks high-fidelity results. VIDIM is fast to sample from as it jointly denoises all the frames to be generated, requires less than a billion parameters per <b>diffusion</b> <b>model</b> to produce compelling results, and still enjoys scalability and improved quality at larger parameter counts.</p></p class="citation"></blockquote><h3 id=7589--75273-monobox-tightness-free-box-supervised-polyp-segmentation-using-monotonicity-constraint-qiang-hu-et-al-2024>(75/89 | 75/273) MonoBox: Tightness-free Box-supervised Polyp Segmentation using Monotonicity Constraint (Qiang Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiang Hu, Zhenyu Yi, Ying Zhou, Ting Li, Fan Huang, Mei Liu, Qiang Li, Zhiwei Wang. (2024)<br><strong>MonoBox: Tightness-free Box-supervised Polyp Segmentation using Monotonicity Constraint</strong><br><button class=copy-to-clipboard title="MonoBox: Tightness-free Box-supervised Polyp Segmentation using Monotonicity Constraint" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Multiple Instance Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01188v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01188v2.pdf filename=2404.01188v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose MonoBox, an innovative box-supervised segmentation method constrained by monotonicity to liberate its training from the user-unfriendly box-tightness assumption. In contrast to conventional box-supervised segmentation, where the box edges must precisely touch the target boundaries, MonoBox leverages imprecisely-annotated boxes to achieve robust pixel-wise segmentation. The &rsquo;linchpin&rsquo; is that, within the noisy zones around box edges, MonoBox discards the traditional misguiding <b>multiple-instance</b> <b>learning</b> <b>loss,</b> and instead optimizes a carefully-designed objective, termed monotonicity constraint. Along directions transitioning from the foreground to background, this new constraint steers responses to adhere to a trend of monotonically decreasing values. Consequently, the originally unreliable learning within the noisy zones is transformed into a correct and effective monotonicity optimization. Moreover, an adaptive label correction is introduced, enabling MonoBox to enhance the tightness of box annotations using predicted masks from the previous epoch and dynamically shrink the noisy zones as training progresses. We verify MonoBox in the box-supervised segmentation task of polyps, where satisfying box-tightness is challenging due to the vague boundaries between the polyp and normal tissues. Experiments on both public synthetic and in-house real noisy datasets demonstrate that MonoBox exceeds other anti-noise state-of-the-arts by improving Dice by at least 5.5% and 3.3%, respectively. Codes are at <a href=https://github.com/Huster-Hq/MonoBox>https://github.com/Huster-Hq/MonoBox</a>.</p></p class="citation"></blockquote><h3 id=7689--76273-cmt-cross-modulation-transformer-with-hybrid-loss-for-pansharpening-wen-jie-shu-et-al-2024>(76/89 | 76/273) CMT: Cross Modulation Transformer with Hybrid Loss for Pansharpening (Wen-Jie Shu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wen-Jie Shu, Hong-Xia Dou, Rui Wen, Xiao Wu, Liang-Jian Deng. (2024)<br><strong>CMT: Cross Modulation Transformer with Hybrid Loss for Pansharpening</strong><br><button class=copy-to-clipboard title="CMT: Cross Modulation Transformer with Hybrid Loss for Pansharpening" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01121v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01121v1.pdf filename=2404.01121v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pansharpening aims to enhance remote sensing image (RSI) quality by merging high-resolution panchromatic (PAN) with multispectral (MS) images. However, prior techniques struggled to optimally fuse PAN and MS images for enhanced spatial and spectral information, due to a lack of a systematic framework capable of effectively coordinating their individual strengths. In response, we present the Cross Modulation <b>Transformer</b> (CMT), a pioneering method that modifies the attention mechanism. This approach utilizes a robust modulation technique from signal processing, integrating it into the attention mechanism&rsquo;s calculations. It dynamically tunes the weights of the carrier&rsquo;s value (V) matrix according to the modulator&rsquo;s features, thus resolving historical challenges and achieving a seamless integration of spatial and spectral attributes. Furthermore, considering that RSI exhibits large-scale features and edge details along with local textures, we crafted a hybrid loss function that combines Fourier and wavelet transforms to effectively capture these characteristics, thereby enhancing both spatial and spectral accuracy in pansharpening. Extensive experiments demonstrate our framework&rsquo;s superior performance over existing state-of-the-art methods. The code will be publicly available to encourage further research.</p></p class="citation"></blockquote><h3 id=7789--77273-motion-blur-decomposition-with-cross-shutter-guidance-xiang-ji-et-al-2024>(77/89 | 77/273) Motion Blur Decomposition with Cross-shutter Guidance (Xiang Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Ji, Haiyang Jiang, Yinqiang Zheng. (2024)<br><strong>Motion Blur Decomposition with Cross-shutter Guidance</strong><br><button class=copy-to-clipboard title="Motion Blur Decomposition with Cross-shutter Guidance" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Disambiguation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01120v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01120v1.pdf filename=2404.01120v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motion blur is a frequently observed image artifact, especially under insufficient illumination where exposure time has to be prolonged so as to collect more photons for a bright enough image. Rather than simply removing such blurring effects, recent researches have aimed at decomposing a blurry image into multiple sharp images with spatial and temporal coherence. Since motion blur decomposition itself is highly ambiguous, priors from neighbouring frames or human annotation are usually needed for motion <b>disambiguation.</b> In this paper, inspired by the complementary exposure characteristics of a global shutter (GS) camera and a rolling shutter (RS) camera, we propose to utilize the ordered scanline-wise delay in a rolling shutter image to robustify motion decomposition of a single blurry image. To evaluate this novel dual imaging setting, we construct a triaxial system to collect realistic data, as well as a deep network architecture that explicitly addresses temporal and contextual information through reciprocal branches for cross-shutter motion blur decomposition. Experiment results have verified the effectiveness of our proposed algorithm, as well as the validity of our dual imaging setting.</p></p class="citation"></blockquote><h3 id=7889--78273-action-detection-via-an-image-diffusion-process-lin-geng-foo-et-al-2024>(78/89 | 78/273) Action Detection via an Image Diffusion Process (Lin Geng Foo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lin Geng Foo, Tianjiao Li, Hossein Rahmani, Jun Liu. (2024)<br><strong>Action Detection via an Image Diffusion Process</strong><br><button class=copy-to-clipboard title="Action Detection via an Image Diffusion Process" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01051v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01051v1.pdf filename=2404.01051v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Action detection aims to localize the starting and ending points of action instances in untrimmed videos, and predict the classes of those instances. In this paper, we make the observation that the outputs of the action detection task can be formulated as images. Thus, from a novel perspective, we tackle action detection via a three-image generation process to generate starting point, ending point and action-class predictions as images via our proposed Action Detection Image Diffusion (ADI-Diff) framework. Furthermore, since our images differ from natural images and exhibit special properties, we further explore a Discrete Action-Detection Diffusion Process and a Row-Column <b>Transformer</b> design to better handle their processing. Our ADI-Diff framework achieves state-of-the-art results on two widely-used datasets.</p></p class="citation"></blockquote><h3 id=7989--79273-flexidreamer-single-image-to-3d-generation-with-flexicubes-ruowen-zhao-et-al-2024>(79/89 | 79/273) FlexiDreamer: Single Image-to-3D Generation with FlexiCubes (Ruowen Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruowen Zhao, Zhengyi Wang, Yikai Wang, Zihan Zhou, Jun Zhu. (2024)<br><strong>FlexiDreamer: Single Image-to-3D Generation with FlexiCubes</strong><br><button class=copy-to-clipboard title="FlexiDreamer: Single Image-to-3D Generation with FlexiCubes" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00987v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00987v1.pdf filename=2404.00987v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D content generation from text <b>prompts</b> or single images has made remarkable progress in quality and speed recently. One of its dominant paradigms involves generating consistent multi-view images followed by a sparse-view reconstruction. However, due to the challenge of directly deforming the mesh representation to approach the target topology, most methodologies learn an implicit representation (such as NeRF) during the sparse-view reconstruction and acquire the target mesh by a post-processing extraction. Although the implicit representation can effectively model rich 3D information, its training typically entails a long convergence time. In addition, the post-extraction operation from the implicit field also leads to undesirable visual artifacts. In this paper, we propose FlexiDreamer, a novel single image-to-3d generation framework that reconstructs the target mesh in an end-to-end manner. By leveraging a flexible gradient-based extraction known as FlexiCubes, our method circumvents the defects brought by the post-processing and facilitates a direct acquisition of the target mesh. Furthermore, we incorporate a multi-resolution hash grid encoding scheme that progressively activates the encoding levels into the implicit field in FlexiCubes to help capture geometric details for per-step optimization. Notably, FlexiDreamer recovers a dense 3D structure from a single-view image in approximately 1 minute on a single NVIDIA A100 GPU, outperforming previous methodologies by a large margin.</p></p class="citation"></blockquote><h3 id=8089--80273-camo-correlation-aware-mask-optimization-with-modulated-reinforcement-learning-xiaoxiao-liang-et-al-2024>(80/89 | 80/273) CAMO: Correlation-Aware Mask Optimization with Modulated Reinforcement Learning (Xiaoxiao Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoxiao Liang, Haoyu Yang, Kang Liu, Bei Yu, Yuzhe Ma. (2024)<br><strong>CAMO: Correlation-Aware Mask Optimization with Modulated Reinforcement Learning</strong><br><button class=copy-to-clipboard title="CAMO: Correlation-Aware Mask Optimization with Modulated Reinforcement Learning" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AR, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00980v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00980v1.pdf filename=2404.00980v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optical proximity correction (OPC) is a vital step to ensure printability in modern VLSI manufacturing. Various OPC approaches based on machine learning have been proposed to pursue performance and efficiency, which are typically data-driven and hardly involve any particular considerations of the OPC problem, leading to potential performance or efficiency bottlenecks. In this paper, we propose CAMO, a <b>reinforcement</b> <b>learning-based</b> OPC system that specifically integrates important principles of the OPC problem. CAMO explicitly involves the spatial correlation among the movements of neighboring segments and an OPC-inspired modulation for movement action selection. Experiments are conducted on both via layer patterns and metal layer patterns. The results demonstrate that CAMO outperforms state-of-the-art OPC engines from both academia and industry.</p></p class="citation"></blockquote><h3 id=8189--81273-exploring-the-efficacy-of-group-normalization-in-deep-learning-models-for-alzheimers-disease-classification-gousia-habib-et-al-2024>(81/89 | 81/273) Exploring the Efficacy of Group-Normalization in Deep Learning Models for Alzheimer&rsquo;s Disease Classification (Gousia Habib et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gousia Habib, Ishfaq Ahmed Malik, Jameel Ahmad, Imtiaz Ahmed, Shaima Qureshi. (2024)<br><strong>Exploring the Efficacy of Group-Normalization in Deep Learning Models for Alzheimer&rsquo;s Disease Classification</strong><br><button class=copy-to-clipboard title="Exploring the Efficacy of Group-Normalization in Deep Learning Models for Alzheimer's Disease Classification" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00946v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00946v1.pdf filename=2404.00946v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Batch Normalization is an important approach to advancing deep learning since it allows multiple networks to train simultaneously. A problem arises when normalizing along the batch dimension because B.N.&rsquo;s error increases significantly as batch size shrinks because batch statistics estimates are inaccurate. As a result, computer vision tasks like detection, segmentation, and video, which require tiny batches based on memory consumption, aren&rsquo;t suitable for using Batch Normalization for larger model training and feature transfer. Here, we explore Group Normalization as an easy alternative to using Batch Normalization A Group Normalization is a channel normalization method in which each group is divided into different channels, and the corresponding mean and variance are calculated for each group. Group Normalization computations are accurate across a wide range of batch sizes and are independent of batch size. When trained using a large ImageNet database on ResNet-50, GN achieves a very low error rate of 10.6% compared to Batch Normalization. when a smaller batch size of only 2 is used. For usual batch sizes, the performance of G.N. is comparable to that of Batch Normalization, but at the same time, it outperforms other normalization techniques. Implementing Group Normalization as a direct alternative to B.N to combat the serious challenges faced by the Batch Normalization in deep learning models with comparable or improved classification accuracy. Additionally, Group Normalization can be naturally transferred from the pre-training to the <b>fine-tuning</b> phase. .</p></p class="citation"></blockquote><h3 id=8289--82273-gyro-based-neural-single-image-deblurring-heemin-yang-et-al-2024>(82/89 | 82/273) Gyro-based Neural Single Image Deblurring (Heemin Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Heemin Yang, Jaesung Rim, Seung-Hwan Baek, Sunghyun Cho. (2024)<br><strong>Gyro-based Neural Single Image Deblurring</strong><br><button class=copy-to-clipboard title="Gyro-based Neural Single Image Deblurring" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Curriculum Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00916v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00916v1.pdf filename=2404.00916v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present GyroDeblurNet, a novel single image deblurring method that utilizes a gyro sensor to effectively resolve the ill-posedness of image deblurring. The gyro sensor provides valuable information about camera motion during exposure time that can significantly improve deblurring quality. However, effectively exploiting real-world gyro data is challenging due to significant errors from various sources including sensor noise, the disparity between the positions of a camera module and a gyro sensor, the absence of translational motion information, and moving objects whose motions cannot be captured by a gyro sensor. To handle gyro error, GyroDeblurNet is equipped with two novel neural network blocks: a gyro refinement block and a gyro deblurring block. The gyro refinement block refines the error-ridden gyro data using the blur information from the input image. On the other hand, the gyro deblurring block removes blur from the input image using the refined gyro data and further compensates for gyro error by leveraging the blur information from the input image. For training a neural network with erroneous gyro data, we propose a training strategy based on the <b>curriculum</b> <b>learning.</b> We also introduce a novel gyro data embedding scheme to represent real-world intricate camera shakes. Finally, we present a synthetic dataset and a real dataset for the training and evaluation of gyro-based single image deblurring. Our experiments demonstrate that our approach achieves state-of-the-art deblurring quality by effectively utilizing erroneous gyro data.</p></p class="citation"></blockquote><h3 id=8389--83273-streaming-dense-video-captioning-xingyi-zhou-et-al-2024>(83/89 | 83/273) Streaming Dense Video Captioning (Xingyi Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan, Austin Myers, Xuehan Xiong, Arsha Nagrani, Cordelia Schmid. (2024)<br><strong>Streaming Dense Video Captioning</strong><br><button class=copy-to-clipboard title="Streaming Dense Video Captioning" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01297v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01297v1.pdf filename=2404.01297v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An ideal model for dense video captioning &ndash; predicting captions localized temporally in a video &ndash; should be able to handle long input videos, predict rich, detailed textual descriptions, and be able to produce outputs before processing the entire video. Current state-of-the-art models, however, process a fixed number of downsampled frames, and make a single full prediction after seeing the whole video. We propose a streaming dense video captioning model that consists of two novel components: First, we propose a new memory module, based on <b>clustering</b> incoming tokens, which can handle arbitrarily long videos as the memory is of a fixed size. Second, we develop a streaming decoding algorithm that enables our model to make predictions before the entire video has been processed. Our model achieves this streaming ability, and significantly improves the state-of-the-art on three dense video captioning <b>benchmarks:</b> ActivityNet, YouCook2 and ViTT. Our code is released at <a href=https://github.com/google-research/scenic>https://github.com/google-research/scenic</a>.</p></p class="citation"></blockquote><h3 id=8489--84273-360x-a-panoptic-multi-modal-scene-understanding-dataset-hao-chen-et-al-2024>(84/89 | 84/273) 360+x: A Panoptic Multi-modal Scene Understanding Dataset (Hao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Chen, Yuqi Hou, Chenyuan Qu, Irene Testini, Xiaohan Hong, Jianbo Jiao. (2024)<br><strong>360+x: A Panoptic Multi-modal Scene Understanding Dataset</strong><br><button class=copy-to-clipboard title="360+x: A Panoptic Multi-modal Scene Understanding Dataset" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-MM, cs-SD, cs.CV, eess-AS<br>Keyword Score: 6<br>Keywords: Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00989v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00989v1.pdf filename=2404.00989v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human perception of the world is shaped by a multitude of viewpoints and modalities. While many existing datasets focus on scene understanding from a certain perspective (e.g. egocentric or third-person views), our dataset offers a panoptic perspective (i.e. multiple viewpoints with multiple data modalities). Specifically, we encapsulate third-person panoramic and front views, as well as egocentric monocular/binocular views with rich modalities including video, multi-channel audio, directional binaural delay, location data and textual scene descriptions within each scene captured, presenting comprehensive observation of the world. Figure 1 offers a glimpse of all 28 scene categories of our 360+x dataset. To the best of our knowledge, this is the first database that covers multiple viewpoints with multiple data modalities to mimic how daily information is accessed in the real world. Through our <b>benchmark</b> analysis, we presented 5 different scene understanding tasks on the proposed 360+x dataset to evaluate the impact and benefit of each data modality and perspective in panoptic scene understanding. We hope this unique dataset could broaden the scope of comprehensive scene understanding and encourage the community to approach these problems from more diverse perspectives.</p></p class="citation"></blockquote><h3 id=8589--85273-scalable-scene-modeling-from-perspective-imaging-physics-based-appearance-and-geometry-inference-shuang-song-2024>(85/89 | 85/273) Scalable Scene Modeling from Perspective Imaging: Physics-based Appearance and Geometry Inference (Shuang Song, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuang Song. (2024)<br><strong>Scalable Scene Modeling from Perspective Imaging: Physics-based Appearance and Geometry Inference</strong><br><button class=copy-to-clipboard title="Scalable Scene Modeling from Perspective Imaging: Physics-based Appearance and Geometry Inference" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01248v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01248v1.pdf filename=2404.01248v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D scene modeling techniques serve as the bedrocks in the geospatial engineering and computer science, which drives many applications ranging from automated driving, terrain mapping, navigation, virtual, augmented, mixed, and extended reality (for gaming and movie industry etc.). This dissertation presents a fraction of contributions that advances 3D scene modeling to its state of the art, in the aspects of both appearance and <b>geometry</b> modeling. In contrast to the prevailing deep learning methods, as a core contribution, this thesis aims to develop algorithms that follow first principles, where sophisticated physic-based models are introduced alongside with simpler learning and inference tasks. The outcomes of these algorithms yield processes that can consume much larger volume of data for highly accurate reconstructing 3D scenes at a scale without losing methodological generality, which are not possible by contemporary complex-model based deep learning methods. Specifically, the dissertation introduces three novel methodologies that address the challenges of inferring appearance and <b>geometry</b> through physics-based modeling. Overall, the research encapsulated in this dissertation marks a series of methodological triumphs in the processing of complex datasets. By navigating the confluence of deep learning, computational <b>geometry,</b> and photogrammetry, this work lays down a robust framework for future exploration and practical application in the rapidly evolving field of 3D scene reconstruction. The outcomes of these studies are evidenced through rigorous experiments and comparisons with existing state-of-the-art methods, demonstrating the efficacy and scalability of the proposed approaches.</p></p class="citation"></blockquote><h3 id=8689--86273-badpart-unified-black-box-adversarial-patch-attacks-against-pixel-wise-regression-tasks-zhiyuan-cheng-et-al-2024>(86/89 | 86/273) BadPart: Unified Black-box Adversarial Patch Attacks against Pixel-wise Regression Tasks (Zhiyuan Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyuan Cheng, Zhaoyi Liu, Tengda Guo, Shiwei Feng, Dongfang Liu, Mingjie Tang, Xiangyu Zhang. (2024)<br><strong>BadPart: Unified Black-box Adversarial Patch Attacks against Pixel-wise Regression Tasks</strong><br><button class=copy-to-clipboard title="BadPart: Unified Black-box Adversarial Patch Attacks against Pixel-wise Regression Tasks" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00924v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00924v1.pdf filename=2404.00924v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pixel-wise regression tasks (e.g., monocular depth estimation (MDE) and optical flow estimation (OFE)) have been widely involved in our daily life in applications like autonomous driving, augmented reality and video composition. Although certain applications are security-critical or bear societal significance, the adversarial robustness of such models are not sufficiently studied, especially in the <b>black-box</b> <b>scenario.</b> In this work, we introduce the first unified <b>black-box</b> <b>adversarial</b> patch attack framework against pixel-wise regression tasks, aiming to identify the vulnerabilities of these models under query-based <b>black-box</b> <b>attacks.</b> We propose a novel square-based adversarial patch optimization framework and employ probabilistic square sampling and score-based gradient estimation techniques to generate the patch effectively and efficiently, overcoming the scalability problem of previous <b>black-box</b> <b>patch</b> attacks. Our attack prototype, named BadPart, is evaluated on both MDE and OFE tasks, utilizing a total of 7 models. BadPart surpasses 3 baseline methods in terms of both attack performance and efficiency. We also apply BadPart on the Google online service for portrait depth estimation, causing 43.5% relative distance error with 50K queries. State-of-the-art (SOTA) countermeasures cannot defend our attack effectively.</p></p class="citation"></blockquote><h3 id=8789--87273-on-train-test-class-overlap-and-detection-for-image-retrieval-chull-hwan-song-et-al-2024>(87/89 | 87/273) On Train-Test Class Overlap and Detection for Image Retrieval (Chull Hwan Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chull Hwan Song, Jooyoung Yoon, Taebaek Hwang, Shunghyun Choi, Yeong Hyeon Gu, Yannis Avrithis. (2024)<br><strong>On Train-Test Class Overlap and Detection for Image Retrieval</strong><br><button class=copy-to-clipboard title="On Train-Test Class Overlap and Detection for Image Retrieval" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01524v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01524v1.pdf filename=2404.01524v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How important is it for training and evaluation sets to not have class overlap in image retrieval? We revisit Google Landmarks v2 clean, the most popular training set, by identifying and removing class overlap with Revisited Oxford and Paris [34], the most popular evaluation set. By comparing the original and the new RGLDv2-clean on a <b>benchmark</b> of reproduced state-of-the-art methods, our findings are striking. Not only is there a dramatic drop in performance, but it is inconsistent across methods, changing the ranking.What does it take to focus on objects or interest and ignore background clutter when indexing? Do we need to train an object detector and the representation separately? Do we need location supervision? We introduce Single-stage Detect-to-Retrieve (CiDeR), an end-to-end, single-stage pipeline to detect objects of interest and extract a global image representation. We outperform previous state-of-the-art on both existing training sets and the new RGLDv2-clean. Our dataset is available at <a href=https://github.com/dealicious-inc/RGLDv2-clean>https://github.com/dealicious-inc/RGLDv2-clean</a>.</p></p class="citation"></blockquote><h3 id=8889--88273-posterllama-bridging-design-ability-of-langauge-model-to-contents-aware-layout-generation-jaejung-seol-et-al-2024>(88/89 | 88/273) PosterLlama: Bridging Design Ability of Langauge Model to Contents-Aware Layout Generation (Jaejung Seol et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaejung Seol, Seojun Kim, Jaejun Yoo. (2024)<br><strong>PosterLlama: Bridging Design Ability of Langauge Model to Contents-Aware Layout Generation</strong><br><button class=copy-to-clipboard title="PosterLlama: Bridging Design Ability of Langauge Model to Contents-Aware Layout Generation" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00995v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00995v2.pdf filename=2404.00995v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual layout plays a critical role in graphic design fields such as advertising, posters, and web UI design. The recent trend towards content-aware layout generation through generative models has shown promise, yet it often overlooks the semantic intricacies of layout design by treating it as a simple numerical optimization. To bridge this gap, we introduce PosterLlama, a network designed for generating visually and textually coherent layouts by reformatting layout elements into HTML code and leveraging the rich design knowledge embedded within language models. Furthermore, we enhance the robustness of our model with a unique depth-based poster augmentation strategy. This ensures our generated layouts remain semantically rich but also visually appealing, even with limited data. Our extensive evaluations across several <b>benchmarks</b> demonstrate that PosterLlama outperforms existing methods in producing authentic and content-aware layouts. It supports an unparalleled range of conditions, including but not limited to unconditional layout generation, element conditional layout generation, layout completion, among others, serving as a highly versatile user manipulation tool.</p></p class="citation"></blockquote><h3 id=8989--89273-mm3dgs-slam-multi-modal-3d-gaussian-splatting-for-slam-using-vision-depth-and-inertial-measurements-lisong-c-sun-et-al-2024>(89/89 | 89/273) MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements (Lisong C. Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu. (2024)<br><strong>MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements</strong><br><button class=copy-to-clipboard title="MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00923v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00923v1.pdf filename=2404.00923v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a <b>multi-modal</b> dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map. Project Webpage: <a href=https://vita-group.github.io/MM3DGS-SLAM>https://vita-group.github.io/MM3DGS-SLAM</a></p></p class="citation"></blockquote><h2 id=cscl-57>cs.CL (57)</h2><h3 id=157--90273-unveiling-divergent-inductive-biases-of-llms-on-temporal-data-sindhu-kishore-et-al-2024>(1/57 | 90/273) Unveiling Divergent Inductive Biases of LLMs on Temporal Data (Sindhu Kishore et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sindhu Kishore, Hangfeng He. (2024)<br><strong>Unveiling Divergent Inductive Biases of LLMs on Temporal Data</strong><br><button class=copy-to-clipboard title="Unveiling Divergent Inductive Biases of LLMs on Temporal Data" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, Question Answering, Question Answering, Textual Entailment, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01453v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01453v1.pdf filename=2404.01453v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Unraveling the intricate details of events in natural language necessitates a subtle understanding of temporal dynamics. Despite the adeptness of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in discerning patterns and relationships from data, their inherent comprehension of temporal dynamics remains a formidable challenge. This research meticulously explores these intrinsic challenges within <b>LLMs,</b> with a specific emphasis on evaluating the performance of <b>GPT-3.5</b> and <b>GPT-4</b> models in the analysis of temporal data. Employing two distinct <b>prompt</b> types, namely <b>Question</b> <b>Answering</b> <b>(QA)</b> format and <b>Textual</b> <b>Entailment</b> (TE) format, our analysis probes into both implicit and explicit events. The findings underscore noteworthy trends, revealing disparities in the performance of <b>GPT-3.5</b> and <b>GPT-4.</b> Notably, biases toward specific temporal relationships come to light, with <b>GPT-3.5</b> demonstrating a preference for &ldquo;AFTER&rsquo;&rsquo; in the <b>QA</b> format for both implicit and explicit events, while <b>GPT-4</b> leans towards &ldquo;BEFORE&rsquo;&rsquo;. Furthermore, a consistent pattern surfaces wherein <b>GPT-3.5</b> tends towards &ldquo;TRUE&rsquo;&rsquo;, and <b>GPT-4</b> exhibits a preference for &ldquo;FALSE&rsquo;&rsquo; in the TE format for both implicit and explicit events. This persistent discrepancy between <b>GPT-3.5</b> and <b>GPT-4</b> in handling temporal data highlights the intricate nature of inductive bias in <b>LLMs,</b> suggesting that the evolution of these models may not merely mitigate bias but may introduce new layers of complexity.</p></p class="citation"></blockquote><h3 id=257--91273-fables-evaluating-faithfulness-and-content-selection-in-book-length-summarization-yekyung-kim-et-al-2024>(2/57 | 91/273) FABLES: Evaluating faithfulness and content selection in book-length summarization (Yekyung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun Manjunatha, Kyle Lo, Tanya Goyal, Mohit Iyyer. (2024)<br><strong>FABLES: Evaluating faithfulness and content selection in book-length summarization</strong><br><button class=copy-to-clipboard title="FABLES: Evaluating faithfulness and content selection in book-length summarization" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Claude, GPT, GPT-3, GPT-3.5, Reasoning, Large Language Model, Large Language Model, Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01261v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01261v1.pdf filename=2404.01261v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While long-context <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can technically <b>summarize</b> book-length documents (>100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first <b>large-scale</b> <b>human</b> <b>evaluation</b> of faithfulness and content selection on <b>LLM-generated</b> summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in <b>LLM-generated</b> summaries of 26 books, at a cost of $5.2K USD, which allows us to rank <b>LLM</b> summarizers based on faithfulness: <b>Claude-3-Opus</b> significantly outperforms all closed-source <b>LLMs,</b> while the open-source Mixtral is on par with <b>GPT-3.5-Turbo.</b> An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect <b>reasoning</b> over the narrative to invalidate. While <b>LLM-based</b> auto-raters have proven reliable for factuality and coherence in other settings, we implement several <b>LLM</b> raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims. Our experiments suggest that detecting unfaithful claims is an important future direction not only for <b>summarization</b> evaluation but also as a testbed for long-context understanding. Finally, we move beyond faithfulness by exploring content selection errors in book-length <b>summarization:</b> we develop a typology of omission errors related to crucial narrative elements and also identify a systematic over-emphasis on events occurring towards the end of the book.</p></p class="citation"></blockquote><h3 id=357--92273-aragog-advanced-rag-output-grading-matouš-eibich-et-al-2024>(3/57 | 92/273) ARAGOG: Advanced RAG Output Grading (Matouš Eibich et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matouš Eibich, Shivay Nagpal, Alexander Fred-Ojala. (2024)<br><strong>ARAGOG: Advanced RAG Output Grading</strong><br><button class=copy-to-clipboard title="ARAGOG: Advanced RAG Output Grading" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-CL, cs-IR, cs.CL<br>Keyword Score: 80<br>Keywords: Rerank, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Cohere, Document Embedding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01037v1.pdf filename=2404.01037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> is essential for integrating external knowledge into <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> outputs. While the literature on <b>RAG</b> is growing, it primarily focuses on systematic reviews and comparisons of new state-of-the-art (SoTA) techniques against their predecessors, with a gap in extensive experimental comparisons. This study begins to address this gap by assessing various <b>RAG</b> methods&rsquo; impacts on <b>retrieval</b> <b>precision</b> <b>and</b> answer similarity. We found that Hypothetical <b>Document</b> <b>Embedding</b> (HyDE) and <b>LLM</b> <b>reranking</b> significantly enhance <b>retrieval</b> <b>precision.</b> <b>However,</b> Maximal Marginal Relevance (MMR) and <b>Cohere</b> <b>rerank</b> did not exhibit notable advantages over a baseline Naive <b>RAG</b> system, and Multi-query approaches underperformed. Sentence Window <b>Retrieval</b> <b>emerged</b> <b>as</b> the most effective for <b>retrieval</b> <b>precision,</b> <b>despite</b> its variable performance on answer similarity. The study confirms the potential of the <b>Document</b> <b>Summary</b> Index as a competent <b>retrieval</b> <b>approach.</b> <b>All</b> resources related to this research are publicly accessible for further investigation through our GitHub repository ARAGOG (<a href=https://github.com/predlico/ARAGOG)>https://github.com/predlico/ARAGOG)</a>. We welcome the community to further this exploratory study in <b>RAG</b> systems.</p></p class="citation"></blockquote><h3 id=457--93273-structured-information-matters-incorporating-abstract-meaning-representation-into-llms-for-improved-open-domain-dialogue-evaluation-bohao-yang-et-al-2024>(4/57 | 93/273) Structured Information Matters: Incorporating Abstract Meaning Representation into LLMs for Improved Open-Domain Dialogue Evaluation (Bohao Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bohao Yang, Kun Zhao, Chen Tang, Liang Zhan, Chenghua Lin. (2024)<br><strong>Structured Information Matters: Incorporating Abstract Meaning Representation into LLMs for Improved Open-Domain Dialogue Evaluation</strong><br><button class=copy-to-clipboard title="Structured Information Matters: Incorporating Abstract Meaning Representation into LLMs for Improved Open-Domain Dialogue Evaluation" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 78<br>Keywords: Graph Attention Networks, Graph, Representation Learning, Open-Domain Dialogue, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01129v1.pdf filename=2404.01129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic <b>open-domain</b> <b>dialogue</b> evaluation has attracted increasing attention. Trainable evaluation metrics are commonly trained with true positive and randomly selected negative responses, resulting in a tendency for them to assign a higher score to the responses that share higher content similarity with a given context. However, adversarial negative responses possess high content similarity with the contexts whilst being semantically different. Therefore, existing evaluation metrics are not robust enough to evaluate such responses, resulting in low correlations with human judgments. While recent studies have shown some efficacy in utilizing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for <b>open-domain</b> <b>dialogue</b> evaluation, they still encounter challenges in effectively handling adversarial negative examples. In this paper, we propose a simple yet effective framework for <b>open-domain</b> <b>dialogue</b> evaluation, which combines domain-specific language models (SLMs) with <b>LLMs.</b> The SLMs can explicitly incorporate Abstract Meaning <b>Representation</b> <b>(AMR)</b> <b>graph</b> information of the dialogue through a <b>gating</b> mechanism for enhanced semantic <b>representation</b> <b>learning.</b> The evaluation result of SLMs and AMR <b>graph</b> information are plugged into the <b>prompt</b> of <b>LLM,</b> for the enhanced <b>in-context</b> <b>learning</b> performance. Experimental results on <b>open-domain</b> <b>dialogue</b> evaluation tasks demonstrate the superiority of our method compared to a wide range of state-of-the-art baselines, especially in discriminating adversarial negative responses. Our code is available at <a href=https://github.com/Bernard-Yang/SIMAMR>https://github.com/Bernard-Yang/SIMAMR</a>.</p></p class="citation"></blockquote><h3 id=557--94273-self-demos-eliciting-out-of-demonstration-generalizability-in-large-language-models-wei-he-et-al-2024>(5/57 | 94/273) Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models (Wei He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei He, Shichun Liu, Jun Zhao, Yiwen Ding, Yi Lu, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang. (2024)<br><strong>Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models</strong><br><button class=copy-to-clipboard title="Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Few-shot, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00884v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00884v1.pdf filename=2404.00884v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown promising abilities of <b>in-context</b> <b>learning</b> <b>(ICL),</b> adapting swiftly to new tasks with only <b>few-shot</b> demonstrations. However, current <b>few-shot</b> methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel <b>prompting</b> method that elicits the inherent generalizability in <b>LLMs</b> by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query. Thorough experiments on our dataset and two public math <b>benchmarks</b> have shown that our method can outperform state-of-the-art baselines in the OOD setting. Moreover, we conduct a range of analyses to validate Self-Demos&rsquo;s generalization and provide more insights.</p></p class="citation"></blockquote><h3 id=657--95273-bailong-bilingual-transfer-learning-based-on-qlora-and-zip-tie-embedding-lung-chuan-chen-et-al-2024>(6/57 | 95/273) Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding (Lung-Chuan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lung-Chuan Chen, Zong-Ru Li. (2024)<br><strong>Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding</strong><br><button class=copy-to-clipboard title="Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Low-Resource, Transfer Learning, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00862v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00862v1.pdf filename=2404.00862v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated exceptional performance in various NLP applications. However, the majority of existing open-source <b>LLMs</b> are pre-trained primarily on English data and little part of other languages. This deficiency in multilingual training data results in suboptimal performance when applied to languages with fewer available resources. Furthermore, enhancing the performance of <b>LLMs</b> on <b>low-resource</b> languages by full-parameter <b>fine-tuning</b> with additional data requires substantial computational resources, posing computational barriers for research organizations and individual researchers. Consequently, several techniques such as parameter-efficient tuning and advanced embedding initialization have been proposed to address these challenges. In this work, we combine them to facilitate cross-lingual <b>transfer</b> <b>on</b> English-dominated open-source <b>LLM.</b> To effectively enhance the model&rsquo;s proficiency in Traditional Chinese, we conduct secondary pre-training on <b>Llama</b> 2 7B with Traditional Chinese data by leveraging QLoRA and our proposed zip-tie embedding initialization. The resulting model called Bailong, which stands for Bilingual <b>trAnsfer</b> <b>learnIng</b> based on qLOra and zip-tie embeddiNG. We present Bailong-instruct 7B, a <b>fine-tuned</b> version of Bailong 7B optimized for multi-turn dialogue scenarios. Recognizing the inadequacy of <b>benchmark</b> datasets in Traditional Chinese, we further introduce Bailong-bench to assess the alignment of models with human preferences and the capability to follow instructions in both Traditional Chinese and English tasks. In our evaluation, Bailong-instruct 7B exhibits competitive performance on Bailong-bench and other <b>benchmark</b> datasets when compared to other open-source models of similar or even larger parameter sizes. Bailong-instruct 7B and Bailong-bench are publicly available with the aim of empowering the community to build upon our efforts.</p></p class="citation"></blockquote><h3 id=757--96273-aadam-at-semeval-2024-task-1-augmentation-and-adaptation-for-multilingual-semantic-textual-relatedness-miaoran-zhang-et-al-2024>(7/57 | 96/273) AAdaM at SemEval-2024 Task 1: Augmentation and Adaptation for Multilingual Semantic Textual Relatedness (Miaoran Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miaoran Zhang, Mingyang Wang, Jesujoba O. Alabi, Dietrich Klakow. (2024)<br><strong>AAdaM at SemEval-2024 Task 1: Augmentation and Adaptation for Multilingual Semantic Textual Relatedness</strong><br><button class=copy-to-clipboard title="AAdaM at SemEval-2024 Task 1: Augmentation and Adaptation for Multilingual Semantic Textual Relatedness" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Data Augmentation, Fine-tuning, Low-Resource, Supervised Learning, Supervised Learning, Zero-shot, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01490v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01490v1.pdf filename=2404.01490v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents our system developed for the SemEval-2024 Task 1: Semantic Textual Relatedness for African and Asian Languages. The shared task aims at measuring the semantic textual relatedness between pairs of sentences, with a focus on a range of under-represented languages. In this work, we propose using <b>machine</b> <b>translation</b> for <b>data</b> <b>augmentation</b> to address the <b>low-resource</b> challenge of limited training <b>data.</b> <b>Moreover,</b> we apply task-adaptive pre-training on unlabeled task <b>data</b> <b>to</b> bridge the gap between pre-training and task adaptation. For model training, we investigate both full <b>fine-tuning</b> and adapter-based tuning, and adopt the adapter framework for effective <b>zero-shot</b> cross-lingual transfer. We achieve competitive results in the shared task: our system performs the best among all ranked teams in both subtask A <b>(supervised</b> <b>learning)</b> and subtask C (cross-lingual transfer).</p></p class="citation"></blockquote><h3 id=857--97273-advancing-ai-with-integrity-ethical-challenges-and-solutions-in-neural-machine-translation-richard-kimera-et-al-2024>(8/57 | 97/273) Advancing AI with Integrity: Ethical Challenges and Solutions in Neural Machine Translation (Richard Kimera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Richard Kimera, Yun-Seon Kim, Heeyoul Choi. (2024)<br><strong>Advancing AI with Integrity: Ethical Challenges and Solutions in Neural Machine Translation</strong><br><button class=copy-to-clipboard title="Advancing AI with Integrity: Ethical Challenges and Solutions in Neural Machine Translation" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fairness, Fine-tuning, BERT, Transformer, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01070v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01070v1.pdf filename=2404.01070v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the ethical challenges of Artificial Intelligence in <b>Neural</b> <b>Machine</b> <b>Translation</b> <b>(NMT)</b> systems, emphasizing the imperative for developers to ensure <b>fairness</b> and cultural sensitivity. We investigate the ethical competence of AI models in <b>NMT,</b> examining the Ethical considerations at each stage of <b>NMT</b> development, including data handling, privacy, data ownership, and consent. We identify and address ethical issues through empirical studies. These include employing <b>Transformer</b> models for Luganda-English translations and enhancing efficiency with sentence mini-batching. And complementary studies that refine data labeling techniques and <b>fine-tune</b> <b>BERT</b> and Longformer models for analyzing Luganda and English social media content. Our second approach is a literature review from databases such as Google Scholar and platforms like GitHub. Additionally, the paper probes the distribution of responsibility between AI systems and humans, underscoring the essential role of human oversight in upholding <b>NMT</b> ethical standards. Incorporating a biblical perspective, we discuss the societal impact of <b>NMT</b> and the broader ethical responsibilities of developers, positing them as stewards accountable for the societal repercussions of their creations.</p></p class="citation"></blockquote><h3 id=957--98273-llm-radjudge-achieving-radiologist-level-evaluation-for-x-ray-report-generation-zilong-wang-et-al-2024>(9/57 | 98/273) LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation (Zilong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zilong Wang, Xufang Luo, Xinyang Jiang, Dongsheng Li, Lili Qiu. (2024)<br><strong>LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation</strong><br><button class=copy-to-clipboard title="LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00998v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00998v1.pdf filename=2404.00998v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task&rsquo;s clinical requirements. This study proposes a novel evaluation framework using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to compare radiology reports for assessment. We compare the performance of various <b>LLMs</b> and demonstrate that, when using <b>GPT-4,</b> our proposed metric achieves evaluation consistency close to that of radiologists. Furthermore, to reduce costs and improve accessibility, making this method practical, we construct a dataset using <b>LLM</b> evaluation results and perform <b>knowledge</b> <b>distillation</b> to train a smaller model. The <b>distilled</b> model achieves evaluation capabilities comparable to <b>GPT-4.</b> Our framework and <b>distilled</b> model offer an accessible and efficient evaluation method for radiology report generation, facilitating the development of more clinically relevant models. The model will be further open-sourced and accessible.</p></p class="citation"></blockquote><h3 id=1057--99273-chatglm-rlhf-practices-of-aligning-large-language-models-with-human-feedback-zhenyu-hou-et-al-2024>(10/57 | 99/273) ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback (Zhenyu Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenyu Hou, Yilin Niu, Zhengxiao Du, Xiaohan Zhang, Xiao Liu, Aohan Zeng, Qinkai Zheng, Minlie Huang, Hongning Wang, Jie Tang, Yuxiao Dong. (2024)<br><strong>ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback</strong><br><button class=copy-to-clipboard title="ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00934v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00934v2.pdf filename=2404.00934v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>ChatGLM is a free-to-use AI service powered by the ChatGLM family of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> In this paper, we present the ChatGLM-RLHF pipeline &ndash; a <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF)</b> system &ndash; designed to enhance ChatGLM&rsquo;s alignment with human preferences. ChatGLM-RLHF encompasses three major components: the collection of human preference data, the training of the reward model, and the optimization of policies. Throughout the process of integrating ChatGLM-RLHF into production, we encountered and addressed several unprecedented challenges. We introduce the strategies to mitigate reward variance for stabilized <b>large-scale</b> <b>training,</b> <b>implement</b> model parallelism with fused gradient-descent, and design regularization constraints to avoid catastrophic forgetting in <b>LLMs.</b> Experiments show that ChatGLM-RLHF brings significant improvements in alignment tasks compared to the <b>supervised</b> <b>fine-tuned</b> (SFT) version of ChatGLM. For instance, it achieves on average 15% more wins against ChatGLM-SFT in Chinese alignment tasks. The work presents our practices of aligning <b>LLMs</b> with human preferences, offering insights into the challenges and solutions in <b>RLHF</b> implementations.</p></p class="citation"></blockquote><h3 id=1157--100273-token-efficient-leverage-learning-in-large-language-models-yuanhao-zeng-et-al-2024>(11/57 | 100/273) Token-Efficient Leverage Learning in Large Language Models (Yuanhao Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanhao Zeng, Min Wang, Yihang Wang, Yingxia Shao. (2024)<br><strong>Token-Efficient Leverage Learning in Large Language Models</strong><br><button class=copy-to-clipboard title="Token-Efficient Leverage Learning in Large Language Models" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, High-Resource, Low-Resource, Quantization, Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00914v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00914v1.pdf filename=2404.00914v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have excelled in various tasks but perform better in <b>high-resource</b> scenarios, which presents challenges in <b>low-resource</b> scenarios. Data scarcity and the inherent difficulty of adapting <b>LLMs</b> to specific tasks compound the challenge. To address the twin hurdles, we introduce \textbf{Leverage Learning}. We present a streamlined implement of this methodology called Token-Efficient Leverage Learning (TELL). TELL showcases the potential of Leverage Learning, demonstrating effectiveness across various <b>LLMs</b> and <b>low-resource</b> tasks, ranging from $10^4$ to $10^6$ tokens. It reduces task data requirements by up to nearly an order of magnitude compared to conventional <b>Supervised</b> <b>Fine-Tuning</b> (SFT) while delivering competitive performance. With the same amount of task data, TELL leads in improving task performance compared to SFT. We discuss the mechanism of Leverage Learning, suggesting it aligns with <b>quantization</b> hypothesis and explore its promising potential through empirical testing.</p></p class="citation"></blockquote><h3 id=1257--101273-set-aligning-framework-for-auto-regressive-event-temporal-graph-generation-xingwei-tan-et-al-2024>(12/57 | 101/273) Set-Aligning Framework for Auto-Regressive Event Temporal Graph Generation (Xingwei Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingwei Tan, Yuxiang Zhou, Gabriele Pergola, Yulan He. (2024)<br><strong>Set-Aligning Framework for Auto-Regressive Event Temporal Graph Generation</strong><br><button class=copy-to-clipboard title="Set-Aligning Framework for Auto-Regressive Event Temporal Graph Generation" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 63<br>Keywords: Graph, Data Augmentation, Zero-shot, Text Generation, Large Language Model, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01532v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01532v1.pdf filename=2404.01532v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Event temporal <b>graphs</b> have been shown as convenient and effective representations of complex temporal relations between events in <b>text.</b> <b>Recent</b> studies, which employ <b>pre-trained</b> <b>language</b> <b>models</b> to auto-regressively generate linearised <b>graphs</b> for constructing event temporal <b>graphs,</b> have shown promising results. However, these methods have often led to suboptimal <b>graph</b> generation as the linearised <b>graphs</b> exhibit set characteristics which are instead treated sequentially by language models. This discrepancy stems from the conventional <b>text</b> <b>generation</b> objectives, leading to erroneous penalisation of correct predictions caused by the misalignment of elements in target sequences. To address these challenges, we reframe the task as a conditional set generation problem, proposing a Set-aligning Framework tailored for the effective utilisation of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> The framework incorporates <b>data</b> <b>augmentations</b> and set-property regularisations designed to alleviate <b>text</b> <b>generation</b> loss penalties associated with the linearised <b>graph</b> edge sequences, thus encouraging the generation of more relation edges. Experimental results show that our framework surpasses existing baselines for event temporal <b>graph</b> generation. Furthermore, under <b>zero-shot</b> settings, the structural knowledge introduced through our framework notably improves model generalisation, particularly when the training examples available are limited.</p></p class="citation"></blockquote><h3 id=1357--102273-effectively-prompting-small-sized-language-models-for-cross-lingual-tasks-via-winning-tickets-mingqi-li-et-al-2024>(13/57 | 102/273) Effectively Prompting Small-sized Language Models for Cross-lingual Tasks via Winning Tickets (Mingqi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingqi Li, Feng Luo. (2024)<br><strong>Effectively Prompting Small-sized Language Models for Cross-lingual Tasks via Winning Tickets</strong><br><button class=copy-to-clipboard title="Effectively Prompting Small-sized Language Models for Cross-lingual Tasks via Winning Tickets" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Low-Resource, Masked Language Model, Pre-trained Language Model, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01242v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01242v1.pdf filename=2404.01242v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current soft <b>prompt</b> <b>methods</b> yield limited performance when applied to small-sized models (fewer than a billion parameters). Deep <b>prompt-tuning,</b> <b>which</b> entails prepending parameters in each layer for enhanced efficacy, presents a solution for <b>prompting</b> <b>small-sized</b> models, albeit requiring carefully designed implementation. In this paper, we introduce the Lottery Ticket <b>Prompt-learning</b> <b>(LTP)</b> framework that integrates winning tickets with soft <b>prompts.</b> <b>The</b> LTP offers a simpler implementation and requires only a one-time execution. We demonstrate LTP on cross-lingual tasks, where prior works rely on external tools like human-designed multilingual templates and bilingual dictionaries, which may not be feasible in a <b>low-resource</b> regime. Specifically, we select a subset of parameters that have been changed the most during the <b>fine-tuning</b> with the <b>Masked</b> <b>Language</b> <b>Modeling</b> objective. Then, we prepend soft <b>prompts</b> <b>to</b> the original <b>pre-trained</b> <b>language</b> <b>model</b> and only update the selected parameters together with <b>prompt-related</b> <b>parameters</b> when adapting to the downstream tasks. We verify the effectiveness of our LTP framework on cross-lingual tasks, specifically targeting <b>low-resource</b> languages. Our approach outperforms the baselines by only updating 20% of the original parameters.</p></p class="citation"></blockquote><h3 id=1457--103273-a-survey-on-multilingual-large-language-models-corpora-alignment-and-bias-yuemei-xu-et-al-2024>(14/57 | 103/273) A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias (Yuemei Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuemei Xu, Ling Hu, Jiayi Zhao, Zihan Qiu, Yuqi Ye, Hanwen Gu. (2024)<br><strong>A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias</strong><br><button class=copy-to-clipboard title="A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: High-Resource, Knowledge Transfer, Low-Resource, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00929v1.pdf filename=2404.00929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Based on the foundation of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> Multilingual <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) have been developed to address the challenges of multilingual natural language processing tasks, hoping to achieve <b>knowledge</b> <b>transfer</b> from <b>high-resource</b> to <b>low-resource</b> languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolution, key techniques, and multilingual capacities. Secondly, we explore widely utilized multilingual corpora for MLLMs&rsquo; training and multilingual datasets oriented for downstream tasks that are crucial for enhancing the cross-lingual capability of MLLMs. Thirdly, we survey the existing studies on multilingual representations and investigate whether the current MLLMs can learn a universal language representation. Fourthly, we discuss bias on MLLMs including its category and evaluation metrics, and <b>summarize</b> the existing debiasing techniques. Finally, we discuss existing challenges and point out promising research directions. By demonstrating these aspects, this paper aims to facilitate a deeper understanding of MLLMs and their potentiality in various domains.</p></p class="citation"></blockquote><h3 id=1557--104273-aispace-at-semeval-2024-task-8-a-class-balanced-soft-voting-system-for-detecting-multi-generator-machine-generated-text-renhua-gu-et-al-2024>(15/57 | 104/273) AISPACE at SemEval-2024 task 8: A Class-balanced Soft-voting System for Detecting Multi-generator Machine-generated Text (Renhua Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renhua Gu, Xiangfeng Meng. (2024)<br><strong>AISPACE at SemEval-2024 task 8: A Class-balanced Soft-voting System for Detecting Multi-generator Machine-generated Text</strong><br><button class=copy-to-clipboard title="AISPACE at SemEval-2024 task 8: A Class-balanced Soft-voting System for Detecting Multi-generator Machine-generated Text" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Transformer, Text Classification, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00950v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00950v1.pdf filename=2404.00950v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>SemEval-2024 Task 8 provides a challenge to detect human-written and machine-generated <b>text.</b> <b>There</b> are 3 subtasks for different detection scenarios. This paper proposes a system that mainly deals with Subtask B. It aims to detect if given full <b>text</b> <b>is</b> written by human or is generated by a specific <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM),</b> which is actually a multi-class <b>text</b> <b>classification</b> task. Our team AISPACE conducted a systematic study of <b>fine-tuning</b> <b>transformer-based</b> models, including encoderonly, decoder-only and encoder-decoder models. We compared their performance on this task and identified that encoder-only models performed exceptionally well. We also applied a weighted Cross Entropy loss function to address the issue of data imbalance of different class samples. Additionally, we employed softvoting strategy over multi-models ensemble to enhance the reliability of our predictions. Our system ranked top 1 in Subtask B, which sets a state-of-the-art <b>benchmark</b> for this new challenge.</p></p class="citation"></blockquote><h3 id=1657--105273-position-aware-parameter-efficient-fine-tuning-approach-for-reducing-positional-bias-in-llms-zheng-zhang-et-al-2024>(16/57 | 105/273) Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs (Zheng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheng Zhang, Fan Yang, Ziyan Jiang, Zheng Chen, Zhengyang Zhao, Chengyuan Ma, Liang Zhao, Yang Liu. (2024)<br><strong>Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs</strong><br><button class=copy-to-clipboard title="Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Data Augmentation, Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01430v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01430v1.pdf filename=2404.01430v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have enhanced their ability to process long input contexts. This development is particularly crucial for tasks that involve retrieving knowledge from an external datastore, which can result in long inputs. However, recent studies show a positional bias in <b>LLMs,</b> demonstrating varying performance depending on the location of useful information within the input sequence. In this study, we conduct extensive experiments to investigate the root causes of positional bias. Our findings indicate that the primary contributor to <b>LLM</b> positional bias stems from the inherent positional preferences of different models. We demonstrate that merely employing <b>prompt-based</b> solutions is inadequate for overcoming the positional preferences. To address this positional bias issue of a pre-trained <b>LLM,</b> we developed a Position-Aware Parameter Efficient <b>Fine-Tuning</b> (PAPEFT) approach which is composed of a <b>data</b> <b>augmentation</b> technique and a parameter efficient adapter, enhancing a uniform attention distribution across the input context. Our experiments demonstrate that the proposed approach effectively reduces positional bias, improving <b>LLMs&rsquo;</b> effectiveness in handling long context sequences for various tasks that require externally retrieved knowledge.</p></p class="citation"></blockquote><h3 id=1757--106273-generating-faithful-and-complete-hospital-course-summaries-from-the-electronic-health-record-griffin-adams-2024>(17/57 | 106/273) Generating Faithful and Complete Hospital-Course Summaries from the Electronic Health Record (Griffin Adams, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Griffin Adams. (2024)<br><strong>Generating Faithful and Complete Hospital-Course Summaries from the Electronic Health Record</strong><br><button class=copy-to-clipboard title="Generating Faithful and Complete Hospital-Course Summaries from the Electronic Health Record" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Mistral, Domain Adaptation, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01189v1.pdf filename=2404.01189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid adoption of Electronic Health Records (EHRs) has been instrumental in streamlining administrative tasks, increasing transparency, and enabling continuity of care across providers. An unintended consequence of the increased documentation burden, however, has been reduced face-time with patients and, concomitantly, a dramatic rise in clinician burnout. In this thesis, we pinpoint a particularly time-intensive, yet critical, documentation task: generating a summary of a patient&rsquo;s hospital admissions, and propose and evaluate automated solutions. In Chapter 2, we construct a dataset based on 109,000 hospitalizations (2M source notes) and perform exploratory analyses to motivate future work on modeling and evaluation [NAACL 2021]. In Chapter 3, we address faithfulness from a modeling perspective by revising noisy references [EMNLP 2022] and, to reduce the reliance on references, directly calibrating model outputs to metrics [ACL 2023]. These works relied heavily on automatic metrics as human annotations were limited. To fill this gap, in Chapter 4, we conduct a fine-grained expert annotation of system errors in order to meta-evaluate existing metrics and better understand task-specific issues of <b>domain</b> <b>adaptation</b> and source-summary alignments. To learn a metric less correlated to extractiveness (copy-and-paste), we derive noisy faithfulness labels from an ensemble of existing metrics and train a faithfulness classifier on these pseudo labels [MLHC 2023]. Finally, in Chapter 5, we demonstrate that <b>fine-tuned</b> <b>LLMs</b> <b>(Mistral</b> and Zephyr) are highly prone to entity hallucinations and cover fewer salient entities. We improve both coverage and faithfulness by performing sentence-level entity planning based on a set of pre-computed salient entities from the source text, which extends our work on entity-guided news <b>summarization</b> [ACL, 2023], [EMNLP, 2023].</p></p class="citation"></blockquote><h3 id=1857--107273-senticse-a-sentiment-aware-contrastive-sentence-embedding-framework-with-sentiment-guided-textual-similarity-jaemin-kim-et-al-2024>(18/57 | 107/273) SentiCSE: A Sentiment-aware Contrastive Sentence Embedding Framework with Sentiment-guided Textual Similarity (Jaemin Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaemin Kim, Yohan Na, Kangmin Kim, Sang Rak Lee, Dong-Kyu Chae. (2024)<br><strong>SentiCSE: A Sentiment-aware Contrastive Sentence Embedding Framework with Sentiment-guided Textual Similarity</strong><br><button class=copy-to-clipboard title="SentiCSE: A Sentiment-aware Contrastive Sentence Embedding Framework with Sentiment-guided Textual Similarity" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T50, I-2-7, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Sentence Embedding, Sentiment Analysis, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01104v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01104v1.pdf filename=2404.01104v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>sentiment-aware</b> <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> demonstrate impressive results in downstream <b>sentiment</b> <b>analysis</b> tasks. However, they neglect to evaluate the quality of their constructed <b>sentiment</b> <b>representations;</b> they just focus on improving the <b>fine-tuning</b> performance, which overshadows the representation quality. We argue that without guaranteeing the representation quality, their downstream performance can be highly dependent on the supervision of the <b>fine-tuning</b> data rather than representation quality. This problem would make them difficult to foray into other <b>sentiment-related</b> <b>domains,</b> especially where labeled data is scarce. We first propose <b>Sentiment-guided</b> <b>Textual</b> Similarity (SgTS), a novel metric for evaluating the quality of <b>sentiment</b> <b>representations,</b> which is designed based on the degree of equivalence in <b>sentiment</b> <b>polarity</b> between two <b>sentences.</b> <b>We</b> then propose SentiCSE, a novel <b>Sentiment-aware</b> <b>Contrastive</b> <b>Sentence</b> <b>Embedding</b> framework for constructing <b>sentiment</b> <b>representations</b> via combined word-level and <b>sentence-level</b> <b>objectives,</b> whose quality is guaranteed by SgTS. Qualitative and quantitative comparison with the previous <b>sentiment-aware</b> <b>PLMs</b> shows the superiority of our work. Our code is available at: <a href=https://github.com/nayohan/SentiCSE>https://github.com/nayohan/SentiCSE</a></p></p class="citation"></blockquote><h3 id=1957--108273-bert-enhanced-retrieval-tool-for-homework-plagiarism-detection-system-jiarong-xian-et-al-2024>(19/57 | 108/273) BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System (Jiarong Xian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiarong Xian, Jibao Yuan, Peiwei Zheng, Dexian Chen. (2024)<br><strong>BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System</strong><br><button class=copy-to-clipboard title="BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 50<br>Keywords: BERT, GPT, GPT-3, GPT-3.5, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01582v1.pdf filename=2404.01582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text</b> <b>plagiarism</b> <b>detection</b> task is a common natural language processing task that aims to detect whether a given <b>text</b> <b>contains</b> <b>plagiarism</b> or copying from other <b>texts.</b> <b>In</b> <b>existing</b> research, detection of high level plagiarism is still a challenge due to the lack of high quality datasets. In this paper, we propose a plagiarized <b>text</b> <b>data</b> <b>generation</b> method based on <b>GPT-3.5,</b> which produces 32,927 pairs of <b>text</b> <b>plagiarism</b> <b>detection</b> datasets covering a wide range of plagiarism methods, bridging the gap in this part of research. Meanwhile, we propose a plagiarism identification method based on Faiss with <b>BERT</b> with high efficiency and high accuracy. Our experiments show that the performance of this model outperforms other models in several metrics, including 98.86%, 98.90%, 98.86%, and 0.9888 for Accuracy, Precision, Recall, and F1 Score, respectively. At the end, we also provide a user-friendly demo platform that allows users to upload a <b>text</b> <b>library</b> <b>and</b> intuitively participate in the plagiarism analysis.</p></p class="citation"></blockquote><h3 id=2057--109273-efficient-prompting-methods-for-large-language-models-a-survey-kaiyan-chang-et-al-2024>(20/57 | 109/273) Efficient Prompting Methods for Large Language Models: A Survey (Kaiyan Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiyan Chang, Songcheng Xu, Chenglong Wang, Yingfeng Luo, Tong Xiao, Jingbo Zhu. (2024)<br><strong>Efficient Prompting Methods for Large Language Models: A Survey</strong><br><button class=copy-to-clipboard title="Efficient Prompting Methods for Large Language Models: A Survey" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01077v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01077v1.pdf filename=2404.01077v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompting</b> has become a mainstream paradigm for adapting <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to specific natural language processing tasks. While this approach opens the door to <b>in-context</b> <b>learning</b> of <b>LLMs,</b> it brings the additional computational burden of model inference and human effort of manual-designed <b>prompts,</b> particularly when using lengthy and complex <b>prompts</b> to guide and control the behavior of <b>LLMs.</b> As a result, the <b>LLM</b> field has seen a remarkable surge in efficient <b>prompting</b> methods. In this paper, we present a comprehensive overview of these methods. At a high level, efficient <b>prompting</b> methods can broadly be categorized into two approaches: <b>prompting</b> with efficient computation and <b>prompting</b> with efficient design. The former involves various ways of compressing <b>prompts,</b> and the latter employs techniques for automatic <b>prompt</b> optimization. We present the basic concepts of <b>prompting,</b> review the advances for efficient <b>prompting,</b> and highlight future research directions.</p></p class="citation"></blockquote><h3 id=2157--110273-source-aware-training-enables-knowledge-attribution-in-language-models-muhammad-khalifa-et-al-2024>(21/57 | 110/273) Source-Aware Training Enables Knowledge Attribution in Language Models (Muhammad Khalifa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Khalifa, David Wadden, Emma Strubell, Honglak Lee, Lu Wang, Iz Beltagy, Hao Peng. (2024)<br><strong>Source-Aware Training Enables Knowledge Attribution in Language Models</strong><br><button class=copy-to-clipboard title="Source-Aware Training Enables Knowledge Attribution in Language Models" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Data Augmentation, Instruction Tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01019v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01019v1.pdf filename=2404.01019v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> learn a vast amount of knowledge during pretraining, but they are often oblivious to the source(s) of such knowledge. We investigate the problem of intrinsic source citation, where <b>LLMs</b> are required to cite the pretraining source supporting a generated response. Intrinsic source citation can enhance <b>LLM</b> transparency, interpretability, and verifiability. To give <b>LLMs</b> such ability, we explore source-aware training &ndash; a post pretraining recipe that involves (i) training the <b>LLM</b> to associate unique source document identifiers with the knowledge in each document, followed by (ii) an <b>instruction-tuning</b> <b>to</b> teach the <b>LLM</b> to cite a supporting pretraining source when <b>prompted.</b> Source-aware training can easily be applied to pretrained <b>LLMs</b> off the shelf, and diverges minimally from existing pretraining/fine-tuning frameworks. Through experiments on carefully curated <b>data,</b> <b>we</b> demonstrate that our training recipe can enable faithful attribution to the pretraining <b>data</b> <b>without</b> a substantial impact on the model&rsquo;s quality compared to standard pretraining. Our results also highlight the importance of <b>data</b> <b>augmentation</b> in achieving attribution.</p></p class="citation"></blockquote><h3 id=2257--111273-developing-safe-and-responsible-large-language-models----a-comprehensive-framework-shaina-raza-et-al-2024>(22/57 | 111/273) Developing Safe and Responsible Large Language Models &ndash; A Comprehensive Framework (Shaina Raza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaina Raza, Oluwanifemi Bamgbose, Shardul Ghuge, Fatemeh Tavakoli, Deepak John Reji. (2024)<br><strong>Developing Safe and Responsible Large Language Models &ndash; A Comprehensive Framework</strong><br><button class=copy-to-clipboard title="Developing Safe and Responsible Large Language Models -- A Comprehensive Framework" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Language Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01399v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01399v1.pdf filename=2404.01399v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the growing concerns around the safety and risks of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> it is essential to develop methods for mitigating these issues. We introduce Safe and Responsible <b>Large</b> <b>Language</b> <b>Model</b> (SR$<em>{\text{LLM}}$) , a model designed to enhance the safety of <b>language</b> <b>generation</b> using <b>LLMs.</b> Our approach incorporates a comprehensive <b>LLM</b> safety risk taxonomy and utilizes a dataset annotated by experts that align with this taxonomy. SR$</em>{\text{LLM}}$ is designed to identify potentially unsafe content and produce benign variations. It employs instruction-based and parameter-efficient <b>fine-tuning</b> methods, making the model not only effective in enhancing safety but also resource-efficient and straightforward to adjust. Through our testing on five <b>benchmark</b> datasets and two proprietary datasets, we observed notable reductions in the generation of unsafe content. Moreover, following the implementation of safety measures, there was a significant improvement in the production of safe content. We detail our <b>fine-tuning</b> processes and how we <b>benchmark</b> safety for SR$_{\text{LLM}}$ with the community engagement and promote the responsible advancement of <b>LLMs.</b> All the data and code are available anonymous at <a href=https://github.com/shainarazavi/Safe-Responsible-LLM>https://github.com/shainarazavi/Safe-Responsible-LLM</a> .</p></p class="citation"></blockquote><h3 id=2357--112273-stable-code-technical-report-nikhil-pinnaparaju-et-al-2024>(23/57 | 112/273) Stable Code Technical Report (Nikhil Pinnaparaju et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung, Jonathan Tow, James Baicoianu, Ashish Datta, Maksym Zhuravinskyi, Dakota Mahan, Marco Bellagente, Carlos Riquelme, Nathan Cooper. (2024)<br><strong>Stable Code Technical Report</strong><br><button class=copy-to-clipboard title="Stable Code Technical Report" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Quantization, Neural Machine Translation, Question Answering, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01226v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01226v1.pdf filename=2404.01226v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Stable Code, the first in our new-generation of code language models series, which serves as a general-purpose base code language model targeting code completion, <b>reasoning,</b> math, and other software engineering-based tasks. Additionally, we introduce an instruction variant named Stable Code Instruct that allows conversing with the model in a natural chat interface for performing <b>question-answering</b> <b>and</b> instruction-based tasks. In this technical report, we detail the data and training procedure leading to both models. Their weights are available via Hugging Face for anyone to download and use at <a href=https://huggingface.co/stabilityai/stable-code-3b>https://huggingface.co/stabilityai/stable-code-3b</a> and <a href=https://huggingface.co/stabilityai/stable-code-instruct-3b>https://huggingface.co/stabilityai/stable-code-instruct-3b</a>. This report contains thorough evaluations of the models, including multilingual programming <b>benchmarks,</b> and the <b>MT</b> <b>benchmark</b> focusing on multi-turn dialogues. At the time of its release, Stable Code is the state-of-the-art open model under 3B parameters and even performs comparably to larger models of sizes 7 billion and 15 billion parameters on the popular Multi-PL <b>benchmark.</b> Stable Code Instruct also exhibits state-of-the-art performance on the <b>MT-Bench</b> coding tasks and on Multi-PL completion compared to other instruction tuned models. Given its appealing small size, we also provide throughput measurements on a number of edge devices. In addition, we open source several <b>quantized</b> checkpoints and provide their performance metrics compared to the original model.</p></p class="citation"></blockquote><h3 id=2457--113273-finding-replicable-human-evaluations-via-stable-ranking-probability-parker-riley-et-al-2024>(24/57 | 113/273) Finding Replicable Human Evaluations via Stable Ranking Probability (Parker Riley et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Parker Riley, Daniel Deutsch, George Foster, Viresh Ratnakar, Ali Dabirmoghaddam, Markus Freitag. (2024)<br><strong>Finding Replicable Human Evaluations via Stable Ranking Probability</strong><br><button class=copy-to-clipboard title="Finding Replicable Human Evaluations via Stable Ranking Probability" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Recommendation, Language Generation, Natural Language Generation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01474v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01474v1.pdf filename=2404.01474v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reliable human evaluation is critical to the development of successful <b>natural</b> <b>language</b> <b>generation</b> models, but achieving it is notoriously difficult. Stability is a crucial requirement when ranking systems by quality: consistent ranking of systems across repeated evaluations is not just desirable, but essential. Without it, there is no reliable foundation for hill-climbing or product launch decisions. In this paper, we use <b>machine</b> <b>translation</b> and its state-of-the-art human evaluation framework, MQM, as a case study to understand how to set up reliable human evaluations that yield stable conclusions. We investigate the optimal configurations for item allocation to raters, number of ratings per item, and score normalization. Our study on two <b>language</b> <b>pairs</b> provides concrete <b>recommendations</b> for designing replicable human evaluation studies. We also collect and release the largest publicly available dataset of multi-segment translations rated by multiple professional translators, consisting of nearly 140,000 segment annotations across two <b>language</b> <b>pairs.</b></p></p class="citation"></blockquote><h3 id=2557--114273-will-the-real-linda-please-stand-upto-large-language-models-examining-the-representativeness-heuristic-in-llms-pengda-wang-et-al-2024>(25/57 | 114/273) Will the Real Linda Please Stand up&mldr;to Large Language Models? Examining the Representativeness Heuristic in LLMs (Pengda Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengda Wang, Zilin Xiao, Hanjie Chen, Frederick L. Oswald. (2024)<br><strong>Will the Real Linda Please Stand up&mldr;to Large Language Models? Examining the Representativeness Heuristic in LLMs</strong><br><button class=copy-to-clipboard title="Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs.CL<br>Keyword Score: 40<br>Keywords: Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01461v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01461v1.pdf filename=2404.01461v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated remarkable proficiency in understanding text and generating human-like text, they may exhibit biases acquired from training data in doing so. Specifically, <b>LLMs</b> may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example versus considering broader facts or statistical evidence. This work investigates the impact of the representativeness heuristic on <b>LLM</b> <b>reasoning.</b> We created REHEAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four <b>LLMs</b> applied to REHEAT all exhibited representativeness heuristic biases. We further identify that the model&rsquo;s <b>reasoning</b> steps are often incorrectly based on a stereotype rather than the problem&rsquo;s description. Interestingly, the performance improves when adding a hint in the <b>prompt</b> to remind the model of using its knowledge. This suggests the uniqueness of the representativeness heuristic compared to traditional biases. It can occur even when <b>LLMs</b> possess the correct knowledge while failing in a cognitive trap. This highlights the importance of future research focusing on the representativeness heuristic in model <b>reasoning</b> and decision-making and on developing solutions to address it.</p></p class="citation"></blockquote><h3 id=2657--115273-ails-ntua-at-semeval-2024-task-6-efficient-model-tuning-for-hallucination-detection-and-analysis-natalia-griogoriadou-et-al-2024>(26/57 | 115/273) AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for hallucination detection and analysis (Natalia Griogoriadou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Natalia Griogoriadou, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou. (2024)<br><strong>AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for hallucination detection and analysis</strong><br><button class=copy-to-clipboard title="AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for hallucination detection and analysis" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Hallucination Detection, Natural Language Inference, Natural Language Inference<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01210v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01210v1.pdf filename=2404.01210v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present our team&rsquo;s submissions for SemEval-2024 Task-6 - SHROOM, a Shared-task on <b>Hallucinations</b> <b>and</b> Related Observable Overgeneration Mistakes. The participants were asked to perform binary classification to identify cases of fluent overgeneration <b>hallucinations.</b> <b>Our</b> experimentation included <b>fine-tuning</b> a pre-trained model on <b>hallucination</b> <b>detection</b> and a <b>Natural</b> <b>Language</b> <b>Inference</b> <b>(NLI)</b> model. The most successful strategy involved creating an ensemble of these models, resulting in accuracy rates of 77.8% and 79.9% on model-agnostic and model-aware datasets respectively, outperforming the organizers&rsquo; baseline and achieving notable results when contrasted with the top-performing results in the competition, which reported accuracies of 84.7% and 81.3% correspondingly.</p></p class="citation"></blockquote><h3 id=2757--116273-llm-attributor-interactive-visual-attribution-for-llm-generation-seongmin-lee-et-al-2024>(27/57 | 116/273) LLM Attributor: Interactive Visual Attribution for LLM Generation (Seongmin Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seongmin Lee, Zijie J. Wang, Aishwarya Chakravarthy, Alec Helbling, ShengYun Peng, Mansi Phute, Duen Horng Chau, Minsuk Kahng. (2024)<br><strong>LLM Attributor: Interactive Visual Attribution for LLM Generation</strong><br><button class=copy-to-clipboard title="LLM Attributor: Interactive Visual Attribution for LLM Generation" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01361v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01361v1.pdf filename=2404.01361v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown remarkable capability to generate convincing <b>text</b> <b>across</b> diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind <b>text</b> <b>generation.</b> We present <b>LLM</b> Attributor, a Python library that provides interactive visualizations for training data attribution of an <b>LLM&rsquo;s</b> <b>text</b> <b>generation.</b> Our library offers a new way to quickly attribute an <b>LLM&rsquo;s</b> <b>text</b> <b>generation</b> to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated <b>text</b> <b>with</b> user-provided <b>text.</b> <b>We</b> describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models <b>fine-tuned</b> with two different datasets: online articles about recent disasters and finance-related question-answer pairs. Thanks to <b>LLM</b> Attributor&rsquo;s broad support for computational notebooks, users can easily integrate it into their workflow to interactively visualize attributions of their models. For easier access and extensibility, we open-source <b>LLM</b> Attributor at <a href=https://github.com/poloclub/>https://github.com/poloclub/</a> <b>LLM-Attribution.</b> The video demo is available at <a href=https://youtu.be/mIG2MDQKQxM>https://youtu.be/mIG2MDQKQxM</a>.</p></p class="citation"></blockquote><h3 id=2857--117273-exploring-the-mystery-of-influential-data-for-mathematical-reasoning-xinzhe-ni-et-al-2024>(28/57 | 117/273) Exploring the Mystery of Influential Data for Mathematical Reasoning (Xinzhe Ni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinzhe Ni, Yeyun Gong, Zhibin Gou, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen. (2024)<br><strong>Exploring the Mystery of Influential Data for Mathematical Reasoning</strong><br><button class=copy-to-clipboard title="Exploring the Mystery of Influential Data for Mathematical Reasoning" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Mathematical Reasoning, Question Answering, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01067v1.pdf filename=2404.01067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Selecting influential data for <b>fine-tuning</b> on downstream tasks is a key factor for both performance and computation efficiency. Recent works have shown that training with only limited data can show a superior performance on general tasks. However, the feasibility on <b>mathematical</b> <b>reasoning</b> tasks has not been validated. To go further, there exist two open questions for <b>mathematical</b> <b>reasoning:</b> how to select influential data and what is an influential data composition. For the former one, we propose a Quality-aware Diverse Selection <b>(QaDS)</b> strategy adaptable for <b>mathematical</b> <b>reasoning.</b> A comparison with other selection strategies validates the superiority of <b>QaDS.</b> For the latter one, we first enlarge our setting and explore the influential data composition. We conduct a series of experiments and highlight: scaling up <b>reasoning</b> data, and training with general data selected by <b>QaDS</b> is helpful. Then, we define our optimal mixture as OpenMathMix, an influential data mixture with open-source data selected by <b>QaDS.</b> With OpenMathMix, we achieve a state-of-the-art 48.8% accuracy on MATH with 7B base model. Additionally, we showcase the use of <b>QaDS</b> in creating efficient <b>fine-tuning</b> mixtures with various selection ratios, and analyze the quality of a wide range of open-source datasets, which can perform as a reference for future works on <b>mathematical</b> <b>reasoning</b> tasks.</p></p class="citation"></blockquote><h3 id=2957--118273-exploring-the-nexus-of-large-language-models-and-legal-systems-a-short-survey-weicong-qin-et-al-2024>(29/57 | 118/273) Exploring the Nexus of Large Language Models and Legal Systems: A Short Survey (Weicong Qin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weicong Qin, Zhongxiang Sun. (2024)<br><strong>Exploring the Nexus of Large Language Models and Legal Systems: A Short Survey</strong><br><button class=copy-to-clipboard title="Exploring the Nexus of Large Language Models and Legal Systems: A Short Survey" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00990v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00990v1.pdf filename=2404.00990v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advancement of Artificial Intelligence (AI) and <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> there is a profound transformation occurring in the realm of natural language processing tasks within the legal domain. The capabilities of <b>LLMs</b> are increasingly demonstrating unique roles in the legal sector, bringing both distinctive benefits and various challenges. This survey delves into the synergy between <b>LLMs</b> and the legal system, such as their applications in tasks like legal text comprehension, case retrieval, and analysis. Furthermore, this survey highlights key challenges faced by <b>LLMs</b> in the legal domain, including bias, interpretability, and ethical considerations, as well as how researchers are addressing these issues. The survey showcases the latest advancements in <b>fine-tuned</b> legal <b>LLMs</b> tailored for various legal systems, along with legal datasets available for <b>fine-tuning</b> <b>LLMs</b> in various languages. Additionally, it proposes directions for future research and development.</p></p class="citation"></blockquote><h3 id=3057--119273-prior-constraints-based-reward-model-training-for-aligning-large-language-models-hang-zhou-et-al-2024>(30/57 | 119/273) Prior Constraints-based Reward Model Training for Aligning Large Language Models (Hang Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hang Zhou, Chenglong Wang, Yimin Hu, Tong Xiao, Chunliang Zhang, Jingbo Zhu. (2024)<br><strong>Prior Constraints-based Reward Model Training for Aligning Large Language Models</strong><br><button class=copy-to-clipboard title="Prior Constraints-based Reward Model Training for Aligning Large Language Models" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Direct Preference Optimization, Reinforcement Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00978v1.pdf filename=2404.00978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> with human feedback for aligning <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> trains a reward model typically using ranking loss with comparison pairs.However, the training procedure suffers from an inherent problem: the uncontrolled scaling of reward scores during <b>reinforcement</b> <b>learning</b> due to the lack of constraints while training the reward model.This paper proposes a Prior Constraints-based Reward Model (namely PCRM) training method to mitigate this problem. PCRM incorporates prior constraints, specifically, length ratio and cosine similarity between outputs of each comparison pair, during reward model training to regulate optimization magnitude and control score margins. We comprehensively evaluate PCRM by examining its rank correlation with human preferences and its effectiveness in aligning <b>LLMs</b> via RL. Experimental results demonstrate that PCRM significantly improves alignment performance by effectively constraining reward score scaling. As another bonus, our method is easily integrated into arbitrary rank-based alignment methods, such as <b>direct</b> <b>preference</b> <b>optimization,</b> and can yield consistent improvement.</p></p class="citation"></blockquote><h3 id=3157--120273-psydial-personality-based-synthetic-dialogue-generation-using-large-language-models-ji-eun-han-et-al-2024>(31/57 | 120/273) PSYDIAL: Personality-based Synthetic Dialogue Generation using Large Language Models (Ji-Eun Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ji-Eun Han, Jun-Seok Koh, Hyeon-Tae Seo, Du-Seong Chang, Kyung-Ah Sohn. (2024)<br><strong>PSYDIAL: Personality-based Synthetic Dialogue Generation using Large Language Models</strong><br><button class=copy-to-clipboard title="PSYDIAL: Personality-based Synthetic Dialogue Generation using Large Language Models" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Chatbot, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00930v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00930v1.pdf filename=2404.00930v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel end-to-end personality-based synthetic dialogue data generation pipeline, specifically designed to elicit responses from <b>large</b> <b>language</b> <b>models</b> via <b>prompting.</b> We design the <b>prompts</b> to generate more human-like dialogues considering real-world scenarios when users engage with <b>chatbots.</b> We introduce PSYDIAL, the first Korean dialogue dataset focused on personality-based dialogues, curated using our proposed pipeline. Notably, we focus on the Extraversion dimension of the Big Five personality model in our research. Experimental results indicate that while pre-trained models and those <b>fine-tuned</b> with a chit-chat dataset struggle to generate responses reflecting personality, models trained with PSYDIAL show significant improvements. The versatility of our pipeline extends beyond dialogue tasks, offering potential for other non-dialogue related applications. This research opens doors for more nuanced, personality-driven conversational AI in Korean and potentially other languages. Our code is publicly available at <a href=https://github.com/jiSilverH/psydial>https://github.com/jiSilverH/psydial</a>.</p></p class="citation"></blockquote><h3 id=3257--121273-a-study-on-scaling-up-multilingual-news-framing-analysis-syeda-sabrina-akter-et-al-2024>(32/57 | 121/273) A Study on Scaling Up Multilingual News Framing Analysis (Syeda Sabrina Akter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Syeda Sabrina Akter, Antonios Anastasopoulos. (2024)<br><strong>A Study on Scaling Up Multilingual News Framing Analysis</strong><br><button class=copy-to-clipboard title="A Study on Scaling Up Multilingual News Framing Analysis" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01481v1.pdf filename=2404.01481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Media framing is the study of strategically selecting and presenting specific aspects of political issues to shape public opinion. Despite its relevance to almost all societies around the world, research has been limited due to the lack of available datasets and other resources. This study explores the possibility of dataset creation through crowdsourcing, utilizing non-expert annotators to develop training corpora. We first extend framing analysis beyond English news to a multilingual context (12 typologically diverse languages) through automatic translation. We also present a novel <b>benchmark</b> in Bengali and Portuguese on the immigration and same-sex marriage domains. Additionally, we show that a system trained on our crowd-sourced dataset, combined with other existing ones, leads to a 5.32 percentage point increase from the baseline, showing that crowdsourcing is a viable option. Last, we study the performance of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for this task, finding that task-specific <b>fine-tuning</b> is a better approach than employing bigger non-specialized models.</p></p class="citation"></blockquote><h3 id=3357--122273-towards-safety-and-helpfulness-balanced-responses-via-controllable-large-language-models-yi-lin-tuan-et-al-2024>(33/57 | 122/273) Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models (Yi-Lin Tuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi-Lin Tuan, Xilun Chen, Eric Michael Smith, Louis Martin, Soumya Batra, Asli Celikyilmaz, William Yang Wang, Daniel M. Bikel. (2024)<br><strong>Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models</strong><br><button class=copy-to-clipboard title="Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01295v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01295v1.pdf filename=2404.01295v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> become easily accessible nowadays, the trade-off between safety and helpfulness can significantly impact user experience. A model that prioritizes safety will cause users to feel less engaged and assisted while prioritizing helpfulness will potentially cause harm. Possible harms include teaching people how to build a bomb, exposing youth to inappropriate content, and hurting users&rsquo; mental health. In this work, we propose to balance safety and helpfulness in diverse use cases by controlling both attributes in <b>LLM.</b> We explore training-free and <b>fine-tuning</b> methods that do not require extra human annotations and analyze the challenges of controlling safety and helpfulness in <b>LLMs.</b> Our experiments demonstrate that our method can rewind a learned model and unlock its controllability.</p></p class="citation"></blockquote><h3 id=3457--123273-large-language-models-are-capable-of-offering-cognitive-reappraisal-if-guided-hongli-zhan-et-al-2024>(34/57 | 123/273) Large Language Models are Capable of Offering Cognitive Reappraisal, if Guided (Hongli Zhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongli Zhan, Allen Zheng, Yoon Kyung Lee, Jina Suh, Junyi Jessy Li, Desmond C. Ong. (2024)<br><strong>Large Language Models are Capable of Offering Cognitive Reappraisal, if Guided</strong><br><button class=copy-to-clipboard title="Large Language Models are Capable of Offering Cognitive Reappraisal, if Guided" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01288v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01288v1.pdf filename=2404.01288v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have offered new opportunities for emotional support, and recent work has shown that they can produce empathic responses to people in distress. However, long-term mental well-being requires emotional self-regulation, where a one-time empathic response falls short. This work takes a first step by engaging with cognitive reappraisals, a strategy from psychology practitioners that uses language to targetedly change negative appraisals that an individual makes of the situation; such appraisals is known to sit at the root of human emotional experience. We hypothesize that psychologically grounded principles could enable such advanced psychology capabilities in <b>LLMs,</b> and design RESORT which consists of a series of reappraisal constitutions across multiple dimensions that can be used as <b>LLM</b> instructions. We conduct a first-of-its-kind expert evaluation (by clinical psychologists with M.S. or Ph.D. degrees) of an <b>LLM&rsquo;s</b> <b>zero-shot</b> ability to generate cognitive reappraisal responses to medium-length social media messages asking for support. This fine-grained evaluation showed that even <b>LLMs</b> at the 7B scale guided by RESORT are capable of generating empathic responses that can help users reappraise their situations.</p></p class="citation"></blockquote><h3 id=3557--124273-mapping-the-increasing-use-of-llms-in-scientific-papers-weixin-liang-et-al-2024>(35/57 | 124/273) Mapping the Increasing Use of LLMs in Scientific Papers (Weixin Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, Diyi Yang, Christopher Potts, Christopher D Manning, James Y. Zou. (2024)<br><strong>Mapping the Increasing Use of LLMs in Scientific Papers</strong><br><button class=copy-to-clipboard title="Mapping the Increasing Use of LLMs in Scientific Papers" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-DL, cs-LG, cs-SI, cs.CL<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01268v1.pdf filename=2404.01268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> like <b>ChatGPT</b> in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by <b>LLMs.</b> To address this gap, we conduct the first systematic, <b>large-scale</b> <b>analysis</b> <b>across</b> 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of <b>LLM-modified</b> content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in <b>LLM</b> usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least <b>LLM</b> modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of <b>LLM-modification</b> are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that <b>LLMs</b> are being broadly used in scientific writings.</p></p class="citation"></blockquote><h3 id=3657--125273-uniark-improving-generalisation-and-consistency-for-factual-knowledge-extraction-through-debiasing-yijun-yang-et-al-2024>(36/57 | 125/273) UniArk: Improving Generalisation and Consistency for Factual Knowledge Extraction through Debiasing (Yijun Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijun Yang, Jie He, Pinzhen Chen, Víctor Gutiérrez-Basulto, Jeff Z. Pan. (2024)<br><strong>UniArk: Improving Generalisation and Consistency for Factual Knowledge Extraction through Debiasing</strong><br><button class=copy-to-clipboard title="UniArk: Improving Generalisation and Consistency for Factual Knowledge Extraction through Debiasing" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Out-of-domain, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01253v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01253v1.pdf filename=2404.01253v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several recent papers have investigated the potential of language models as knowledge bases as well as the existence of severe biases when extracting factual knowledge. In this work, we focus on the factual probing performance over unseen <b>prompts</b> from tuning, and using a probabilistic view we show the inherent misalignment between pre-training and downstream tuning objectives in language models for probing knowledge. We hypothesize that simultaneously debiasing these objectives can be the key to generalisation over unseen <b>prompts.</b> We propose an adapter-based framework, UniArk, for generalised and consistent factual knowledge extraction through simple methods without introducing extra parameters. Extensive experiments show that UniArk can significantly improve the model&rsquo;s <b>out-of-domain</b> generalisation as well as consistency under various <b>prompts.</b> Additionally, we construct ParaTrex, a <b>large-scale</b> <b>and</b> <b>diverse</b> dataset for measuring the inconsistency and <b>out-of-domain</b> generation of models. Further, ParaTrex offers a reference method for constructing paraphrased datasets using <b>large</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=3757--126273-llm-as-a-mastermind-a-survey-of-strategic-reasoning-with-large-language-models-yadong-zhang-et-al-2024>(37/57 | 126/273) LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models (Yadong Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, Furu Wei. (2024)<br><strong>LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models</strong><br><button class=copy-to-clipboard title="LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01230v1.pdf filename=2404.01230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a comprehensive survey of the current status and opportunities for <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in strategic <b>reasoning,</b> a sophisticated form of <b>reasoning</b> that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly. Strategic <b>reasoning</b> is distinguished by its focus on the dynamic and uncertain nature of interactions among multi-agents, where comprehending the environment and anticipating the behavior of others is crucial. We explore the scopes, applications, methodologies, and evaluation metrics related to strategic <b>reasoning</b> with <b>LLMs,</b> highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance. It aims to systematize and clarify the scattered literature on this subject, providing a systematic review that underscores the importance of strategic <b>reasoning</b> as a critical cognitive capability and offers insights into future research directions and potential improvements.</p></p class="citation"></blockquote><h3 id=3857--127273-the-fine-line-navigating-large-language-model-pretraining-with-down-streaming-capability-analysis-chen-yang-et-al-2024>(38/57 | 127/273) The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis (Chen Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Yang, Junzhuo Li, Xinyao Niu, Xinrun Du, Songyang Gao, Haoran Zhang, Zhaoliang Chen, Xingwei Qu, Ruibin Yuan, Yizhi Li, Jiaheng Liu, Stephen W. Huang, Shawn Yue, Wenhu Chen, Jie Fu, Ge Zhang. (2024)<br><strong>The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis</strong><br><button class=copy-to-clipboard title="The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01204v1.pdf filename=2404.01204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Uncovering early-stage metrics that reflect final model performance is one core principle for <b>large-scale</b> <b>pretraining.</b> <b>The</b> existing <b>scaling</b> <b>law</b> demonstrates the power-law correlation between pretraining loss and training flops, which serves as an important indicator of the current training state for <b>large</b> <b>language</b> <b>models.</b> However, this principle only focuses on the model&rsquo;s compression properties on the training data, resulting in an inconsistency with the ability improvements on the downstream tasks. Some follow-up works attempted to extend the <b>scaling-law</b> <b>to</b> more complex metrics (such as hyperparameters), but still lacked a comprehensive analysis of the dynamic differences among various capabilities during pretraining. To address the aforementioned limitations, this paper undertakes a comprehensive comparison of model capabilities at various pretraining intermediate checkpoints. Through this analysis, we confirm that specific downstream metrics exhibit similar training dynamics across models of different sizes, up to 67 billion parameters. In addition to our core findings, we&rsquo;ve reproduced Amber and OpenLLaMA, releasing their intermediate checkpoints. This initiative offers valuable resources to the research community and facilitates the verification and exploration of <b>LLM</b> pretraining by open-source researchers. Besides, we provide empirical summaries, including performance comparisons of different models and capabilities, and tuition of key metrics for different training phases. Based on these findings, we provide a more user-friendly strategy for evaluating the optimization state, offering guidance for establishing a stable pretraining process.</p></p class="citation"></blockquote><h3 id=3957--128273-a-neuro-symbolic-approach-to-monitoring-salt-content-in-food-anuja-tayal-et-al-2024>(39/57 | 128/273) A Neuro-Symbolic Approach to Monitoring Salt Content in Food (Anuja Tayal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anuja Tayal, Barbara Di Eugenio, Devika Salunke, Andrew D. Boyd, Carolyn A Dickens, Eulalia P Abril, Olga Garcia-Bedoya, Paula G Allen-Meares. (2024)<br><strong>A Neuro-Symbolic Approach to Monitoring Salt Content in Food</strong><br><button class=copy-to-clipboard title="A Neuro-Symbolic Approach to Monitoring Salt Content in Food" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SC, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Transformer, Dialogue System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01182v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01182v1.pdf filename=2404.01182v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a <b>dialogue</b> <b>system</b> that enables heart failure patients to inquire about salt content in foods and help them monitor and reduce salt intake. Addressing the lack of specific datasets for food-based salt content inquiries, we develop a template-based conversational dataset. The dataset is structured to ask clarification questions to identify food items and their salt content. Our findings indicate that while <b>fine-tuning</b> <b>transformer-based</b> models on the dataset yields limited performance, the integration of Neuro-Symbolic Rules significantly enhances the system&rsquo;s performance. Our experiments show that by integrating neuro-symbolic rules, our system achieves an improvement in joint goal accuracy of over 20% across different data sizes compared to naively <b>fine-tuning</b> <b>transformer-based</b> models.</p></p class="citation"></blockquote><h3 id=4057--129273-ails-ntua-at-semeval-2024-task-9-cracking-brain-teasers-transformer-models-for-lateral-thinking-puzzles-ioannis-panagiotopoulos-et-al-2024>(40/57 | 129/273) AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models for Lateral Thinking Puzzles (Ioannis Panagiotopoulos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ioannis Panagiotopoulos, Giorgos Filandrianos, Maria Lymperaiou, Giorgos Stamou. (2024)<br><strong>AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models for Lateral Thinking Puzzles</strong><br><button class=copy-to-clipboard title="AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models for Lateral Thinking Puzzles" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, ChatGPT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01084v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01084v1.pdf filename=2404.01084v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we outline our submission for the SemEval-2024 Task 9 competition: &lsquo;BRAINTEASER: A Novel Task Defying Common Sense&rsquo;. We engage in both sub-tasks: Sub-task A-Sentence Puzzle and Sub-task B-Word Puzzle. We evaluate a plethora of pre-trained <b>transformer-based</b> language models of different sizes through <b>fine-tuning.</b> Subsequently, we undertake an analysis of their scores and responses to aid future researchers in understanding and utilizing these models effectively. Our top-performing approaches secured competitive positions on the competition leaderboard across both sub-tasks. In the evaluation phase, our best submission attained an average accuracy score of 81.7% in the Sentence Puzzle, and 85.4% in the Word Puzzle, significantly outperforming the best neural baseline <b>(ChatGPT)</b> by more than 20% and 30% respectively.</p></p class="citation"></blockquote><h3 id=4157--130273-lite-modeling-environmental-ecosystems-with-multimodal-large-language-models-haoran-li-et-al-2024>(41/57 | 130/273) LITE: Modeling Environmental Ecosystems with Multimodal Large Language Models (Haoran Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoran Li, Junqi Liu, Zexian Wang, Shiyuan Luo, Xiaowei Jia, Huaxiu Yao. (2024)<br><strong>LITE: Modeling Environmental Ecosystems with Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="LITE: Modeling Environmental Ecosystems with Multimodal Large Language Models" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 29<br>Keywords: Graph, Distribution Shift, Distribution Shift, Multi-modal, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01165v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01165v1.pdf filename=2404.01165v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The modeling of environmental ecosystems plays a pivotal role in the sustainable management of our planet. Accurate prediction of key environmental variables over space and time can aid in informed policy and decision-making, thus improving people&rsquo;s livelihood. Recently, deep learning-based methods have shown promise in modeling the spatial-temporal relationships for predicting environmental variables. However, these approaches often fall short in handling incomplete features and <b>distribution</b> <b>shifts,</b> which are commonly observed in environmental data due to the substantial cost of data collection and malfunctions in measuring instruments. To address these issues, we propose LITE &ndash; a <b>multimodal</b> <b>large</b> <b>language</b> <b>model</b> for environmental ecosystems modeling. Specifically, LITE unifies different environmental variables by transforming them into natural language descriptions and line <b>graph</b> images. Then, LITE utilizes unified encoders to capture spatial-temporal dynamics and correlations in different modalities. During this step, the incomplete features are imputed by a sparse Mixture-of-Experts framework, and the <b>distribution</b> <b>shift</b> is handled by incorporating multi-granularity information from past observations. Finally, guided by domain instructions, a language model is employed to fuse the <b>multimodal</b> representations for the prediction. Our experiments demonstrate that LITE significantly enhances performance in environmental spatial-temporal prediction across different domains compared to the best baseline, with a 41.25% reduction in prediction error. This justifies its effectiveness. Our data and code are available at <a href=https://github.com/hrlics/LITE>https://github.com/hrlics/LITE</a>.</p></p class="citation"></blockquote><h3 id=4257--131273-evaluating-the-factuality-of-large-language-models-using-large-scale-knowledge-graphs-xiaoze-liu-et-al-2024>(42/57 | 131/273) Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs (Xiaoze Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoze Liu, Feijie Wu, Tianyang Xu, Zhuo Chen, Yichi Zhang, Xiaoqian Wang, Jing Gao. (2024)<br><strong>Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs</strong><br><button class=copy-to-clipboard title="Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 28<br>Keywords: Graph, Knowledge Graph, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00942v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00942v1.pdf filename=2404.00942v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has significantly transformed the AI landscape, enhancing machine learning and AI capabilities. Factuality issue is a critical concern for <b>LLMs,</b> as they may generate factually incorrect responses. In this paper, we propose GraphEval to evaluate an <b>LLM&rsquo;s</b> performance using a substantially <b>large</b> <b>test</b> <b>dataset.</b> Specifically, the test dataset is retrieved from a <b>large</b> <b>knowledge</b> <b>graph</b> with more than 10 million facts without expensive human efforts. Unlike conventional methods that evaluate <b>LLMs</b> based on generated responses, GraphEval streamlines the evaluation process by creating a judge model to estimate the correctness of the answers given by the <b>LLM.</b> Our experiments demonstrate that the judge model&rsquo;s factuality assessment aligns closely with the correctness of the <b>LLM&rsquo;s</b> generated outputs, while also substantially reducing evaluation costs. Besides, our findings offer valuable insights into <b>LLM</b> performance across different metrics and highlight the potential for future improvements in ensuring the factual integrity of <b>LLM</b> outputs. The code is publicly available at <a href=https://github.com/xz-liu/GraphEval>https://github.com/xz-liu/GraphEval</a>.</p></p class="citation"></blockquote><h3 id=4357--132273-open-vocabulary-federated-learning-with-multimodal-prototyping-huimin-zeng-et-al-2024>(43/57 | 132/273) Open-Vocabulary Federated Learning with Multimodal Prototyping (Huimin Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huimin Zeng, Zhenrui Yue, Dong Wang. (2024)<br><strong>Open-Vocabulary Federated Learning with Multimodal Prototyping</strong><br><button class=copy-to-clipboard title="Open-Vocabulary Federated Learning with Multimodal Prototyping" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 26<br>Keywords: Federated Learning, Multi-modal, Multi-modal, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01232v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01232v2.pdf filename=2404.01232v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>federated</b> <b>learning</b> (FL) studies usually assume the training label space and test label space are identical. However, in real-world applications, this assumption is too ideal to be true. A new user could come up with queries that involve data from unseen classes, and such open-vocabulary queries would directly defect such FL systems. Therefore, in this work, we explicitly focus on the under-explored open-vocabulary challenge in FL. That is, for a new user, the global server shall understand her/his query that involves arbitrary unknown classes. To address this problem, we leverage the pre-trained <b>vision-language</b> models (VLMs). In particular, we present a novel adaptation framework tailored for VLMs in the context of FL, named as <b>Federated</b> <b>Multimodal</b> Prototyping (Fed-MP). Fed-MP adaptively aggregates the local model weights based on light-weight client residuals, and makes predictions based on a novel <b>multimodal</b> prototyping mechanism. Fed-MP exploits the knowledge learned from the seen classes, and robustifies the adapted VLM to unseen categories. Our empirical evaluation on various datasets validates the effectiveness of Fed-MP.</p></p class="citation"></blockquote><h3 id=4457--133273-enterprise-use-cases-combining-knowledge-graphs-and-natural-language-processing-phillip-schneider-et-al-2024>(44/57 | 133/273) Enterprise Use Cases Combining Knowledge Graphs and Natural Language Processing (Phillip Schneider et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Phillip Schneider, Tim Schopf, Juraj Vladika, Florian Matthes. (2024)<br><strong>Enterprise Use Cases Combining Knowledge Graphs and Natural Language Processing</strong><br><button class=copy-to-clipboard title="Enterprise Use Cases Combining Knowledge Graphs and Natural Language Processing" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01443v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01443v1.pdf filename=2404.01443v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>management</b> is a critical challenge for enterprises in today&rsquo;s digital world, as the volume and complexity of data being generated and collected continue to grow incessantly. <b>Knowledge</b> <b>graphs</b> <b>(KG)</b> emerged as a promising solution to this problem by providing a flexible, scalable, and semantically rich way to organize and make sense of data. This paper builds upon a recent survey of the research literature on combining <b>KGs</b> and Natural Language Processing (NLP). Based on selected application scenarios from enterprise context, we discuss synergies that result from such a combination. We cover various approaches from the three core areas of <b>KG</b> construction, <b>reasoning</b> as well as <b>KG-based</b> NLP tasks. In addition to explaining innovative enterprise use cases, we assess their maturity in terms of practical applicability and conclude with an outlook on emergent application areas for the future.</p></p class="citation"></blockquote><h3 id=4557--134273-paireval-open-domain-dialogue-evaluation-with-pairwise-comparison-chaehun-park-et-al-2024>(45/57 | 134/273) PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison (ChaeHun Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>ChaeHun Park, Minseok Choi, Dohyun Lee, Jaegul Choo. (2024)<br><strong>PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison</strong><br><button class=copy-to-clipboard title="PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Dialogue System, Open-Domain Dialogue<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01015v1.pdf filename=2404.01015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Building a reliable and automated evaluation metric is a necessary but challenging problem for <b>open-domain</b> <b>dialogue</b> <b>systems.</b> Recent studies proposed evaluation metrics that assess generated responses by considering their relevance to previous <b>dialogue</b> <b>histories.</b> Although effective, these metrics evaluate individual responses directly rather than considering their relative quality compared to other responses. To handle this, we propose PairEval, a novel <b>dialogue</b> <b>evaluation</b> metric for assessing responses by comparing their quality against responses in different conversations. PairEval is built on top of open-sourced and moderate-size language models, and we make them specialized in pairwise comparison between <b>dialogue</b> <b>responses.</b> Extensive experiments on multiple <b>benchmarks</b> demonstrate that our metric exhibits a higher correlation with human judgments than baseline metrics. We also find that the proposed comparative metric is more robust in detecting common failures from <b>open-domain</b> <b>dialogue</b> <b>systems,</b> including repetition and speaker insensitivity.</p></p class="citation"></blockquote><h3 id=4657--135273-creating-emoji-lexica-from-unsupervised-sentiment-analysis-of-their-descriptions-milagros-fernández-gavilanes-et-al-2024>(46/57 | 135/273) Creating emoji lexica from unsupervised sentiment analysis of their descriptions (Milagros Fernández-Gavilanes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Milagros Fernández-Gavilanes, Jonathan Juncal-Martínez, Silvia García-Méndez, Enrique Costa-Montenegro, Francisco Javier González-Castaño. (2024)<br><strong>Creating emoji lexica from unsupervised sentiment analysis of their descriptions</strong><br><button class=copy-to-clipboard title="Creating emoji lexica from unsupervised sentiment analysis of their descriptions" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01439v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01439v1.pdf filename=2404.01439v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Online media, such as blogs and social networking sites, generate massive volumes of unstructured data of great interest to analyze the opinions and <b>sentiments</b> <b>of</b> individuals and organizations. Novel approaches beyond Natural Language Processing are necessary to quantify these opinions with polarity metrics. So far, the <b>sentiment</b> <b>expressed</b> by emojis has received little attention. The use of symbols, however, has boomed in the past four years. About twenty billion are typed in Twitter nowadays, and new emojis keep appearing in each new Unicode version, making them increasingly relevant to <b>sentiment</b> <b>analysis</b> tasks. This has motivated us to propose a novel approach to predict the <b>sentiments</b> <b>expressed</b> by emojis in online textual messages, such as tweets, that does not require human effort to manually annotate data and saves valuable time for other analysis tasks. For this purpose, we automatically constructed a novel emoji <b>sentiment</b> <b>lexicon</b> using an <b>unsupervised</b> <b>sentiment</b> <b>analysis</b> system based on the definitions given by emoji creators in Emojipedia. Additionally, we automatically created lexicon variants by also considering the <b>sentiment</b> <b>distribution</b> of the informal texts accompanying emojis. All these lexica are evaluated and compared regarding the improvement obtained by including them in <b>sentiment</b> <b>analysis</b> of the annotated datasets provided by Kralj Novak et al. (2015). The results confirm the competitiveness of our approach.</p></p class="citation"></blockquote><h3 id=4757--136273-artificial-intelligence-and-the-spatial-documentation-of-languages-hakam-ghanim-2024>(47/57 | 136/273) Artificial Intelligence and the Spatial Documentation of Languages (Hakam Ghanim, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hakam Ghanim. (2024)<br><strong>Artificial Intelligence and the Spatial Documentation of Languages</strong><br><button class=copy-to-clipboard title="Artificial Intelligence and the Spatial Documentation of Languages" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: GPT, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01263v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01263v1.pdf filename=2404.01263v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancement in technology has made interdisciplinary research more accessible. Particularly the breakthrough in Artificial Intelligence AI has given huge advantages to researchers working in interdisciplinary and multidisciplinary fields. This study investigates the ability of AI models, particularly <b>GPT4</b> and <b>GPT</b> Data Analyst in creating language maps for language documentation. The study Integrates documentary linguistics linguistic geography and AI by showcasing how AI models facilitate the spatial documentation of languages through the creation of language maps with minimal cartographic expertise. The study is conducted using a CSV file and a GeoJSON file both obtained from HDX and from the researchers fieldwork. The study data is then applied in realtime conversations with the AI models in order to generate the language distribution maps. The study highlights the two AI models capabilities in generating highquality static and interactive web maps and streamlining the mapmaking process, despite facing challenges like inconsistencies and difficulties in adding legends. The findings suggest a promising future for AI in generating language maps and enhancing the work of documentary linguists as they collect their data in the field pointing towards the need for further development to fully harness AI potential in this field.</p></p class="citation"></blockquote><h3 id=4857--137273-an-image-speaks-a-thousand-words-but-can-everyone-listen-on-translating-images-for-cultural-relevance-simran-khanuja-et-al-2024>(48/57 | 137/273) An image speaks a thousand words, but can everyone listen? On translating images for cultural relevance (Simran Khanuja et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simran Khanuja, Sathyanarayanan Ramamoorthy, Yueqi Song, Graham Neubig. (2024)<br><strong>An image speaks a thousand words, but can everyone listen? On translating images for cultural relevance</strong><br><button class=copy-to-clipboard title="An image speaks a thousand words, but can everyone listen? On translating images for cultural relevance" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 20<br>Keywords: Neural Machine Translation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01247v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01247v1.pdf filename=2404.01247v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, <b>machine</b> <b>translation</b> systems remain confined to dealing with language in speech and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset: i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image, and ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can be improved by leveraging <b>LLMs</b> and retrievers in the loop. Best pipelines can only translate 5% of images for some countries in the easier concept dataset and no translation is successful for some countries in the application dataset, highlighting the challenging nature of the task. Our code and data is released here: <a href=https://github.com/simran-khanuja/image-transcreation>https://github.com/simran-khanuja/image-transcreation</a>.</p></p class="citation"></blockquote><h3 id=4957--138273-green-ai-exploring-carbon-footprints-mitigation-strategies-and-trade-offs-in-large-language-model-training-vivian-liu-et-al-2024>(49/57 | 138/273) Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade Offs in Large Language Model Training (Vivian Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vivian Liu, Yiqiao Yin. (2024)<br><strong>Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade Offs in Large Language Model Training</strong><br><button class=copy-to-clipboard title="Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade Offs in Large Language Model Training" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-PF, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01157v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01157v1.pdf filename=2404.01157v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prominent works in the field of Natural Language Processing have long attempted to create new innovative models by improving upon previous model training approaches, altering model architecture, and developing more in-depth datasets to better their performance. However, with the quickly advancing field of NLP comes increased greenhouse gas emissions, posing concerns over the environmental damage caused by training <b>LLMs.</b> Gaining a comprehensive understanding of the various costs, particularly those pertaining to environmental aspects, that are associated with artificial intelligence serves as the foundational basis for ensuring safe AI models. Currently, investigations into the CO2 emissions of AI models remain an emerging area of research, and as such, in this paper, we evaluate the CO2 emissions of well-known <b>large</b> <b>language</b> <b>models,</b> which have an especially high carbon footprint due to their significant amount of model parameters. We argue for the training of <b>LLMs</b> in a way that is responsible and sustainable by suggesting measures for reducing carbon emissions. Furthermore, we discuss how the choice of hardware affects CO2 emissions by contrasting the CO2 emissions during model training for two widely used GPUs. Based on our results, we present the benefits and drawbacks of our proposed solutions and make the argument for the possibility of training more environmentally safe AI models without sacrificing their robustness and performance.</p></p class="citation"></blockquote><h3 id=5057--139273-do-llms-find-human-answers-to-fact-driven-questions-perplexing-a-case-study-on-reddit-parker-seegmiller-et-al-2024>(50/57 | 139/273) Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case Study on Reddit (Parker Seegmiller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Parker Seegmiller, Joseph Gatto, Omar Sharif, Madhusudan Basak, Sarah Masud Preum. (2024)<br><strong>Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case Study on Reddit</strong><br><button class=copy-to-clipboard title="Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case Study on Reddit" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01147v1.pdf filename=2404.01147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been shown to be proficient in correctly answering questions in the context of online discourse. However, the study of using <b>LLMs</b> to model human-like answers to fact-driven social media questions is still under-explored. In this work, we investigate how <b>LLMs</b> model the wide variety of human answers to fact-driven questions posed on several topic-specific Reddit communities, or subreddits. We collect and release a dataset of 409 fact-driven questions and 7,534 diverse, human-rated answers from 15 r/Ask{Topic} communities across 3 categories: profession, social identity, and geographic location. We find that <b>LLMs</b> are considerably better at modeling highly-rated human answers to such questions, as opposed to poorly-rated human answers. We present several directions for future research based on our initial findings.</p></p class="citation"></blockquote><h3 id=5157--140273-koconovel-annotated-dataset-of-character-coreference-in-korean-novels-kyuhee-kim-et-al-2024>(51/57 | 140/273) KoCoNovel: Annotated Dataset of Character Coreference in Korean Novels (Kyuhee Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyuhee Kim, Surin Lee, Sangah Lee. (2024)<br><strong>KoCoNovel: Annotated Dataset of Character Coreference in Korean Novels</strong><br><button class=copy-to-clipboard title="KoCoNovel: Annotated Dataset of Character Coreference in Korean Novels" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: BERT, Coreference Resolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01140v1.pdf filename=2404.01140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present KoCoNovel, an novel character <b>coreference</b> <b>dataset</b> derived from Korean literary texts, complete with detailed annotation guidelines. Comprising 178K tokens from 50 modern and contemporary Korean novels, KoCoNovel stands as the second-largest public <b>coreference</b> <b>resolution</b> corpus in Korean, after the NIKL corpus, and the first to be based on literary texts. To broaden its utility, we provide four distinct versions of KoCoNovel, offering options for the perspectives of the omniscient author and readers, and for handling multiple entities as either separate or overlapping. This approach integrates existing discourse surrounding <b>coreference</b> <b>resolution</b> in literary texts, providing a comprehensive dataset for exploration. One of KoCoNovel&rsquo;s distinctive features is that 24% of all character mentions are single common nouns, lacking possessive markers or articles. This feature is particularly influenced by the nuances of Korean address term culture, which favors the use of terms denoting social relationships and kinship over personal names. In experiments with a <b>BERT-based</b> <b>coreference</b> <b>model,</b> we observed notable performance enhancements with KoCoNovel in comparison to the NIKL corpus. Such findings underscore KoCoNovel&rsquo;s potential to significantly enhance <b>coreference</b> <b>resolution</b> models through the integration of Korean cultural and linguistic dynamics.</p></p class="citation"></blockquote><h3 id=5257--141273-regularized-best-of-n-sampling-to-mitigate-reward-hacking-for-language-model-alignment-yuu-jinnai-et-al-2024>(52/57 | 141/273) Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment (Yuu Jinnai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, Kenshi Abe. (2024)<br><strong>Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment</strong><br><button class=copy-to-clipboard title="Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01054v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01054v2.pdf filename=2404.01054v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization (e.g., KL regularization), which ensures that the language model remains close to the reference model. In this research, we propose Regularized Best-of-N (RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating a proximity term in response selection, similar to preference learning techniques. We evaluate two variants of RBoN on the AlpacaFarm dataset and find that they outperform BoN, especially when the proxy reward model has a low correlation with the true objective.</p></p class="citation"></blockquote><h3 id=5357--142273-evalverse-unified-and-accessible-library-for-large-language-model-evaluation-jihoo-kim-et-al-2024>(53/57 | 142/273) Evalverse: Unified and Accessible Library for Large Language Model Evaluation (Jihoo Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jihoo Kim, Wonho Song, Dahyun Kim, Yunsu Kim, Yungi Kim, Chanjun Park. (2024)<br><strong>Evalverse: Unified and Accessible Library for Large Language Model Evaluation</strong><br><button class=copy-to-clipboard title="Evalverse: Unified and Accessible Library for Large Language Model Evaluation" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00943v1.pdf filename=2404.00943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces Evalverse, a novel library that streamlines the evaluation of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> by unifying disparate evaluation tools into a single, user-friendly framework. Evalverse enables individuals with limited knowledge of artificial intelligence to easily request <b>LLM</b> evaluations and receive detailed reports, facilitated by an integration with communication platforms like Slack. Thus, Evalverse serves as a powerful tool for the comprehensive assessment of <b>LLMs,</b> offering both researchers and practitioners a centralized and easily accessible evaluation framework. Finally, we also provide a demo video for Evalverse, showcasing its capabilities and implementation in a two-minute format.</p></p class="citation"></blockquote><h3 id=5457--143273-tm-trek-at-semeval-2024-task-8-towards-llm-based-automatic-boundary-detection-for-human-machine-mixed-text-xiaoyan-qu-et-al-2024>(54/57 | 143/273) TM-TREK at SemEval-2024 Task 8: Towards LLM-Based Automatic Boundary Detection for Human-Machine Mixed Text (Xiaoyan Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyan Qu, Xiangfeng Meng. (2024)<br><strong>TM-TREK at SemEval-2024 Task 8: Towards LLM-Based Automatic Boundary Detection for Human-Machine Mixed Text</strong><br><button class=copy-to-clipboard title="TM-TREK at SemEval-2024 Task 8: Towards LLM-Based Automatic Boundary Detection for Human-Machine Mixed Text" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00899v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00899v1.pdf filename=2404.00899v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasing prevalence of text generated by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> there is a growing concern about distinguishing between <b>LLM-generated</b> and human-written texts in order to prevent the misuse of <b>LLMs,</b> such as the dissemination of misleading information and academic dishonesty. Previous research has primarily focused on classifying text as either entirely human-written or <b>LLM-generated,</b> neglecting the detection of mixed texts that contain both types of content. This paper explores <b>LLMs&rsquo;</b> ability to identify boundaries in human-written and machine-generated mixed texts. We approach this task by transforming it into a token classification problem and regard the label turning point as the boundary. Notably, our ensemble model of <b>LLMs</b> achieved first place in the &lsquo;Human-Machine Mixed Text Detection&rsquo; sub-task of the SemEval'24 Competition Task 8. Additionally, we investigate factors that influence the capability of <b>LLMs</b> in detecting boundaries within mixed texts, including the incorporation of extra layers on top of <b>LLMs,</b> combination of segmentation loss, and the impact of pretraining. Our findings aim to provide valuable insights for future research in this area.</p></p class="citation"></blockquote><h3 id=5557--144273-verifying-claims-about-metaphors-with-large-scale-automatic-metaphor-identification-kotaro-aono-et-al-2024>(55/57 | 144/273) Verifying Claims About Metaphors with Large-Scale Automatic Metaphor Identification (Kotaro Aono et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kotaro Aono, Ryohei Sasano, Koichi Takeda. (2024)<br><strong>Verifying Claims About Metaphors with Large-Scale Automatic Metaphor Identification</strong><br><button class=copy-to-clipboard title="Verifying Claims About Metaphors with Large-Scale Automatic Metaphor Identification" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Metaphor Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01029v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01029v1.pdf filename=2404.01029v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There are several linguistic claims about situations where words are more likely to be used as <b>metaphors.</b> <b>However,</b> few studies have sought to verify such claims with large corpora. This study entails a large-scale, corpus-based analysis of certain existing claims about verb <b>metaphors,</b> <b>by</b> applying <b>metaphor</b> <b>detection</b> to sentences extracted from Common Crawl and using the statistics obtained from the results. The verification results indicate that the direct objects of verbs used as <b>metaphors</b> <b>tend</b> to have lower degrees of concreteness, imageability, and familiarity, and that <b>metaphors</b> <b>are</b> more likely to be used in emotional and subjective sentences.</p></p class="citation"></blockquote><h3 id=5657--145273-constructing-and-expanding-low-resource-and-underrepresented-parallel-datasets-for-indonesian-local-languages-joanito-agili-lopo-et-al-2024>(56/57 | 145/273) Constructing and Expanding Low-Resource and Underrepresented Parallel Datasets for Indonesian Local Languages (Joanito Agili Lopo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joanito Agili Lopo, Radius Tanone. (2024)<br><strong>Constructing and Expanding Low-Resource and Underrepresented Parallel Datasets for Indonesian Local Languages</strong><br><button class=copy-to-clipboard title="Constructing and Expanding Low-Resource and Underrepresented Parallel Datasets for Indonesian Local Languages" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Low-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01009v1.pdf filename=2404.01009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Indonesia, local languages play an integral role in the culture. However, the available Indonesian language resources still fall into the category of limited data in the Natural Language Processing (NLP) field. This is become problematic when build NLP model for these languages. To address this gap, we introduce Bhinneka Korpus, a multilingual parallel corpus featuring five Indonesian local languages. Our goal is to enhance access and utilization of these resources, extending their reach within the country. We explained in a detail the dataset collection process and associated challenges. Additionally, we experimented with translation task using the IBM Model 1 due to data constraints. The result showed that the performance of each language already shows good indications for further development. Challenges such as lexical variation, smoothing effects, and cross-linguistic variability are discussed. We intend to evaluate the corpus using advanced NLP techniques for <b>low-resource</b> languages, paving the way for multilingual translation models.</p></p class="citation"></blockquote><h3 id=5757--146273-dialogue-with-robots-proposals-for-broadening-participation-and-research-in-the-slivar-community-casey-kennington-et-al-2024>(57/57 | 146/273) Dialogue with Robots: Proposals for Broadening Participation and Research in the SLIVAR Community (Casey Kennington et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Casey Kennington, Malihe Alikhani, Heather Pon-Barry, Katherine Atwell, Yonatan Bisk, Daniel Fried, Felix Gervits, Zhao Han, Mert Inan, Michael Johnston, Raj Korpan, Diane Litman, Matthew Marge, Cynthia Matuszek, Ross Mead, Shiwali Mohan, Raymond Mooney, Natalie Parde, Jivko Sinapov, Angela Stewart, Matthew Stone, Stefanie Tellex, Tom Williams. (2024)<br><strong>Dialogue with Robots: Proposals for Broadening Participation and Research in the SLIVAR Community</strong><br><button class=copy-to-clipboard title="Dialogue with Robots: Proposals for Broadening Participation and Research in the SLIVAR Community" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-RO, cs.CL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01158v1.pdf filename=2404.01158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability to interact with machines using natural human language is becoming not just commonplace, but expected. The next step is not just text interfaces, but speech interfaces and not just with computers, but with all machines including robots. In this paper, we chronicle the recent history of this growing field of spoken dialogue with robots and offer the community three proposals, the first focused on education, the second on <b>benchmarks,</b> and the third on the modeling of language when it comes to spoken interaction with robots. The three proposals should act as white papers for any researcher to take and build upon.</p></p class="citation"></blockquote><h2 id=csse-5>cs.SE (5)</h2><h3 id=15--147273-large-language-model-evaluation-via-multi-ai-agents-preliminary-results-zeeshan-rasheed-et-al-2024>(1/5 | 147/273) Large Language Model Evaluation Via Multi AI Agents: Preliminary results (Zeeshan Rasheed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeeshan Rasheed, Muhammad Waseem, Kari Systä, Pekka Abrahamsson. (2024)<br><strong>Large Language Model Evaluation Via Multi AI Agents: Preliminary results</strong><br><button class=copy-to-clipboard title="Large Language Model Evaluation Via Multi AI Agents: Preliminary results" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 93<br>Keywords: Benchmarking, Bard, GPT, GPT-3, GPT-3.5, GPT-4, GPT-4 turbo, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01023v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01023v1.pdf filename=2404.01023v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have become integral to both research and daily operations, rigorous evaluation is crucial. This assessment is important not only for individual tasks but also for understanding their societal impact and potential risks. Despite extensive efforts to examine <b>LLMs</b> from various perspectives, there is a noticeable lack of multi-agent AI models specifically designed to evaluate the performance of different <b>LLMs.</b> To address this gap, we introduce a novel multi-agent AI model that aims to assess and compare the performance of various <b>LLMs.</b> Our model consists of eight distinct AI agents, each responsible for retrieving code based on a common description from different advanced language models, including <b>GPT-3.5,</b> <b>GPT-3.5</b> Turbo, <b>GPT-4,</b> <b>GPT-4</b> <b>Turbo,</b> Google <b>Bard,</b> <b>LLAMA,</b> and Hugging Face. Our developed model utilizes the API of each language model to retrieve code for a given high-level description. Additionally, we developed a verification agent, tasked with the critical role of evaluating the code generated by its counterparts. We integrate the HumanEval <b>benchmark</b> into our verification agent to assess the generated code&rsquo;s performance, providing insights into their respective capabilities and efficiencies. Our initial results indicate that the <b>GPT-3.5</b> Turbo model&rsquo;s performance is comparatively better than the other models. This preliminary analysis serves as a <b>benchmark,</b> comparing their performances side by side. Our future goal is to enhance the evaluation process by incorporating the Massively Multitask <b>Benchmark</b> for Python (MBPP) <b>benchmark,</b> which is expected to further refine our assessment. Additionally, we plan to share our developed model with twenty practitioners from various backgrounds to test our model and collect their feedback for further improvement.</p></p class="citation"></blockquote><h3 id=25--148273-exploring-and-evaluating-hallucinations-in-llm-powered-code-generation-fang-liu-et-al-2024>(2/5 | 148/273) Exploring and Evaluating Hallucinations in LLM-Powered Code Generation (Fang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, Li Zhang. (2024)<br><strong>Exploring and Evaluating Hallucinations in LLM-Powered Code Generation</strong><br><button class=copy-to-clipboard title="Exploring and Evaluating Hallucinations in LLM-Powered Code Generation" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 83<br>Keywords: Benchmarking, Code Generation, Hallucination Detection, Language Generation, Natural Language Generation, Natural Language Generation, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00971v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00971v1.pdf filename=2404.00971v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has significantly advanced many applications on software engineering tasks, particularly in <b>code</b> <b>generation.</b> Despite the promising performance, <b>LLMs</b> are prone to generate <b>hallucinations,</b> <b>which</b> means <b>LLMs</b> might produce outputs that deviate from users&rsquo; intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of <b>LLMs</b> potentially risky in a wide range of applications. Existing work mainly focuses on investing the <b>hallucination</b> <b>in</b> the domain of <b>natural</b> <b>language</b> <b>generation</b> <b>(NLG),</b> leaving a gap in understanding the types and extent of <b>hallucinations</b> <b>in</b> the context of <b>code</b> <b>generation.</b> To bridge the gap, we conducted a thematic analysis of the <b>LLM-generated</b> <b>code</b> <b>to</b> <b>summarize</b> and categorize the <b>hallucinations</b> <b>present</b> in it. Our study established a comprehensive taxonomy of <b>hallucinations</b> <b>in</b> <b>LLM-generated</b> <b>code,</b> <b>encompassing</b> 5 primary categories of <b>hallucinations</b> <b>depending</b> on the conflicting objectives and varying degrees of deviation observed in <b>code</b> <b>generation.</b> Furthermore, we systematically analyzed the distribution of <b>hallucinations,</b> <b>exploring</b> variations among different <b>LLMs</b> and their correlation with <b>code</b> <b>correctness.</b> Based on the results, we proposed HalluCode, a <b>benchmark</b> for evaluating the performance of <b>code</b> <b>LLMs</b> in recognizing <b>hallucinations.</b> <b>Hallucination</b> <b>recognition</b> and mitigation experiments with HalluCode and HumanEval show existing <b>LLMs</b> face great challenges in recognizing <b>hallucinations,</b> <b>particularly</b> in identifying their types, and are hardly able to mitigate <b>hallucinations.</b> <b>We</b> believe our findings will shed light on future research about <b>hallucination</b> <b>evaluation,</b> detection, and mitigation, ultimately paving the way for building more effective and reliable <b>code</b> <b>LLMs</b> in the future.</p></p class="citation"></blockquote><h3 id=35--149273-syntactic-robustness-for-llm-based-code-generation-laboni-sarker-et-al-2024>(3/5 | 149/273) Syntactic Robustness for LLM-based Code Generation (Laboni Sarker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Laboni Sarker, Mara Downing, Achintya Desai, Tevfik Bultan. (2024)<br><strong>Syntactic Robustness for LLM-based Code Generation</strong><br><button class=copy-to-clipboard title="Syntactic Robustness for LLM-based Code Generation" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 80<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, Code Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01535v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01535v1.pdf filename=2404.01535v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rapid advances in the field of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have made <b>LLM-based</b> <b>code</b> <b>generation</b> an important area for investigation. An <b>LLM-based</b> <b>code</b> <b>generator</b> takes a <b>prompt</b> as input and produces <b>code</b> <b>that</b> implements the requirements specified in the <b>prompt.</b> Many software requirements include mathematical formulas that specify the expected behavior of the <b>code</b> <b>to</b> be generated. Given a <b>code</b> <b>generation</b> <b>prompt</b> that includes a mathematical formula, a reasonable expectation is that, if the formula is syntactically modified without changing its semantics, the generated <b>code</b> <b>for</b> the modified <b>prompt</b> should be semantically equivalent. We formalize this concept as syntactic robustness and investigate the syntactic robustness of <b>GPT-3.5-Turbo</b> and <b>GPT-4</b> as <b>code</b> <b>generators.</b> To test syntactic robustness, we generate syntactically different but semantically equivalent versions of <b>prompts</b> using a set of mutators that only modify mathematical formulas in <b>prompts.</b> In this paper, we focus on <b>prompts</b> that ask for <b>code</b> <b>that</b> generates solutions to variables in an equation, when given coefficients of the equation as input. Our experimental evaluation demonstrates that <b>GPT-3.5-Turbo</b> and <b>GPT-4</b> are not syntactically robust for this type of <b>prompts.</b> To improve syntactic robustness, we define a set of reductions that transform the formulas to a simplified form and use these reductions as a pre-processing step. Our experimental results indicate that the syntactic robustness of <b>LLM-based</b> <b>code</b> <b>generation</b> can be improved using our approach.</p></p class="citation"></blockquote><h3 id=45--150273-enabling-memory-safety-of-c-programs-using-llms-nausheen-mohammed-et-al-2024>(4/5 | 150/273) Enabling Memory Safety of C Programs using LLMs (Nausheen Mohammed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nausheen Mohammed, Akash Lal, Aseem Rastogi, Subhajit Roy, Rahul Sharma. (2024)<br><strong>Enabling Memory Safety of C Programs using LLMs</strong><br><button class=copy-to-clipboard title="Enabling Memory Safety of C Programs using LLMs" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-PL, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01096v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01096v1.pdf filename=2404.01096v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Memory safety violations in low-level code, written in languages like C, continues to remain one of the major sources of software vulnerabilities. One method of removing such violations by construction is to port C code to a safe C dialect. Such dialects rely on programmer-supplied annotations to guarantee safety with minimal runtime overhead. This porting, however, is a manual process that imposes significant burden on the programmer and, hence, there has been limited adoption of this technique. The task of porting not only requires inferring annotations, but may also need refactoring/rewriting of the code to make it amenable to such annotations. In this paper, we use <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> towards addressing both these concerns. We show how to harness <b>LLM</b> capabilities to do complex code <b>reasoning</b> as well as rewriting of <b>large</b> <b>codebases.</b> <b>We</b> also present a novel framework for whole-program transformations that leverages lightweight static analysis to break the transformation into smaller steps that can be carried out effectively by an <b>LLM.</b> We implement our ideas in a tool called MSA that targets the CheckedC dialect. We evaluate MSA on several micro-benchmarks, as well as real-world code ranging up to 20K lines of code. We showcase superior performance compared to a vanilla <b>LLM</b> baseline, as well as demonstrate improvement over a state-of-the-art symbolic (non-LLM) technique.</p></p class="citation"></blockquote><h3 id=55--151273-aurora-navigating-ui-tarpits-via-automated-neural-screen-understanding-safwat-ali-khan-et-al-2024>(5/5 | 151/273) AURORA: Navigating UI Tarpits via Automated Neural Screen Understanding (Safwat Ali Khan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Safwat Ali Khan, Wenyu Wang, Yiran Ren, Bin Zhu, Jiangfan Shi, Alyssa McGowan, Wing Lam, Kevin Moran. (2024)<br><strong>AURORA: Navigating UI Tarpits via Automated Neural Screen Understanding</strong><br><button class=copy-to-clipboard title="AURORA: Navigating UI Tarpits via Automated Neural Screen Understanding" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-CV, cs-HC, cs-SE, cs.SE<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01240v1.pdf filename=2404.01240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nearly a decade of research in software engineering has focused on automating mobile app testing to help engineers in overcoming the unique challenges associated with the software platform. Much of this work has come in the form of Automated Input Generation tools (AIG tools) that dynamically explore app screens. However, such tools have repeatedly been demonstrated to achieve lower-than-expected code coverage - particularly on sophisticated proprietary apps. Prior work has illustrated that a primary cause of these coverage deficiencies is related to so-called tarpits, or complex screens that are difficult to navigate. In this paper, we take a critical step toward enabling AIG tools to effectively navigate tarpits during app exploration through a new form of automated semantic screen understanding. We introduce AURORA, a technique that learns from the visual and textual patterns that exist in mobile app UIs to automatically detect common screen designs and navigate them accordingly. The key idea of AURORA is that there are a finite number of mobile app screen designs, albeit with subtle variations, such that the general patterns of different categories of UI designs can be learned. As such, AURORA employs a <b>multi-modal,</b> neural screen classifier that is able to recognize the most common types of UI screen designs. After recognizing a given screen, it then applies a set of flexible and generalizable heuristics to properly navigate the screen. We evaluated AURORA both on a set of 12 apps with known tarpits from prior work, and on a new set of five of the most popular apps from the Google Play store. Our results indicate that AURORA is able to effectively navigate tarpit screens, outperforming prior approaches that avoid tarpits by 19.6% in terms of method coverage. The improvements can be attributed to AURORA&rsquo;s UI design classification and heuristic navigation techniques.</p></p class="citation"></blockquote><h2 id=csai-3>cs.AI (3)</h2><h3 id=13--152273-isobench-benchmarking-multimodal-foundation-models-on-isomorphic-representations-deqing-fu-et-al-2024>(1/3 | 152/273) IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations (Deqing Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deqing Fu, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, Willie Neiswanger. (2024)<br><strong>IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations</strong><br><button class=copy-to-clipboard title="IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 82<br>Keywords: Benchmarking, Benchmarking, Foundation Model, Multi-modal, Multi-modal, Claude, GPT, GPT-4, GPT-4 turbo, Gemini, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01266v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01266v2.pdf filename=2404.01266v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current <b>foundation</b> <b>models</b> exhibit impressive capabilities when <b>prompted</b> either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose $\textbf{IsoBench}$, a <b>benchmark</b> dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple $\textbf{isomorphic representations}$ of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various <b>foundation</b> <b>models,</b> we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, <b>Claude-3</b> Opus performs 28.7 points worse when provided with images instead of text; similarly, <b>GPT-4</b> <b>Turbo</b> is 18.7 points worse and <b>Gemini</b> Pro is 14.9 points worse. Finally, we present two <b>prompting</b> techniques, $\textit{IsoCombination}$ and $\textit{IsoScratchPad}$, which improve model performance by considering combinations of, and translations between, different input representations.</p></p class="citation"></blockquote><h3 id=23--153273-some-orders-are-important-partially-preserving-orders-in-top-quality-planning-michael-katz-et-al-2024>(2/3 | 153/273) Some Orders Are Important: Partially Preserving Orders in Top-Quality Planning (Michael Katz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Katz, Junkyu Lee, Jungkoo Kang, Shirin Sohrabi. (2024)<br><strong>Some Orders Are Important: Partially Preserving Orders in Top-Quality Planning</strong><br><button class=copy-to-clipboard title="Some Orders Are Important: Partially Preserving Orders in Top-Quality Planning" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01503v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01503v1.pdf filename=2404.01503v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability to generate multiple plans is central to using planning in real-life applications. Top-quality planners generate sets of such top-cost plans, allowing flexibility in determining equivalent ones. In terms of the order between actions in a plan, the literature only considers two extremes &ndash; either all orders are important, making each plan unique, or all orders are unimportant, treating two plans differing only in the order of actions as equivalent. To allow flexibility in selecting important orders, we propose specifying a subset of actions the orders between which are important, interpolating between the top-quality and unordered top-quality planning problems. We explore the ways of adapting partial order reduction search <b>pruning</b> techniques to address this new computational problem and present experimental evaluations demonstrating the benefits of exploiting such techniques in this setting.</p></p class="citation"></blockquote><h3 id=33--154273-mtlight-efficient-multi-task-reinforcement-learning-for-traffic-signal-control-liwen-zhu-et-al-2024>(3/3 | 154/273) MTLight: Efficient Multi-Task Reinforcement Learning for Traffic Signal Control (Liwen Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liwen Zhu, Peixi Peng, Zongqing Lu, Yonghong Tian. (2024)<br><strong>MTLight: Efficient Multi-Task Reinforcement Learning for Traffic Signal Control</strong><br><button class=copy-to-clipboard title="MTLight: Efficient Multi-Task Reinforcement Learning for Traffic Signal Control" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00886v1.pdf filename=2404.00886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traffic signal control has a great impact on alleviating traffic congestion in modern cities. Deep <b>reinforcement</b> <b>learning</b> (RL) has been widely used for this task in recent years, demonstrating promising performance but also facing many challenges such as limited performances and sample inefficiency. To handle these challenges, MTLight is proposed to enhance the agent observation with a latent state, which is learned from numerous traffic indicators. Meanwhile, multiple auxiliary and supervisory tasks are constructed to learn the latent state, and two types of embedding latent features, the task-specific feature and task-shared feature, are used to make the latent state more abundant. Extensive experiments conducted on CityFlow demonstrate that MTLight has leading convergence speed and asymptotic performance. We further simulate under peak-hour pattern in all scenarios with increasing control difficulty and the results indicate that MTLight is highly adaptable.</p></p class="citation"></blockquote><h2 id=cslg-36>cs.LG (36)</h2><h3 id=136--155273-prompt-prompted-mixture-of-experts-for-efficient-llm-generation-harry-dong-et-al-2024>(1/36 | 155/273) Prompt-prompted Mixture of Experts for Efficient LLM Generation (Harry Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harry Dong, Beidi Chen, Yuejie Chi. (2024)<br><strong>Prompt-prompted Mixture of Experts for Efficient LLM Generation</strong><br><button class=copy-to-clipboard title="Prompt-prompted Mixture of Experts for Efficient LLM Generation" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Pruning, LLaMA, Transformer, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01365v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01365v2.pdf filename=2404.01365v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the development of <b>transformer-based</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as <b>pruning</b> or constructing a mixture of experts (MoE) aim at exploiting sparsity in <b>transformer</b> feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free MoE that selects unique FF experts at the sequence level for efficient generation across a plethora of <b>LLMs</b> with different non-ReLU activation functions. This is possible due to a critical observation that many trained <b>LLMs</b> naturally produce highly structured FF activation patterns within a sequence, which we call flocking. Despite our method&rsquo;s simplicity, we show with 50% of the FF parameters, GRIFFIN maintains the original model&rsquo;s performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.25$\times$ speed-up in <b>Llama</b> 2 13B on an NVIDIA L40). Code is available at <a href=https://github.com/hdong920/GRIFFIN>https://github.com/hdong920/GRIFFIN</a>.</p></p class="citation"></blockquote><h3 id=236--156273-new-logarithmic-step-size-for-stochastic-gradient-descent-m-soheil-shamaee-et-al-2024>(2/36 | 156/273) New logarithmic step size for stochastic gradient descent (M. Soheil Shamaee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>M. Soheil Shamaee, S. Fathi Hafshejani, Z. Saeidian. (2024)<br><strong>New logarithmic step size for stochastic gradient descent</strong><br><button class=copy-to-clipboard title="New logarithmic step size for stochastic gradient descent" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01257v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01257v1.pdf filename=2404.01257v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a novel warm restart technique using a new logarithmic step size for the <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> approach. For smooth and non-convex functions, we establish an $O(\frac{1}{\sqrt{T}})$ convergence rate for the <b>SGD.</b> We conduct a comprehensive implementation to demonstrate the efficiency of the newly proposed step size on the ~FashionMinst,~ CIFAR10, and CIFAR100 datasets. Moreover, we compare our results with nine other existing approaches and demonstrate that the new logarithmic step size improves test accuracy by $0.9%$ for the CIFAR100 dataset when we utilize a <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> model.</p></p class="citation"></blockquote><h3 id=336--157273-incorporating-domain-differential-equations-into-graph-convolutional-networks-to-lower-generalization-discrepancy-yue-sun-et-al-2024>(3/36 | 157/273) Incorporating Domain Differential Equations into Graph Convolutional Networks to Lower Generalization Discrepancy (Yue Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yue Sun, Chao Chen, Yuesheng Xu, Sihong Xie, Rick S. Blum, Parv Venkitasubramaniam. (2024)<br><strong>Incorporating Domain Differential Equations into Graph Convolutional Networks to Lower Generalization Discrepancy</strong><br><button class=copy-to-clipboard title="Incorporating Domain Differential Equations into Graph Convolutional Networks to Lower Generalization Discrepancy" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01217v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01217v1.pdf filename=2404.01217v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensuring both accuracy and robustness in time series prediction is critical to many applications, ranging from urban planning to pandemic management. With sufficient training data where all spatiotemporal patterns are well-represented, existing deep-learning models can make reasonably accurate predictions. However, existing methods fail when the training data are drawn from different circumstances (e.g., traffic patterns on regular days) compared to test data (e.g., traffic patterns after a natural disaster). Such challenges are usually classified under domain generalization. In this work, we show that one way to address this challenge in the context of spatiotemporal prediction is by incorporating domain differential equations into <b>Graph</b> <b>Convolutional</b> <b>Networks</b> <b>(GCNs).</b> We theoretically derive conditions where <b>GCNs</b> incorporating such domain differential equations are robust to mismatched training and testing data compared to baseline domain agnostic models. To support our theory, we propose two domain-differential-equation-informed networks called Reaction-Diffusion <b>Graph</b> <b>Convolutional</b> <b>Network</b> (RDGCN), which incorporates differential equations for traffic speed evolution, and Susceptible-Infectious-Recovered <b>Graph</b> <b>Convolutional</b> <b>Network</b> (SIRGCN), which incorporates a disease propagation model. Both RDGCN and SIRGCN are based on reliable and interpretable domain differential equations that allow the models to generalize to unseen patterns. We experimentally show that RDGCN and SIRGCN are more robust with mismatched testing data than the state-of-the-art deep learning methods.</p></p class="citation"></blockquote><h3 id=436--158273-decentralized-collaborative-learning-framework-with-external-privacy-leakage-analysis-tsuyoshi-idé-et-al-2024>(4/36 | 158/273) Decentralized Collaborative Learning Framework with External Privacy Leakage Analysis (Tsuyoshi Idé et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tsuyoshi Idé, Dzung T. Phan, Rudy Raymond. (2024)<br><strong>Decentralized Collaborative Learning Framework with External Privacy Leakage Analysis</strong><br><button class=copy-to-clipboard title="Decentralized Collaborative Learning Framework with External Privacy Leakage Analysis" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-DC, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Anomaly Detection, Autoencoder, Variational Autoencoder, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01270v1.pdf filename=2404.01270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents two methodological advancements in decentralized multi-task learning under privacy constraints, aiming to pave the way for future developments in next-generation Blockchain platforms. First, we expand the existing framework for collaborative dictionary learning (CollabDict), which has previously been limited to Gaussian mixture models, by incorporating deep <b>variational</b> <b>autoencoders</b> (VAEs) into the framework, with a particular focus on <b>anomaly</b> <b>detection.</b> We demonstrate that the VAE-based <b>anomaly</b> <b>score</b> function shares the same mathematical structure as the non-deep model, and provide comprehensive qualitative comparison. Second, considering the widespread use of &ldquo;pre-trained models,&rdquo; we provide a mathematical analysis on data privacy leakage when models trained with CollabDict are shared externally. We show that the CollabDict approach, when applied to Gaussian mixtures, adheres to a Renyi <b>differential</b> <b>privacy</b> criterion. Additionally, we propose a practical metric for monitoring internal privacy breaches during the learning process.</p></p class="citation"></blockquote><h3 id=536--159273-machine-unlearning-for-traditional-models-and-large-language-models-a-short-survey-yi-xu-2024>(5/36 | 159/273) Machine Unlearning for Traditional Models and Large Language Models: A Short Survey (Yi Xu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Xu. (2024)<br><strong>Machine Unlearning for Traditional Models and Large Language Models: A Short Survey</strong><br><button class=copy-to-clipboard title="Machine Unlearning for Traditional Models and Large Language Models: A Short Survey" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Machine Unlearning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01206v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01206v1.pdf filename=2404.01206v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the implementation of personal data privacy regulations, the field of <b>machine</b> <b>learning</b> (ML) faces the challenge of the &ldquo;right to be forgotten&rdquo;. <b>Machine</b> <b>unlearning</b> has emerged to address this issue, aiming to delete data and reduce its impact on models according to user requests. Despite the widespread interest in <b>machine</b> <b>unlearning,</b> comprehensive surveys on its latest advancements, especially in the field of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is lacking. This survey aims to fill this gap by providing an in-depth exploration of <b>machine</b> <b>unlearning,</b> including the definition, classification and evaluation criteria, as well as challenges in different environments and their solutions. Specifically, this paper categorizes and investigates unlearning on both traditional models and <b>LLMs,</b> and proposes methods for evaluating the effectiveness and efficiency of unlearning, and standards for performance measurement. This paper reveals the limitations of current unlearning techniques and emphasizes the importance of a comprehensive unlearning evaluation to avoid arbitrary forgetting. This survey not only <b>summarizes</b> the key concepts of unlearning technology but also points out its prominent issues and feasible directions for future research, providing valuable guidance for scholars in the field.</p></p class="citation"></blockquote><h3 id=636--160273-whats-in-your-safe-data-identifying-benign-data-that-breaks-safety-luxi-he-et-al-2024>(6/36 | 160/273) What&rsquo;s in Your &lsquo;Safe&rsquo; Data?: Identifying Benign Data that Breaks Safety (Luxi He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luxi He, Mengzhou Xia, Peter Henderson. (2024)<br><strong>What&rsquo;s in Your &lsquo;Safe&rsquo; Data?: Identifying Benign Data that Breaks Safety</strong><br><button class=copy-to-clipboard title="What's in Your 'Safe' Data?: Identifying Benign Data that Breaks Safety" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01099v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01099v1.pdf filename=2404.01099v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further <b>fine-tuning</b> an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign <b>fine-tuning</b> inadvertently contributes to jailbreaking. First, we represent <b>fine-tuning</b> data through two lenses: representation and gradient spaces. Furthermore, we propose a bi-directional anchoring method that prioritizes data points that are close to harmful examples and distant from benign ones. By doing so, our approach effectively identifies subsets of benign data that are more likely to degrade the model&rsquo;s safety after <b>fine-tuning.</b> Training on just 100 of these seemingly benign datapoints can lead to the <b>fine-tuned</b> model affirmatively responding to > 70% of tested harmful requests, compared to &lt; 20% after <b>fine-tuning</b> on randomly selected data. We further find that selected data are often in the form of lists and bullet points, or math questions.</p></p class="citation"></blockquote><h3 id=736--161273-machine-learning-robustness-a-primer-houssem-ben-braiek-et-al-2024>(7/36 | 161/273) Machine Learning Robustness: A Primer (Houssem Ben Braiek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Houssem Ben Braiek, Foutse Khomh. (2024)<br><strong>Machine Learning Robustness: A Primer</strong><br><button class=copy-to-clipboard title="Machine Learning Robustness: A Primer" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SE, cs.LG<br>Keyword Score: 40<br>Keywords: Adversarial Learning, Pruning, Transfer Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00897v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00897v1.pdf filename=2404.00897v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This chapter explores the foundational concept of robustness in Machine Learning (ML) and its integral role in establishing trustworthiness in Artificial Intelligence (AI) systems. The discussion begins with a detailed definition of robustness, portraying it as the ability of ML models to maintain stable performance across varied and unexpected environmental conditions. ML robustness is dissected through several lenses: its complementarity with generalizability; its status as a requirement for trustworthy AI; its <b>adversarial</b> <b>vs</b> non-adversarial aspects; its quantitative metrics; and its indicators such as reproducibility and explainability. The chapter delves into the factors that impede robustness, such as data bias, model complexity, and the pitfalls of underspecified ML pipelines. It surveys key techniques for robustness assessment from a broad perspective, including <b>adversarial</b> <b>attacks,</b> encompassing both digital and physical realms. It covers non-adversarial data shifts and nuances of Deep Learning (DL) software testing methodologies. The discussion progresses to explore amelioration strategies for bolstering robustness, starting with data-centric approaches like debiasing and augmentation. Further examination includes a variety of model-centric methods such as <b>transfer</b> <b>learning,</b> <b>adversarial</b> <b>training,</b> and randomized smoothing. Lastly, post-training methods are discussed, including ensemble techniques, <b>pruning,</b> and model repairs, emerging as cost-effective strategies to make models more resilient against the unpredictable. This chapter underscores the ongoing challenges and limitations in estimating and achieving ML robustness by existing approaches. It offers insights and directions for future research on this crucial concept, as a prerequisite for trustworthy AI systems.</p></p class="citation"></blockquote><h3 id=836--162273-lipsum-ft-robust-fine-tuning-of-zero-shot-models-using-random-text-guidance-giung-nam-et-al-2024>(8/36 | 162/273) Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance (Giung Nam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giung Nam, Byeongho Heo, Juho Lee. (2024)<br><strong>Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance</strong><br><button class=copy-to-clipboard title="Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Distribution Shift, Distribution Shift, Fine-tuning, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00860v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00860v1.pdf filename=2404.00860v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale contrastive <b>vision-language</b> pre-trained models provide the <b>zero-shot</b> model achieving competitive performance across a range of image classification tasks without requiring training on downstream data. Recent works have confirmed that while additional <b>fine-tuning</b> of the <b>zero-shot</b> model on the reference data results in enhanced downstream performance, it compromises the model&rsquo;s robustness against <b>distribution</b> <b>shifts.</b> Our investigation begins by examining the conditions required to achieve the goals of robust <b>fine-tuning,</b> employing descriptions based on feature distortion theory and joint energy-based models. Subsequently, we propose a novel robust <b>fine-tuning</b> algorithm, Lipsum-FT, that effectively utilizes the language modeling aspect of the <b>vision-language</b> pre-trained models. Extensive experiments conducted on <b>distribution</b> <b>shift</b> scenarios in DomainNet and ImageNet confirm the superiority of our proposed Lipsum-FT approach over existing robust <b>fine-tuning</b> methods.</p></p class="citation"></blockquote><h3 id=936--163273-addressing-heterogeneity-in-federated-load-forecasting-with-personalization-layers-shourya-bose-et-al-2024>(9/36 | 163/273) Addressing Heterogeneity in Federated Load Forecasting with Personalization Layers (Shourya Bose et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shourya Bose, Yu Zhang, Kibaek Kim. (2024)<br><strong>Addressing Heterogeneity in Federated Load Forecasting with Personalization Layers</strong><br><button class=copy-to-clipboard title="Addressing Heterogeneity in Federated Load Forecasting with Personalization Layers" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 30<br>Keywords: Federated Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01517v1.pdf filename=2404.01517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of smart meters has enabled pervasive collection of energy consumption data for training short-term load forecasting models. In response to privacy concerns, <b>federated</b> <b>learning</b> (FL) has been proposed as a privacy-preserving approach for training, but the quality of trained models degrades as client data becomes heterogeneous. In this paper we propose the use of personalization layers for load forecasting in a general framework called PL-FL. We show that PL-FL outperforms FL and purely local training, while requiring lower communication bandwidth than FL. This is done through extensive <b>simulations</b> on three different datasets from the NREL ComStock repository.</p></p class="citation"></blockquote><h3 id=1036--164273-are-large-language-models-superhuman-chemists-adrian-mirza-et-al-2024>(10/36 | 164/273) Are large language models superhuman chemists? (Adrian Mirza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Benedict Emoekabu, Aswanth Krishnan, Mara Wilhelmi, Macjonathan Okereke, Juliane Eberhardt, Amir Mohammad Elahi, Maximilian Greiner, Caroline T. Holick, Tanya Gupta, Mehrdad Asgari, Christina Glaubitz, Lea C. Klepsch, Yannik Köster, Jakob Meyer, Santiago Miret, Tim Hoffmann, Fabian Alexander Kreth, Michael Ringleb, Nicole Roesner, Ulrich S. Schubert, Leanne M. Stafast, Dinga Wonanke, Michael Pieler, Philippe Schwaller, Kevin Maik Jablonka. (2024)<br><strong>Are large language models superhuman chemists?</strong><br><button class=copy-to-clipboard title="Are large language models superhuman chemists?" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cond-mat-mtrl-sci, cs-AI, cs-LG, cs.LG, physics-chem-ph<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01475v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01475v1.pdf filename=2404.01475v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have gained widespread interest due to their ability to process human language and perform tasks on which they have not been explicitly trained. This is relevant for the chemical sciences, which face the problem of small and diverse datasets that are frequently in the form of text. <b>LLMs</b> have shown promise in addressing these issues and are increasingly being harnessed to predict chemical properties, optimize reactions, and even design and conduct experiments autonomously. However, we still have only a very limited systematic understanding of the chemical <b>reasoning</b> capabilities of <b>LLMs,</b> which would be required to improve models and mitigate potential harms. Here, we introduce &ldquo;ChemBench,&rdquo; an automated framework designed to rigorously evaluate the chemical knowledge and <b>reasoning</b> abilities of state-of-the-art <b>LLMs</b> against the expertise of human chemists. We curated more than 7,000 question-answer pairs for a wide array of subfields of the chemical sciences, evaluated leading open and closed-source <b>LLMs,</b> and found that the best models outperformed the best human chemists in our study on average. The models, however, struggle with some chemical <b>reasoning</b> tasks that are easy for human experts and provide overconfident, misleading predictions, such as about chemicals&rsquo; safety profiles. These findings underscore the dual reality that, although <b>LLMs</b> demonstrate remarkable proficiency in chemical tasks, further research is critical to enhancing their safety and utility in chemical sciences. Our findings also indicate a need for adaptations to chemistry curricula and highlight the importance of continuing to develop evaluation frameworks to improve safe and useful <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1136--165273-is-model-collapse-inevitable-breaking-the-curse-of-recursion-by-accumulating-real-and-synthetic-data-matthias-gerstgrasser-et-al-2024>(11/36 | 165/273) Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data (Matthias Gerstgrasser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthias Gerstgrasser, Rylan Schaeffer, Apratim Dey, Rafael Rafailov, Henry Sleight, John Hughes, Tomasz Korbak, Rajashree Agrawal, Dhruv Pai, Andrey Gromov, Daniel A. Roberts, Diyi Yang, David L. Donoho, Sanmi Koyejo. (2024)<br><strong>Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data</strong><br><button class=copy-to-clipboard title="Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-ET, cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Diffusion Model, Autoencoder, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01413v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01413v1.pdf filename=2404.01413v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs? Recent investigations into model-data feedback loops discovered that such loops can lead to model collapse, a phenomenon where performance progressively degrades with each model-fitting iteration until the latest model becomes useless. However, several recent papers studying model collapse assumed that new data replace old data over time rather than assuming data accumulate over time. In this paper, we compare these two settings and show that accumulating data prevents model collapse. We begin by studying an analytically tractable setup in which a sequence of linear models are fit to the previous models&rsquo; predictions. Previous work showed if data are replaced, the test error increases linearly with the number of model-fitting iterations; we extend this result by proving that if data instead accumulate, the test error has a finite upper bound independent of the number of iterations. We next empirically test whether accumulating data similarly prevents model collapse by pretraining sequences of language models on text corpora. We confirm that replacing data does indeed cause model collapse, then demonstrate that accumulating data prevents model collapse; these results hold across a range of model sizes, architectures and hyperparameters. We further show that similar results hold for other deep generative models on real data: <b>diffusion</b> <b>models</b> for molecule generation and <b>variational</b> <b>autoencoders</b> for image generation. Our work provides consistent theoretical and empirical evidence that data accumulation mitigates model collapse.</p></p class="citation"></blockquote><h3 id=1236--166273-efficiently-distilling-llms-for-edge-applications-achintya-kundu-et-al-2024>(12/36 | 166/273) Efficiently Distilling LLMs for Edge Applications (Achintya Kundu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Achintya Kundu, Fabian Lim, Aaron Chew, Laura Wynter, Penny Chong, Rhui Dih Lee. (2024)<br><strong>Efficiently Distilling LLMs for Edge Applications</strong><br><button class=copy-to-clipboard title="Efficiently Distilling LLMs for Edge Applications" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Knowledge Distillation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01353v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01353v1.pdf filename=2404.01353v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Supernet training of <b>LLMs</b> is of great interest in industrial applications as it confers the ability to produce a palette of smaller models at constant cost, regardless of the number of models (of different size / latency) produced. We propose a new method called Multistage Low-rank <b>Fine-tuning</b> of Super-transformers (MLFS) for parameter-efficient supernet training. We show that it is possible to obtain high-quality encoder models that are suitable for commercial edge applications, and that while decoder-only models are resistant to a comparable degree of compression, decoders can be effectively sliced for a significant reduction in training time.</p></p class="citation"></blockquote><h3 id=1336--167273-rethinking-the-relationship-between-recurrent-and-non-recurrent-neural-networks-a-study-in-sparsity-quincy-hershey-et-al-2024>(13/36 | 167/273) Rethinking the Relationship between Recurrent and Non-Recurrent Neural Networks: A Study in Sparsity (Quincy Hershey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quincy Hershey, Randy Paffenroth, Harsh Pathak, Simon Tavener. (2024)<br><strong>Rethinking the Relationship between Recurrent and Non-Recurrent Neural Networks: A Study in Sparsity</strong><br><button class=copy-to-clipboard title="Rethinking the Relationship between Recurrent and Non-Recurrent Neural Networks: A Study in Sparsity" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Recurrent Neural Network, Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00880v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00880v1.pdf filename=2404.00880v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural networks (NN) can be divided into two broad categories, <b>recurrent</b> <b>and</b> <b>non-recurrent.</b> Both types of neural networks are popular and extensively studied, but they are often treated as distinct families of machine learning algorithms. In this position paper, we argue that there is a closer relationship between these two types of neural networks than is normally appreciated. We show that many common neural network models, such as <b>Recurrent</b> <b>Neural</b> <b>Networks</b> <b>(RNN),</b> Multi-Layer Perceptrons (MLP), and even deep multi-layer <b>transformers,</b> can all be represented as iterative maps. The close relationship between <b>RNNs</b> and other types of NNs should not be surprising. In particular, <b>RNNs</b> are known to be Turing complete, and therefore capable of representing any computable function (such as any other types of NNs), but herein we argue that the relationship runs deeper and is more practical than this. For example, <b>RNNs</b> are often thought to be more difficult to train than other types of NNs, with <b>RNNs</b> being plagued by issues such as vanishing or exploding gradients. However, as we demonstrate in this paper, MLPs, <b>RNNs,</b> and many other NNs lie on a continuum, and this perspective leads to several insights that illuminate both theoretical and practical aspects of NNs.</p></p class="citation"></blockquote><h3 id=1436--168273-continual-learning-for-smart-city-a-survey-li-yang-et-al-2024>(14/36 | 168/273) Continual Learning for Smart City: A Survey (Li Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Yang, Zhipeng Luo, Shiming Zhang, Fei Teng, Tianrui Li. (2024)<br><strong>Continual Learning for Smart City: A Survey</strong><br><button class=copy-to-clipboard title="Continual Learning for Smart City: A Survey" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Graph, Continual Learning, Federated Learning, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00983v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00983v1.pdf filename=2404.00983v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the digitization of modern cities, large data volumes and powerful computational resources facilitate the rapid update of intelligent models deployed in smart cities. <b>Continual</b> <b>learning</b> (CL) is a novel machine learning paradigm that constantly updates models to adapt to changing environments, where the learning tasks, data, and distributions can vary over time. Our survey provides a comprehensive review of <b>continual</b> <b>learning</b> methods that are widely used in smart city development. The content consists of three parts: 1) Methodology-wise. We categorize a large number of basic CL methods and advanced CL frameworks in combination with other learning paradigms including <b>graph</b> learning, spatial-temporal learning, <b>multi-modal</b> learning, and <b>federated</b> <b>learning.</b> 2) Application-wise. We present numerous CL applications covering transportation, environment, public health, safety, networks, and associated datasets related to urban computing. 3) Challenges. We discuss current problems and challenges and envision several promising research directions. We believe this survey can help relevant researchers quickly familiarize themselves with the current state of <b>continual</b> <b>learning</b> research used in smart city development and direct them to future research trends.</p></p class="citation"></blockquote><h3 id=1536--169273-diffusion-driven-domain-adaptation-for-generating-3d-molecules-haokai-hong-et-al-2024>(15/36 | 169/273) Diffusion-Driven Domain Adaptation for Generating 3D Molecules (Haokai Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haokai Hong, Wanyu Lin, Kay Chen Tan. (2024)<br><strong>Diffusion-Driven Domain Adaptation for Generating 3D Molecules</strong><br><button class=copy-to-clipboard title="Diffusion-Driven Domain Adaptation for Generating 3D Molecules" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-chem-ph, q-bio-BM<br>Keyword Score: 26<br>Keywords: Autoencoder, Benchmarking, Benchmarking, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00962v1.pdf filename=2404.00962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Can we train a molecule generator that can generate 3D molecules from a new <b>domain,</b> <b>circumventing</b> the need to collect data? This problem can be cast as the problem of <b>domain</b> <b>adaptive</b> molecule generation. This work presents a novel and principled diffusion-based approach, called GADM, that allows shifting a generative model to desired new <b>domains</b> <b>without</b> the need to collect even a single molecule. As the <b>domain</b> <b>shift</b> is typically caused by the structure variations of molecules, e.g., scaffold variations, we leverage a designated equivariant masked <b>autoencoder</b> (MAE) along with various masking strategies to capture the structural-grained representations of the in-domain varieties. In particular, with an asymmetric encoder-decoder module, the MAE can generalize to unseen structure variations from the target <b>domains.</b> <b>These</b> structure variations are encoded with an equivariant encoder and treated as <b>domain</b> <b>supervisors</b> to control denoising. We show that, with these encoded structural-grained <b>domain</b> <b>supervisors,</b> GADM can generate effective molecules within the desired new <b>domains.</b> <b>We</b> conduct extensive experiments across various <b>domain</b> <b>adaptation</b> tasks over <b>benchmarking</b> datasets. We show that our approach can improve up to 65.6% in terms of success rate defined based on molecular validity, uniqueness, and novelty compared to alternative baselines.</p></p class="citation"></blockquote><h3 id=1636--170273-a-survey-on-hypergraph-neural-networks-an-in-depth-and-step-by-step-guide-sunwoo-kim-et-al-2024>(16/36 | 170/273) A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step Guide (Sunwoo Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sunwoo Kim, Soo Yong Lee, Yue Gao, Alessia Antelmi, Mirko Polato, Kijung Shin. (2024)<br><strong>A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step Guide</strong><br><button class=copy-to-clipboard title="A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step Guide" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Message-Passing, Recommendation, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01039v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01039v1.pdf filename=2404.01039v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Higher-order interactions (HOIs) are ubiquitous in real-world complex systems and applications, and thus investigation of deep learning for HOIs has become a valuable agenda for the data mining and machine learning communities. As networks of HOIs are expressed mathematically as hypergraphs, hypergraph neural networks (HNNs) have emerged as a powerful tool for <b>representation</b> <b>learning</b> on hypergraphs. Given the emerging trend, we present the first survey dedicated to HNNs, with an in-depth and step-by-step guide. Broadly, the present survey overviews HNN architectures, training strategies, and applications. First, we break existing HNNs down into four design components: (i) input features, (ii) input structures, (iii) <b>message-passing</b> schemes, and (iv) training strategies. Second, we examine how HNNs address and learn HOIs with each of their components. Third, we overview the recent applications of HNNs in <b>recommendation,</b> biological and medical science, time series analysis, and computer vision. Lastly, we conclude with a discussion on limitations and future directions.</p></p class="citation"></blockquote><h3 id=1736--171273-securing-social-spaces-harnessing-deep-learning-to-eradicate-cyberbullying-rohan-biswas-et-al-2024>(17/36 | 171/273) Securing Social Spaces: Harnessing Deep Learning to Eradicate Cyberbullying (Rohan Biswas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohan Biswas, Kasturi Ganguly, Arijit Das, Diganta Saha. (2024)<br><strong>Securing Social Spaces: Harnessing Deep Learning to Eradicate Cyberbullying</strong><br><button class=copy-to-clipboard title="Securing Social Spaces: Harnessing Deep Learning to Eradicate Cyberbullying" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: BERT, Hate Speech Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03686v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03686v1.pdf filename=2404.03686v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In today&rsquo;s digital world, cyberbullying is a serious problem that can harm the mental and physical health of people who use social media. This paper explains just how serious cyberbullying is and how it really affects indi-viduals exposed to it. It also stresses how important it is to find better ways to detect cyberbullying so that online spaces can be safer. Plus, it talks about how making more accurate tools to spot cyberbullying will be really helpful in the future. Our paper introduces a deep learning-based ap-proach, primarily employing <b>BERT</b> and BiLSTM architectures, to effective-ly address cyberbullying. This approach is designed to analyse large vol-umes of posts and predict potential instances of cyberbullying in online spaces. Our results demonstrate the superiority of the hateBERT model, an extension of <b>BERT</b> focused on <b>hate</b> <b>speech</b> <b>detection,</b> among the five mod-els, achieving an accuracy rate of 89.16%. This research is a significant con-tribution to &ldquo;Computational Intelligence for Social Transformation,&rdquo; prom-ising a safer and more inclusive digital landscape.</p></p class="citation"></blockquote><h3 id=1836--172273-can-llms-get-help-from-other-llms-without-revealing-private-information-florian-hartmann-et-al-2024>(18/36 | 172/273) Can LLMs get help from other LLMs without revealing private information? (Florian Hartmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian Hartmann, Duc-Hieu Tran, Peter Kairouz, Victor Cărbune, Blaise Aguera y Arcas. (2024)<br><strong>Can LLMs get help from other LLMs without revealing private information?</strong><br><button class=copy-to-clipboard title="Can LLMs get help from other LLMs without revealing private information?" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs-MA, cs.LG<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01041v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01041v2.pdf filename=2404.01041v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cascades are a common type of machine learning systems in which a <b>large,</b> <b>remote</b> <b>model</b> can be queried if a local model is not able to accurately label a user&rsquo;s data by itself. Serving stacks for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> increasingly use cascades due to their ability to preserve task performance while dramatically reducing inference costs. However, applying cascade systems in situations where the local model has access to sensitive data constitutes a significant privacy risk for users since such data could be forwarded to the remote model. In this work, we show the feasibility of applying cascade systems in such setups by equipping the local model with privacy-preserving techniques that reduce the risk of leaking private information when querying the remote model. To quantify information leakage in such setups, we introduce two privacy measures. We then propose a system that leverages the recently introduced social learning paradigm in which <b>LLMs</b> collaboratively learn from each other by exchanging natural language. Using this paradigm, we demonstrate on several datasets that our methods minimize the privacy loss while at the same time improving task performance compared to a non-cascade baseline.</p></p class="citation"></blockquote><h3 id=1936--173273-the-double-edged-sword-of-input-perturbations-to-robust-accurate-fairness-xuran-li-et-al-2024>(19/36 | 173/273) The Double-Edged Sword of Input Perturbations to Robust Accurate Fairness (Xuran Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuran Li, Peng Wu, Yanting Chen, Xingjun Ma, Zhen Zhang, Kaixiang Dong. (2024)<br><strong>The Double-Edged Sword of Input Perturbations to Robust Accurate Fairness</strong><br><button class=copy-to-clipboard title="The Double-Edged Sword of Input Perturbations to Robust Accurate Fairness" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fairness, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01356v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01356v1.pdf filename=2404.01356v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks (DNNs) are known to be sensitive to <b>adversarial</b> <b>input</b> perturbations, leading to a reduction in either prediction accuracy or individual <b>fairness.</b> To jointly characterize the susceptibility of prediction accuracy and individual <b>fairness</b> to <b>adversarial</b> <b>perturbations,</b> we introduce a novel robustness definition termed robust accurate <b>fairness.</b> Informally, robust accurate <b>fairness</b> requires that predictions for an instance and its similar counterparts consistently align with the ground truth when subjected to input perturbations. We propose an <b>adversarial</b> <b>attack</b> approach dubbed RAFair to expose false or biased <b>adversarial</b> <b>defects</b> in DNN, which either deceive accuracy or compromise individual <b>fairness.</b> Then, we show that such <b>adversarial</b> <b>instances</b> can be effectively addressed by carefully designed benign perturbations, correcting their predictions to be accurate and fair. Our work explores the double-edged sword of input perturbations to robust accurate <b>fairness</b> in DNN and the potential of using benign perturbations to correct <b>adversarial</b> <b>instances.</b></p></p class="citation"></blockquote><h3 id=2036--174273-make-continual-learning-stronger-via-c-flat-ang-bian-et-al-2024>(20/36 | 174/273) Make Continual Learning Stronger via C-Flat (Ang Bian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ang Bian, Wei Li, Hangjie Yuan, Chengrong Yu, Zixiang Zhao, Mang Wang, Aojun Lu, Tao Feng. (2024)<br><strong>Make Continual Learning Stronger via C-Flat</strong><br><button class=copy-to-clipboard title="Make Continual Learning Stronger via C-Flat" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Continual Learning, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00986v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00986v1.pdf filename=2404.00986v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model generalization ability upon incrementally acquiring dynamically updating knowledge from sequentially arriving tasks is crucial to tackle the sensitivity-stability dilemma in <b>Continual</b> <b>Learning</b> (CL). Weight loss landscape sharpness minimization seeking for flat minima lying in neighborhoods with uniform low loss or smooth gradient is proven to be a strong training regime improving model generalization compared with loss minimization based optimizer like <b>SGD.</b> Yet only a few works have discussed this training regime for CL, proving that dedicated designed zeroth-order sharpness optimizer can improve CL performance. In this work, we propose a <b>Continual</b> <b>Flatness</b> (C-Flat) method featuring a flatter loss landscape tailored for CL. C-Flat could be easily called with only one line of code and is plug-and-play to any CL methods. A general framework of C-Flat applied to all CL categories and a thorough comparison with loss minima optimizer and flat minima based CL approaches is presented in this paper, showing that our method can boost CL performance in almost all cases. Code will be publicly available upon publication.</p></p class="citation"></blockquote><h3 id=2136--175273-stream-of-search-sos-learning-to-search-in-language-kanishk-gandhi-et-al-2024>(21/36 | 175/273) Stream of Search (SoS): Learning to Search in Language (Kanishk Gandhi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, Noah D. Goodman. (2024)<br><strong>Stream of Search (SoS): Learning to Search in Language</strong><br><button class=copy-to-clipboard title="Stream of Search (SoS): Learning to Search in Language" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03683v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03683v1.pdf filename=2404.03683v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models are rarely shown fruitful mistakes while training. They then struggle to look beyond the next token, suffering from a snowballing of errors and struggling to predict the consequence of their actions several steps ahead. In this paper, we show how language models can be taught to search by representing the process of search in language, as a flattened string &ndash; a stream of search (SoS). We propose a unified language for search that captures an array of different symbolic search strategies. We demonstrate our approach using the simple yet difficult game of Countdown, where the goal is to combine input numbers with arithmetic operations to reach a target number. We pretrain a <b>transformer-based</b> language model from scratch on a dataset of streams of search generated by heuristic solvers. We find that SoS pretraining increases search accuracy by 25% over models trained to predict only the optimal search trajectory. We further <b>finetune</b> this model with two policy improvement methods: Advantage-Induced Policy Alignment (APA) and Self-Taught Reasoner (STaR). The <b>finetuned</b> SoS models solve 36% of previously unsolved problems, including problems that cannot be solved by any of the heuristic solvers. Our results indicate that language models can learn to solve problems via search, self-improve to flexibly use different search strategies, and potentially discover new ones.</p></p class="citation"></blockquote><h3 id=2236--176273-caap-class-dependent-automatic-data-augmentation-based-on-adaptive-policies-for-time-series-tien-yu-chang-et-al-2024>(22/36 | 176/273) CAAP: Class-Dependent Automatic Data Augmentation Based On Adaptive Policies For Time Series (Tien-Yu Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tien-Yu Chang, Hao Dai, Vincent S. Tseng. (2024)<br><strong>CAAP: Class-Dependent Automatic Data Augmentation Based On Adaptive Policies For Time Series</strong><br><button class=copy-to-clipboard title="CAAP: Class-Dependent Automatic Data Augmentation Based On Adaptive Policies For Time Series" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Data Augmentation, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00898v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00898v1.pdf filename=2404.00898v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Data</b> <b>Augmentation</b> is a common technique used to enhance the performance of deep learning models by expanding the training dataset. Automatic <b>Data</b> <b>Augmentation</b> (ADA) methods are getting popular because of their capacity to generate policies for various datasets. However, existing ADA methods primarily focused on overall performance improvement, neglecting the problem of class-dependent bias that leads to performance reduction in specific classes. This bias poses significant challenges when deploying models in real-world applications. Furthermore, ADA for time series remains an underexplored domain, highlighting the need for advancements in this field. In particular, applying ADA techniques to vital signals like an electrocardiogram (ECG) is a compelling example due to its potential in medical domains such as heart disease diagnostics. We propose a novel deep learning-based approach called Class-dependent Automatic Adaptive Policies (CAAP) framework to overcome the notable class-dependent bias problem while maintaining the overall improvement in time-series <b>data</b> <b>augmentation.</b> Specifically, we utilize the policy network to generate effective sample-wise policies with balanced difficulty through class and feature <b>information</b> <b>extraction.</b> Second, we design the augmentation probability regulation method to minimize class-dependent bias. Third, we introduce the <b>information</b> <b>region</b> concepts into the ADA framework to preserve essential regions in the sample. Through a series of experiments on real-world ECG datasets, we demonstrate that CAAP outperforms representative methods in achieving lower class-dependent bias combined with superior overall performance. These results highlight the reliability of CAAP as a promising ADA method for time series modeling that fits for the demands of real-world applications.</p></p class="citation"></blockquote><h3 id=2336--177273-ts-causalnn-learning-temporal-causal-relations-from-non-linear-non-stationary-time-series-data-omar-faruque-et-al-2024>(23/36 | 177/273) TS-CausalNN: Learning Temporal Causal Relations from Non-linear Non-stationary Time Series Data (Omar Faruque et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omar Faruque, Sahara Ali, Xue Zheng, Jianwu Wang. (2024)<br><strong>TS-CausalNN: Learning Temporal Causal Relations from Non-linear Non-stationary Time Series Data</strong><br><button class=copy-to-clipboard title="TS-CausalNN: Learning Temporal Causal Relations from Non-linear Non-stationary Time Series Data" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ME<br>Keyword Score: 13<br>Keywords: Graph, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01466v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01466v1.pdf filename=2404.01466v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing availability and importance of time series data across various domains, including environmental science, epidemiology, and economics, has led to an increasing need for time-series causal discovery methods that can identify the intricate relationships in the non-stationary, non-linear, and often noisy real world data. However, the majority of current time series causal discovery methods assume stationarity and linear relations in data, making them infeasible for the task. Further, the recent deep learning-based methods rely on the traditional causal structure learning approaches making them computationally expensive. In this paper, we propose a Time-Series Causal Neural Network (TS-CausalNN) - a deep learning technique to discover contemporaneous and lagged causal relations simultaneously. Our proposed architecture comprises (i) <b>convolutional</b> blocks comprising parallel custom causal layers, (ii) acyclicity constraint, and (iii) optimization techniques using the augmented Lagrangian approach. In addition to the simple parallel design, an advantage of the proposed model is that it naturally handles the non-stationarity and non-linearity of the data. Through experiments on multiple synthetic and real world datasets, we demonstrate the empirical proficiency of our proposed approach as compared to several state-of-the-art methods. The inferred <b>graphs</b> for the real world dataset are in good agreement with the domain understanding.</p></p class="citation"></blockquote><h3 id=2436--178273-novel-node-category-detection-under-subpopulation-shift-hsing-huan-chung-et-al-2024>(24/36 | 178/273) Novel Node Category Detection Under Subpopulation Shift (Hsing-Huan Chung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hsing-Huan Chung, Shravan Chaudhari, Yoav Wald, Xing Han, Joydeep Ghosh. (2024)<br><strong>Novel Node Category Detection Under Subpopulation Shift</strong><br><button class=copy-to-clipboard title="Novel Node Category Detection Under Subpopulation Shift" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG, stat-ML<br>Keyword Score: 13<br>Keywords: Graph, Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01216v1.pdf filename=2404.01216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In real-world <b>graph</b> data, <b>distribution</b> <b>shifts</b> can manifest in various ways, such as the emergence of new categories and changes in the relative proportions of existing categories. It is often important to detect nodes of novel categories under such <b>distribution</b> <b>shifts</b> for safety or insight discovery purposes. We introduce a new approach, Recall-Constrained Optimization with Selective Link Prediction (RECO-SLIP), to detect nodes belonging to novel categories in attributed <b>graphs</b> under subpopulation shifts. By integrating a recall-constrained learning framework with a sample-efficient link prediction mechanism, RECO-SLIP addresses the dual challenges of resilience against subpopulation shifts and the effective exploitation of <b>graph</b> structure. Our extensive empirical evaluation across multiple <b>graph</b> datasets demonstrates the superior performance of RECO-SLIP over existing methods.</p></p class="citation"></blockquote><h3 id=2536--179273-explainable-ai-integrated-feature-engineering-for-wildfire-prediction-di-fan-et-al-2024>(25/36 | 179/273) Explainable AI Integrated Feature Engineering for Wildfire Prediction (Di Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Di Fan, Ayan Biswas, James Paul Ahrens. (2024)<br><strong>Explainable AI Integrated Feature Engineering for Wildfire Prediction</strong><br><button class=copy-to-clipboard title="Explainable AI Integrated Feature Engineering for Wildfire Prediction" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01487v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01487v1.pdf filename=2404.01487v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wildfires present intricate challenges for prediction, necessitating the use of sophisticated machine learning techniques for effective modeling\cite{jain2020review}. In our research, we conducted a thorough assessment of various machine learning algorithms for both classification and regression tasks relevant to predicting wildfires. We found that for classifying different types or stages of wildfires, the XGBoost model outperformed others in terms of accuracy and robustness. Meanwhile, the Random Forest regression model showed superior results in predicting the extent of wildfire-affected areas, excelling in both prediction error and explained variance. Additionally, we developed a hybrid neural network model that integrates numerical data and image information for simultaneous classification and regression. To gain deeper insights into the decision-making processes of these models and identify key contributing features, we utilized <b>eXplainable</b> <b>Artificial</b> Intelligence (XAI) techniques, including TreeSHAP, LIME, Partial Dependence Plots (PDP), and Gradient-weighted Class Activation Mapping (Grad-CAM). These interpretability tools shed light on the significance and interplay of various features, highlighting the complex factors influencing wildfire predictions. Our study not only demonstrates the effectiveness of specific machine learning models in wildfire-related tasks but also underscores the critical role of model transparency and interpretability in environmental science applications.</p></p class="citation"></blockquote><h3 id=2636--180273-drive-dual-gradient-based-rapid-iterative-pruning-dhananjay-saikumar-et-al-2024>(26/36 | 180/273) DRIVE: Dual Gradient-Based Rapid Iterative Pruning (Dhananjay Saikumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dhananjay Saikumar, Blesson Varghese. (2024)<br><strong>DRIVE: Dual Gradient-Based Rapid Iterative Pruning</strong><br><button class=copy-to-clipboard title="DRIVE: Dual Gradient-Based Rapid Iterative Pruning" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03687v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03687v1.pdf filename=2404.03687v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern deep neural networks (DNNs) consist of millions of parameters, necessitating high-performance computing during training and inference. <b>Pruning</b> is one solution that significantly reduces the space and time complexities of DNNs. Traditional <b>pruning</b> methods that are applied post-training focus on streamlining inference, but there are recent efforts to leverage sparsity early on by <b>pruning</b> before training. <b>Pruning</b> methods, such as iterative magnitude-based <b>pruning</b> (IMP) achieve up to a 90% parameter reduction while retaining accuracy comparable to the original model. However, this leads to impractical runtime as it relies on multiple train-prune-reset cycles to identify and eliminate redundant parameters. In contrast, training agnostic early <b>pruning</b> methods, such as SNIP and SynFlow offer fast <b>pruning</b> but fall short of the accuracy achieved by IMP at high sparsities. To bridge this gap, we present Dual Gradient-Based Rapid Iterative <b>Pruning</b> (DRIVE), which leverages dense training for initial epochs to counteract the randomness inherent at the initialization. Subsequently, it employs a unique dual gradient-based metric for parameter ranking. It has been experimentally demonstrated for VGG and ResNet architectures on CIFAR-10/100 and Tiny ImageNet, and ResNet on ImageNet that DRIVE consistently has superior performance over other training-agnostic early <b>pruning</b> methods in accuracy. Notably, DRIVE is 43$\times$ to 869$\times$ faster than IMP for <b>pruning.</b></p></p class="citation"></blockquote><h3 id=2736--181273-openchemie-an-information-extraction-toolkit-for-chemistry-literature-vincent-fan-et-al-2024>(27/36 | 181/273) OpenChemIE: An Information Extraction Toolkit For Chemistry Literature (Vincent Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vincent Fan, Yujie Qian, Alex Wang, Amber Wang, Connor W. Coley, Regina Barzilay. (2024)<br><strong>OpenChemIE: An Information Extraction Toolkit For Chemistry Literature</strong><br><button class=copy-to-clipboard title="OpenChemIE: An Information Extraction Toolkit For Chemistry Literature" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-IR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01462v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01462v1.pdf filename=2404.01462v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Information</b> <b>extraction</b> from chemistry literature is vital for constructing up-to-date reaction databases for data-driven chemistry. Complete extraction requires combining <b>information</b> <b>across</b> text, tables, and figures, whereas prior work has mainly investigated extracting reactions from single modalities. In this paper, we present OpenChemIE to address this complex challenge and enable the extraction of reaction data at the document level. OpenChemIE approaches the problem in two steps: extracting relevant <b>information</b> <b>from</b> individual modalities and then integrating the results to obtain a final list of reactions. For the first step, we employ specialized neural models that each address a specific task for chemistry <b>information</b> <b>extraction,</b> such as parsing molecules or reactions from text or figures. We then integrate the <b>information</b> <b>from</b> these modules using chemistry-informed algorithms, allowing for the extraction of fine-grained reaction data from reaction condition and substrate scope investigations. Our machine learning models attain state-of-the-art performance when evaluated individually, and we meticulously annotate a challenging dataset of reaction schemes with R-groups to evaluate our pipeline as a whole, achieving an F1 score of 69.5%. Additionally, the reaction extraction results of \ours attain an accuracy score of 64.3% when directly compared against the Reaxys chemical database. We provide OpenChemIE freely to the public as an open-source package, as well as through a web interface.</p></p class="citation"></blockquote><h3 id=2836--182273-twin-gpt-digital-twins-for-clinical-trials-via-large-language-model-yue-wang-et-al-2024>(28/36 | 182/273) TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model (Yue Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yue Wang, Yingzhou Lu, Yinlong Xu, Zihan Ma, Hongxia Xu, Bang Du, Honghao Gao, Jian Wu. (2024)<br><strong>TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model</strong><br><button class=copy-to-clipboard title="TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG, stat-ME<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01273v1.pdf filename=2404.01273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been a burgeoning interest in virtual clinical trials, which simulate real-world scenarios and hold the potential to significantly enhance patient safety, expedite development, reduce costs, and contribute to the broader scientific knowledge in healthcare. Existing research often focuses on leveraging electronic health records (EHRs) to support clinical trial outcome prediction. Yet, trained with limited clinical trial outcome data, existing approaches frequently struggle to perform accurate predictions. Some research has attempted to generate EHRs to augment model development but has fallen short in personalizing the generation for individual patient profiles. Recently, the emergence of <b>large</b> <b>language</b> <b>models</b> has illuminated new possibilities, as their embedded comprehensive clinical knowledge has proven beneficial in addressing medical issues. In this paper, we propose a <b>large</b> <b>language</b> <b>model-based</b> digital twin creation approach, called TWIN-GPT. TWIN-GPT can establish cross-dataset associations of medical information given limited data, generating unique personalized digital twins for different patients, thereby preserving individual patient characteristics. Comprehensive experiments show that using digital twins created by TWIN-GPT can boost clinical trial outcome prediction, exceeding various previous prediction approaches. Besides, we also demonstrate that TWIN-GPT can generate high-fidelity trial data that closely approximate specific patients, aiding in more accurate result predictions in data-scarce situations. Moreover, our study provides practical evidence for the application of digital twins in healthcare, highlighting its potential significance.</p></p class="citation"></blockquote><h3 id=2936--183273-information-plane-analysis-visualization-in-deep-learning-via-transfer-entropy-adrian-moldovan-et-al-2024>(29/36 | 183/273) Information Plane Analysis Visualization in Deep Learning via Transfer Entropy (Adrian Moldovan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adrian Moldovan, Angel Cataron, Razvan Andonie. (2024)<br><strong>Information Plane Analysis Visualization in Deep Learning via Transfer Entropy</strong><br><button class=copy-to-clipboard title="Information Plane Analysis Visualization in Deep Learning via Transfer Entropy" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-HC, cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01364v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01364v1.pdf filename=2404.01364v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In a feedforward network, Transfer Entropy (TE) can be used to measure the influence that one layer has on another by quantifying the information transfer between them during training. According to the Information Bottleneck principle, a neural model&rsquo;s internal representation should compress the input data as much as possible while still retaining sufficient information about the output. Information Plane analysis is a visualization technique used to understand the trade-off between compression and information preservation in the context of the Information Bottleneck method by plotting the amount of information in the input data against the compressed representation. The claim that there is a causal link between information-theoretic compression and generalization, measured by <b>mutual</b> <b>information,</b> is plausible, but results from different studies are conflicting. In contrast to <b>mutual</b> <b>information,</b> TE can capture temporal relationships between variables. To explore such links, in our novel approach we use TE to quantify information transfer between neural layers and perform Information Plane analysis. We obtained encouraging experimental results, opening the possibility for further investigations.</p></p class="citation"></blockquote><h3 id=3036--184273-nearly-tight-approximation-guarantees-for-the-improving-multi-armed-bandits-problem-avrim-blum-et-al-2024>(30/36 | 184/273) Nearly-tight Approximation Guarantees for the Improving Multi-Armed Bandits Problem (Avrim Blum et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Avrim Blum, Kavya Ravichandran. (2024)<br><strong>Nearly-tight Approximation Guarantees for the Improving Multi-Armed Bandits Problem</strong><br><button class=copy-to-clipboard title="Nearly-tight Approximation Guarantees for the Improving Multi-Armed Bandits Problem" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DS, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01198v1.pdf filename=2404.01198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We give nearly-tight upper and lower bounds for the improving multi-armed <b>bandits</b> problem. An instance of this problem has $k$ arms, each of whose reward function is a concave and increasing function of the number of times that arm has been pulled so far. We show that for any randomized online algorithm, there exists an instance on which it must suffer at least an $\Omega(\sqrt{k})$ approximation factor relative to the optimal reward. We then provide a randomized online algorithm that guarantees an $O(\sqrt{k})$ approximation factor, if it is told the maximum reward achievable by the optimal arm in advance. We then show how to remove this assumption at the cost of an extra $O(\log k)$ approximation factor, achieving an overall $O(\sqrt{k} \log k)$ approximation relative to optimal.</p></p class="citation"></blockquote><h3 id=3136--185273-sok-a-review-of-differentially-private-linear-models-for-high-dimensional-data-amol-khanna-et-al-2024>(31/36 | 185/273) SoK: A Review of Differentially Private Linear Models For High-Dimensional Data (Amol Khanna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amol Khanna, Edward Raff, Nathan Inkawhich. (2024)<br><strong>SoK: A Review of Differentially Private Linear Models For High-Dimensional Data</strong><br><button class=copy-to-clipboard title="SoK: A Review of Differentially Private Linear Models For High-Dimensional Data" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2, cs-CR, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01141v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01141v1.pdf filename=2404.01141v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Linear models are ubiquitous in data science, but are particularly prone to overfitting and data memorization in high dimensions. To guarantee the privacy of training data, <b>differential</b> <b>privacy</b> can be used. Many papers have proposed optimization techniques for high-dimensional differentially private linear models, but a systematic comparison between these methods does not exist. We close this gap by providing a comprehensive review of optimization methods for private high-dimensional linear models. Empirical tests on all methods demonstrate robust and coordinate-optimized algorithms perform best, which can inform future research. Code for implementing all methods is released online.</p></p class="citation"></blockquote><h3 id=3236--186273-energy-model-based-accurate-shapley-value-estimation-for-interpretable-deep-learning-predictive-modelling-cheng-lu-et-al-2024>(32/36 | 186/273) Energy Model-based Accurate Shapley Value Estimation for Interpretable Deep Learning Predictive Modelling (Cheng Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Lu, Jiusun Zeng, Yu Xia, Jinhui Cai, Shihua Luo. (2024)<br><strong>Energy Model-based Accurate Shapley Value Estimation for Interpretable Deep Learning Predictive Modelling</strong><br><button class=copy-to-clipboard title="Energy Model-based Accurate Shapley Value Estimation for Interpretable Deep Learning Predictive Modelling" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Graph Attention Networks<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01078v1.pdf filename=2404.01078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a favorable tool for explainable artificial intelligence (XAI), Shapley value has been widely used to interpret deep learning based predictive models. However, accurate and efficient estimation of Shapley value is a difficult task since the computation load grows exponentially with the increase of input features. Most existing accelerated Shapley value estimation methods have to compromise on estimation accuracy with efficiency. In this article, we present EmSHAP(Energy model-based Shapley value estimation), which can effectively approximate the expectation of Shapley contribution function/deep learning model under arbitrary subset of features given the rest. In order to determine the proposal conditional distribution in the energy model, a <b>gated</b> recurrent unit(GRU) is introduced by mapping the input features onto a hidden space, so that the impact of input feature orderings can be eliminated. In addition, a dynamic masking scheme is proposed to improve the generalization ability. It is proved in Theorems 1, 2 and 3 that EmSHAP achieves tighter error bound than state-of-the-art methods like KernelSHAP and VAEAC, leading to higher estimation accuracy. Finally, case studies on a medical application and an industrial application show that the proposed Shapley value-based explainable framework exhibits enhanced estimation accuracy without compromise on efficiency.</p></p class="citation"></blockquote><h3 id=3336--187273-aetta-label-free-accuracy-estimation-for-test-time-adaptation-taeckyung-lee-et-al-2024>(33/36 | 187/273) AETTA: Label-Free Accuracy Estimation for Test-Time Adaptation (Taeckyung Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taeckyung Lee, Sorn Chottananurak, Taesik Gong, Sung-Ju Lee. (2024)<br><strong>AETTA: Label-Free Accuracy Estimation for Test-Time Adaptation</strong><br><button class=copy-to-clipboard title="AETTA: Label-Free Accuracy Estimation for Test-Time Adaptation" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01351v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01351v1.pdf filename=2404.01351v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Test-time adaptation (TTA) has emerged as a viable solution to adapt pre-trained models to domain shifts using unlabeled test data. However, TTA faces challenges of adaptation failures due to its reliance on blind adaptation to unknown test samples in dynamic scenarios. Traditional methods for <b>out-of-distribution</b> performance estimation are limited by unrealistic assumptions in the TTA context, such as requiring labeled data or re-training models. To address this issue, we propose AETTA, a label-free accuracy estimation algorithm for TTA. We propose the prediction disagreement as the accuracy estimate, calculated by comparing the target model prediction with dropout inferences. We then improve the prediction disagreement to extend the applicability of AETTA under adaptation failures. Our extensive evaluation with four baselines and six TTA methods demonstrates that AETTA shows an average of 19.8%p more accurate estimation compared with the baselines. We further demonstrate the effectiveness of accuracy estimation with a model recovery case study, showcasing the practicality of our model recovery based on accuracy estimation. The source code is available at <a href=https://github.com/taeckyung/AETTA>https://github.com/taeckyung/AETTA</a>.</p></p class="citation"></blockquote><h3 id=3436--188273-modeling-output-level-task-relatedness-in-multi-task-learning-with-feedback-mechanism-xiangming-xi-et-al-2024>(34/36 | 188/273) Modeling Output-Level Task Relatedness in Multi-Task Learning with Feedback Mechanism (Xiangming Xi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangming Xi, Feng Gao, Jun Xu, Fangtai Guo, Tianlei Jin. (2024)<br><strong>Modeling Output-Level Task Relatedness in Multi-Task Learning with Feedback Mechanism</strong><br><button class=copy-to-clipboard title="Modeling Output-Level Task Relatedness in Multi-Task Learning with Feedback Mechanism" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Graph Attention Networks<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00885v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00885v1.pdf filename=2404.00885v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-task learning (MTL) is a paradigm that simultaneously learns multiple tasks by sharing information at different levels, enhancing the performance of each individual task. While previous research has primarily focused on feature-level or parameter-level task relatedness, and proposed various model architectures and learning algorithms to improve learning performance, we aim to explore output-level task relatedness. This approach introduces a posteriori information into the model, considering that different tasks may produce correlated outputs with mutual influences. We achieve this by incorporating a feedback mechanism into MTL models, where the output of one task serves as a hidden feature for another task, thereby transforming a static MTL model into a dynamic one. To ensure the training process converges, we introduce a convergence loss that measures the trend of a task&rsquo;s outputs during each iteration. Additionally, we propose a Gumbel <b>gating</b> mechanism to determine the optimal projection of feedback signals. We validate the effectiveness of our method and evaluate its performance through experiments conducted on several baseline models in spoken language understanding.</p></p class="citation"></blockquote><h3 id=3536--189273-do-language-models-plan-ahead-for-future-tokens-wilson-wu-et-al-2024>(35/36 | 189/273) Do language models plan ahead for future tokens? (Wilson Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wilson Wu, John X. Morris, Lionel Levine. (2024)<br><strong>Do language models plan ahead for future tokens?</strong><br><button class=copy-to-clipboard title="Do language models plan ahead for future tokens?" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00859v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00859v1.pdf filename=2404.00859v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Do <b>transformers</b> &ldquo;think ahead&rdquo; during inference at a given position? It is known <b>transformers</b> prepare information in the hidden states of the forward pass at $t$ that is then used in future forward passes $t+\tau$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present in training result in the model computing features at $t$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step $t$ are already the same as those that would most benefit inference at time $t+\tau$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis.</p></p class="citation"></blockquote><h3 id=3636--190273-interpretable-multi-view-clustering-based-on-anchor-graph-tensor-factorization-jing-li-et-al-2024>(36/36 | 190/273) Interpretable Multi-View Clustering Based on Anchor Graph Tensor Factorization (Jing Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Li, Quanxue Gao, Cheng Deng, Qianqian Wang, Ming Yang. (2024)<br><strong>Interpretable Multi-View Clustering Based on Anchor Graph Tensor Factorization</strong><br><button class=copy-to-clipboard title="Interpretable Multi-View Clustering Based on Anchor Graph Tensor Factorization" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Graph, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00883v1.pdf filename=2404.00883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>clustering</b> method based on the anchor <b>graph</b> has gained significant attention due to its exceptional <b>clustering</b> performance and ability to process large-scale data. One common approach is to learn bipartite <b>graphs</b> with K-connected components, helping avoid the need for post-processing. However, this method has strict parameter requirements and may not always get K-connected components. To address this issue, an alternative approach is to directly obtain the cluster label matrix by performing non-negative matrix factorization (NMF) on the anchor <b>graph.</b> Nevertheless, existing multi-view <b>clustering</b> methods based on anchor <b>graph</b> factorization lack adequate cluster interpretability for the decomposed matrix and often overlook the inter-view information. We address this limitation by using non-negative tensor factorization to decompose an anchor <b>graph</b> tensor that combines anchor <b>graphs</b> from multiple views. This approach allows us to consider inter-view information comprehensively. The decomposed tensors, namely the sample indicator tensor and the anchor indicator tensor, enhance the interpretability of the factorization. Extensive experiments validate the effectiveness of this method.</p></p class="citation"></blockquote><h2 id=eessiv-4>eess.IV (4)</h2><h3 id=14--191273-diffusion-based-zero-shot-medical-image-to-image-translation-for-cross-modality-segmentation-zihao-wang-et-al-2024>(1/4 | 191/273) Diffusion based Zero-shot Medical Image-to-Image Translation for Cross Modality Segmentation (Zihao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihao Wang, Yingyu Yang, Yuzhou Chen, Tingting Yuan, Maxime Sermesant, Herve Delingette. (2024)<br><strong>Diffusion based Zero-shot Medical Image-to-Image Translation for Cross Modality Segmentation</strong><br><button class=copy-to-clipboard title="Diffusion based Zero-shot Medical Image-to-Image Translation for Cross Modality Segmentation" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 60<br>Keywords: Supervised Learning, Supervised Learning, Unsupervised Learning, Zero-shot, Image2text, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01102v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01102v1.pdf filename=2404.01102v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-modality <b>image</b> <b>segmentation</b> aims to segment the target modalities using a method designed in the source modality. Deep generative models can translate the target modality <b>images</b> <b>into</b> the source modality, thus enabling cross-modality segmentation. However, a vast body of existing cross-modality <b>image</b> <b>translation</b> methods relies on <b>supervised</b> <b>learning.</b> In this work, we aim to address the challenge of <b>zero-shot</b> <b>learning-based</b> <b>image</b> <b>translation</b> tasks (extreme scenarios in the target modality is unseen in the training phase). To leverage generative learning for <b>zero-shot</b> <b>cross-modality</b> <b>image</b> <b>segmentation,</b> we propose a novel <b>unsupervised</b> <b>image</b> <b>translation</b> method. The framework learns to translate the unseen source <b>image</b> <b>to</b> the target modality for <b>image</b> <b>segmentation</b> by leveraging the inherent statistical consistency between different modalities for diffusion guidance. Our framework captures identical cross-modality features in the statistical domain, offering diffusion guidance without relying on direct mappings between the source and target domains. This advantage allows our method to adapt to changing source domains without the need for retraining, making it highly practical when sufficient labeled source domain data is not available. The proposed framework is validated in <b>zero-shot</b> <b>cross-modality</b> <b>image</b> <b>segmentation</b> tasks through empirical comparisons with influential generative models, including adversarial-based and diffusion-based models.</p></p class="citation"></blockquote><h3 id=24--192273-harnessing-data-and-physics-for-deep-learning-phase-recovery-kaiqiang-wang-et-al-2024>(2/4 | 192/273) Harnessing Data and Physics for Deep Learning Phase Recovery (Kaiqiang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiqiang Wang, Edmund Y. Lam. (2024)<br><strong>Harnessing Data and Physics for Deep Learning Phase Recovery</strong><br><button class=copy-to-clipboard title="Harnessing Data and Physics for Deep Learning Phase Recovery" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, eess-IV, eess.IV, physics-optics<br>Keyword Score: 40<br>Keywords: Self-supervised Learning, Self-supervised Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01360v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01360v1.pdf filename=2404.01360v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Phase recovery, calculating the phase of a light wave from its intensity measurements, is essential for various applications, such as coherent diffraction imaging, adaptive optics, and biomedical imaging. It enables the reconstruction of an object&rsquo;s refractive index distribution or topography as well as the correction of imaging system aberrations. In recent years, deep learning has been proven to be highly effective in addressing phase recovery problems. Two main deep learning phase recovery strategies are data-driven (DD) with <b>supervised</b> <b>learning</b> mode and physics-driven (PD) with <b>self-supervised</b> <b>learning</b> mode. DD and PD achieve the same goal in different ways and lack the necessary study to reveal similarities and differences. Therefore, in this paper, we comprehensively compare these two deep learning phase recovery strategies in terms of time consumption, accuracy, generalization ability, ill-posedness adaptability, and prior capacity. What&rsquo;s more, we propose a co-driven (CD) strategy of combining datasets and physics for the balance of high- and low-frequency information. The codes for DD, PD, and CD are publicly available at <a href=https://github.com/kqwang/DLPR>https://github.com/kqwang/DLPR</a>.</p></p class="citation"></blockquote><h3 id=34--193273-data-efficient-unsupervised-interpolation-without-any-intermediate-frame-for-4d-medical-images-jungeun-kim-et-al-2024>(3/4 | 193/273) Data-Efficient Unsupervised Interpolation Without Any Intermediate Frame for 4D Medical Images (JungEun Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>JungEun Kim, Hangyul Yoon, Geondo Park, Kyungsu Kim, Eunho Yang. (2024)<br><strong>Data-Efficient Unsupervised Interpolation Without Any Intermediate Frame for 4D Medical Images</strong><br><button class=copy-to-clipboard title="Data-Efficient Unsupervised Interpolation Without Any Intermediate Frame for 4D Medical Images" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 23<br>Keywords: Benchmarking, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01464v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01464v1.pdf filename=2404.01464v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>4D medical images, which represent 3D images with temporal information, are crucial in clinical practice for capturing dynamic changes and monitoring long-term disease progression. However, acquiring 4D medical images poses challenges due to factors such as radiation exposure and imaging duration, necessitating a balance between achieving high temporal resolution and minimizing adverse effects. Given these circumstances, not only is data acquisition challenging, but increasing the frame rate for each dataset also proves difficult. To address this challenge, this paper proposes a simple yet effective <b>Unsupervised</b> Volumetric Interpolation framework, UVI-Net. This framework facilitates temporal interpolation without the need for any intermediate frames, distinguishing it from the majority of other existing <b>unsupervised</b> methods. Experiments on <b>benchmark</b> datasets demonstrate significant improvements across diverse evaluation metrics compared to <b>unsupervised</b> and <b>supervised</b> baselines. Remarkably, our approach achieves this superior performance even when trained with a dataset as small as one, highlighting its exceptional robustness and efficiency in scenarios with sparse supervision. This positions UVI-Net as a compelling alternative for 4D medical imaging, particularly in settings where data availability is limited. The source code is available at <a href=https://github.com/jungeun122333/UVI-Net>https://github.com/jungeun122333/UVI-Net</a>.</p></p class="citation"></blockquote><h3 id=44--194273-imd4gc-incomplete-multimodal-data-integration-to-advance-precise-treatment-response-prediction-and-survival-analysis-for-gastric-cancer-fengtao-zhou-et-al-2024>(4/4 | 194/273) iMD4GC: Incomplete Multimodal Data Integration to Advance Precise Treatment Response Prediction and Survival Analysis for Gastric Cancer (Fengtao Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fengtao Zhou, Yingxue Xu, Yanfen Cui, Shenyan Zhang, Yun Zhu, Weiyang He, Jiguang Wang, Xin Wang, Ronald Chan, Louis Ho Shing Lau, Chu Han, Dafu Zhang, Zhenhui Li, Hao Chen. (2024)<br><strong>iMD4GC: Incomplete Multimodal Data Integration to Advance Precise Treatment Response Prediction and Survival Analysis for Gastric Cancer</strong><br><button class=copy-to-clipboard title="iMD4GC: Incomplete Multimodal Data Integration to Advance Precise Treatment Response Prediction and Survival Analysis for Gastric Cancer" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01192v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01192v1.pdf filename=2404.01192v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gastric cancer (GC) is a prevalent malignancy worldwide, ranking as the fifth most common cancer with over 1 million new cases and 700 thousand deaths in 2020. Locally advanced gastric cancer (LAGC) accounts for approximately two-thirds of GC diagnoses, and neoadjuvant chemotherapy (NACT) has emerged as the standard treatment for LAGC. However, the effectiveness of NACT varies significantly among patients, with a considerable subset displaying treatment resistance. Ineffective NACT not only leads to adverse effects but also misses the optimal therapeutic window, resulting in lower survival rate. However, existing <b>multimodal</b> learning methods assume the availability of all modalities for each patient, which does not align with the reality of clinical practice. The limited availability of modalities for each patient would cause information loss, adversely affecting predictive accuracy. In this study, we propose an incomplete <b>multimodal</b> data integration framework for GC (iMD4GC) to address the challenges posed by incomplete <b>multimodal</b> data, enabling precise response prediction and survival analysis. Specifically, iMD4GC incorporates unimodal attention layers for each modality to capture intra-modal information. Subsequently, the cross-modal interaction layers explore potential inter-modal interactions and capture complementary information across modalities, thereby enabling information compensation for missing modalities. To evaluate iMD4GC, we collected three <b>multimodal</b> datasets for GC study: GastricRes (698 cases) for response prediction, GastricSur (801 cases) for survival analysis, and TCGA-STAD (400 cases) for survival analysis. The scale of our datasets is significantly larger than previous studies. The iMD4GC achieved impressive performance with an 80.2% AUC on GastricRes, 71.4% C-index on GastricSur, and 66.1% C-index on TCGA-STAD, significantly surpassing other compared methods.</p></p class="citation"></blockquote><h2 id=csir-7>cs.IR (7)</h2><h3 id=17--195273-query-performance-prediction-using-relevance-judgments-generated-by-large-language-models-chuan-meng-et-al-2024>(1/7 | 195/273) Query Performance Prediction using Relevance Judgments Generated by Large Language Models (Chuan Meng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuan Meng, Negar Arabzadeh, Arian Askari, Mohammad Aliannejadi, Maarten de Rijke. (2024)<br><strong>Query Performance Prediction using Relevance Judgments Generated by Large Language Models</strong><br><button class=copy-to-clipboard title="Query Performance Prediction using Relevance Judgments Generated by Large Language Models" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: H-3-3, cs-AI, cs-CL, cs-IR, cs-LG, cs.IR<br>Keyword Score: 60<br>Keywords: Fine-tuning, LLaMA, Information Retrieval, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01012v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01012v1.pdf filename=2404.01012v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific <b>information</b> <b>retrieval</b> (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgments as pseudo-labels; Also, this allows us to interpret predicted IR evaluation measures, and identify, track and rectify errors in generated relevance judgments to improve QPP quality. We judge relevance by leveraging a leading open-source <b>large</b> <b>language</b> <b>model</b> <b>(LLM),</b> <b>LLaMA,</b> to ensure scientific reproducibility. In doing so, we address two main challenges: (i) excessive computational costs of judging the entire corpus for predicting a recall-based metric, and (ii) poor performance in <b>prompting</b> <b>LLaMA</b> in a zero-/few-shot manner. We devise an approximation strategy to predict a recall-oriented IR measure and propose to <b>fine-tune</b> <b>LLaMA</b> using human-labeled relevance judgments. Experiments on the TREC 2019-2022 deep learning tracks show that QPP-GenRE achieves state-of-the-art QPP accuracy for both lexical and neural rankers in both precision- and recall-oriented metrics.</p></p class="citation"></blockquote><h3 id=27--196273-using-chaos-estimator-as-a-stopping-criterion-for-technology-assisted-review-michiel-p-bron-et-al-2024>(2/7 | 196/273) Using Chao&rsquo;s Estimator as a Stopping Criterion for Technology-Assisted Review (Michiel P. Bron et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michiel P. Bron, Peter G. M. van der Heijden, Ad J. Feelders, Arno P. J. M. Siebes. (2024)<br><strong>Using Chao&rsquo;s Estimator as a Stopping Criterion for Technology-Assisted Review</strong><br><button class=copy-to-clipboard title="Using Chao's Estimator as a Stopping Criterion for Technology-Assisted Review" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Active Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01176v1.pdf filename=2404.01176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Technology-Assisted Review (TAR) aims to reduce the human effort required for screening processes such as abstract screening for systematic literature reviews. Human reviewers label documents as relevant or irrelevant during this process, while the system incrementally updates a prediction model based on the reviewers&rsquo; previous decisions. After each model update, the system proposes new documents it deems relevant, to prioritize relevant documentsover irrelevant ones. A stopping criterion is necessary to guide users in stopping the review process to minimize the number of missed relevant documents and the number of read irrelevant documents. In this paper, we propose and evaluate a new ensemble-based <b>Active</b> <b>Learning</b> strategy and a stopping criterion based on Chao&rsquo;s Population Size Estimator that estimates the prevalence of relevant documents in the dataset. Our <b>simulation</b> study demonstrates that this criterion performs well on several datasets and is compared to other methods presented in the literature.</p></p class="citation"></blockquote><h3 id=37--197273-eeg-svrec-an-eeg-dataset-with-user-multidimensional-affective-engagement-labels-in-short-video-recommendation-shaorun-zhang-et-al-2024>(3/7 | 197/273) EEG-SVRec: An EEG Dataset with User Multidimensional Affective Engagement Labels in Short Video Recommendation (Shaorun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaorun Zhang, Zhiyu He, Ziyi Ye, Peijie Sun, Qingyao Ai, Min Zhang, Yiqun Liu. (2024)<br><strong>EEG-SVRec: An EEG Dataset with User Multidimensional Affective Engagement Labels in Short Video Recommendation</strong><br><button class=copy-to-clipboard title="EEG-SVRec: An EEG Dataset with User Multidimensional Affective Engagement Labels in Short Video Recommendation" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 23<br>Keywords: Benchmarking, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01008v1.pdf filename=2404.01008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, short video platforms have gained widespread popularity, making the quality of video <b>recommendations</b> crucial for retaining users. Existing <b>recommendation</b> systems primarily rely on behavioral data, which faces limitations when inferring user preferences due to issues such as data sparsity and noise from accidental interactions or personal habits. To address these challenges and provide a more comprehensive understanding of user affective experience and cognitive activity, we propose EEG-SVRec, the first EEG dataset with User Multidimensional Affective Engagement Labels in Short Video <b>Recommendation.</b> The study involves 30 participants and collects 3,657 interactions, offering a rich dataset that can be used for a deeper exploration of user preference and cognitive activity. By incorporating selfassessment techniques and real-time, low-cost EEG signals, we offer a more detailed understanding user affective experiences (valence, arousal, immersion, interest, visual and auditory) and the cognitive mechanisms behind their behavior. We establish <b>benchmarks</b> for rating prediction by the <b>recommendation</b> algorithm, showing significant improvement with the inclusion of EEG signals. Furthermore, we demonstrate the potential of this dataset in gaining insights into the affective experience and cognitive activity behind user behaviors in <b>recommender</b> <b>systems.</b> This work presents a novel perspective for enhancing short video <b>recommendation</b> by leveraging the rich information contained in EEG signals and multidimensional affective engagement scores, paving the way for future research in short video <b>recommendation</b> systems.</p></p class="citation"></blockquote><h3 id=47--198273-higher-education-assessment-practice-in-the-era-of-generative-ai-tools-bayode-ogunleye-et-al-2024>(4/7 | 198/273) Higher education assessment practice in the era of generative AI tools (Bayode Ogunleye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bayode Ogunleye, Kudirat Ibilola Zakariyyah, Oluwaseun Ajao, Olakunle Olayinka, Hemlata Sharma. (2024)<br><strong>Higher education assessment practice in the era of generative AI tools</strong><br><button class=copy-to-clipboard title="Higher education assessment practice in the era of generative AI tools" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: I-2-7; I-2-10; H-3-3, cs-AI, cs-CV, cs-IR, cs-LG, cs.IR<br>Keyword Score: 20<br>Keywords: Generative AI, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01036v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01036v1.pdf filename=2404.01036v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The higher education (HE) sector benefits every nation&rsquo;s economy and society at large. However, their contributions are challenged by advanced technologies like <b>generative</b> <b>artificial</b> intelligence (GenAI) tools. In this paper, we provide a comprehensive assessment of GenAI tools towards assessment and pedagogic practice and, subsequently, discuss the potential impacts. This study experimented using three assessment instruments from data science, data analytics, and construction management disciplines. Our findings are two-fold: first, the findings revealed that GenAI tools exhibit subject knowledge, problem-solving, analytical, critical thinking, and presentation skills and thus can limit learning when used unethically. Secondly, the design of the assessment of certain disciplines revealed the limitations of the GenAI tools. Based on our findings, we made <b>recommendations</b> on how AI tools can be utilised for teaching and learning in HE.</p></p class="citation"></blockquote><h3 id=57--199273-cross-channel-recommendation-for-multi-channel-retail-yijin-choi-et-al-2024>(5/7 | 199/273) Cross-channel Recommendation for Multi-channel Retail (Yijin Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijin Choi, Jongkyung Shin, Chiehyeon Lim. (2024)<br><strong>Cross-channel Recommendation for Multi-channel Retail</strong><br><button class=copy-to-clipboard title="Cross-channel Recommendation for Multi-channel Retail" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00972v1.pdf filename=2404.00972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An increasing number of brick-and-mortar retailers are expanding their channels to the online domain, transforming them into multi-channel retailers. This transition emphasizes the need for cross-channel <b>recommender</b> <b>systems,</b> aiming to enhance revenue across both offline and online channels. Given that each retail channel represents a separate domain with a unique context, this can be regarded as a cross-domain <b>recommendation</b> (CDR). However, the existing studies on CDR did not address the scenarios where both users and items partially overlap across multi-retail channels which we define as &ldquo;cross-channel retail <b>recommendation</b> (CCRR)&rdquo;. This paper introduces our original work on CCRR using real-world datasets from a multi-channel retail store. Specifically, (1) we present significant challenges in integrating user preferences across both channels. (2) Accordingly, we propose a novel model for CCRR using a channel-wise attention mechanism to capture different user preferences for the same item on each channel. We empirically validate our model&rsquo;s superiority in addressing CCRR over existing models. (3) Finally, we offer implications for future research on CCRR, delving into our experiment results.</p></p class="citation"></blockquote><h3 id=67--200273-maximizing-user-experience-with-llmops-driven-personalized-recommendation-systems-chenxi-shi-et-al-2024>(6/7 | 200/273) Maximizing User Experience with LLMOps-Driven Personalized Recommendation Systems (Chenxi Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenxi Shi, Penghao Liang, Yichao Wu, Tong Zhan, Zhengyu Jin. (2024)<br><strong>Maximizing User Experience with LLMOps-Driven Personalized Recommendation Systems</strong><br><button class=copy-to-clipboard title="Maximizing User Experience with LLMOps-Driven Personalized Recommendation Systems" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00903v1.pdf filename=2404.00903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of LLMOps into personalized <b>recommendation</b> systems marks a significant advancement in managing <b>LLM-driven</b> applications. This innovation presents both opportunities and challenges for enterprises, requiring specialized teams to navigate the complexity of engineering technology while prioritizing data security and model interpretability. By leveraging LLMOps, enterprises can enhance the efficiency and reliability of large-scale machine learning models, driving personalized <b>recommendations</b> aligned with user preferences. Despite ethical considerations, LLMOps is poised for widespread adoption, promising more efficient and secure machine learning services that elevate user experience and shape the future of personalized <b>recommendation</b> systems.</p></p class="citation"></blockquote><h3 id=77--201273-towards-an-in-depth-comprehension-of-case-relevance-for-better-legal-retrieval-haitao-li-et-al-2024>(7/7 | 201/273) Towards an In-Depth Comprehension of Case Relevance for Better Legal Retrieval (Haitao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haitao Li, You Chen, Zhekai Ge, Qingyao Ai, Yiqun Liu, Quan Zhou, Shuai Huo. (2024)<br><strong>Towards an In-Depth Comprehension of Case Relevance for Better Legal Retrieval</strong><br><button class=copy-to-clipboard title="Towards an In-Depth Comprehension of Case Relevance for Better Legal Retrieval" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00947v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00947v1.pdf filename=2404.00947v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Legal retrieval techniques play an important role in preserving the <b>fairness</b> and equality of the judicial system. As an annually well-known international competition, COLIEE aims to advance the development of state-of-the-art retrieval models for legal texts. This paper elaborates on the methodology employed by the TQM team in COLIEE2024.Specifically, we explored various lexical matching and semantic retrieval models, with a focus on enhancing the understanding of case relevance. Additionally, we endeavor to integrate various features using the learning-to-rank technique. Furthermore, fine heuristic pre-processing and post-processing methods have been proposed to mitigate irrelevant information. Consequently, our methodology achieved remarkable performance in COLIEE2024, securing first place in Task 1 and third place in Task 3. We anticipate that our proposed approach can contribute valuable insights to the advancement of legal retrieval technology.</p></p class="citation"></blockquote><h2 id=cscr-6>cs.CR (6)</h2><h3 id=16--202273-privacy-backdoors-enhancing-membership-inference-through-poisoning-pre-trained-models-yuxin-wen-et-al-2024>(1/6 | 202/273) Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models (Yuxin Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxin Wen, Leo Marchyok, Sanghyun Hong, Jonas Geiping, Tom Goldstein, Nicholas Carlini. (2024)<br><strong>Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models</strong><br><button class=copy-to-clipboard title="Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 55<br>Keywords: Black Box, Fine-tuning, Fine-tuning, Foundation Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01231v1.pdf filename=2404.01231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is commonplace to produce application-specific models by <b>fine-tuning</b> <b>large</b> <b>pre-trained</b> <b>models</b> using a small bespoke dataset. The widespread availability of <b>foundation</b> <b>model</b> checkpoints on the web poses considerable risks, including the vulnerability to backdoor attacks. In this paper, we unveil a new vulnerability: the privacy backdoor attack. This <b>black-box</b> <b>privacy</b> attack aims to amplify the privacy leakage that arises when <b>fine-tuning</b> a model: when a victim <b>fine-tunes</b> a backdoored model, their training data will be leaked at a significantly higher rate than if they had <b>fine-tuned</b> a typical model. We conduct extensive experiments on various datasets and models, including both <b>vision-language</b> models (CLIP) and <b>large</b> <b>language</b> <b>models,</b> demonstrating the broad applicability and effectiveness of such an attack. Additionally, we carry out multiple ablation studies with different <b>fine-tuning</b> methods and inference strategies to thoroughly analyze this new threat. Our findings highlight a critical privacy concern within the machine learning community and call for a reevaluation of safety protocols in the use of open-source pre-trained models.</p></p class="citation"></blockquote><h3 id=26--203273-enhancing-reasoning-capacity-of-slm-using-cognitive-enhancement-jonathan-pan-et-al-2024>(2/6 | 203/273) Enhancing Reasoning Capacity of SLM using Cognitive Enhancement (Jonathan Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Pan, Swee Liang Wong, Xin Wei Chia, Yidi Yuan. (2024)<br><strong>Enhancing Reasoning Capacity of SLM using Cognitive Enhancement</strong><br><button class=copy-to-clipboard title="Enhancing Reasoning Capacity of SLM using Cognitive Enhancement" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01135v1.pdf filename=2404.01135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have been applied to automate cyber security activities and processes including cyber investigation and digital forensics. However, the use of such models for cyber investigation and digital forensics should address accountability and security considerations. Accountability ensures models have the means to provide explainable <b>reasonings</b> and outcomes. This information can be extracted through explicit <b>prompt</b> requests. For security considerations, it is crucial to address privacy and confidentiality of the involved data during data processing as well. One approach to deal with this consideration is to have the data processed locally using a local instance of the model. Due to limitations of locally available resources, namely memory and GPU capacities, a Smaller <b>Large</b> <b>Language</b> <b>Model</b> (SLM) will typically be used. These SLMs have significantly fewer parameters compared to the <b>LLMs.</b> However, such size reductions have notable performance reduction, especially when tasked to provide <b>reasoning</b> explanations. In this paper, we aim to mitigate performance reduction through the integration of cognitive strategies that humans use for problem-solving. We term this as cognitive enhancement through <b>prompts.</b> Our experiments showed significant improvement gains of the SLMs&rsquo; performances when such enhancements were applied. We believe that our exploration study paves the way for further investigation into the use of cognitive enhancement to optimize SLM for cyber security applications.</p></p class="citation"></blockquote><h3 id=36--204273-maglive-near-field-magnetic-sensing-based-voice-liveness-detection-on-smartphones-xiping-sun-et-al-2024>(3/6 | 204/273) MagLive: Near-Field Magnetic Sensing-Based Voice Liveness Detection on Smartphones (Xiping Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiping Sun, Jing Chen, Cong Wu, Kun He, Haozhe Xu, Yebo Feng, Ruiying Du, Xianhao Chen. (2024)<br><strong>MagLive: Near-Field Magnetic Sensing-Based Voice Liveness Detection on Smartphones</strong><br><button class=copy-to-clipboard title="MagLive: Near-Field Magnetic Sensing-Based Voice Liveness Detection on Smartphones" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: Contrastive Learning, Convolutional Neural Network, Supervised Learning, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01106v1.pdf filename=2404.01106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Voice authentication has been widely used on smartphones. However, it remains vulnerable to spoofing attacks, where the attacker replays recorded voice samples from authentic humans using loudspeakers to bypass the voice authentication system. In this paper, we present MagLive, a robust voice liveness detection scheme designed for smartphones to mitigate such spoofing attacks. MagLive leverages differences in magnetic field patterns generated by different speakers (i.e., humans or loudspeakers) when speaking for liveness detection. It uses the built-in magnetometer on smartphones to capture these magnetic field changes. Specifically, MagLive utilizes two <b>CNN-based</b> submodels and a <b>self-attention-based</b> feature fusion model to extract effective and robust features. <b>Supervised</b> <b>contrastive</b> <b>learning</b> is then employed to achieve user-irrelevance, device-irrelevance, and content-irrelevance. MagLive imposes no additional burdens on users and does not rely on active sensing or extra devices. We conducted comprehensive experiments with various settings to evaluate the security and robustness of MagLive. Our results demonstrate that MagLive effectively distinguishes between humans and attackers (i.e., loudspeakers), achieving a balanced accuracy of 99.01% and an equal error rate of 0.77%.</p></p class="citation"></blockquote><h3 id=46--205273-poisoning-decentralized-collaborative-recommender-system-and-its-countermeasures-ruiqi-zheng-et-al-2024>(4/6 | 205/273) Poisoning Decentralized Collaborative Recommender System and Its Countermeasures (Ruiqi Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruiqi Zheng, Liang Qu, Tong Chen, Kai Zheng, Yuhui Shi, Hongzhi Yin. (2024)<br><strong>Poisoning Decentralized Collaborative Recommender System and Its Countermeasures</strong><br><button class=copy-to-clipboard title="Poisoning Decentralized Collaborative Recommender System and Its Countermeasures" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-IR, cs.CR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01177v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01177v1.pdf filename=2404.01177v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To make room for privacy and efficiency, the deployment of many <b>recommender</b> <b>systems</b> is experiencing a shift from central servers to personal devices, where the federated <b>recommender</b> <b>systems</b> (FedRecs) and decentralized collaborative <b>recommender</b> <b>systems</b> (DecRecs) are arguably the two most representative paradigms. While both leverage knowledge (e.g., gradients) sharing to facilitate learning local models, FedRecs rely on a central server to coordinate the optimization process, yet in DecRecs, the knowledge sharing directly happens between clients. Knowledge sharing also opens a backdoor for model poisoning attacks, where adversaries disguise themselves as benign clients and disseminate polluted knowledge to achieve malicious goals like promoting an item&rsquo;s exposure rate. Although research on such poisoning attacks provides valuable insights into finding security loopholes and corresponding countermeasures, existing attacks mostly focus on FedRecs, and are either inapplicable or ineffective for DecRecs. Compared with FedRecs where the tampered information can be universally distributed to all clients once uploaded to the cloud, each adversary in DecRecs can only communicate with neighbor clients of a small size, confining its impact to a limited range. To fill the gap, we present a novel attack method named Poisoning with Adaptive Malicious Neighbors (PAMN). With item promotion in top-K <b>recommendation</b> as the attack objective, PAMN effectively boosts target items&rsquo; ranks with several adversaries that emulate benign clients and transfers adaptively crafted gradients conditioned on each adversary&rsquo;s neighbors. Moreover, with the vulnerabilities of DecRecs uncovered, a dedicated defensive mechanism based on user-level gradient clipping with sparsified updating is proposed. Extensive experiments demonstrate the effectiveness of the poisoning attack and the robustness of our defensive mechanism.</p></p class="citation"></blockquote><h3 id=56--206273-towards-automated-generation-of-smart-grid-cyber-range-for-cybersecurity-experiments-and-training-daisuke-mashima-et-al-2024>(5/6 | 206/273) Towards Automated Generation of Smart Grid Cyber Range for Cybersecurity Experiments and Training (Daisuke Mashima et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daisuke Mashima, Muhammad M. Roomi, Bennet Ng, Zbigniew Kalbarczyk, S. M. Suhail Hussain, Ee-chien Chang. (2024)<br><strong>Towards Automated Generation of Smart Grid Cyber Range for Cybersecurity Experiments and Training</strong><br><button class=copy-to-clipboard title="Towards Automated Generation of Smart Grid Cyber Range for Cybersecurity Experiments and Training" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00869v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00869v1.pdf filename=2404.00869v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Assurance of cybersecurity is crucial to ensure dependability and resilience of smart power grid systems. In order to evaluate the impact of potential cyber attacks, to assess deployability and effectiveness of cybersecurity measures, and to enable hands-on exercise and training of personals, an interactive, virtual environment that emulates the behaviour of a smart grid system, namely smart grid cyber range, has been demanded by industry players as well as academia. A smart grid cyber range is typically implemented as a combination of cyber system emulation, which allows interactivity, and physical system (i.e., power grid) <b>simulation</b> that are tightly coupled for consistent cyber and physical behaviours. However, its design and implementation require intensive expertise and efforts in cyber and physical aspects of smart power systems as well as software/system engineering. While many industry players, including power grid operators, device vendors, research and education sectors are interested, availability of the smart grid cyber range is limited to a small number of research labs. To address this challenge, we have developed a framework for modelling a smart grid cyber range using an XML-based language, called SG-ML, and for &ldquo;compiling&rdquo; the model into an operational cyber range with minimal engineering efforts. The modelling language includes standardized schema from IEC 61850 and IEC 61131, which allows industry players to utilize their existing configurations. The SG-ML framework aims at making a smart grid cyber range available to broader user bases to facilitate cybersecurity R&amp;D and hands-on exercises.</p></p class="citation"></blockquote><h3 id=66--207273-ufid-a-unified-framework-for-input-level-backdoor-detection-on-diffusion-models-zihan-guan-et-al-2024>(6/6 | 207/273) UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models (Zihan Guan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihan Guan, Mengxuan Hu, Sheng Li, Anil Vullikanti. (2024)<br><strong>UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models</strong><br><button class=copy-to-clipboard title="UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CV, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01101v1.pdf filename=2404.01101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>Models</b> are vulnerable to backdoor attacks, where malicious attackers inject backdoors by poisoning some parts of the training samples during the training stage. This poses a serious threat to the downstream users, who query the <b>diffusion</b> <b>models</b> through the API or directly download them from the internet. To mitigate the threat of backdoor attacks, there have been a plethora of investigations on backdoor detections. However, none of them designed a specialized backdoor detection method for <b>diffusion</b> <b>models,</b> rendering the area much under-explored. Moreover, these prior methods mainly focus on the traditional neural networks in the classification task, which cannot be adapted to the backdoor detections on the generative task easily. Additionally, most of the prior methods require white-box access to model weights and architectures, or the probability logits as additional information, which are not always practical. In this paper, we propose a Unified Framework for Input-level backdoor Detection (UFID) on the <b>diffusion</b> <b>models,</b> which is motivated by observations in the <b>diffusion</b> <b>models</b> and further validated with a theoretical causality analysis. Extensive experiments across different datasets on both conditional and unconditional <b>diffusion</b> <b>models</b> show that our method achieves a superb performance on detection effectiveness and run-time efficiency. The code is available at <a href=https://github.com/GuanZihan/official_UFID>https://github.com/GuanZihan/official_UFID</a>.</p></p class="citation"></blockquote><h2 id=cshc-5>cs.HC (5)</h2><h3 id=15--208273-a-preliminary-roadmap-for-llms-as-assistants-in-exploring-analyzing-and-visualizing-knowledge-graphs-harry-li-et-al-2024>(1/5 | 208/273) A Preliminary Roadmap for LLMs as Assistants in Exploring, Analyzing, and Visualizing Knowledge Graphs (Harry Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harry Li, Gabriel Appleby, Ashley Suh. (2024)<br><strong>A Preliminary Roadmap for LLMs as Assistants in Exploring, Analyzing, and Visualizing Knowledge Graphs</strong><br><button class=copy-to-clipboard title="A Preliminary Roadmap for LLMs as Assistants in Exploring, Analyzing, and Visualizing Knowledge Graphs" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 43<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01425v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01425v1.pdf filename=2404.01425v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a mixed-methods study to explore how <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can assist users in the visual exploration and analysis of <b>knowledge</b> <b>graphs</b> <b>(KGs).</b> We surveyed and interviewed 20 professionals from industry, government laboratories, and academia who regularly work with <b>KGs</b> and <b>LLMs,</b> either collaboratively or concurrently. Our findings show that participants overwhelmingly want an <b>LLM</b> to facilitate data retrieval from <b>KGs</b> through joint query construction, to identify interesting relationships in the <b>KG</b> through multi-turn conversation, and to create on-demand visualizations from the <b>KG</b> that enhance their trust in the <b>LLM&rsquo;s</b> outputs. To interact with an <b>LLM,</b> participants strongly prefer a chat-based &lsquo;widget,&rsquo; built on top of their regular analysis workflows, with the ability to guide the <b>LLM</b> using their interactions with a visualization. When viewing an <b>LLM&rsquo;s</b> outputs, participants similarly prefer a combination of annotated visuals (e.g., subgraphs or tables extracted from the <b>KG)</b> alongside summarizing text. However, participants also expressed concerns about an <b>LLM&rsquo;s</b> ability to maintain semantic intent when translating natural language questions into <b>KG</b> queries, the risk of an <b>LLM</b> &lsquo;hallucinating&rsquo; false data from the <b>KG,</b> and the difficulties of engineering a &lsquo;perfect <b>prompt.&rsquo;</b> From the analysis of our interviews, we contribute a preliminary roadmap for the design of <b>LLM-driven</b> <b>knowledge</b> <b>graph</b> exploration systems and outline future opportunities in this emergent design space.</p></p class="citation"></blockquote><h3 id=25--209273-how-can-large-language-models-enable-better-socially-assistive-human-robot-interaction-a-brief-survey-zhonghao-shi-et-al-2024>(2/5 | 209/273) How Can Large Language Models Enable Better Socially Assistive Human-Robot Interaction: A Brief Survey (Zhonghao Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhonghao Shi, Ellen Landrum, Amy O&rsquo; Connell, Mina Kian, Leticia Pinto-Alva, Kaleen Shrestha, Xiaoyuan Zhu, Maja J Matarić. (2024)<br><strong>How Can Large Language Models Enable Better Socially Assistive Human-Robot Interaction: A Brief Survey</strong><br><button class=copy-to-clipboard title="How Can Large Language Models Enable Better Socially Assistive Human-Robot Interaction: A Brief Survey" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CL, cs-CV, cs-HC, cs-RO, cs.HC<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00938v1.pdf filename=2404.00938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Socially assistive robots (SARs) have shown great success in providing personalized cognitive-affective support for user populations with special needs such as older adults, children with autism spectrum disorder (ASD), and individuals with mental health challenges. The <b>large</b> <b>body</b> <b>of</b> work on SAR demonstrates its potential to provide at-home support that complements clinic-based interventions delivered by mental health professionals, making these interventions more effective and accessible. However, there are still several major technical challenges that hinder SAR-mediated interactions and interventions from reaching human-level social intelligence and efficacy. With the recent advances in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> there is an increased potential for novel applications within the field of SAR that can significantly expand the current capabilities of SARs. However, incorporating <b>LLMs</b> introduces new risks and ethical concerns that have not yet been encountered, and must be carefully be addressed to safely deploy these more advanced systems. In this work, we aim to conduct a brief survey on the use of <b>LLMs</b> in SAR technologies, and discuss the potentials and risks of applying <b>LLMs</b> to the following three major technical challenges of SAR: 1) natural language dialog; 2) <b>multimodal</b> understanding; 3) <b>LLMs</b> as robot policies.</p></p class="citation"></blockquote><h3 id=35--210273-towards-a-potential-paradigm-shift-in-health-data-collection-and-analysis-david-josef-herzog-et-al-2024>(3/5 | 210/273) Towards a potential paradigm shift in health data collection and analysis (David Josef Herzog et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Josef Herzog, Nitsa Judith Herzog. (2024)<br><strong>Towards a potential paradigm shift in health data collection and analysis</strong><br><button class=copy-to-clipboard title="Towards a potential paradigm shift in health data collection and analysis" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 25<br>Keywords: Black Box, Explainable AI, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01403v1.pdf filename=2404.01403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Industrial Revolution 4.0 transforms healthcare systems. The first three technological revolutions changed the relationship between human and machine interaction due to the exponential growth of machine numbers. The fourth revolution put humans into a situation where heterogeneous data is produced with unmatched quantity and quality not only by traditional methods, enforced by digitization, but also by ubiquitous computing, machine-to-machine interactions and smart environment. The modern cyber-physical space underlines the role of the person in the expanding context of computerization and big data processing. In healthcare, where data collection and analysis particularly depend on human efforts, the disruptive nature of these developments is evident. Adaptation to this process requires deep scrutiny of the trends and recognition of future medical data technologies` evolution. Significant difficulties arise from discrepancies in requirements by healthcare, administrative and technology stakeholders. <b>Black</b> <b>box</b> and grey box decisions made in medical imaging and diagnostic Decision Support Software are often not transparent enough for the professional, social and medico-legal requirements. While <b>Explainable</b> <b>AI</b> proposes a partial solution for AI applications in medicine, the approach has to be wider and multiplex. <b>LLM</b> potential and limitations are also discussed. This paper lists the most significant issues in these topics and describes possible solutions.</p></p class="citation"></blockquote><h3 id=45--211273-delve-into-earths-past-a-visualization-based-exhibit-deployed-across-multiple-museum-contexts-mara-solen-et-al-2024>(4/5 | 211/273) DeLVE into Earth&rsquo;s Past: A Visualization-Based Exhibit Deployed Across Multiple Museum Contexts (Mara Solen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mara Solen, Nigar Sultana, Laura Lukes, Tamara Munzner. (2024)<br><strong>DeLVE into Earth&rsquo;s Past: A Visualization-Based Exhibit Deployed Across Multiple Museum Contexts</strong><br><button class=copy-to-clipboard title="DeLVE into Earth's Past: A Visualization-Based Exhibit Deployed Across Multiple Museum Contexts" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01488v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01488v1.pdf filename=2404.01488v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While previous work has found success in deploying visualizations as museum exhibits, differences in visitor behaviour across varying museum contexts are understudied. We present an interactive Deep-time Literacy Visualization Exhibit (DeLVE) to help museum visitors understand deep time (lengths of extremely long geological processes) by improving proportional <b>reasoning</b> skills through comparison of different time periods. DeLVE uses a new visualization idiom, Connected Multi-Tier Ranges, to visualize curated datasets of past events across multiple scales of time, relating extreme scales with concrete scales that have more familiar magnitudes and units. Museum staff at three separate museums approved the deployment of DeLVE as a digital kiosk, and devoted time to curating a unique dataset in each of them. We collect data from two sources, an observational study and system trace logs, yielding evidence of successfully meeting our requirements. We discuss the importance of context: similar museum exhibits in different contexts were received very differently by visitors. We additionally discuss differences in our process from standard design study methodology which is focused on design studies for data analysis purposes, rather than for presentation. Supplemental materials are available at: <a href="https://osf.io/z53dq/?view_only=4df33aad207144aca149982412125541">https://osf.io/z53dq/?view_only=4df33aad207144aca149982412125541</a></p></p class="citation"></blockquote><h3 id=55--212273-chat-modeling-natural-language-based-procedural-modeling-of-biological-structures-without-training-donggang-jia-et-al-2024>(5/5 | 212/273) Chat Modeling: Natural Language-based Procedural Modeling of Biological Structures without Training (Donggang Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Donggang Jia, Yunhai Wang, Ivan Viola. (2024)<br><strong>Chat Modeling: Natural Language-based Procedural Modeling of Biological Structures without Training</strong><br><button class=copy-to-clipboard title="Chat Modeling: Natural Language-based Procedural Modeling of Biological Structures without Training" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-GR, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01063v1.pdf filename=2404.01063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D modeling of biological structures is an inherently complex process, necessitating both biological and geometric understanding. Additionally, the complexity of user interfaces of 3D modeling tools and the associated steep learning curve further exacerbate the difficulty of authoring a 3D model. In this paper, we introduce a novel framework to address the challenge of using 3D modeling software by converting users&rsquo; textual inputs into modeling actions within an interactive procedural modeling system. The framework incorporates a code generator of a novel code format and a corresponding code interpreter. The major technical innovation includes the user-refinement mechanism that captures the degree of user dissatisfaction with the modeling outcome, offers an interactive revision, and leverages this feedback for future improved 3D modeling. This entire framework is powered by <b>large</b> <b>language</b> <b>models</b> and eliminates the need for a traditional training process. We develop a prototype tool named Chat Modeling, offering both automatic and step-by-step 3D modeling approaches. Our evaluation of the framework with structural biologists highlights the potential of our approach being utilized in their scientific workflows. All supplemental materials are available at <a href=https://osf.io/x4qb7/>https://osf.io/x4qb7/</a>.</p></p class="citation"></blockquote><h2 id=mathst-2>math.ST (2)</h2><h3 id=12--213273-a-statistical-framework-of-watermarks-for-large-language-models-pivot-detection-efficiency-and-optimal-rules-xiang-li-et-al-2024>(1/2 | 213/273) A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules (Xiang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su. (2024)<br><strong>A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules</strong><br><button class=copy-to-clipboard title="A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.ST<br>Categories: cs-CL, cs-CR, cs-LG, math-ST, math.ST, stat-ML, stat-TH<br>Keyword Score: 40<br>Keywords: ChatGPT, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01245v1.pdf filename=2404.01245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since <b>ChatGPT</b> was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> also known as watermarking, has been used as a principled approach to provable detection of <b>LLM-generated</b> text from its human-written counterpart. In this paper, we introduce a general and flexible framework for <b>reasoning</b> about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key &ndash; provided by the <b>LLM</b> to the verifier &ndash; to enable controlling the false positive rate (the error of mistakenly detecting human-written text as <b>LLM-generated).</b> Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying <b>LLM-generated</b> text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks &ndash; one of which has been internally implemented at OpenAI &ndash; and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments.</p></p class="citation"></blockquote><h3 id=22--214273-optimal-ridge-regularization-for-out-of-distribution-prediction-pratik-patil-et-al-2024>(2/2 | 214/273) Optimal Ridge Regularization for Out-of-Distribution Prediction (Pratik Patil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pratik Patil, Jin-Hong Du, Ryan J. Tibshirani. (2024)<br><strong>Optimal Ridge Regularization for Out-of-Distribution Prediction</strong><br><button class=copy-to-clipboard title="Optimal Ridge Regularization for Out-of-Distribution Prediction" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.ST<br>Categories: cs-LG, math-ST, math.ST, stat-ML, stat-TH<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01233v1.pdf filename=2404.01233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the behavior of optimal ridge regularization and optimal ridge risk for <b>out-of-distribution</b> prediction, where the test distribution deviates arbitrarily from the train distribution. We establish general conditions that determine the sign of the optimal regularization level under covariate and regression shifts. These conditions capture the alignment between the covariance and signal structures in the train and test data and reveal stark differences compared to the in-distribution setting. For example, a negative regularization level can be optimal under covariate shift or regression shift, even when the training features are isotropic or the design is underparameterized. Furthermore, we prove that the optimally-tuned risk is monotonic in the data aspect ratio, even in the <b>out-of-distribution</b> setting and when optimizing over negative regularization levels. In general, our results do not make any modeling assumptions for the train or the test distributions, except for moment bounds, and allow for arbitrary shifts and the widest possible range of (negative) regularization levels.</p></p class="citation"></blockquote><h2 id=statml-3>stat.ML (3)</h2><h3 id=13--215273-transfusion-covariate-shift-robust-transfer-learning-for-high-dimensional-regression-zelin-he-et-al-2024>(1/3 | 215/273) TransFusion: Covariate-Shift Robust Transfer Learning for High-Dimensional Regression (Zelin He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zelin He, Ying Sun, Jingyuan Liu, Runze Li. (2024)<br><strong>TransFusion: Covariate-Shift Robust Transfer Learning for High-Dimensional Regression</strong><br><button class=copy-to-clipboard title="TransFusion: Covariate-Shift Robust Transfer Learning for High-Dimensional Regression" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-DC, cs-LG, math-ST, stat-ME, stat-ML, stat-TH, stat.ML<br>Keyword Score: 40<br>Keywords: Distribution Shift, Distribution Shift, Supervised Learning, Supervised Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01153v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01153v1.pdf filename=2404.01153v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The main challenge that sets <b>transfer</b> <b>learning</b> apart from traditional <b>supervised</b> <b>learning</b> is the <b>distribution</b> <b>shift,</b> reflected as the shift between the source and target models and that between the marginal covariate <b>distributions.</b> <b>In</b> this work, we tackle model shifts in the presence of covariate shifts in the high-dimensional regression setting. Specifically, we propose a two-step method with a novel fused-regularizer that effectively leverages samples from source tasks to improve the learning performance on a target task with limited samples. Nonasymptotic bound is provided for the estimation error of the target model, showing the robustness of the proposed method to covariate shifts. We further establish conditions under which the estimator is minimax-optimal. Additionally, we extend the method to a distributed setting, allowing for a pretraining-finetuning strategy, requiring just one round of communication while retaining the estimation rate of the centralized version. Numerical tests validate our theory, highlighting the method&rsquo;s robustness to covariate shifts.</p></p class="citation"></blockquote><h3 id=23--216273-fair-mp-boost-fair-and-interpretable-minipatch-boosting-camille-olivia-little-et-al-2024>(2/3 | 216/273) Fair MP-BOOST: Fair and Interpretable Minipatch Boosting (Camille Olivia Little et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Camille Olivia Little, Genevera I. Allen. (2024)<br><strong>Fair MP-BOOST: Fair and Interpretable Minipatch Boosting</strong><br><button class=copy-to-clipboard title="Fair MP-BOOST: Fair and Interpretable Minipatch Boosting" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 13<br>Keywords: Benchmarking, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01521v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01521v1.pdf filename=2404.01521v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensemble methods, particularly boosting, have established themselves as highly effective and widely embraced machine learning techniques for tabular data. In this paper, we aim to leverage the robust predictive power of traditional boosting methods while enhancing <b>fairness</b> and interpretability. To achieve this, we develop Fair MP-Boost, a stochastic boosting scheme that balances <b>fairness</b> and accuracy by adaptively learning features and observations during training. Specifically, Fair MP-Boost sequentially samples small subsets of observations and features, termed minipatches (MP), according to adaptively learned feature and observation sampling probabilities. We devise these probabilities by combining loss functions, or by combining feature importance scores to address accuracy and <b>fairness</b> simultaneously. Hence, Fair MP-Boost prioritizes important and fair features along with challenging instances, to select the most relevant minipatches for learning. The learned probability distributions also yield intrinsic interpretations of feature importance and important observations in Fair MP-Boost. Through empirical evaluation of simulated and <b>benchmark</b> datasets, we showcase the interpretability, accuracy, and <b>fairness</b> of Fair MP-Boost.</p></p class="citation"></blockquote><h3 id=33--217273-large-scale-non-convex-stochastic-constrained-distributionally-robust-optimization-qi-zhang-et-al-2024>(3/3 | 217/273) Large-Scale Non-convex Stochastic Constrained Distributionally Robust Optimization (Qi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Zhang, Yi Zhou, Ashley Prater-Bennette, Lixin Shen, Shaofeng Zou. (2024)<br><strong>Large-Scale Non-convex Stochastic Constrained Distributionally Robust Optimization</strong><br><button class=copy-to-clipboard title="Large-Scale Non-convex Stochastic Constrained Distributionally Robust Optimization" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01200v1.pdf filename=2404.01200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributionally robust optimization (DRO) is a powerful framework for training robust models against data <b>distribution</b> <b>shifts.</b> This paper focuses on constrained DRO, which has an explicit characterization of the robustness level. Existing studies on constrained DRO mostly focus on convex loss function, and exclude the practical and challenging case with non-convex loss function, e.g., neural network. This paper develops a stochastic algorithm and its performance analysis for non-convex constrained DRO. The computational complexity of our stochastic algorithm at each iteration is independent of the overall dataset size, and thus is suitable for large-scale applications. We focus on the general Cressie-Read family divergence defined uncertainty set which includes $\chi^2$-divergences as a special case. We prove that our algorithm finds an $\epsilon$-stationary point with a computational complexity of $\mathcal O(\epsilon^{-3k_<em>-5})$, where $k_</em>$ is the parameter of the Cressie-Read divergence. The numerical results indicate that our method outperforms existing methods.} Our method also applies to the smoothed conditional value at risk (CVaR) DRO.</p></p class="citation"></blockquote><h2 id=csro-12>cs.RO (12)</h2><h3 id=112--218273-quad-query-based-interpretable-neural-motion-planning-for-autonomous-driving-sourav-biswas-et-al-2024>(1/12 | 218/273) QuAD: Query-based Interpretable Neural Motion Planning for Autonomous Driving (Sourav Biswas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sourav Biswas, Sergio Casas, Quinlan Sykora, Ben Agro, Abbas Sadat, Raquel Urtasun. (2024)<br><strong>QuAD: Query-based Interpretable Neural Motion Planning for Autonomous Driving</strong><br><button class=copy-to-clipboard title="QuAD: Query-based Interpretable Neural Motion Planning for Autonomous Driving" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Object Detection, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01486v1.pdf filename=2404.01486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A self-driving vehicle must understand its environment to determine the appropriate action. Traditional autonomy systems rely on <b>object</b> <b>detection</b> to find the agents in the scene. However, <b>object</b> <b>detection</b> assumes a discrete set of <b>objects</b> <b>and</b> loses information about uncertainty, so any errors compound when predicting the future behavior of those agents. Alternatively, dense occupancy grid maps have been utilized to understand free-space. However, predicting a grid for the entire scene is wasteful since only certain spatio-temporal regions are reachable and relevant to the self-driving vehicle. We present a unified, interpretable, and efficient autonomy framework that moves away from cascading modules that first perceive, then predict, and finally plan. Instead, we shift the paradigm to have the planner query occupancy at relevant spatio-temporal points, restricting the computation to those regions of interest. Exploiting this representation, we evaluate candidate trajectories around key factors such as collision avoidance, comfort, and progress for safety and interpretability. Our approach achieves better highway driving quality than the state-of-the-art in high-fidelity closed-loop <b>simulations.</b></p></p class="citation"></blockquote><h3 id=212--219273-force-evt-a-closer-look-at-robotic-gripper-force-measurement-with-event-based-vision-transformer-qianyu-guo-et-al-2024>(2/12 | 219/273) Force-EvT: A Closer Look at Robotic Gripper Force Measurement with Event-based Vision Transformer (Qianyu Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianyu Guo, Ziqing Yu, Jiaming Fu, Yawen Lu, Yahya Zweiri, Dongming Gan. (2024)<br><strong>Force-EvT: A Closer Look at Robotic Gripper Force Measurement with Event-based Vision Transformer</strong><br><button class=copy-to-clipboard title="Force-EvT: A Closer Look at Robotic Gripper Force Measurement with Event-based Vision Transformer" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO, eess-IV<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01170v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01170v1.pdf filename=2404.01170v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic grippers are receiving increasing attention in various industries as essential components of robots for interacting and manipulating objects. While significant progress has been made in the past, conventional rigid grippers still have limitations in handling irregular objects and can damage fragile objects. We have shown that soft grippers offer deformability to adapt to a variety of object shapes and maximize object protection. At the same time, dynamic <b>vision</b> <b>sensors</b> (e.g., event-based cameras) are capable of capturing small changes in brightness and streaming them asynchronously as events, unlike RGB cameras, which do not perform well in low-light and fast-moving environments. In this paper, a dynamic-vision-based algorithm is proposed to measure the force applied to the gripper. In particular, we first set up a DVXplorer Lite series event camera to capture twenty-five sets of event data. Second, motivated by the impressive performance of the <b>Vision</b> <b>Transformer</b> (ViT) algorithm in dense image prediction tasks, we propose a new approach that demonstrates the potential for real-time force estimation and meets the requirements of real-world scenarios. We extensively evaluate the proposed algorithm on a wide range of scenarios and settings, and show that it consistently outperforms recent approaches.</p></p class="citation"></blockquote><h3 id=312--220273-nvins-robust-visual-inertial-navigation-fused-with-nerf-augmented-camera-pose-regressor-and-uncertainty-quantification-juyeop-han-et-al-2024>(3/12 | 220/273) NVINS: Robust Visual Inertial Navigation Fused with NeRF-augmented Camera Pose Regressor and Uncertainty Quantification (Juyeop Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juyeop Han, Lukas Lao Beyer, Guilherme V. Cavalheiro, Sertac Karaman. (2024)<br><strong>NVINS: Robust Visual Inertial Navigation Fused with NeRF-augmented Camera Pose Regressor and Uncertainty Quantification</strong><br><button class=copy-to-clipboard title="NVINS: Robust Visual Inertial Navigation Fused with NeRF-augmented Camera Pose Regressor and Uncertainty Quantification" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01400v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01400v1.pdf filename=2404.01400v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, Neural Radiance Fields (NeRF) have emerged as a powerful tool for 3D reconstruction and novel view synthesis. However, the computational cost of NeRF rendering and degradation in quality due to the presence of artifacts pose significant challenges for its application in real-time and robust robotic tasks, especially on embedded systems. This paper introduces a novel framework that integrates NeRF-derived localization information with Visual-Inertial Odometry(VIO) to provide a robust solution for robotic navigation in a real-time. By training an absolute pose regression network with augmented image data rendered from a NeRF and quantifying its uncertainty, our approach effectively counters positional drift and enhances system reliability. We also establish a mathematically sound foundation for combining visual inertial navigation with camera localization neural networks, considering uncertainty under a Bayesian framework. Experimental validation in the photorealistic <b>simulation</b> environment demonstrates significant improvements in accuracy compared to a conventional VIO approach.</p></p class="citation"></blockquote><h3 id=412--221273-ltl-d-incrementally-optimal-replanning-for-feasible-and-infeasible-tasks-in-linear-temporal-logic-specifications-jiming-ren-et-al-2024>(4/12 | 221/273) LTL-D*: Incrementally Optimal Replanning for Feasible and Infeasible Tasks in Linear Temporal Logic Specifications (Jiming Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiming Ren, Haris Miller, Karen M. Feigh, Samuel Coogan, Ye Zhao. (2024)<br><em><em>LTL-D</em>: Incrementally Optimal Replanning for Feasible and Infeasible Tasks in Linear Temporal Logic Specifications</em>*<br><button class=copy-to-clipboard title="LTL-D*: Incrementally Optimal Replanning for Feasible and Infeasible Tasks in Linear Temporal Logic Specifications" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-FL, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01219v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01219v1.pdf filename=2404.01219v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an incremental replanning algorithm, dubbed LTL-D*, for temporal-logic-based task planning in a dynamically changing environment. Unexpected changes in the environment may lead to failures in satisfying a task specification in the form of a Linear Temporal Logic (LTL). In this study, the considered failures are categorized into two classes: (i) the desired LTL specification can be satisfied via replanning, and (ii) the desired LTL specification is infeasible to meet strictly and can only be satisfied in a &ldquo;relaxed&rdquo; fashion. To address these failures, the proposed algorithm finds an optimal replanning solution that minimally violates desired task specifications. In particular, our approach leverages the D* Lite algorithm and employs a distance metric within the synthesized automaton to quantify the degree of the task violation and then replan incrementally. This ensures plan optimality and reduces planning time, especially when frequent replanning is required. Our approach is implemented in a robot navigation <b>simulation</b> to demonstrate a significant improvement in the computational efficiency for replanning by two orders of magnitude.</p></p class="citation"></blockquote><h3 id=512--222273-visual-inertial-state-estimation-based-on-chebyshev-polynomial-optimization-hongyu-zhang-et-al-2024>(5/12 | 222/273) Visual-inertial state estimation based on Chebyshev polynomial optimization (Hongyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyu Zhang, Maoran Zhu, Qi Cai, Yuanxin Wu. (2024)<br><strong>Visual-inertial state estimation based on Chebyshev polynomial optimization</strong><br><button class=copy-to-clipboard title="Visual-inertial state estimation based on Chebyshev polynomial optimization" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01150v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01150v1.pdf filename=2404.01150v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes an innovative state estimation method for visual-inertial fusion based on Chebyshev polynomial optimization. Specifically, the pose is modeled as a Chebyshev polynomial of a certain order, and its time derivatives are used to calculate linear acceleration and angular velocity, which, along with inertial measurements, constitute dynamic constraints. This is coupled with a visual measurement model to construct a visual-inertial bundle adjustment formulation. <b>Simulation</b> and public dataset experiments show that the proposed method has better accuracy than the discrete-form preintegration method.</p></p class="citation"></blockquote><h3 id=612--223273-a-center-of-mass-shifting-aerial-manipulation-platform-for-heavy-tool-handling-on-non-horizontal-surfaces-tong-hui-et-al-2024>(6/12 | 223/273) A Center-of-Mass Shifting Aerial Manipulation Platform for Heavy-Tool Handling on Non-Horizontal Surfaces (Tong Hui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tong Hui, Stefan Rucareanu, Haotian Liu, Matteo Fumagalli. (2024)<br><strong>A Center-of-Mass Shifting Aerial Manipulation Platform for Heavy-Tool Handling on Non-Horizontal Surfaces</strong><br><button class=copy-to-clipboard title="A Center-of-Mass Shifting Aerial Manipulation Platform for Heavy-Tool Handling on Non-Horizontal Surfaces" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01110v1.pdf filename=2404.01110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aerial vehicles equipped with manipulators can serve contact-based industrial applications, where fundamental tasks like drilling and grinding often necessitate aerial platforms to handle heavy tools. Industrial environments often involve non-horizontal surfaces. Existing aerial manipulation platforms based on multirotors typically feature a fixed CoM (Center of Mass) within the rotor-defined area, leading to a considerable moment arm between the EE (End-Effector) tip and the CoM for operations on such surfaces. Carrying heavy tools at the EE tip of the manipulator with an extended moment arm can lead to system instability and potential damage to the servo actuators used in the manipulator. To tackle this issue, we present a novel aerial vehicle tailored for handling heavy tools on non-horizontal surfaces. In this work, we provide the platform&rsquo;s system design, modeling, and control strategies. This platform can carry heavy manipulators within the rotor-defined area during free flight. During interactions, the manipulator can shift towards the work surface outside the rotor-defined area, resulting in a displaced CoM location with a significantly shorter moment arm. Furthermore, we propose a method for automatically determining the manipulator&rsquo;s position to reach the maximum CoM displacement towards the work surface. Our proposed concepts are validated through <b>simulations</b> that closely capture the developed physical prototype of the platform.</p></p class="citation"></blockquote><h3 id=712--224273-versatile-navigation-under-partial-observability-via-value-guided-diffusion-policy-gengyu-zhang-et-al-2024>(7/12 | 224/273) Versatile Navigation under Partial Observability via Value-guided Diffusion Policy (Gengyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gengyu Zhang, Hao Tang, Yan Yan. (2024)<br><strong>Versatile Navigation under Partial Observability via Value-guided Diffusion Policy</strong><br><button class=copy-to-clipboard title="Versatile Navigation under Partial Observability via Value-guided Diffusion Policy" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02176v1.pdf filename=2404.02176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Route planning for navigation under partial observability plays a crucial role in modern robotics and autonomous driving. Existing route planning approaches can be categorized into two main classes: traditional autoregressive and diffusion-based methods. The former often fails due to its myopic nature, while the latter either assumes full observability or struggles to adapt to unfamiliar scenarios, due to strong couplings with behavior cloning from experts. To address these deficiencies, we propose a versatile diffusion-based approach for both 2D and 3D route planning under partial observability. Specifically, our value-guided diffusion policy first generates plans to predict actions across various timesteps, providing ample foresight to the planning. It then employs a differentiable planner with state estimations to derive a value function, directing the agent&rsquo;s exploration and goal-seeking behaviors without seeking experts while explicitly addressing partial observability. During inference, our policy is further enhanced by a best-plan-selection strategy, substantially boosting the planning success rate. Moreover, we propose projecting point clouds, derived from RGB-D inputs, onto 2D grid-based bird-eye-view maps via semantic segmentation, generalizing to 3D environments. This simple yet effective adaption enables <b>zero-shot</b> transfer from 2D-trained policy to 3D, cutting across the laborious training for 3D policy, and thus certifying our versatility. Experimental results demonstrate our superior performance, particularly in navigating situations beyond expert demonstrations, surpassing state-of-the-art autoregressive and diffusion-based baselines for both 2D and 3D scenarios.</p></p class="citation"></blockquote><h3 id=812--225273-contacthandover-contact-guided-robot-to-human-object-handover-zixi-wang-et-al-2024>(8/12 | 225/273) ContactHandover: Contact-Guided Robot-to-Human Object Handover (Zixi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zixi Wang, Zeyi Liu, Nicolas Ouporov, Shuran Song. (2024)<br><strong>ContactHandover: Contact-Guided Robot-to-Human Object Handover</strong><br><button class=copy-to-clipboard title="ContactHandover: Contact-Guided Robot-to-Human Object Handover" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Rerank<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01402v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01402v1.pdf filename=2404.01402v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robot-to-human object handover is an important step in many human robot collaboration tasks. A successful handover requires the robot to maintain a stable grasp on the object while making sure the human receives the object in a natural and easy-to-use manner. We propose ContactHandover, a robot to human handover system that consists of two phases: a contact-guided grasping phase and an object delivery phase. During the grasping phase, ContactHandover predicts both 6-DoF robot grasp poses and a 3D affordance map of human contact points on the object. The robot grasp poses are <b>reranked</b> by penalizing those that block human contact points, and the robot executes the highest ranking grasp. During the delivery phase, the robot end effector pose is computed by maximizing human contact points close to the human while minimizing the human arm joint torques and displacements. We evaluate our system on 27 diverse household objects and show that our system achieves better visibility and reachability of human contacts to the receiver compared to several baselines. More results can be found on <a href=https://clairezixiwang.github.io/ContactHandover.github.io>https://clairezixiwang.github.io/ContactHandover.github.io</a></p></p class="citation"></blockquote><h3 id=912--226273-entity-centric-reinforcement-learning-for-object-manipulation-from-pixels-dan-haramati-et-al-2024>(9/12 | 226/273) Entity-Centric Reinforcement Learning for Object Manipulation from Pixels (Dan Haramati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dan Haramati, Tal Daniel, Aviv Tamar. (2024)<br><strong>Entity-Centric Reinforcement Learning for Object Manipulation from Pixels</strong><br><button class=copy-to-clipboard title="Entity-Centric Reinforcement Learning for Object Manipulation from Pixels" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01220v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01220v1.pdf filename=2404.01220v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Manipulating objects is a hallmark of human intelligence, and an important task in domains such as robotics. In principle, <b>Reinforcement</b> <b>Learning</b> (RL) offers a general approach to learn object manipulation. In practice, however, domains with more than a few objects are difficult for RL agents due to the curse of dimensionality, especially when learning from raw image observations. In this work we propose a structured approach for visual RL that is suitable for representing multiple objects and their interaction, and use it to learn goal-conditioned manipulation of several objects. Key to our method is the ability to handle goals with dependencies between the objects (e.g., moving objects in a certain order). We further relate our architecture to the generalization capability of the trained agent, based on a theoretical result for compositional generalization, and demonstrate agents that learn with 3 objects but generalize to similar tasks with over 10 objects. Videos and code are available on the project website: <a href=https://sites.google.com/view/entity-centric-rl>https://sites.google.com/view/entity-centric-rl</a></p></p class="citation"></blockquote><h3 id=1012--227273-physreaction-physically-plausible-real-time-humanoid-reaction-synthesis-via-forward-dynamics-guided-4d-imitation-yunze-liu-et-al-2024>(10/12 | 227/273) PhysReaction: Physically Plausible Real-Time Humanoid Reaction Synthesis via Forward Dynamics Guided 4D Imitation (Yunze Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunze Liu, Changxi Chen, Chenjing Ding, Li Yi. (2024)<br><strong>PhysReaction: Physically Plausible Real-Time Humanoid Reaction Synthesis via Forward Dynamics Guided 4D Imitation</strong><br><button class=copy-to-clipboard title="PhysReaction: Physically Plausible Real-Time Humanoid Reaction Synthesis via Forward Dynamics Guided 4D Imitation" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01081v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01081v1.pdf filename=2404.01081v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humanoid Reaction Synthesis is pivotal for creating highly interactive and empathetic robots that can seamlessly integrate into human environments, enhancing the way we live, work, and communicate. However, it is difficult to learn the diverse interaction patterns of multiple humans and generate physically plausible reactions. The kinematics-based approaches face challenges, including issues like floating feet, sliding, penetration, and other problems that defy physical plausibility. The existing physics-based method often relies on kinematics-based methods to generate reference states, which struggle with the challenges posed by kinematic noise during action execution. Constrained by their reliance on <b>diffusion</b> <b>models,</b> these methods are unable to achieve real-time inference. In this work, we propose a Forward Dynamics Guided 4D Imitation method to generate physically plausible human-like reactions. The learned policy is capable of generating physically plausible and human-like reactions in real-time, significantly improving the speed(x33) and quality of reactions compared with the existing method. Our experiments on the InterHuman and Chi3D datasets, along with ablation studies, demonstrate the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=1112--228273-efficient-motion-planning-for-manipulators-with-control-barrier-function-induced-neural-controller-mingxin-yu-et-al-2024>(11/12 | 228/273) Efficient Motion Planning for Manipulators with Control Barrier Function-Induced Neural Controller (Mingxin Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingxin Yu, Chenning Yu, M-Mahdi Naddaf-Sh, Devesh Upadhyay, Sicun Gao, Chuchu Fan. (2024)<br><strong>Efficient Motion Planning for Manipulators with Control Barrier Function-Induced Neural Controller</strong><br><button class=copy-to-clipboard title="Efficient Motion Planning for Manipulators with Control Barrier Function-Induced Neural Controller" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01184v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01184v1.pdf filename=2404.01184v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sampling-based motion planning methods for manipulators in crowded environments often suffer from expensive collision checking and high sampling complexity, which make them difficult to use in real time. To address this issue, we propose a new generalizable control barrier function (CBF)-based steering controller to reduce the number of samples needed in a sampling-based motion planner RRT. Our method combines the strength of CBF for real-time collision-avoidance control and RRT for long-horizon motion planning, by using CBF-induced neural controller (CBF-INC) to generate control signals that steer the system towards sampled configurations by RRT. CBF-INC is learned as Neural Networks and has two variants handling different inputs, respectively: state (signed distance) input and point-cloud input from LiDAR. In the latter case, we also study two different settings: fully and partially observed environmental information. Compared to manually crafted CBF which suffers from over-approximating robot <b>geometry,</b> CBF-INC can balance safety and goal-reaching better without being over-conservative. Given state-based input, our neural CBF-induced neural controller-enhanced RRT (CBF-INC-RRT) can increase the success rate by 14% while reducing the number of nodes explored by 30%, compared with vanilla RRT on hard test cases. Given LiDAR input where vanilla RRT is not directly applicable, we demonstrate that our CBF-INC-RRT can improve the success rate by 10%, compared with planning with other steering controllers. Our project page with supplementary material is at <a href=https://mit-realm.github.io/CBF-INC-RRT-website/>https://mit-realm.github.io/CBF-INC-RRT-website/</a>.</p></p class="citation"></blockquote><h3 id=1212--229273-scalable-radar-based-its-self-localization-and-occupancy-heat-map-for-traffic-analysis-longfei-han-et-al-2024>(12/12 | 229/273) Scalable Radar-based ITS: Self-localization and Occupancy Heat Map for Traffic Analysis (Longfei Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Longfei Han, Klaus Kefferpütz, Qiuyu Xu, Ying Lu, Gordon Elger, Jürgen Beyerer. (2024)<br><strong>Scalable Radar-based ITS: Self-localization and Occupancy Heat Map for Traffic Analysis</strong><br><button class=copy-to-clipboard title="Scalable Radar-based ITS: Self-localization and Occupancy Heat Map for Traffic Analysis" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01166v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01166v1.pdf filename=2404.01166v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>4D mmWave radar sensors are well suited for city scale Intelligent Transportation Systems (ITS) given their long sensing range, weatherproof functionality, simple mechanical design, and low manufacturing cost. In this paper, we investigate radar-based ITS for scalable traffic analysis. Localization of these radar sensors in a city scale range is a fundamental task in ITS. For mobile ITS setups it requires more endeavor. To address this task, we propose a self-localization approach that matches two descriptions of &ldquo;road&rdquo;: the one from the <b>geometry</b> of the motion trajectories of cumulatively observed vehicles, and the other one from the aerial laser scan. An ICP (iterative closest point) algorithm is used to register the motion trajectory into the road section of the laser scan to estimate the sensor pose. We evaluates the results and show that it outperforms other map-based radar localization methods, especially for the orientation estimation. Beyond the localization result, we project radar sensor data onto city scale laser scan and generate an scalable occupancy heat map as a traffic analysis tool. This is demonstrated using two radar sensors monitoring an urban area in the real world.</p></p class="citation"></blockquote><h2 id=quant-ph-4>quant-ph (4)</h2><h3 id=14--230273-exploring-quantum-enhanced-machine-learning-for-computer-vision-applications-and-insights-on-noisy-intermediate-scale-quantum-devices-purnachandra-mandadapu-2024>(1/4 | 230/273) Exploring Quantum-Enhanced Machine Learning for Computer Vision: Applications and Insights on Noisy Intermediate-Scale Quantum Devices (Purnachandra Mandadapu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Purnachandra Mandadapu. (2024)<br><strong>Exploring Quantum-Enhanced Machine Learning for Computer Vision: Applications and Insights on Noisy Intermediate-Scale Quantum Devices</strong><br><button class=copy-to-clipboard title="Exploring Quantum-Enhanced Machine Learning for Computer Vision: Applications and Insights on Noisy Intermediate-Scale Quantum Devices" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, quant-ph, quant-ph<br>Keyword Score: 30<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02177v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02177v1.pdf filename=2404.02177v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As medium-scale quantum computers progress, the application of quantum algorithms across diverse fields like simulating physical systems, chemistry, optimization, and cryptography becomes more prevalent. However, these quantum computers, known as Noisy Intermediate Scale Quantum (NISQ), are susceptible to noise, <b>prompting</b> the search for applications that can capitalize on quantum advantage without extensive error correction procedures. Since, Machine Learning (ML), particularly Deep Learning (DL), faces challenges due to resource-intensive training and algorithmic opacity. Therefore, this study explores the intersection of quantum computing and ML, focusing on computer vision tasks. Specifically, it evaluates the effectiveness of hybrid quantum-classical algorithms, such as the data re-uploading scheme and the patch <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GAN)</b> model, on small-scale quantum devices. Through practical implementation and testing, the study reveals comparable or superior performance of these algorithms compared to classical counterparts, highlighting the potential of leveraging quantum algorithms in ML tasks.</p></p class="citation"></blockquote><h3 id=24--231273-no-go-theorem-for-probabilistic-one-way-secret-key-distillation-vishal-singh-et-al-2024>(2/4 | 231/273) No-go theorem for probabilistic one-way secret-key distillation (Vishal Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vishal Singh, Mark M. Wilde. (2024)<br><strong>No-go theorem for probabilistic one-way secret-key distillation</strong><br><button class=copy-to-clipboard title="No-go theorem for probabilistic one-way secret-key distillation" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-IT, math-IT, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01392v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01392v1.pdf filename=2404.01392v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The probabilistic one-way distillable secret key is equal to the largest expected rate at which perfect secret key bits can be probabilistically <b>distilled</b> from a bipartite state by means of local operations and one-way classical communication. Here we define the set of super two-extendible states and prove that an arbitrary state in this set cannot be used for probabilistic one-way secret-key <b>distillation.</b> This broad class of states includes both erased states and all full-rank states. Comparing the probabilistic one-way distillable secret key with the more commonly studied approximate one-way distillable secret key, our results demonstrate an extreme gap between them for many states of interest, with the approximate one-way distillable secret key being much larger. Our findings naturally extend to probabilistic one-way entanglement <b>distillation,</b> with similar conclusions.</p></p class="citation"></blockquote><h3 id=34--232273-random-circuit-sampling-fourier-expansion-and-statistics-gil-kalai-et-al-2024>(3/4 | 232/273) Random Circuit Sampling: Fourier Expansion and Statistics (Gil Kalai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gil Kalai, Yosef Rinott, Tomer Shoham. (2024)<br><strong>Random Circuit Sampling: Fourier Expansion and Statistics</strong><br><button class=copy-to-clipboard title="Random Circuit Sampling: Fourier Expansion and Statistics" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-CC, math-ST, quant-ph, quant-ph, stat-TH<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00935v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00935v1.pdf filename=2404.00935v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Considerable effort in experimental quantum computing is devoted to noisy intermediate scale quantum computers (NISQ computers). Understanding the effect of noise is important for various aspects of this endeavor including notable claims for achieving quantum supremacy and attempts to demonstrate quantum error correcting codes. In this paper we use Fourier methods combined with statistical analysis to study the effect of noise. In particular, we use Fourier analysis to refine the linear cross-entropy fidelity estimator. We use both analytical methods and <b>simulations</b> to study the effect of readout and gate errors, and we use our analysis to study the samples of Google&rsquo;s 2019 quantum supremacy experiment.</p></p class="citation"></blockquote><h3 id=44--233273-parallel-proportional-fusion-of-spiking-quantum-neural-network-for-optimizing-image-classification-zuyu-xu-et-al-2024>(4/4 | 233/273) Parallel Proportional Fusion of Spiking Quantum Neural Network for Optimizing Image Classification (Zuyu Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zuyu Xu, Kang Shen, Pengnian Cai, Tao Yang, Yuanming Hu, Shixian Chen, Yunlai Zhu, Zuheng Wu, Yuehua Dai, Jun Wang, Fei Yang. (2024)<br><strong>Parallel Proportional Fusion of Spiking Quantum Neural Network for Optimizing Image Classification</strong><br><button class=copy-to-clipboard title="Parallel Proportional Fusion of Spiking Quantum Neural Network for Optimizing Image Classification" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-AI, cs-NE, quant-ph, quant-ph<br>Keyword Score: 10<br>Keywords: MNIST<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01359v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01359v1.pdf filename=2404.01359v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent emergence of the hybrid quantum-classical neural network (HQCNN) architecture has garnered considerable attention due to the potential advantages associated with integrating quantum principles to enhance various facets of machine learning algorithms and computations. However, the current investigated serial structure of HQCNN, wherein information sequentially passes from one network to another, often imposes limitations on the trainability and expressivity of the network. In this study, we introduce a novel architecture termed Parallel Proportional Fusion of Quantum and Spiking Neural Networks (PPF-QSNN). The dataset information is simultaneously fed into both the spiking neural network and the variational quantum circuits, with the outputs amalgamated in proportion to their individual contributions. We systematically assess the impact of diverse PPF-QSNN parameters on network performance for image classification, aiming to identify the optimal configuration. Numerical results on the <b>MNIST</b> dataset unequivocally illustrate that our proposed PPF-QSNN outperforms both the existing spiking neural network and the serial quantum neural network across metrics such as accuracy, loss, and robustness. This study introduces a novel and effective amalgamation approach for HQCNN, thereby laying the groundwork for the advancement and application of quantum advantage in artificial intelligent computations.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--234273-a-novel-audio-representation-for-music-genre-identification-in-mir-navin-kamuni-et-al-2024>(1/2 | 234/273) A Novel Audio Representation for Music Genre Identification in MIR (Navin Kamuni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Navin Kamuni, Mayank Jindal, Arpita Soni, Sukender Reddy Mallreddy, Sharath Chandra Macha. (2024)<br><strong>A Novel Audio Representation for Music Genre Identification in MIR</strong><br><button class=copy-to-clipboard title="A Novel Audio Representation for Music Genre Identification in MIR" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-IR, cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 30<br>Keywords: Quantization, Transformer, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01058v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01058v1.pdf filename=2404.01058v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For Music <b>Information</b> <b>Retrieval</b> downstream tasks, the most common audio representation is time-frequency-based, such as Mel spectrograms. In order to identify musical genres, this study explores the possibilities of a new form of audio representation one of the most usual MIR downstream tasks. Therefore, to discretely encoding music using deep vector <b>quantization;</b> a novel audio representation was created for the innovative generative music model i.e. Jukebox. The effectiveness of Jukebox&rsquo;s audio representation is compared to Mel spectrograms using a dataset that is almost equivalent to State-of-the-Art (SOTA) and an almost same <b>transformer</b> design. The results of this study imply that, at least when the <b>transformers</b> are pretrained using a very modest dataset of 20k tracks, Jukebox&rsquo;s audio representation is not superior to Mel spectrograms. This could be explained by the fact that Jukebox&rsquo;s audio representation does not sufficiently take into account the peculiarities of human hearing perception. On the other hand, Mel spectrograms are specifically created with the human auditory sense in mind.</p></p class="citation"></blockquote><h3 id=22--235273-removing-speaker-information-from-speech-representation-using-variable-length-soft-pooling-injune-hwang-et-al-2024>(2/2 | 235/273) Removing Speaker Information from Speech Representation using Variable-Length Soft Pooling (Injune Hwang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Injune Hwang, Kyogu Lee. (2024)<br><strong>Removing Speaker Information from Speech Representation using Variable-Length Soft Pooling</strong><br><button class=copy-to-clipboard title="Removing Speaker Information from Speech Representation using Variable-Length Soft Pooling" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00856v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00856v1.pdf filename=2404.00856v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there have been efforts to encode the linguistic information of speech using a <b>self-supervised</b> framework for speech synthesis. However, predicting representations from surrounding representations can inadvertently entangle speaker information in the speech representation. This paper aims to remove speaker information by exploiting the structured nature of speech, composed of discrete units like phonemes with clear boundaries. A neural network predicts these boundaries, enabling variable-length pooling for event-based representation extraction instead of fixed-rate methods. The boundary predictor outputs a probability for the boundary between 0 and 1, making pooling soft. The model is trained to minimize the difference with the pooled representation of the data augmented by time-stretch and pitch-shift. To confirm that the learned representation includes contents information but is independent of speaker information, the model was evaluated with libri-light&rsquo;s phonetic ABX task and SUPERB&rsquo;s speaker identification task.</p></p class="citation"></blockquote><h2 id=astro-phim-1>astro-ph.IM (1)</h2><h3 id=11--236273-a-novel-sector-based-algorithm-for-an-optimized-star-galaxy-classification-anumanchi-agastya-sai-ram-likhit-et-al-2024>(1/1 | 236/273) A Novel Sector-Based Algorithm for an Optimized Star-Galaxy Classification (Anumanchi Agastya Sai Ram Likhit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anumanchi Agastya Sai Ram Likhit, Divyansh Tripathi, Akshay Agarwal. (2024)<br><strong>A Novel Sector-Based Algorithm for an Optimized Star-Galaxy Classification</strong><br><button class=copy-to-clipboard title="A Novel Sector-Based Algorithm for an Optimized Star-Galaxy Classification" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.IM<br>Categories: astro-ph-IM, astro-ph.IM, cs-LG<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01049v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01049v1.pdf filename=2404.01049v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel sector-based methodology for star-galaxy classification, leveraging the latest Sloan Digital Sky Survey data (SDSS-DR18). By strategically segmenting the sky into sectors aligned with SDSS observational patterns and employing a dedicated <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN),</b> we achieve state-of-the-art performance for star galaxy classification. Our preliminary results demonstrate a promising pathway for efficient and precise astronomical analysis, especially in real-time observational settings.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--237273-utilizing-ai-and-social-media-analytics-to-discover-adverse-side-effects-of-glp-1-receptor-agonists-alon-bartal-et-al-2024>(1/1 | 237/273) Utilizing AI and Social Media Analytics to Discover Adverse Side Effects of GLP-1 Receptor Agonists (Alon Bartal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alon Bartal, Kathleen M. Jagodnik, Nava Pliskin, Abraham Seidmann. (2024)<br><strong>Utilizing AI and Social Media Analytics to Discover Adverse Side Effects of GLP-1 Receptor Agonists</strong><br><button class=copy-to-clipboard title="Utilizing AI and Social Media Analytics to Discover Adverse Side Effects of GLP-1 Receptor Agonists" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: 62, cs-AI, cs-CL, cs-IR, cs-LG, cs-SI, q-bio-QM, q-bio.QM<br>Keyword Score: 30<br>Keywords: ChatGPT, Named Entity Recognition, Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01358v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01358v1.pdf filename=2404.01358v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adverse side effects (ASEs) of drugs, revealed after FDA approval, pose a threat to patient safety. To promptly detect overlooked ASEs, we developed a digital health methodology capable of analyzing massive public data from social media, published clinical research, manufacturers&rsquo; reports, and <b>ChatGPT.</b> We uncovered ASEs associated with the glucagon-like peptide 1 receptor agonists (GLP-1 RA), a market expected to grow exponentially to $133.5 billion USD by 2030. Using a <b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER)</b> model, our method successfully detected 21 potential ASEs overlooked upon FDA approval, including irritability and numbness. Our data-analytic approach revolutionizes the detection of unreported ASEs associated with newly deployed drugs, leveraging cutting-edge AI-driven social media analytics. It can increase the safety of new drugs in the marketplace by unlocking the power of social media to support regulators and manufacturers in the rapid discovery of hidden ASE risks.</p></p class="citation"></blockquote><h2 id=csit-7>cs.IT (7)</h2><h3 id=17--238273-joint-beam-scheduling-and-beamforming-design-for-cooperative-positioning-in-multi-beam-leo-satellite-networks-hongtao-xv-et-al-2024>(1/7 | 238/273) Joint Beam Scheduling and Beamforming Design for Cooperative Positioning in Multi-beam LEO Satellite Networks (Hongtao Xv et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongtao Xv, Yaohua Sun, Yafei Zhao, Mugen Peng, Shijie Zhang. (2024)<br><strong>Joint Beam Scheduling and Beamforming Design for Cooperative Positioning in Multi-beam LEO Satellite Networks</strong><br><button class=copy-to-clipboard title="Joint Beam Scheduling and Beamforming Design for Cooperative Positioning in Multi-beam LEO Satellite Networks" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01148v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01148v1.pdf filename=2404.01148v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cooperative positioning with multiple low earth orbit (LEO) satellites is promising in providing location-based services and enhancing satellite-terrestrial communication. However, positioning accuracy is greatly affected by inter-beam interference and satellite-terrestrial topology <b>geometry.</b> To select the best combination of satellites from visible ones and suppress inter-beam interference, this paper explores the utilization of flexible beam scheduling and beamforming of multi-beam LEO satellites that can adjust beam directions toward the same earth-fixed cell to send positioning signals simultaneously. By leveraging Cram'{e}r-Rao lower bound (CRLB) to characterize user Time Difference of Arrival (TDOA) positioning accuracy, the concerned problem is formulated, aiming at optimizing user positioning accuracy under beam scheduling and beam transmission power constraints. To deal with the mixed-integer-nonconvex problem, we decompose it into an inner beamforming design problem and an outer beam scheduling problem. For the former, we first prove the monotonic relationship between user positioning accuracy and its perceived signal-to-interference-plus-noise ratio (SINR) to reformulate the problem, and then semidefinite relaxation (SDR) is adopted for beamforming design. For the outer problem, a heuristic low-complexity beam scheduling scheme is proposed, whose core idea is to schedule users with lower channel correlation to mitigate inter-beam interference while seeking a proper satellite-terrestrial topology <b>geometry.</b> <b>Simulation</b> results verify the superior positioning performance of our proposed positioning-oriented beamforming and beam scheduling scheme, and it is shown that average user positioning accuracy is improved by $17.1%$ and $55.9%$ when the beam transmission power is 20 dBw, compared to conventional beamforming and beam scheduling schemes, respectively.</p></p class="citation"></blockquote><h3 id=27--239273-distribution-agnostic-database-de-anonymization-under-obfuscation-and-synchronization-errors-serhat-bakirtas-et-al-2024>(2/7 | 239/273) Distribution-Agnostic Database De-Anonymization Under Obfuscation And Synchronization Errors (Serhat Bakirtas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Serhat Bakirtas, Elza Erkip. (2024)<br><strong>Distribution-Agnostic Database De-Anonymization Under Obfuscation And Synchronization Errors</strong><br><button class=copy-to-clipboard title="Distribution-Agnostic Database De-Anonymization Under Obfuscation And Synchronization Errors" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01366v1.pdf filename=2404.01366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Database de-anonymization typically involves matching an anonymized database with correlated publicly available data. Existing research focuses either on practical aspects without requiring knowledge of the data distribution yet provides limited guarantees, or on theoretical aspects assuming known distributions. This paper aims to bridge these two approaches, offering theoretical guarantees for database de-anonymization under synchronization errors and obfuscation without prior knowledge of data distribution. Using a modified replica detection algorithm and a new seeded deletion detection algorithm, we establish sufficient conditions on the database growth rate for successful matching, demonstrating a double-logarithmic seed size relative to row size is sufficient for detecting deletions in the database. Importantly, our findings indicate that these sufficient de-anonymization conditions are tight and are the same as in the distribution-aware setting, avoiding asymptotic performance loss due to unknown distributions. Finally, we evaluate the performance of our proposed algorithms through <b>simulations,</b> confirming their effectiveness in more practical, non-asymptotic, scenarios.</p></p class="citation"></blockquote><h3 id=37--240273-density-evolution-analysis-of-generalized-low-density-parity-check-codes-under-a-posteriori-probability-decoder-dongxu-chang-et-al-2024>(3/7 | 240/273) Density Evolution Analysis of Generalized Low-density Parity-check Codes under a Posteriori Probability Decoder (Dongxu Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongxu Chang, Qingqing Peng, Guanghui Wang, Dawei Yin. (2024)<br><strong>Density Evolution Analysis of Generalized Low-density Parity-check Codes under a Posteriori Probability Decoder</strong><br><button class=copy-to-clipboard title="Density Evolution Analysis of Generalized Low-density Parity-check Codes under a Posteriori Probability Decoder" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01136v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01136v1.pdf filename=2404.01136v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, the performance of generalized low-density parity-check (GLDPC) codes under the a posteriori probability (APP) decoder is analyzed. We explore the concentration, symmetry, and monotonicity properties of GLDPC codes under the APP decoder, extending the applicability of density evolution to GLDPC codes. We demonstrate that with an appropriate proportion of generalized constraint (GC) nodes, GLDPC codes can reduce the original gap to capacity compared to their original LDPC counterparts over the BEC and BI-AWGN channels. Additionally, on the BI-AWGN channel, we adopt Gaussian mixture distributions to approximate the message distributions from variable nodes and Gaussian distributions for those from constraint nodes. This approximation technique significantly enhances the precision of the channel parameter threshold compared to traditional Gaussian approximations while maintaining a low computational complexity similar to that of Gaussian approximations. Our <b>simulation</b> experiments provide empirical evidence that GLDPC codes, when decoded with the APP decoder and equipped with the right fraction of GC nodes, can achieve substantial performance improvements compared to low-density parity-check (LDPC) codes.</p></p class="citation"></blockquote><h3 id=47--241273-performance-evaluation-of-ris-assisted-spatial-modulation-for-downlink-transmission-xusheng-zhu-et-al-2024>(4/7 | 241/273) Performance Evaluation of RIS-Assisted Spatial Modulation for Downlink Transmission (Xusheng Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xusheng Zhu, Qingqing Wu, Wen Chen. (2024)<br><strong>Performance Evaluation of RIS-Assisted Spatial Modulation for Downlink Transmission</strong><br><button class=copy-to-clipboard title="Performance Evaluation of RIS-Assisted Spatial Modulation for Downlink Transmission" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00872v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00872v1.pdf filename=2404.00872v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the performance of reconfigurable intelligent surface (RIS) assisted spatial modulation (SM) downlink communication systems, focusing on the average bit error probability (ABEP). Notably, in scenarios with a large number of reflecting units, the composite channel can be approximated by a Gaussian distribution using the central limit theorem. The receiver utilizes a maximum likelihood detector to recover information in both spatial and symbol domains. In the proposed RIS-SM system, we analytically derive a closed-form expression for the union tight upper bound of ABEP, employing the Gaussian-Chebyshev quadrature method. The validity of these results is rigorously confirmed through exhaustive Monte Carlo <b>simulations.</b></p></p class="citation"></blockquote><h3 id=57--242273-measuring-the-redundancy-of-information-from-a-source-failure-perspective-jesse-milzman-2024>(5/7 | 242/273) Measuring the Redundancy of Information from a Source Failure Perspective (Jesse Milzman, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jesse Milzman. (2024)<br><strong>Measuring the Redundancy of Information from a Source Failure Perspective</strong><br><button class=copy-to-clipboard title="Measuring the Redundancy of Information from a Source Failure Perspective" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: 94A17, cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01470v1.pdf filename=2404.01470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we define a new measure of the redundancy of information from a fault tolerance perspective. The partial information decomposition (PID) emerged last decade as a framework for decomposing the multi-source <b>mutual</b> <b>information</b> $I(T;X_1, &mldr;, X_n)$ into atoms of redundant, synergistic, and unique information. It built upon the notion of redundancy/synergy from McGill&rsquo;s interaction information (McGill 1954). Separately, the redundancy of system components has served as a principle of fault tolerant engineering, for sensing, routing, and control applications. Here, redundancy is understood as the level of duplication necessary for the fault tolerant performance of a system. With these two perspectives in mind, we propose a new PID-based measure of redundancy $I_{\text{ft}}$, based upon the presupposition that redundant information is robust to individual source failures. We demonstrate that this new measure satisfies the common PID axioms from (Williams 2010). In order to do so, we establish an order-reversing correspondence between collections of source-fallible instantiations of a system, on the one hand, and the PID lattice from (Williams 2010), on the other.</p></p class="citation"></blockquote><h3 id=67--243273-rethinking-resource-management-in-edge-learning-a-joint-pre-training-and-fine-tuning-design-paradigm-zhonghao-lyu-et-al-2024>(6/7 | 243/273) Rethinking Resource Management in Edge Learning: A Joint Pre-training and Fine-tuning Design Paradigm (Zhonghao Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhonghao Lyu, Yuchen Li, Guangxu Zhu, Jie Xu, H. Vincent Poor, Shuguang Cui. (2024)<br><strong>Rethinking Resource Management in Edge Learning: A Joint Pre-training and Fine-tuning Design Paradigm</strong><br><button class=copy-to-clipboard title="Rethinking Resource Management in Edge Learning: A Joint Pre-training and Fine-tuning Design Paradigm" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-DC, cs-IT, cs-LG, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00836v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00836v1.pdf filename=2404.00836v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In some applications, edge learning is experiencing a shift in focusing from conventional learning from scratch to new two-stage learning unifying pre-training and task-specific <b>fine-tuning.</b> This paper considers the problem of joint communication and computation resource management in a two-stage edge learning system. In this system, model pre-training is first conducted at an edge server via centralized learning on local pre-stored general data, and then task-specific <b>fine-tuning</b> is performed at edge devices based on the pre-trained model via federated edge learning. For the two-stage learning model, we first analyze the convergence behavior (in terms of the average squared gradient norm bound), which characterizes the impacts of various system parameters such as the number of learning rounds and batch sizes in the two stages on the convergence rate. Based on our analytical results, we then propose a joint communication and computation resource management design to minimize an average squared gradient norm bound, subject to constraints on the transmit power, overall system energy consumption, and training delay. The decision variables include the number of learning rounds, batch sizes, clock frequencies, and transmit power control for both pre-training and <b>fine-tuning</b> stages. Finally, numerical results are provided to evaluate the effectiveness of our proposed design. It is shown that the proposed joint resource management over the pre-training and <b>fine-tuning</b> stages well balances the system performance trade-off among the training accuracy, delay, and energy consumption. The proposed design is also shown to effectively leverage the inherent trade-off between pre-training and <b>fine-tuning,</b> which arises from the differences in data distribution between pre-stored general data versus real-time task-specific data, thus efficiently optimizing overall system performance.</p></p class="citation"></blockquote><h3 id=77--244273-star-ris-aided-secure-mimo-communication-systems-xiequn-dong-et-al-2024>(7/7 | 244/273) STAR-RIS Aided Secure MIMO Communication Systems (Xiequn Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiequn Dong, Zesong Fei, Xinyi Wang, Meng Hua, Qingqing Wu. (2024)<br><strong>STAR-RIS Aided Secure MIMO Communication Systems</strong><br><button class=copy-to-clipboard title="STAR-RIS Aided Secure MIMO Communication Systems" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01059v1.pdf filename=2404.01059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates simultaneous transmission and reflection reconfigurable intelligent surface (STAR-RIS) aided physical layer security (PLS) in multiple-input multiple-output (MIMO) systems, where the base station (BS) transmits secrecy information with the aid of STAR-RIS against multiple eavesdroppers equipped with multiple antennas. We aim to maximize the secrecy rate by jointly optimizing the active beamforming at the BS and passive beamforming at the STAR-RIS, subject to the hardware constraint for STAR-RIS. To handle the coupling variables, a minimum mean-square error (MMSE) based alternating optimization (AO) algorithm is applied. In particular, the amplitudes and phases of STAR-RIS are divided into two blocks to simplify the algorithm design. Besides, by applying the Majorization-Minimization (MM) method, we derive a closed-form expression of the STAR-RIS&rsquo;s phase shifts. Numerical results show that the proposed scheme significantly outperforms various <b>benchmark</b> schemes, especially as the number of STAR-RIS elements increases.</p></p class="citation"></blockquote><h2 id=physicsflu-dyn-1>physics.flu-dyn (1)</h2><h3 id=11--245273-numerical-modelling-of-flame-spread-over-thin-circular-ducts-vipin-kumar-et-al-2024>(1/1 | 245/273) Numerical modelling of flame spread over thin circular ducts (Vipin Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vipin Kumar, Kambam Naresh, Amit Kumar. (2024)<br><strong>Numerical modelling of flame spread over thin circular ducts</strong><br><button class=copy-to-clipboard title="Numerical modelling of flame spread over thin circular ducts" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-NA, math-NA, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01016v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01016v1.pdf filename=2404.01016v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a numerical investigation into the phenomenon of flame spread over thin circular ducts in normal gravity and microgravity environments. Flame spread over such <b>geometry</b> is of significant interest due to its relevance in various practical applications, including tubes for flow purpose in medical system, fire safety in spacecrafts, ducts as well as wiring tubes. This study comprises of a comprehensive investigation of key parameters affecting flame spread rate, including fuel radius and opposed flow speed in normal gravity and microgravity environments. A 2-D axisymmetric flame spread model accounted for char and numerical <b>simulations</b> were performed which revealed valuable insights into the underlying mechanisms governing flame spread over such <b>geometry.</b> The results computed from the numerical model is compared with the experimentally observed flame spread rate to validate the numerical model which can be used to gain a comprehensive understanding of the underlying physical phenomena. As the radius of circular duct increases the flame spread rate increases both in normal gravity and microgravity environments. The conduction heat feedback and radiation heat gain coming from hot char through gas phase at inner core region are the two major mechanisms which controls the flame spread phenomena over the circular duct fuels. The flame spread rate at different flow ranging from quiescent (0 cm/s) to 30 cm/s is also evaluated and 21 % oxygen and found a non-monotonic increasing decreasing trend of flame spread rate at different opposed flow speed in both normal gravity and microgravity environments.</p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=11--246273-distributed-satellite-terrestrial-cooperative-routing-strategy-based-on-minimum-hop-count-analysis-in-mega-leo-satellite-constellation-xinao-feng-et-al-2024>(1/1 | 246/273) Distributed Satellite-Terrestrial Cooperative Routing Strategy Based on Minimum Hop-Count Analysis in Mega LEO Satellite Constellation (Xin&rsquo;ao Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin&rsquo;ao Feng, Yaohua Sun, Mugen Peng. (2024)<br><strong>Distributed Satellite-Terrestrial Cooperative Routing Strategy Based on Minimum Hop-Count Analysis in Mega LEO Satellite Constellation</strong><br><button class=copy-to-clipboard title="Distributed Satellite-Terrestrial Cooperative Routing Strategy Based on Minimum Hop-Count Analysis in Mega LEO Satellite Constellation" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00988v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00988v1.pdf filename=2404.00988v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mega low earth orbit (LEO) satellite constellation is promising in achieving global coverage with high capacity. However, forwarding packets in mega constellation faces long end-to-end delay caused by multi-hop routing and high-complexity routing table construction, which will detrimentally impair the network transmission efficiency. To overcome this issue, a distributed low-complexity satellite-terrestrial cooperative routing approach is proposed in this paper, and its core idea is that each node forwards packets to next-hop node under the constraints of minimum end-to-end hop-count and queuing delay. Particularly, to achieve an accurate and low-complexity minimum end-to-end hop-count estimation in satellite-terrestrial cooperative routing scenario, we first introduce a satellite real-time position based <b>graph</b> (RTPG) to simplify the description of three-dimensional constellation, and further abstract RTPG into a key node based <b>graph</b> (KNBG). Considering the frequent regeneration of KNBG due to satellite movement, a low complexity generation method of KNBG is studied as well. Finally, utilizing KNBG as input, we design the minimum end-to-end hop-count estimation method (KNBG-MHCE). Meanwhile, the computational complexity, routing path survival probability and practical implementation of our proposal are all deeply discussed. Extensive <b>simulations</b> are also conducted in systems with Ka and laser band inter-satellite links to verify the superiority of our proposal.</p></p class="citation"></blockquote><h2 id=eesssy-7>eess.SY (7)</h2><h3 id=17--247273-using-dynamic-safety-margins-as-control-barrier-functions-victor-freire-et-al-2024>(1/7 | 247/273) Using Dynamic Safety Margins as Control Barrier Functions (Victor Freire et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor Freire, Marco M. Nicotra. (2024)<br><strong>Using Dynamic Safety Margins as Control Barrier Functions</strong><br><button class=copy-to-clipboard title="Using Dynamic Safety Margins as Control Barrier Functions" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01445v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01445v1.pdf filename=2404.01445v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper provides an approach to design control barrier functions (CBFs) using the notion of dynamic safety margins (DSMs). In particular, it is shown that DSMs are CBFs for an augmented system. The proposed approach can handle multiple state and input constraints using the control-sharing property of CBFs. Moreover, it makes no assumption on the relative degree of the constraints. Numerical <b>simulations</b> show that the method outperforms existing DSM-based approaches, while also guaranteeing safety and recursive feasibility.</p></p class="citation"></blockquote><h3 id=27--248273-foundations-of-cyber-resilience-the-confluence-of-game-control-and-learning-theories-quanyan-zhu-2024>(2/7 | 248/273) Foundations of Cyber Resilience: The Confluence of Game, Control, and Learning Theories (Quanyan Zhu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quanyan Zhu. (2024)<br><strong>Foundations of Cyber Resilience: The Confluence of Game, Control, and Learning Theories</strong><br><button class=copy-to-clipboard title="Foundations of Cyber Resilience: The Confluence of Game, Control, and Learning Theories" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-CR, cs-GT, cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Foundation Model, Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01205v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01205v2.pdf filename=2404.01205v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cyber resilience is a complementary concept to cybersecurity, focusing on the preparation, response, and recovery from cyber threats that are challenging to prevent. Organizations increasingly face such threats in an evolving cyber threat landscape. Understanding and establishing <b>foundations</b> <b>for</b> cyber resilience provide a quantitative and systematic approach to cyber risk assessment, mitigation policy evaluation, and risk-informed defense design. A systems-scientific view toward cyber risks provides holistic and system-level solutions. This chapter starts with a systemic view toward cyber risks and presents the confluence of game theory, control theory, and learning theories, which are three major pillars for the design of cyber resilience mechanisms to counteract increasingly sophisticated and evolving threats in our networks and organizations. Game and control theoretic methods provide a set of modeling frameworks to capture the strategic and dynamic interactions between defenders and attackers. Control and learning frameworks together provide a feedback-driven mechanism that enables autonomous and adaptive responses to threats. Game and learning frameworks offer a data-driven approach to proactively reason about adversarial behaviors and resilient strategies. The confluence of the three lays the theoretical <b>foundations</b> <b>for</b> the analysis and design of cyber resilience. This chapter presents various theoretical paradigms, including dynamic asymmetric games, moving horizon control, conjectural learning, and <b>meta-learning,</b> <b>as</b> recent advances at the intersection. This chapter concludes with future directions and discussions of the role of neurosymbolic learning and the synergy between <b>foundation</b> <b>models</b> and game models in cyber resilience.</p></p class="citation"></blockquote><h3 id=37--249273-research-on-mechanism-of-voltage-oscillation-caused-by-repeated-lvrt-of-wind-turbine-based-on-switched-system-theory-qiping-lai-et-al-2024>(3/7 | 249/273) Research on Mechanism of Voltage Oscillation Caused by Repeated LVRT of Wind Turbine Based on Switched System Theory (Qiping Lai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiping Lai, Chen Shen, Dongsheng Li. (2024)<br><strong>Research on Mechanism of Voltage Oscillation Caused by Repeated LVRT of Wind Turbine Based on Switched System Theory</strong><br><button class=copy-to-clipboard title="Research on Mechanism of Voltage Oscillation Caused by Repeated LVRT of Wind Turbine Based on Switched System Theory" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01155v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01155v1.pdf filename=2404.01155v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The electrical distance between the wind power collection sending end grid and the main grid is relatively long, lacking synchronous power supply support, showing the characteristics of weak grid. Therefore, the voltage oscillation phenomenon is easy to happen, threatening the safe and stable operation of the grid. Its dynamic process and evolution mechanism need to be studied urgently. This paper firstly analyzes conditions for voltage oscillations caused by repeated low voltage ride through (LVRT) of wind turbine through steady-state power flow calculation. Then, based on the switched system theory, considering the external connected impedance and internal control dynamics of the wind turbine, the switched system model for the grid-side converter (GSC) of wind turbine is established. After that, the relevant parameters are substituted to analyze the dynamic evolution process of each electrical quantity during the process that wind turbine repeatedly enters and exits LVRT, revealing the evolution mechanism of voltage oscillations. Finally, the voltage oscillation phenomenon of the grid-connected point of wind turbine is reproduced through <b>simulation,</b> verifying the correctness and effectiveness of theoretical analysis results. Furthermore, the main factors which influence characteristics of voltage oscillations are explored as well.</p></p class="citation"></blockquote><h3 id=47--250273-second-order-newton-based-extremum-seeking-for-multivariable-static-maps-azad-ghaffari-et-al-2024>(4/7 | 250/273) Second-Order Newton-Based Extremum Seeking for Multivariable Static Maps (Azad Ghaffari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Azad Ghaffari, Tiago Roux Oliveira. (2024)<br><strong>Second-Order Newton-Based Extremum Seeking for Multivariable Static Maps</strong><br><button class=copy-to-clipboard title="Second-Order Newton-Based Extremum Seeking for Multivariable Static Maps" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01103v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01103v1.pdf filename=2404.01103v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A second-order Newton-based extremum seeking (SONES) algorithm is presented to estimate directional inflection points for multivariable static maps. The design extends the first-order Newton-based extremum seeking algorithm that drives the system toward its peak point. This work provides perturbation matrices to estimate the second- and third-order derivatives necessary for implementation of the SONES. A set of conditions are provided for the probing frequencies that ensure accurate estimation of the derivatives. A differential Riccati filter is used to calculate the inverse of the third-order derivative. The local stability of the new algorithm is proven for general multivariable static maps using averaging analysis. The proposed algorithm ensures uniform convergence toward directional inflection point without requiring information about the curvature of the map and its gradient. <b>Simulation</b> results show the effectiveness of the proposed algorithm.</p></p class="citation"></blockquote><h3 id=57--251273-finite-sample-frequency-domain-identification-anastasios-tsiamis-et-al-2024>(5/7 | 251/273) Finite Sample Frequency Domain Identification (Anastasios Tsiamis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anastasios Tsiamis, Mohamed Abdalmoaty, Roy S. Smith, John Lygeros. (2024)<br><strong>Finite Sample Frequency Domain Identification</strong><br><button class=copy-to-clipboard title="Finite Sample Frequency Domain Identification" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY, math-OC, stat-ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01100v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01100v1.pdf filename=2404.01100v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study non-parametric frequency-domain system identification from a finite-sample perspective. We assume an open loop scenario where the excitation input is periodic and consider the Empirical Transfer Function Estimate (ETFE), where the goal is to estimate the frequency response at certain desired (evenly-spaced) frequencies, given input-output samples. We show that under sub-Gaussian colored noise (in time-domain) and stability assumptions, the ETFE estimates are concentrated around the true values. The error rate is of the order of $\mathcal{O}((d_{\mathrm{u}}+\sqrt{d_{\mathrm{u}}d_{\mathrm{y}}})\sqrt{M/N_{\mathrm{tot}}})$, where $N_{\mathrm{tot}}$ is the total number of samples, $M$ is the number of desired frequencies, and $d_{\mathrm{u}},,d_{\mathrm{y}}$ are the dimensions of the input and output signals respectively. This rate remains valid for general irrational transfer functions and does not require a finite order state-space representation. By tuning $M$, we obtain a $N_{\mathrm{tot}}^{-1/3}$ finite-sample rate for learning the frequency response over all frequencies in the $ \mathcal{H}_{\infty}$ norm. Our result draws upon an extension of the Hanson-Wright inequality to semi-infinite matrices. We study the finite-sample behavior of ETFE in <b>simulations.</b></p></p class="citation"></blockquote><h3 id=67--252273-performance-triggered-adaptive-model-reduction-for-soil-moisture-estimation-in-precision-irrigation-sarupa-debnath-et-al-2024>(6/7 | 252/273) Performance triggered adaptive model reduction for soil moisture estimation in precision irrigation (Sarupa Debnath et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sarupa Debnath, Bernard T. Agyeman, Soumya R. Sahoo, Xunyuan Yin, Jinfeng Liu. (2024)<br><strong>Performance triggered adaptive model reduction for soil moisture estimation in precision irrigation</strong><br><button class=copy-to-clipboard title="Performance triggered adaptive model reduction for soil moisture estimation in precision irrigation" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-DS, stat-AP<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01468v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01468v1.pdf filename=2404.01468v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate soil moisture information is crucial for developing precise irrigation control strategies to enhance water use efficiency. Soil moisture estimation based on limited soil moisture sensors is crucial for obtaining comprehensive soil moisture information when dealing with large-scale agricultural fields. The major challenge in soil moisture estimation lies in the high dimensionality of the spatially discretized agro-hydrological models. In this work, we propose a performance-triggered adaptive model reduction approach to address this challenge. The proposed approach employs a trajectory-based <b>unsupervised</b> machine learning technique, and a prediction performance-based triggering scheme is designed to govern model updates adaptively in a way such that the prediction error between the reduced model and the original model over a prediction horizon is maintained below a predetermined threshold. An adaptive extended Kalman filter (EKF) is designed based on the reduced model for soil moisture estimation. The applicability and performance of the proposed approach are evaluated extensively through the application to a simulated large-scale agricultural field.</p></p class="citation"></blockquote><h3 id=77--253273-orchestrating-uavs-for-prioritized-data-harvesting-a-cross-layer-optimization-perspective-bharath-keshavamurthy-et-al-2024>(7/7 | 253/273) Orchestrating UAVs for Prioritized Data Harvesting: A Cross-Layer Optimization Perspective (Bharath Keshavamurthy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bharath Keshavamurthy, Nicolo Michelusi. (2024)<br><strong>Orchestrating UAVs for Prioritized Data Harvesting: A Cross-Layer Optimization Perspective</strong><br><button class=copy-to-clipboard title="Orchestrating UAVs for Prioritized Data Harvesting: A Cross-Layer Optimization Perspective" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 6<br>Keywords: Graph, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00961v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00961v1.pdf filename=2404.00961v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work describes the orchestration of a fleet of rotary-wing Unmanned Aerial Vehicles (UAVs) for harvesting prioritized traffic from random distributions of heterogeneous users with Multiple Input Multiple Output (MIMO) capabilities. In a finite-horizon offline setting, the goal is to optimize the beam-forming design, the 3D UAV positioning and trajectory solution, and the user association/scheduling policy, to maximize the cumulative fleet-wide reward obtained by satisfying the quality-of-service mandates imposed on each user uplink request, subject to an average per-UAV mobility power constraint. With a probabilistic air-to-ground channel model, a multi-user MIMO uplink communication model with prioritized traffic, and a novel 3D mobility model for rotary-wing UAVs, the fleet-wide reward maximization problem is solved via a cross-layer optimization framework: first, K-means <b>clustering</b> is employed to obtain user clusters; then, equipped with a zero-forcing beam-forming design, the positions of the UAVs are optimized via two-stage grid search; next, treating these optimal positions as the <b>graph</b> vertices of a fully-connected mesh, the 3D UAV trajectories (i.e., <b>graph</b> edges) are designed via a learning based competitive swarm optimization algorithm, under an average UAV power consumption constraint, coupled with projected subgradient ascent for dual optimization; consequently, the user association/scheduling strategy is solved via a graphical branch-and-bound method on the underlying multiple traveling salesman problem. Numerical evaluations demonstrate that the proposed solution outperforms static UAV deployments, adaptive Voronoi decomposition techniques, and state-of-the-art iterative fleet control algorithms, with respect to user quality-of-service and per-UAV average power consumption.</p></p class="citation"></blockquote><h2 id=mathna-3>math.NA (3)</h2><h3 id=13--254273-convergence-acceleration-of-favre-averaged-non-linear-harmonic-method-feng-wang-et-al-2024>(1/3 | 254/273) Convergence Acceleration of Favre-Averaged Non-Linear Harmonic Method (Feng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feng Wang, Kurt Webber, David Radford, Luca di Mare, Marcus Meyer. (2024)<br><strong>Convergence Acceleration of Favre-Averaged Non-Linear Harmonic Method</strong><br><button class=copy-to-clipboard title="Convergence Acceleration of Favre-Averaged Non-Linear Harmonic Method" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-MP, math-NA, math-ph, math.NA, physics-flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01407v1.pdf filename=2404.01407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper develops a numerical procedure to accelerate the convergence of the Favre-averaged Non-Linear Harmonic (FNLH) method. The scheme provides a unified mathematical framework for solving the sparse linear systems formed by the mean flow and the time-linearized harmonic flows of FNLH in an explicit or implicit fashion. The approach explores the similarity of the sparse linear systems of FNLH and leads to a memory efficient procedure, so that its memory consumption does not depend on the number of harmonics to compute. The proposed method has been implemented in the industrial CFD solver HYDRA. Two test cases are used to conduct a comparative study of explicit and implicit schemes in terms of convergence, computational efficiency, and memory consumption. Comparisons show that the implicit scheme yields better convergence than the explicit scheme and is also roughly 7 to 10 times more computationally efficient than the explicit scheme with 4 levels of multigrid. Furthermore, the implicit scheme consumes only approximately $50%$ of the explicit scheme with four levels of multigrid. Compared with the full annulus unsteady Reynolds averaged Navier-Stokes (URANS) <b>simulations,</b> the implicit scheme produces comparable results to URANS with computational time and memory consumption that are two orders of magnitude smaller.</p></p class="citation"></blockquote><h3 id=23--255273-capturing-shock-waves-by-relaxation-neural-networks-nan-zhou-et-al-2024>(2/3 | 255/273) Capturing Shock Waves by Relaxation Neural Networks (Nan Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nan Zhou, Zheng Ma. (2024)<br><strong>Capturing Shock Waves by Relaxation Neural Networks</strong><br><button class=copy-to-clipboard title="Capturing Shock Waves by Relaxation Neural Networks" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 76L05, 35D99, 68T07, 65D15, cs-AI, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01163v1.pdf filename=2404.01163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we put forward a neural network framework to solve the nonlinear hyperbolic systems. This framework, named relaxation neural networks(RelaxNN), is a simple and scalable extension of physics-informed neural networks(PINN). It is shown later that a typical PINN framework struggles to handle shock waves that arise in hyperbolic systems&rsquo; solutions. This ultimately results in the failure of optimization that is based on gradient descent in the training process. Relaxation systems provide a smooth asymptotic to the discontinuity solution, under the expectation that macroscopic problems can be solved from a microscopic perspective. Based on relaxation systems, the RelaxNN framework alleviates the conflict of losses in the training process of the PINN framework. In addition to the remarkable results demonstrated in numerical <b>simulations,</b> most of the acceleration techniques and improvement strategies aimed at the standard PINN framework can also be applied to the RelaxNN framework.</p></p class="citation"></blockquote><h3 id=33--256273-adaptive-hybrid-high-order-method-for-guaranteed-lower-eigenvalue-bounds-carsten-carstensen-et-al-2024>(3/3 | 256/273) Adaptive hybrid high-order method for guaranteed lower eigenvalue bounds (Carsten Carstensen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carsten Carstensen, Benedikt Gräßle, Ngoc Tien Tran. (2024)<br><strong>Adaptive hybrid high-order method for guaranteed lower eigenvalue bounds</strong><br><button class=copy-to-clipboard title="Adaptive hybrid high-order method for guaranteed lower eigenvalue bounds" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65N12, 65N30, 65Y20, cs-NA, math-NA, math.NA<br>Keyword Score: 13<br>Keywords: Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01228v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01228v1.pdf filename=2404.01228v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The higher-order guaranteed lower eigenvalue bounds of the Laplacian in the recent work by Carstensen, Ern, and Puttkammer [Numer. Math. 149, 2021] require a parameter $C_{\mathrm{st},1}$ that is found $\textit{not}$ robust as the polynomial degree $p$ increases. This is related to the $H^1$ stability bound of the $L^2$ projection onto polynomials of degree at most $p$ and its growth $C_{\rm st, 1}\propto (p+1)^{1/2}$ as $p \to \infty$. A similar estimate for the Galerkin projection holds with a $p$-robust constant $C_{\mathrm{st},2}$ and $C_{\mathrm{st},2} \le 2$ for right-isosceles triangles. This paper utilizes the new inequality with the constant $C_{\mathrm{st},2}$ to design a modified hybrid high-order (HHO) eigensolver that directly computes guaranteed lower eigenvalue bounds under the idealized hypothesis of exact solve of the generalized algebraic eigenvalue problem and a mild explicit condition on the maximal mesh-size in the simplicial mesh. A key advance is a $p$-robust parameter selection. The analysis of the new method with a different <b>fine-tuned</b> volume stabilization allows for a priori quasi-best approximation and improved $L^2$ error estimates as well as a stabilization-free reliable and efficient a posteriori error control. The associated adaptive mesh-refining algorithm performs superior in computer <b>benchmarks</b> with striking numerical evidence for optimal higher empirical convergence rates.</p></p class="citation"></blockquote><h2 id=mathoc-2>math.OC (2)</h2><h3 id=12--257273-dynamics-and-optimization-in-spatially-distributed-electrical-vehicle-charging-fernando-paganini-et-al-2024>(1/2 | 257/273) Dynamics and Optimization in Spatially Distributed Electrical Vehicle Charging (Fernando Paganini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fernando Paganini, Andres Ferragut. (2024)<br><strong>Dynamics and Optimization in Spatially Distributed Electrical Vehicle Charging</strong><br><button class=copy-to-clipboard title="Dynamics and Optimization in Spatially Distributed Electrical Vehicle Charging" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01259v1.pdf filename=2404.01259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a spatially distributed demand for electrical vehicle recharging, that must be covered by a fixed set of charging stations. Arriving EVs receive feedback on transport times to each station, and waiting times at congested ones, based on which they make a selfish selection. This selection determines total arrival rates in station queues, which are represented by a fluid state; departure rates are modeled under the assumption that clients have a given sojourn time in the system. The resulting differential equation system is analyzed with tools of optimization. We characterize the equilibrium as the solution to a specific convex program, which has connections to optimal transport problems, and also with road traffic theory. In particular a price of anarchy appears with respect to a social planner&rsquo;s allocation. From a dynamical perspective, global convergence to equilibrium is established, with tools of Lagrange duality and Lyapunov theory. An extension of the model that makes customer demand elastic to observed delays is also presented, and analyzed with extensions of the optimization machinery. <b>Simulations</b> to illustrate the global behavior are presented, which also help validate the model beyond the fluid approximation.</p></p class="citation"></blockquote><h3 id=22--258273-multiple-joint-chance-constraints-approximation-for-uncertainty-modeling-in-dispatch-problems-yilin-wen-et-al-2024>(2/2 | 258/273) Multiple Joint Chance Constraints Approximation for Uncertainty Modeling in Dispatch Problems (Yilin Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yilin Wen, Yi Guo, Zechun Hu, Gabriela Hug. (2024)<br><strong>Multiple Joint Chance Constraints Approximation for Uncertainty Modeling in Dispatch Problems</strong><br><button class=copy-to-clipboard title="Multiple Joint Chance Constraints Approximation for Uncertainty Modeling in Dispatch Problems" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01167v1.pdf filename=2404.01167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Uncertainty modeling has become increasingly important in power system decision-making. The widely-used tractable uncertainty modeling method-chance constraints with Conditional Value at Risk (CVaR) approximation, can be overconservative and even turn an originally feasible problem into an infeasible one. This paper proposes a new approximation method for multiple joint chance constraints (JCCs) to model the uncertainty in dispatch problems, which solves the conservativeness and potential infeasibility concerns of CVaR. The proposed method is also convenient for controlling the risk levels of different JCCs, which is necessary for power system applications since different resources may be affected by varying degrees of uncertainty or have different importance to the system. We then formulate a data-driven distributionally robust chance-constrained programming model for the power system multiperiod dispatch problem and leverage the proposed approximation method to solve it. In the numerical <b>simulations,</b> two small general examples clearly demonstrate the superiority of the proposed method, and the results of the multiperiod dispatch problem on IEEE test cases verify its practicality.</p></p class="citation"></blockquote><h2 id=csma-2>cs.MA (2)</h2><h3 id=12--259273-a-crisp-dm-based-methodology-for-assessing-agent-based-simulation-models-using-process-mining-rob-h-bemthuis-et-al-2024>(1/2 | 259/273) A CRISP-DM-based Methodology for Assessing Agent-based Simulation Models using Process Mining (Rob H. Bemthuis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rob H. Bemthuis, Ruben R. Govers, Amin Asadi. (2024)<br><strong>A CRISP-DM-based Methodology for Assessing Agent-based Simulation Models using Process Mining</strong><br><button class=copy-to-clipboard title="A CRISP-DM-based Methodology for Assessing Agent-based Simulation Models using Process Mining" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01114v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01114v1.pdf filename=2404.01114v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Agent-based <b>simulation</b> (ABS) models are potent tools for analyzing complex systems. However, understanding and validating ABS models can be a significant challenge. To address this challenge, cutting-edge data-driven techniques offer sophisticated capabilities for analyzing the outcomes of ABS models. One such technique is process mining, which encompasses a range of methods for discovering, monitoring, and enhancing processes by extracting knowledge from event logs. However, applying process mining to event logs derived from ABSs is not trivial, and deriving meaningful insights from the resulting process models adds an additional layer of complexity. Although process mining is invaluable in extracting insights from ABS models, there is a lack of comprehensive methodological guidance for its application in ABS evaluation in the research landscape. In this paper, we propose a methodology, based on the CRoss-Industry Standard Process for Data Mining (CRISP-DM) methodology, to assess ABS models using process mining techniques. We incorporate process mining techniques into the stages of the CRISP-DM methodology, facilitating the analysis of ABS model behaviors and their underlying processes. We demonstrate our methodology using an established agent-based model, Schelling model of segregation. Our results show that our proposed methodology can effectively assess ABS models through produced event logs, potentially paving the way for enhanced agent-based model validity and more insightful decision-making.</p></p class="citation"></blockquote><h3 id=22--260273-gov-rek-governed-reward-engineering-kernels-for-designing-robust-multi-agent-reinforcement-learning-systems-ashish-rana-et-al-2024>(2/2 | 260/273) GOV-REK: Governed Reward Engineering Kernels for Designing Robust Multi-Agent Reinforcement Learning Systems (Ashish Rana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashish Rana, Michael Oesterle, Jannik Brinkmann. (2024)<br><strong>GOV-REK: Governed Reward Engineering Kernels for Designing Robust Multi-Agent Reinforcement Learning Systems</strong><br><button class=copy-to-clipboard title="GOV-REK: Governed Reward Engineering Kernels for Designing Robust Multi-Agent Reinforcement Learning Systems" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-MA, cs.MA<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01131v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01131v1.pdf filename=2404.01131v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For multi-agent <b>reinforcement</b> <b>learning</b> systems (MARLS), the problem formulation generally involves investing massive reward engineering effort specific to a given problem. However, this effort often cannot be translated to other problems; worse, it gets wasted when system dynamics change drastically. This problem is further exacerbated in sparse reward scenarios, where a meaningful heuristic can assist in the policy convergence task. We propose GOVerned Reward Engineering Kernels (GOV-REK), which dynamically assign reward distributions to agents in MARLS during its learning stage. We also introduce governance kernels, which exploit the underlying structure in either state or joint action space for assigning meaningful agent reward distributions. During the agent learning stage, it iteratively explores different reward distribution configurations with a Hyperband-like algorithm to learn ideal agent reward models in a problem-agnostic manner. Our experiments demonstrate that our meaningful reward priors robustly jumpstart the learning process for effectively learning different MARL problems.</p></p class="citation"></blockquote><h2 id=nlinao-1>nlin.AO (1)</h2><h3 id=11--261273-nonlinear-impulse-pattern-formulation-dynamical-social-and-political-prediction-algorithm-for-city-planning-and-public-participation-rolf-bader-et-al-2024>(1/1 | 261/273) Nonlinear Impulse Pattern Formulation dynamical social and political prediction algorithm for city planning and public participation (Rolf Bader et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rolf Bader, Simon Linke, Stefanie Gernert. (2024)<br><strong>Nonlinear Impulse Pattern Formulation dynamical social and political prediction algorithm for city planning and public participation</strong><br><button class=copy-to-clipboard title="Nonlinear Impulse Pattern Formulation dynamical social and political prediction algorithm for city planning and public participation" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: nlin.AO<br>Categories: cs-AI, math-DS, nlin-AO, nlin.AO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00977v1.pdf filename=2404.00977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A nonlinear-dynamical algorithm for city planning is proposed as an Impulse Pattern Formulation (IPF) for predicting relevant parameters like health, artistic freedom, or financial developments of different social or political stakeholders over the cause of a planning process. The IPF has already shown high predictive precision at low computational cost in musical instrument <b>simulations,</b> brain dynamics, and human-human interactions. The social and political IPF consists of three basic equations of system state developments, self-adaptation of stakeholders, two adaptive interactions, and external impact terms suitable for respective planning situations. Typical scenarios of stakeholder interactions and developments are modeled by adjusting a set of system parameters. These include stakeholder reaction to external input, enhanced system stability through self-adaptation, stakeholder convergence due to mediative interaction adaptation, as well as complex dynamics in terms of direct stakeholder impacts. A workflow for implementing the algorithm in real city planning scenarios is outlined. This workflow includes machine learning of a suitable set of parameters suggesting best-practice planning to aim at the desired development of the planning process and its output.</p></p class="citation"></blockquote><h2 id=eesssp-2>eess.SP (2)</h2><h3 id=12--262273-digital-twins-for-supporting-ai-research-with-autonomous-vehicle-networks-anıl-gürses-et-al-2024>(1/2 | 262/273) Digital Twins for Supporting AI Research with Autonomous Vehicle Networks (Anıl Gürses et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anıl Gürses, Gautham Reddy, Saad Masrur, Özgür Özdemir, İsmail Güvenç, Mihail L. Sichitiu, Alphan Şahin, Ahmed Alkhateeb, Rudra Dutta. (2024)<br><strong>Digital Twins for Supporting AI Research with Autonomous Vehicle Networks</strong><br><button class=copy-to-clipboard title="Digital Twins for Supporting AI Research with Autonomous Vehicle Networks" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-NI, eess-SP, eess.SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00954v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00954v1.pdf filename=2404.00954v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Digital twins (DTs), which are virtual environments that simulate, predict, and optimize the performance of their physical counterparts, are envisioned to be essential technologies for advancing next-generation wireless networks. While DTs have been studied extensively for wireless networks, their use in conjunction with autonomous vehicles with programmable mobility remains relatively under-explored. In this paper, we study DTs used as a development environment to design, deploy, and test artificial intelligence (AI) techniques that use real-time observations, e.g. radio key performance indicators, for vehicle trajectory and network optimization decisions in an autonomous vehicle networks (AVN). We first compare and contrast the use of <b>simulation,</b> digital twin (software in the loop (SITL)), sandbox (hardware-in-the-loop (HITL)), and physical testbed environments for their suitability in developing and testing AI algorithms for AVNs. We then review various representative use cases of DTs for AVN scenarios. Finally, we provide an example from the NSF AERPAW platform where a DT is used to develop and test AI-aided solutions for autonomous unmanned aerial vehicles for localizing a signal source based solely on link quality measurements. Our results in the physical testbed show that SITL DTs, when supplemented with data from real-world (RW) measurements and <b>simulations,</b> can serve as an ideal environment for developing and testing innovative AI solutions for AVNs.</p></p class="citation"></blockquote><h3 id=22--263273-wideband-channel-capacity-maximization-with-beyond-diagonal-ris-reflection-matrices-özlem-tuğfe-demir-et-al-2024>(2/2 | 263/273) Wideband Channel Capacity Maximization With Beyond Diagonal RIS Reflection Matrices (Özlem Tuğfe Demir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Özlem Tuğfe Demir, Emil Björnson. (2024)<br><strong>Wideband Channel Capacity Maximization With Beyond Diagonal RIS Reflection Matrices</strong><br><button class=copy-to-clipboard title="Wideband Channel Capacity Maximization With Beyond Diagonal RIS Reflection Matrices" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00982v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00982v1.pdf filename=2404.00982v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Following the promising beamforming gains offered by reconfigurable intelligent surfaces (RISs), a new hardware architecture, known as \emph{beyond diagonal RIS (BD-RIS)}, has recently been proposed. This architecture enables controllable signal flows between the RIS elements, thereby providing greater design flexibility. However, the physics-imposed symmetry and orthogonality conditions on the non-diagonal reflection matrix make the design challenging. In this letter, we analyze how a BD-RIS can improve a wideband channel, starting from fundamental principles and deriving the capacity. Our analysis considers the effects of various channel taps and their frequency-domain characteristics. We introduce a new algorithm designed to optimize the configuration of the BD-RIS to maximize wideband capacity. The proposed algorithm has better performance than the <b>benchmarks.</b> A BD-RIS is beneficial compared to a conventional RIS in the absence of static path or when the Rician $\kappa$-factor is smaller than $10$.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--264273-a-hat-trick-automatically-verifying-representation-invariants-using-symbolic-finite-automata-zhe-zhou-et-al-2024>(1/1 | 264/273) A HAT Trick: Automatically Verifying Representation Invariants Using Symbolic Finite Automata (Zhe Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhe Zhou, Qianchuan Ye, Benjamin Delaware, Suresh Jagannathan. (2024)<br><strong>A HAT Trick: Automatically Verifying Representation Invariants Using Symbolic Finite Automata</strong><br><button class=copy-to-clipboard title="A HAT Trick: Automatically Verifying Representation Invariants Using Symbolic Finite Automata" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: D-3-0; F-3-1, cs-PL, cs.PL<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01484v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01484v1.pdf filename=2404.01484v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Functional programs typically interact with stateful libraries that hide state behind typed abstractions. One particularly important class of applications are data structure implementations that rely on such libraries to provide a level of efficiency and scalability that may be otherwise difficult to achieve. However, because the specifications of the methods provided by these libraries are necessarily general and rarely specialized to the needs of any specific client, any required application-level invariants must often be expressed in terms of additional constraints on the (often) opaque state maintained by the library. In this paper, we consider the specification and verification of such representation invariants using symbolic finite automata (SFA). We show that SFAs can be used to succinctly and precisely capture fine-grained temporal and data-dependent histories of interactions between functional clients and stateful libraries. To facilitate modular and compositional <b>reasoning,</b> we integrate SFAs into a refinement type system to qualify stateful computations resulting from such interactions. The particular instantiation we consider, Hoare Automata Types (HATs), allows us to both specify and automatically type-check the representation invariants of a datatype, even when its implementation depends on stateful library methods that operate over hidden state. We also develop a new bidirectional type checking algorithm that implements an efficient subtyping inclusion check over HATs, enabling their translation into a form amenable for SMT-based automated verification. We present extensive experimental results on an implementation of this algorithm that demonstrates the feasibility of type-checking complex and sophisticated HAT-specified OCaml data structure implementations.</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--265273-game-theoretic-deep-reinforcement-learning-to-minimize-carbon-emissions-and-energy-costs-for-ai-inference-workloads-in-geo-distributed-data-centers-ninad-hogade-et-al-2024>(1/1 | 265/273) Game-Theoretic Deep Reinforcement Learning to Minimize Carbon Emissions and Energy Costs for AI Inference Workloads in Geo-Distributed Data Centers (Ninad Hogade et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ninad Hogade, Sudeep Pasricha. (2024)<br><strong>Game-Theoretic Deep Reinforcement Learning to Minimize Carbon Emissions and Energy Costs for AI Inference Workloads in Geo-Distributed Data Centers</strong><br><button class=copy-to-clipboard title="Game-Theoretic Deep Reinforcement Learning to Minimize Carbon Emissions and Energy Costs for AI Inference Workloads in Geo-Distributed Data Centers" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-AI, cs-DC, cs-LG, cs.DC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01459v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01459v1.pdf filename=2404.01459v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data centers are increasingly using more energy due to the rise in Artificial Intelligence (AI) workloads, which negatively impacts the environment and raises operational costs. Reducing operating expenses and carbon emissions while maintaining performance in data centers is a challenging problem. This work introduces a unique approach combining Game Theory (GT) and Deep <b>Reinforcement</b> <b>Learning</b> (DRL) for optimizing the distribution of AI inference workloads in geo-distributed data centers to reduce carbon emissions and cloud operating (energy + data transfer) costs. The proposed technique integrates the principles of non-cooperative Game Theory into a DRL framework, enabling data centers to make intelligent decisions regarding workload allocation while considering the heterogeneity of hardware resources, the dynamic nature of electricity prices, inter-data center data transfer costs, and carbon footprints. We conducted extensive experiments comparing our game-theoretic DRL (GT-DRL) approach with current DRL-based and other optimization techniques. The results demonstrate that our strategy outperforms the state-of-the-art in reducing carbon emissions and minimizing cloud operating costs without compromising computational performance. This work has significant implications for achieving sustainability and cost-efficiency in data centers handling AI inference workloads across diverse geographic locations.</p></p class="citation"></blockquote><h2 id=physicsmed-ph-1>physics.med-ph (1)</h2><h3 id=11--266273-prior-frequency-guided-diffusion-model-for-limited-angle-la-cbct-reconstruction-jiacheng-xie-et-al-2024>(1/1 | 266/273) Prior Frequency Guided Diffusion Model for Limited Angle (LA)-CBCT Reconstruction (Jiacheng Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiacheng Xie, Hua-Chieh Shao, Yunxiang Li, You Zhang. (2024)<br><strong>Prior Frequency Guided Diffusion Model for Limited Angle (LA)-CBCT Reconstruction</strong><br><button class=copy-to-clipboard title="Prior Frequency Guided Diffusion Model for Limited Angle (LA)-CBCT Reconstruction" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.med-ph<br>Categories: cs-LG, physics-med-ph, physics.med-ph<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01448v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01448v1.pdf filename=2404.01448v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cone-beam computed tomography (CBCT) is widely used in image-guided radiotherapy. Reconstructing CBCTs from limited-angle acquisitions (LA-CBCT) is highly desired for improved imaging efficiency, dose reduction, and better mechanical clearance. LA-CBCT reconstruction, however, suffers from severe under-sampling artifacts, making it a highly ill-posed inverse problem. <b>Diffusion</b> <b>models</b> can generate data/images by reversing a data-noising process through learned data distributions; and can be incorporated as a denoiser/regularizer in LA-CBCT reconstruction. In this study, we developed a <b>diffusion</b> <b>model-based</b> framework, prior frequency-guided <b>diffusion</b> <b>model</b> (PFGDM), for robust and structure-preserving LA-CBCT reconstruction. PFGDM uses a conditioned <b>diffusion</b> <b>model</b> as a regularizer for LA-CBCT reconstruction, and the condition is based on high-frequency information extracted from patient-specific prior CT scans which provides a strong anatomical prior for LA-CBCT reconstruction. Specifically, we developed two variants of PFGDM (PFGDM-A and PFGDM-B) with different conditioning schemes. PFGDM-A applies the high-frequency CT information condition until a pre-optimized iteration step, and drops it afterwards to enable both similar and differing CT/CBCT anatomies to be reconstructed. PFGDM-B, on the other hand, continuously applies the prior CT information condition in every reconstruction step, while with a decaying mechanism, to gradually phase out the reconstruction guidance from the prior CT scans. The two variants of PFGDM were tested and compared with current available LA-CBCT reconstruction solutions, via metrics including PSNR and SSIM. PFGDM outperformed all traditional and <b>diffusion</b> <b>model-based</b> methods. PFGDM reconstructs high-quality LA-CBCTs under very-limited gantry angles, allowing faster and more flexible CBCT scans with dose reductions.</p></p class="citation"></blockquote><h2 id=econth-1>econ.TH (1)</h2><h3 id=11--267273-symmetric-mechanisms-for-two-sided-matching-problems-daniela-bubboloni-et-al-2024>(1/1 | 267/273) Symmetric mechanisms for two-sided matching problems (Daniela Bubboloni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniela Bubboloni, Michele Gori, Claudia Meo. (2024)<br><strong>Symmetric mechanisms for two-sided matching problems</strong><br><button class=copy-to-clipboard title="Symmetric mechanisms for two-sided matching problems" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.TH<br>Categories: 20B05, cs-GT, econ-TH, econ.TH, math-GR<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01404v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01404v1.pdf filename=2404.01404v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We focus on the basic one-to-one two-sided matching model, where there are two disjoint sets of agents of equal size, and each agent in a set has preferences on the agents in the other set, modelled by linear orders. The goal is to find a matching that associates each agent in one set with one and only one agent in the other set based on the agents&rsquo; preferences. A mechanism is a rule that associates a set of matchings to each preference profile. Stability, which refers to the capability to select only stable matchings, is an important property a mechanism should fulfill. Another crucial property, especially useful for applications, is resoluteness, which requires that the mechanism always selects a unique matching. The two versions of the deferred acceptance algorithm are examples of stable and resolute mechanisms. However, these mechanisms are severely unfair since they strongly favor one of the two sides of the market. In this paper, we introduce a property that mechanisms may meet which relates to <b>fairness.</b> Such property, called symmetry, is formulated in a way able to capture different levels of <b>fairness</b> within and across the two sets of agents and generalize existing notions. We prove several possibility and impossibility results, mainly involving the most general notion of symmetry, known as gender <b>fairness:</b> among others, a resolute and gender fair mechanism exists if and only if each side of the market consists of an odd number of agents; there exists no resolute, stable and gender fair mechanism.</p></p class="citation"></blockquote><h2 id=physicssoc-ph-2>physics.soc-ph (2)</h2><h3 id=12--268273-cooperative-evolutionary-pressure-and-diminishing-returns-might-explain-the-fermi-paradox-on-what-super-ais-are-like-daniel-vallstrom-2024>(1/2 | 268/273) Cooperative Evolutionary Pressure and Diminishing Returns Might Explain the Fermi Paradox: On What Super-AIs Are Like (Daniel Vallstrom, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Vallstrom. (2024)<br><strong>Cooperative Evolutionary Pressure and Diminishing Returns Might Explain the Fermi Paradox: On What Super-AIs Are Like</strong><br><button class=copy-to-clipboard title="Cooperative Evolutionary Pressure and Diminishing Returns Might Explain the Fermi Paradox: On What Super-AIs Are Like" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cs-AI, physics-soc-ph, physics.soc-ph<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03685v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03685v1.pdf filename=2404.03685v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With an evolutionary approach, the basis of morality can be explained as adaptations to problems of cooperation. With &rsquo;evolution&rsquo; taken in a broad sense, evolving AIs that satisfy the conditions for evolution to apply will be subject to the same cooperative evolutionary pressure as biological entities. Here the adaptiveness of increased cooperation as material safety and wealth increase is discussed &ndash; for humans, for other societies, and for AIs. Diminishing beneficial returns from increased access to material resources also suggests the possibility that, on the whole, there will be no incentive to for instance colonize entire galaxies, thus providing a possible explanation of the Fermi paradox, wondering where everybody is. It is further argued that old societies could engender, give way to, super-AIs, since it is likely that super-AIs are feasible, and fitter. Closing is an aside on effective ways for morals and goals to affect life and society, emphasizing environments, cultures, and laws, and exemplified by how to eat. Appended are an algorithm for colonizing for example a galaxy quickly, models of the evolution of cooperation and <b>fairness</b> under diminishing returns, and software for simulating signaling development. It is also noted that there can be no exponential colonization or reproduction, for mathematical reasons, as each entity takes up a certain amount of space.</p></p class="citation"></blockquote><h3 id=22--269273-social-dynamics-of-consumer-response-a-unified-framework-integrating-statistical-physics-and-marketing-dynamics-javier-marin-2024>(2/2 | 269/273) Social Dynamics of Consumer Response: A Unified Framework Integrating Statistical Physics and Marketing Dynamics (Javier Marin, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Javier Marin. (2024)<br><strong>Social Dynamics of Consumer Response: A Unified Framework Integrating Statistical Physics and Marketing Dynamics</strong><br><button class=copy-to-clipboard title="Social Dynamics of Consumer Response: A Unified Framework Integrating Statistical Physics and Marketing Dynamics" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cs-LG, physics-soc-ph, physics.soc-ph, q-fin-GN<br>Keyword Score: 10<br>Keywords: Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02175v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02175v1.pdf filename=2404.02175v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Comprehending how consumers react to advertising inputs is essential for marketers aiming to optimize advertising strategies and improve campaign effectiveness. This study examines the complex nature of consumer behaviour by applying theoretical frameworks derived from physics and social psychology. We present an innovative equation that captures the relation between spending on advertising and consumer response, using concepts such as symmetries, <b>scaling</b> <b>laws,</b> and phase transitions. By validating our equation against well-known models such as the Michaelis-Menten and Hill equations, we prove its effectiveness in accurately representing the complexity of consumer response dynamics. The analysis emphasizes the importance of key model parameters, such as marketing effectiveness, response sensitivity, and behavioural sensitivity, in influencing consumer behaviour. The work explores the practical implications for advertisers and marketers, as well as discussing the limitations and future research directions. In summary, this study provides a thorough framework for comprehending and forecasting consumer reactions to advertising, which has implications for optimizing advertising strategies and allocating resources.</p></p class="citation"></blockquote><h2 id=cset-1>cs.ET (1)</h2><h3 id=11--270273-mitigating-transient-bullwhip-effects-under-imperfect-demand-forecasts-sarah-h-q-li-et-al-2024>(1/1 | 270/273) Mitigating Transient Bullwhip Effects Under Imperfect Demand Forecasts (Sarah H. Q. Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sarah H. Q. Li, Florian Dörfler. (2024)<br><strong>Mitigating Transient Bullwhip Effects Under Imperfect Demand Forecasts</strong><br><button class=copy-to-clipboard title="Mitigating Transient Bullwhip Effects Under Imperfect Demand Forecasts" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.ET<br>Categories: cs-ET, cs.ET, math-OC<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01090v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01090v1.pdf filename=2404.01090v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by how forecast errors exacerbate order fluctuations in supply chains, we use tools from robust control theory to characterize and compute the worst-case order fluctuation experienced by an individual supply chain vendor under bounded forecast errors and demand fluctuations. Building on existing <b>discrete</b> <b>time,</b> linear time-invariant (LTI) models of supply chains, we separately model forecast error and demand fluctuations as inputs to the inventory dynamics. We then define a transient Bullwhip measure to evaluate the vendor&rsquo;s worst-case order fluctuation and show that for bounded forecast errors and demand fluctuations, this measure is equivalent to the disturbance to control peak gain. To compute the controller that minimizes the worst-case peak gain, we formulate an optimization problem with bilinear matrix inequalities and show that solving this problem is equivalent to minimizing a quasi-convex function on a bounded domain. In contrast to the existing Bullwhip measure in literature, the transient Bullwhip measure has an explicit dependency on the forecast error and does not need the forecast to be a deterministic function of the demand history. This explicit dependency enables us to separately quantify the transient Bullwhip measure&rsquo;s sensitivity to forecast error and demand fluctuations. We empirically verify our model for vendors with non-zero perishable rates and order backlogging rates.</p></p class="citation"></blockquote><h2 id=csne-1>cs.NE (1)</h2><h3 id=11--271273-gpu-accelerated-evolutionary-multiobjective-optimization-using-tensorized-rvea-zhenyu-liang-et-al-2024>(1/1 | 271/273) GPU-accelerated Evolutionary Multiobjective Optimization Using Tensorized RVEA (Zhenyu Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenyu Liang, Tao Jiang, Kebin Sun, Ran Cheng. (2024)<br><strong>GPU-accelerated Evolutionary Multiobjective Optimization Using Tensorized RVEA</strong><br><button class=copy-to-clipboard title="GPU-accelerated Evolutionary Multiobjective Optimization Using Tensorized RVEA" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01159v1.pdf filename=2404.01159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evolutionary multiobjective optimization has witnessed remarkable progress during the past decades. However, existing algorithms often encounter computational challenges in large-scale scenarios, primarily attributed to the absence of hardware acceleration. In response, we introduce a Tensorized Reference Vector Guided Evolutionary Algorithm (TensorRVEA) for harnessing the advancements of GPU acceleration. In TensorRVEA, the key data structures and operators are fully transformed into tensor forms for leveraging GPU-based parallel computing. In numerical <b>benchmark</b> tests involving large-scale populations and problem dimensions, TensorRVEA consistently demonstrates high computational performance, achieving up to over 1000$\times$ speedups. Then, we applied TensorRVEA to the domain of multiobjective neuroevolution for addressing complex challenges in robotic control tasks. Furthermore, we assessed TensorRVEA&rsquo;s extensibility by altering several tensorized reproduction operators. Experimental results demonstrate promising scalability and robustness of TensorRVEA. Source codes are available at <a href=https://github.com/EMI-Group/tensorrvea>https://github.com/EMI-Group/tensorrvea</a>.</p></p class="citation"></blockquote><h2 id=csds-1>cs.DS (1)</h2><h3 id=11--272273-on-the-complexity-of-minimizing-energy-consumption-of-partitioning-dag-tasks-wei-liu-et-al-2024>(1/1 | 272/273) On the Complexity of Minimizing Energy Consumption of Partitioning DAG Tasks (Wei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Liu, Jian-Jia Chen, Yongjie Yang. (2024)<br><strong>On the Complexity of Minimizing Energy Consumption of Partitioning DAG Tasks</strong><br><button class=copy-to-clipboard title="On the Complexity of Minimizing Energy Consumption of Partitioning DAG Tasks" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-CC, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01022v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01022v2.pdf filename=2404.01022v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a <b>graph</b> partition problem where we are given a directed acyclic <b>graph</b> (DAG) whose vertices and arcs can be respectively regarded as tasks and dependencies among tasks. The objective of the problem is to minimize the total energy consumed for completing these tasks by assigning the tasks to k heterogeneous machines. We first show that the problem is NP-hard. Then, we present polynomial-time algorithms for two special cases where there are only two machines and where the input DAG is a directed path. Finally, we study a natural variant where there are only two machines with one of them being capable of executing a limited number of tasks. We show that this special case remains computationally hard.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--273273-data-analytics-for-improving-energy-efficiency-in-short-sea-shipping-mohamed-abuella-et-al-2024>(1/1 | 273/273) Data Analytics for Improving Energy Efficiency in Short Sea Shipping (Mohamed Abuella et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed Abuella, Hadi Fanaee, M. Amine Atou, Slawomir Nowaczyk, Simon Johansson, Ethan Faghani. (2024)<br><strong>Data Analytics for Improving Energy Efficiency in Short Sea Shipping</strong><br><button class=copy-to-clipboard title="Data Analytics for Improving Energy Efficiency in Short Sea Shipping" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: 90Bxx, 90Cxx, I-2-7; I-6; H-4, cs-CE, cs.CE<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00902v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00902v1.pdf filename=2404.00902v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To meet the urgent requirements for the climate change mitigation, several proactive measures of energy efficiency have been implemented in maritime industry. Many of these practices depend highly on the onboard data of vessel&rsquo;s operation and environmental conditions. In this paper, a high resolution onboard data from passenger vessels in short-sea shipping (SSS) have been collected and preprocessed. We first investigated the available data to deploy it effectively to model the physics of the vessel, and hence the vessel performance. Since in SSS, the weather measurements and forecasts might have not been in temporal and spatial resolutions that accurately representing the actual environmental conditions. Then, We proposed a data-driven modeling approach for vessel energy efficiency. This approach addresses the challenges of data representation and energy modeling by combining and aggregating data from multiple sources and seamlessly integrates explainable artificial intelligence (XAI) to attain clear insights about the energy efficiency for a vessel in SSS. After that, the developed model of energy efficiency has been utilized in developing a framework for optimizing the vessel voyage to minimize the fuel consumption and meeting the constraint of arrival time. Moreover, we developed a spatial <b>clustering</b> approach for labeling the vessel paths to detect the paths for vessels with operating routes of repeatable and semi-repeatable paths.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.04.02</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.04.04</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscv-89>cs.CV (89)</a><ul><li><a href=#189--1273-llama-excitor-general-instruction-tuning-via-indirect-feature-interaction-bo-zou-et-al-2024>(1/89 | 1/273) LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction (Bo Zou et al., 2024)</a></li><li><a href=#289--2273-nerf-mae--masked-autoencoders-for-self-supervised-3d-representation-learning-for-neural-radiance-fields-muhammad-zubair-irshad-et-al-2024>(2/89 | 2/273) NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields (Muhammad Zubair Irshad et al., 2024)</a></li><li><a href=#389--3273-instance-aware-group-quantization-for-vision-transformers-jaehyeon-moon-et-al-2024>(3/89 | 3/273) Instance-Aware Group Quantization for Vision Transformers (Jaehyeon Moon et al., 2024)</a></li><li><a href=#489--4273-learning-by-correction-efficient-tuning-task-for-zero-shot-generative-vision-language-reasoning-rongjie-li-et-al-2024>(4/89 | 4/273) Learning by Correction: Efficient Tuning Task for Zero-Shot Generative Vision-Language Reasoning (Rongjie Li et al., 2024)</a></li><li><a href=#589--5273-evaluating-text-to-visual-generation-with-image-to-text-generation-zhiqiu-lin-et-al-2024>(5/89 | 5/273) Evaluating Text-to-Visual Generation with Image-to-Text Generation (Zhiqiu Lin et al., 2024)</a></li><li><a href=#689--6273-sugar-pre-training-3d-visual-representations-for-robotics-shizhe-chen-et-al-2024>(6/89 | 6/273) SUGAR: Pre-training 3D Visual Representations for Robotics (Shizhe Chen et al., 2024)</a></li><li><a href=#789--7273-direct-preference-optimization-of-video-large-multimodal-models-from-language-model-reward-ruohong-zhang-et-al-2024>(7/89 | 7/273) Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward (Ruohong Zhang et al., 2024)</a></li><li><a href=#889--8273-vision-language-models-for-decoding-provider-attention-during-neonatal-resuscitation-felipe-parodi-et-al-2024>(8/89 | 8/273) Vision-language models for decoding provider attention during neonatal resuscitation (Felipe Parodi et al., 2024)</a></li><li><a href=#989--9273-detect2interact-localizing-object-key-field-in-visual-question-answering-vqa-with-llms-jialou-wang-et-al-2024>(9/89 | 9/273) Detect2Interact: Localizing Object Key Field in Visual Question Answering (VQA) with LLMs (Jialou Wang et al., 2024)</a></li><li><a href=#1089--10273-structured-initialization-for-attention-in-vision-transformers-jianqiao-zheng-et-al-2024>(10/89 | 10/273) Structured Initialization for Attention in Vision Transformers (Jianqiao Zheng et al., 2024)</a></li><li><a href=#1189--11273-harnessing-large-language-models-for-training-free-video-anomaly-detection-luca-zanella-et-al-2024>(11/89 | 11/273) Harnessing Large Language Models for Training-free Video Anomaly Detection (Luca Zanella et al., 2024)</a></li><li><a href=#1289--12273-transfer-learning-with-point-transformers-kartik-gupta-et-al-2024>(12/89 | 12/273) Transfer Learning with Point Transformers (Kartik Gupta et al., 2024)</a></li><li><a href=#1389--13273-s2rc-gcn-a-spatial-spectral-reliable-contrastive-graph-convolutional-network-for-complex-land-cover-classification-using-hyperspectral-images-renxiang-guan-et-al-2024>(13/89 | 13/273) S2RC-GCN: A Spatial-Spectral Reliable Contrastive Graph Convolutional Network for Complex Land Cover Classification Using Hyperspectral Images (Renxiang Guan et al., 2024)</a></li><li><a href=#1489--14273-prompt-learning-for-oriented-power-transmission-tower-detection-in-high-resolution-sar-images-tianyang-li-et-al-2024>(14/89 | 14/273) Prompt Learning for Oriented Power Transmission Tower Detection in High-Resolution SAR Images (Tianyang Li et al., 2024)</a></li><li><a href=#1589--15273-t-mamba-frequency-enhanced-gated-long-range-dependency-for-tooth-3d-cbct-segmentation-jing-hao-et-al-2024>(15/89 | 15/273) T-Mamba: Frequency-Enhanced Gated Long-Range Dependency for Tooth 3D CBCT Segmentation (Jing Hao et al., 2024)</a></li><li><a href=#1689--16273-harnessing-the-power-of-attention-for-patch-based-biomedical-image-classification-gousia-habib-et-al-2024>(16/89 | 16/273) Harnessing The Power of Attention For Patch-Based Biomedical Image Classification (Gousia Habib et al., 2024)</a></li><li><a href=#1789--17273-model-agnostic-human-preference-inversion-in-diffusion-models-jeeyung-kim-et-al-2024>(17/89 | 17/273) Model-Agnostic Human Preference Inversion in Diffusion Models (Jeeyung Kim et al., 2024)</a></li><li><a href=#1889--18273-prompt-learning-via-meta-regularization-jinyoung-park-et-al-2024>(18/89 | 18/273) Prompt Learning via Meta-Regularization (Jinyoung Park et al., 2024)</a></li><li><a href=#1989--19273-equivariant-local-reference-frames-for-unsupervised-non-rigid-point-cloud-shape-correspondence-ling-wang-et-al-2024>(19/89 | 19/273) Equivariant Local Reference Frames for Unsupervised Non-rigid Point Cloud Shape Correspondence (Ling Wang et al., 2024)</a></li><li><a href=#2089--20273-magicmirror-fast-and-high-quality-avatar-generation-with-a-constrained-search-space-armand-comas-massagué-et-al-2024>(20/89 | 20/273) MagicMirror: Fast and High-Quality Avatar Generation with a Constrained Search Space (Armand Comas-Massagué et al., 2024)</a></li><li><a href=#2189--21273-videodistill-language-aware-vision-distillation-for-video-question-answering-bo-zou-et-al-2024>(21/89 | 21/273) VideoDistill: Language-aware Vision Distillation for Video Question Answering (Bo Zou et al., 2024)</a></li><li><a href=#2289--22273-causalchaos-dataset-for-comprehensive-causal-action-question-answering-over-longer-causal-chains-grounded-in-dynamic-visual-scenes-ting-en-lam-et-al-2024>(22/89 | 22/273) CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes (Ting En Lam et al., 2024)</a></li><li><a href=#2389--23273-cosmicman-a-text-to-image-foundation-model-for-humans-shikai-li-et-al-2024>(23/89 | 23/273) CosmicMan: A Text-to-Image Foundation Model for Humans (Shikai Li et al., 2024)</a></li><li><a href=#2489--24273-getting-it-right-improving-spatial-consistency-in-text-to-image-models-agneet-chatterjee-et-al-2024>(24/89 | 24/273) Getting it Right: Improving Spatial Consistency in Text-to-Image Models (Agneet Chatterjee et al., 2024)</a></li><li><a href=#2589--25273-diagnosis-of-skin-cancer-using-vgg16-and-vgg19-based-transfer-learning-models-amir-faghihi-et-al-2024>(25/89 | 25/273) Diagnosis of Skin Cancer Using VGG16 and VGG19 Based Transfer Learning Models (Amir Faghihi et al., 2024)</a></li><li><a href=#2689--26273-syncmask-synchronized-attentional-masking-for-fashion-centric-vision-language-pretraining-chull-hwan-song-et-al-2024>(26/89 | 26/273) SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining (Chull Hwan Song et al., 2024)</a></li><li><a href=#2789--27273-cliptone-unsupervised-learning-for-text-based-image-tone-adjustment-hyeongmin-lee-et-al-2024>(27/89 | 27/273) CLIPtone: Unsupervised Learning for Text-based Image Tone Adjustment (Hyeongmin Lee et al., 2024)</a></li><li><a href=#2889--28273-traveler-a-multi-lmm-agent-framework-for-video-question-answering-chuyi-shang-et-al-2024>(28/89 | 28/273) TraveLER: A Multi-LMM Agent Framework for Video Question-Answering (Chuyi Shang et al., 2024)</a></li><li><a href=#2989--29273-feature-splatting-language-driven-physics-based-scene-synthesis-and-editing-ri-zhao-qiu-et-al-2024>(29/89 | 29/273) Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing (Ri-Zhao Qiu et al., 2024)</a></li><li><a href=#3089--30273-gov-nesf-generalizable-open-vocabulary-neural-semantic-fields-yunsong-wang-et-al-2024>(30/89 | 30/273) GOV-NeSF: Generalizable Open-Vocabulary Neural Semantic Fields (Yunsong Wang et al., 2024)</a></li><li><a href=#3189--31273-from-pixels-to-graphs-open-vocabulary-scene-graph-generation-with-vision-language-models-rongjie-li-et-al-2024>(31/89 | 31/273) From Pixels to Graphs: Open-Vocabulary Scene Graph Generation with Vision-Language Models (Rongjie Li et al., 2024)</a></li><li><a href=#3289--32273-modality-translation-for-object-detection-adaptation-without-forgetting-prior-knowledge-heitor-rapela-medeiros-et-al-2024>(32/89 | 32/273) Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge (Heitor Rapela Medeiros et al., 2024)</a></li><li><a href=#3389--33273-finding-regions-of-interest-in-whole-slide-images-using-multiple-instance-learning-martim-afonso-et-al-2024>(33/89 | 33/273) Finding Regions of Interest in Whole Slide Images Using Multiple Instance Learning (Martim Afonso et al., 2024)</a></li><li><a href=#3489--34273-dpmesh-exploiting-diffusion-prior-for-occluded-human-mesh-recovery-yixuan-zhu-et-al-2024>(34/89 | 34/273) DPMesh: Exploiting Diffusion Prior for Occluded Human Mesh Recovery (Yixuan Zhu et al., 2024)</a></li><li><a href=#3589--35273-on-the-faithfulness-of-vision-transformer-explanations-junyi-wu-et-al-2024>(35/89 | 35/273) On the Faithfulness of Vision Transformer Explanations (Junyi Wu et al., 2024)</a></li><li><a href=#3689--36273-ovfoodseg-elevating-open-vocabulary-food-image-segmentation-via-image-informed-textual-representation-xiongwei-wu-et-al-2024>(36/89 | 36/273) OVFoodSeg: Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation (Xiongwei Wu et al., 2024)</a></li><li><a href=#3789--37273-uncovering-the-text-embedding-in-text-to-image-diffusion-models-hu-yu-et-al-2024>(37/89 | 37/273) Uncovering the Text Embedding in Text-to-Image Diffusion Models (Hu Yu et al., 2024)</a></li><li><a href=#3889--38273-medical-visual-prompting-mvp-a-unified-framework-for-versatile-and-high-quality-medical-image-segmentation-yulin-chen-et-al-2024>(38/89 | 38/273) Medical Visual Prompting (MVP): A Unified Framework for Versatile and High-Quality Medical Image Segmentation (Yulin Chen et al., 2024)</a></li><li><a href=#3989--39273-few-shot-point-cloud-reconstruction-and-denoising-via-learned-guassian-splats-renderings-and-fine-tuned-diffusion-features-pietro-bonazzi-2024>(39/89 | 39/273) Few-shot point cloud reconstruction and denoising via learned Guassian splats renderings and fine-tuned diffusion features (Pietro Bonazzi, 2024)</a></li><li><a href=#4089--40273-survey-of-bias-in-text-to-image-generation-definition-evaluation-and-mitigation-yixin-wan-et-al-2024>(40/89 | 40/273) Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and Mitigation (Yixin Wan et al., 2024)</a></li><li><a href=#4189--41273-towards-memorization-free-diffusion-models-chen-chen-et-al-2024>(41/89 | 41/273) Towards Memorization-Free Diffusion Models (Chen Chen et al., 2024)</a></li><li><a href=#4289--42273-rethinking-saliency-guided-weakly-supervised-semantic-segmentation-beomyoung-kim-et-al-2024>(42/89 | 42/273) Rethinking Saliency-Guided Weakly-Supervised Semantic Segmentation (Beomyoung Kim et al., 2024)</a></li><li><a href=#4389--43273-meta-episodic-learning-with-dynamic-task-sampling-for-clip-based-point-cloud-classification-shuvozit-ghose-et-al-2024>(43/89 | 43/273) Meta Episodic learning with Dynamic Task Sampling for CLIP-based Point Cloud Classification (Shuvozit Ghose et al., 2024)</a></li><li><a href=#4489--44273-pdf-a-probability-driven-framework-for-open-world-3d-point-cloud-semantic-segmentation-jinfeng-xu-et-al-2024>(44/89 | 44/273) PDF: A Probability-Driven Framework for Open World 3D Point Cloud Semantic Segmentation (Jinfeng Xu et al., 2024)</a></li><li><a href=#4589--45273-object-conditioned-bag-of-instances-for-few-shot-personalized-instance-recognition-umberto-michieli-et-al-2024>(45/89 | 45/273) Object-conditioned Bag of Instances for Few-Shot Personalized Instance Recognition (Umberto Michieli et al., 2024)</a></li><li><a href=#4689--46273-losa-long-short-range-adapter-for-scaling-end-to-end-temporal-action-localization-akshita-gupta-et-al-2024>(46/89 | 46/273) LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action Localization (Akshita Gupta et al., 2024)</a></li><li><a href=#4789--47273-biper-binary-neural-networks-using-a-periodic-function-edwin-vargas-et-al-2024>(47/89 | 47/273) BiPer: Binary Neural Networks using a Periodic Function (Edwin Vargas et al., 2024)</a></li><li><a href=#4889--48273-llms-are-good-sign-language-translators-jia-gong-et-al-2024>(48/89 | 48/273) LLMs are Good Sign Language Translators (Jia Gong et al., 2024)</a></li><li><a href=#4989--49273-tryon-adapter-efficient-fine-grained-clothing-identity-adaptation-for-high-fidelity-virtual-try-on-jiazheng-xing-et-al-2024>(49/89 | 49/273) TryOn-Adapter: Efficient Fine-Grained Clothing Identity Adaptation for High-Fidelity Virtual Try-On (Jiazheng Xing et al., 2024)</a></li><li><a href=#5089--50273-collaborative-learning-of-anomalies-with-privacy-clap-for-unsupervised-video-anomaly-detection-a-new-baseline-anas-al-lahham-et-al-2024>(50/89 | 50/273) Collaborative Learning of Anomalies with Privacy (CLAP) for Unsupervised Video Anomaly Detection: A New Baseline (Anas Al-lahham et al., 2024)</a></li><li><a href=#5189--51273-temporally-consistent-unbalanced-optimal-transport-for-unsupervised-action-segmentation-ming-xu-et-al-2024>(51/89 | 51/273) Temporally Consistent Unbalanced Optimal Transport for Unsupervised Action Segmentation (Ming Xu et al., 2024)</a></li><li><a href=#5289--52273-bigger-is-not-always-better-scaling-properties-of-latent-diffusion-models-kangfu-mei-et-al-2024>(52/89 | 52/273) Bigger is not Always Better: Scaling Properties of Latent Diffusion Models (Kangfu Mei et al., 2024)</a></li><li><a href=#5389--53273-measuring-style-similarity-in-diffusion-models-gowthami-somepalli-et-al-2024>(53/89 | 53/273) Measuring Style Similarity in Diffusion Models (Gowthami Somepalli et al., 2024)</a></li><li><a href=#5489--54273-language-guided-domain-generalized-medical-image-segmentation-shahina-kunhimon-et-al-2024>(54/89 | 54/273) Language Guided Domain Generalized Medical Image Segmentation (Shahina Kunhimon et al., 2024)</a></li><li><a href=#5589--55273-structldm-structured-latent-diffusion-for-3d-human-generation-tao-hu-et-al-2024>(55/89 | 55/273) StructLDM: Structured Latent Diffusion for 3D Human Generation (Tao Hu et al., 2024)</a></li><li><a href=#5689--56273-adaptive-query-prompting-for-multi-domain-landmark-detection-qiusen-wei-et-al-2024>(56/89 | 56/273) Adaptive Query Prompting for Multi-Domain Landmark Detection (Qiusen Wei et al., 2024)</a></li><li><a href=#5789--57273-condition-aware-neural-network-for-controlled-image-generation-han-cai-et-al-2024>(57/89 | 57/273) Condition-Aware Neural Network for Controlled Image Generation (Han Cai et al., 2024)</a></li><li><a href=#5889--58273-texture-preserving-diffusion-models-for-high-fidelity-virtual-try-on-xu-yang-et-al-2024>(58/89 | 58/273) Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On (Xu Yang et al., 2024)</a></li><li><a href=#5989--59273-a-comprehensive-review-of-knowledge-distillation-in-computer-vision-sheikh-musa-kaleem-et-al-2024>(59/89 | 59/273) A Comprehensive Review of Knowledge Distillation in Computer Vision (Sheikh Musa Kaleem et al., 2024)</a></li><li><a href=#6089--60273-disr-nerf-diffusion-guided-view-consistent-super-resolution-nerf-jie-long-lee-et-al-2024>(60/89 | 60/273) DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF (Jie Long Lee et al., 2024)</a></li><li><a href=#6189--61273-spikemba-multi-modal-spiking-saliency-mamba-for-temporal-video-grounding-wenrui-li-et-al-2024>(61/89 | 61/273) SpikeMba: Multi-Modal Spiking Saliency Mamba for Temporal Video Grounding (Wenrui Li et al., 2024)</a></li><li><a href=#6289--62273-sgcnerf-few-shot-neural-rendering-via-sparse-geometric-consistency-guidance-yuru-xiao-et-al-2024>(62/89 | 62/273) SGCNeRF: Few-Shot Neural Rendering via Sparse Geometric Consistency Guidance (Yuru Xiao et al., 2024)</a></li><li><a href=#6389--63273-large-motion-model-for-unified-multi-modal-motion-generation-mingyuan-zhang-et-al-2024>(63/89 | 63/273) Large Motion Model for Unified Multi-Modal Motion Generation (Mingyuan Zhang et al., 2024)</a></li><li><a href=#6489--64273-what-is-point-supervision-worth-in-video-instance-segmentation-shuaiyi-huang-et-al-2024>(64/89 | 64/273) What is Point Supervision Worth in Video Instance Segmentation? (Shuaiyi Huang et al., 2024)</a></li><li><a href=#6589--65273-bem-balanced-and-entropy-based-mix-for-long-tailed-semi-supervised-learning-hongwei-zheng-et-al-2024>(65/89 | 65/273) BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised Learning (Hongwei Zheng et al., 2024)</a></li><li><a href=#6689--66273-roadside-monocular-3d-detection-via-2d-detection-prompting-yechi-ma-et-al-2024>(66/89 | 66/273) Roadside Monocular 3D Detection via 2D Detection Prompting (Yechi Ma et al., 2024)</a></li><li><a href=#6789--67273-aigcoiqa2024-perceptual-quality-assessment-of-ai-generated-omnidirectional-images-liu-yang-et-al-2024>(67/89 | 67/273) AIGCOIQA2024: Perceptual Quality Assessment of AI Generated Omnidirectional Images (Liu Yang et al., 2024)</a></li><li><a href=#6889--68273-generating-content-for-hdr-deghosting-from-frequency-view-tao-hu-et-al-2024>(68/89 | 68/273) Generating Content for HDR Deghosting from Frequency View (Tao Hu et al., 2024)</a></li><li><a href=#6989--69273-3mos-multi-sources-multi-resolutions-and-multi-scenes-dataset-for-optical-sar-image-matching-yibin-ye-et-al-2024>(69/89 | 69/273) 3MOS: Multi-sources, Multi-resolutions, and Multi-scenes dataset for Optical-SAR image matching (Yibin Ye et al., 2024)</a></li><li><a href=#7089--70273-can-biases-in-imagenet-models-explain-generalization-paul-gavrikov-et-al-2024>(70/89 | 70/273) Can Biases in ImageNet Models Explain Generalization? (Paul Gavrikov et al., 2024)</a></li><li><a href=#7189--71273-bridging-remote-sensors-with-multisensor-geospatial-foundation-models-boran-han-et-al-2024>(71/89 | 71/273) Bridging Remote Sensors with Multisensor Geospatial Foundation Models (Boran Han et al., 2024)</a></li><li><a href=#7289--72273-fireants-adaptive-riemannian-optimization-for-multi-scale-diffeomorphic-registration-rohit-jena-et-al-2024>(72/89 | 72/273) FireANTs: Adaptive Riemannian Optimization for Multi-Scale Diffeomorphic Registration (Rohit Jena et al., 2024)</a></li><li><a href=#7389--73273-a-unified-and-interpretable-emotion-representation-and-expression-generation-reni-paskaleva-et-al-2024>(73/89 | 73/273) A Unified and Interpretable Emotion Representation and Expression Generation (Reni Paskaleva et al., 2024)</a></li><li><a href=#7489--74273-video-interpolation-with-diffusion-models-siddhant-jain-et-al-2024>(74/89 | 74/273) Video Interpolation with Diffusion Models (Siddhant Jain et al., 2024)</a></li><li><a href=#7589--75273-monobox-tightness-free-box-supervised-polyp-segmentation-using-monotonicity-constraint-qiang-hu-et-al-2024>(75/89 | 75/273) MonoBox: Tightness-free Box-supervised Polyp Segmentation using Monotonicity Constraint (Qiang Hu et al., 2024)</a></li><li><a href=#7689--76273-cmt-cross-modulation-transformer-with-hybrid-loss-for-pansharpening-wen-jie-shu-et-al-2024>(76/89 | 76/273) CMT: Cross Modulation Transformer with Hybrid Loss for Pansharpening (Wen-Jie Shu et al., 2024)</a></li><li><a href=#7789--77273-motion-blur-decomposition-with-cross-shutter-guidance-xiang-ji-et-al-2024>(77/89 | 77/273) Motion Blur Decomposition with Cross-shutter Guidance (Xiang Ji et al., 2024)</a></li><li><a href=#7889--78273-action-detection-via-an-image-diffusion-process-lin-geng-foo-et-al-2024>(78/89 | 78/273) Action Detection via an Image Diffusion Process (Lin Geng Foo et al., 2024)</a></li><li><a href=#7989--79273-flexidreamer-single-image-to-3d-generation-with-flexicubes-ruowen-zhao-et-al-2024>(79/89 | 79/273) FlexiDreamer: Single Image-to-3D Generation with FlexiCubes (Ruowen Zhao et al., 2024)</a></li><li><a href=#8089--80273-camo-correlation-aware-mask-optimization-with-modulated-reinforcement-learning-xiaoxiao-liang-et-al-2024>(80/89 | 80/273) CAMO: Correlation-Aware Mask Optimization with Modulated Reinforcement Learning (Xiaoxiao Liang et al., 2024)</a></li><li><a href=#8189--81273-exploring-the-efficacy-of-group-normalization-in-deep-learning-models-for-alzheimers-disease-classification-gousia-habib-et-al-2024>(81/89 | 81/273) Exploring the Efficacy of Group-Normalization in Deep Learning Models for Alzheimer&rsquo;s Disease Classification (Gousia Habib et al., 2024)</a></li><li><a href=#8289--82273-gyro-based-neural-single-image-deblurring-heemin-yang-et-al-2024>(82/89 | 82/273) Gyro-based Neural Single Image Deblurring (Heemin Yang et al., 2024)</a></li><li><a href=#8389--83273-streaming-dense-video-captioning-xingyi-zhou-et-al-2024>(83/89 | 83/273) Streaming Dense Video Captioning (Xingyi Zhou et al., 2024)</a></li><li><a href=#8489--84273-360x-a-panoptic-multi-modal-scene-understanding-dataset-hao-chen-et-al-2024>(84/89 | 84/273) 360+x: A Panoptic Multi-modal Scene Understanding Dataset (Hao Chen et al., 2024)</a></li><li><a href=#8589--85273-scalable-scene-modeling-from-perspective-imaging-physics-based-appearance-and-geometry-inference-shuang-song-2024>(85/89 | 85/273) Scalable Scene Modeling from Perspective Imaging: Physics-based Appearance and Geometry Inference (Shuang Song, 2024)</a></li><li><a href=#8689--86273-badpart-unified-black-box-adversarial-patch-attacks-against-pixel-wise-regression-tasks-zhiyuan-cheng-et-al-2024>(86/89 | 86/273) BadPart: Unified Black-box Adversarial Patch Attacks against Pixel-wise Regression Tasks (Zhiyuan Cheng et al., 2024)</a></li><li><a href=#8789--87273-on-train-test-class-overlap-and-detection-for-image-retrieval-chull-hwan-song-et-al-2024>(87/89 | 87/273) On Train-Test Class Overlap and Detection for Image Retrieval (Chull Hwan Song et al., 2024)</a></li><li><a href=#8889--88273-posterllama-bridging-design-ability-of-langauge-model-to-contents-aware-layout-generation-jaejung-seol-et-al-2024>(88/89 | 88/273) PosterLlama: Bridging Design Ability of Langauge Model to Contents-Aware Layout Generation (Jaejung Seol et al., 2024)</a></li><li><a href=#8989--89273-mm3dgs-slam-multi-modal-3d-gaussian-splatting-for-slam-using-vision-depth-and-inertial-measurements-lisong-c-sun-et-al-2024>(89/89 | 89/273) MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements (Lisong C. Sun et al., 2024)</a></li></ul></li><li><a href=#cscl-57>cs.CL (57)</a><ul><li><a href=#157--90273-unveiling-divergent-inductive-biases-of-llms-on-temporal-data-sindhu-kishore-et-al-2024>(1/57 | 90/273) Unveiling Divergent Inductive Biases of LLMs on Temporal Data (Sindhu Kishore et al., 2024)</a></li><li><a href=#257--91273-fables-evaluating-faithfulness-and-content-selection-in-book-length-summarization-yekyung-kim-et-al-2024>(2/57 | 91/273) FABLES: Evaluating faithfulness and content selection in book-length summarization (Yekyung Kim et al., 2024)</a></li><li><a href=#357--92273-aragog-advanced-rag-output-grading-matouš-eibich-et-al-2024>(3/57 | 92/273) ARAGOG: Advanced RAG Output Grading (Matouš Eibich et al., 2024)</a></li><li><a href=#457--93273-structured-information-matters-incorporating-abstract-meaning-representation-into-llms-for-improved-open-domain-dialogue-evaluation-bohao-yang-et-al-2024>(4/57 | 93/273) Structured Information Matters: Incorporating Abstract Meaning Representation into LLMs for Improved Open-Domain Dialogue Evaluation (Bohao Yang et al., 2024)</a></li><li><a href=#557--94273-self-demos-eliciting-out-of-demonstration-generalizability-in-large-language-models-wei-he-et-al-2024>(5/57 | 94/273) Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models (Wei He et al., 2024)</a></li><li><a href=#657--95273-bailong-bilingual-transfer-learning-based-on-qlora-and-zip-tie-embedding-lung-chuan-chen-et-al-2024>(6/57 | 95/273) Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding (Lung-Chuan Chen et al., 2024)</a></li><li><a href=#757--96273-aadam-at-semeval-2024-task-1-augmentation-and-adaptation-for-multilingual-semantic-textual-relatedness-miaoran-zhang-et-al-2024>(7/57 | 96/273) AAdaM at SemEval-2024 Task 1: Augmentation and Adaptation for Multilingual Semantic Textual Relatedness (Miaoran Zhang et al., 2024)</a></li><li><a href=#857--97273-advancing-ai-with-integrity-ethical-challenges-and-solutions-in-neural-machine-translation-richard-kimera-et-al-2024>(8/57 | 97/273) Advancing AI with Integrity: Ethical Challenges and Solutions in Neural Machine Translation (Richard Kimera et al., 2024)</a></li><li><a href=#957--98273-llm-radjudge-achieving-radiologist-level-evaluation-for-x-ray-report-generation-zilong-wang-et-al-2024>(9/57 | 98/273) LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation (Zilong Wang et al., 2024)</a></li><li><a href=#1057--99273-chatglm-rlhf-practices-of-aligning-large-language-models-with-human-feedback-zhenyu-hou-et-al-2024>(10/57 | 99/273) ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback (Zhenyu Hou et al., 2024)</a></li><li><a href=#1157--100273-token-efficient-leverage-learning-in-large-language-models-yuanhao-zeng-et-al-2024>(11/57 | 100/273) Token-Efficient Leverage Learning in Large Language Models (Yuanhao Zeng et al., 2024)</a></li><li><a href=#1257--101273-set-aligning-framework-for-auto-regressive-event-temporal-graph-generation-xingwei-tan-et-al-2024>(12/57 | 101/273) Set-Aligning Framework for Auto-Regressive Event Temporal Graph Generation (Xingwei Tan et al., 2024)</a></li><li><a href=#1357--102273-effectively-prompting-small-sized-language-models-for-cross-lingual-tasks-via-winning-tickets-mingqi-li-et-al-2024>(13/57 | 102/273) Effectively Prompting Small-sized Language Models for Cross-lingual Tasks via Winning Tickets (Mingqi Li et al., 2024)</a></li><li><a href=#1457--103273-a-survey-on-multilingual-large-language-models-corpora-alignment-and-bias-yuemei-xu-et-al-2024>(14/57 | 103/273) A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias (Yuemei Xu et al., 2024)</a></li><li><a href=#1557--104273-aispace-at-semeval-2024-task-8-a-class-balanced-soft-voting-system-for-detecting-multi-generator-machine-generated-text-renhua-gu-et-al-2024>(15/57 | 104/273) AISPACE at SemEval-2024 task 8: A Class-balanced Soft-voting System for Detecting Multi-generator Machine-generated Text (Renhua Gu et al., 2024)</a></li><li><a href=#1657--105273-position-aware-parameter-efficient-fine-tuning-approach-for-reducing-positional-bias-in-llms-zheng-zhang-et-al-2024>(16/57 | 105/273) Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs (Zheng Zhang et al., 2024)</a></li><li><a href=#1757--106273-generating-faithful-and-complete-hospital-course-summaries-from-the-electronic-health-record-griffin-adams-2024>(17/57 | 106/273) Generating Faithful and Complete Hospital-Course Summaries from the Electronic Health Record (Griffin Adams, 2024)</a></li><li><a href=#1857--107273-senticse-a-sentiment-aware-contrastive-sentence-embedding-framework-with-sentiment-guided-textual-similarity-jaemin-kim-et-al-2024>(18/57 | 107/273) SentiCSE: A Sentiment-aware Contrastive Sentence Embedding Framework with Sentiment-guided Textual Similarity (Jaemin Kim et al., 2024)</a></li><li><a href=#1957--108273-bert-enhanced-retrieval-tool-for-homework-plagiarism-detection-system-jiarong-xian-et-al-2024>(19/57 | 108/273) BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System (Jiarong Xian et al., 2024)</a></li><li><a href=#2057--109273-efficient-prompting-methods-for-large-language-models-a-survey-kaiyan-chang-et-al-2024>(20/57 | 109/273) Efficient Prompting Methods for Large Language Models: A Survey (Kaiyan Chang et al., 2024)</a></li><li><a href=#2157--110273-source-aware-training-enables-knowledge-attribution-in-language-models-muhammad-khalifa-et-al-2024>(21/57 | 110/273) Source-Aware Training Enables Knowledge Attribution in Language Models (Muhammad Khalifa et al., 2024)</a></li><li><a href=#2257--111273-developing-safe-and-responsible-large-language-models----a-comprehensive-framework-shaina-raza-et-al-2024>(22/57 | 111/273) Developing Safe and Responsible Large Language Models &ndash; A Comprehensive Framework (Shaina Raza et al., 2024)</a></li><li><a href=#2357--112273-stable-code-technical-report-nikhil-pinnaparaju-et-al-2024>(23/57 | 112/273) Stable Code Technical Report (Nikhil Pinnaparaju et al., 2024)</a></li><li><a href=#2457--113273-finding-replicable-human-evaluations-via-stable-ranking-probability-parker-riley-et-al-2024>(24/57 | 113/273) Finding Replicable Human Evaluations via Stable Ranking Probability (Parker Riley et al., 2024)</a></li><li><a href=#2557--114273-will-the-real-linda-please-stand-upto-large-language-models-examining-the-representativeness-heuristic-in-llms-pengda-wang-et-al-2024>(25/57 | 114/273) Will the Real Linda Please Stand up&mldr;to Large Language Models? Examining the Representativeness Heuristic in LLMs (Pengda Wang et al., 2024)</a></li><li><a href=#2657--115273-ails-ntua-at-semeval-2024-task-6-efficient-model-tuning-for-hallucination-detection-and-analysis-natalia-griogoriadou-et-al-2024>(26/57 | 115/273) AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for hallucination detection and analysis (Natalia Griogoriadou et al., 2024)</a></li><li><a href=#2757--116273-llm-attributor-interactive-visual-attribution-for-llm-generation-seongmin-lee-et-al-2024>(27/57 | 116/273) LLM Attributor: Interactive Visual Attribution for LLM Generation (Seongmin Lee et al., 2024)</a></li><li><a href=#2857--117273-exploring-the-mystery-of-influential-data-for-mathematical-reasoning-xinzhe-ni-et-al-2024>(28/57 | 117/273) Exploring the Mystery of Influential Data for Mathematical Reasoning (Xinzhe Ni et al., 2024)</a></li><li><a href=#2957--118273-exploring-the-nexus-of-large-language-models-and-legal-systems-a-short-survey-weicong-qin-et-al-2024>(29/57 | 118/273) Exploring the Nexus of Large Language Models and Legal Systems: A Short Survey (Weicong Qin et al., 2024)</a></li><li><a href=#3057--119273-prior-constraints-based-reward-model-training-for-aligning-large-language-models-hang-zhou-et-al-2024>(30/57 | 119/273) Prior Constraints-based Reward Model Training for Aligning Large Language Models (Hang Zhou et al., 2024)</a></li><li><a href=#3157--120273-psydial-personality-based-synthetic-dialogue-generation-using-large-language-models-ji-eun-han-et-al-2024>(31/57 | 120/273) PSYDIAL: Personality-based Synthetic Dialogue Generation using Large Language Models (Ji-Eun Han et al., 2024)</a></li><li><a href=#3257--121273-a-study-on-scaling-up-multilingual-news-framing-analysis-syeda-sabrina-akter-et-al-2024>(32/57 | 121/273) A Study on Scaling Up Multilingual News Framing Analysis (Syeda Sabrina Akter et al., 2024)</a></li><li><a href=#3357--122273-towards-safety-and-helpfulness-balanced-responses-via-controllable-large-language-models-yi-lin-tuan-et-al-2024>(33/57 | 122/273) Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models (Yi-Lin Tuan et al., 2024)</a></li><li><a href=#3457--123273-large-language-models-are-capable-of-offering-cognitive-reappraisal-if-guided-hongli-zhan-et-al-2024>(34/57 | 123/273) Large Language Models are Capable of Offering Cognitive Reappraisal, if Guided (Hongli Zhan et al., 2024)</a></li><li><a href=#3557--124273-mapping-the-increasing-use-of-llms-in-scientific-papers-weixin-liang-et-al-2024>(35/57 | 124/273) Mapping the Increasing Use of LLMs in Scientific Papers (Weixin Liang et al., 2024)</a></li><li><a href=#3657--125273-uniark-improving-generalisation-and-consistency-for-factual-knowledge-extraction-through-debiasing-yijun-yang-et-al-2024>(36/57 | 125/273) UniArk: Improving Generalisation and Consistency for Factual Knowledge Extraction through Debiasing (Yijun Yang et al., 2024)</a></li><li><a href=#3757--126273-llm-as-a-mastermind-a-survey-of-strategic-reasoning-with-large-language-models-yadong-zhang-et-al-2024>(37/57 | 126/273) LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models (Yadong Zhang et al., 2024)</a></li><li><a href=#3857--127273-the-fine-line-navigating-large-language-model-pretraining-with-down-streaming-capability-analysis-chen-yang-et-al-2024>(38/57 | 127/273) The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis (Chen Yang et al., 2024)</a></li><li><a href=#3957--128273-a-neuro-symbolic-approach-to-monitoring-salt-content-in-food-anuja-tayal-et-al-2024>(39/57 | 128/273) A Neuro-Symbolic Approach to Monitoring Salt Content in Food (Anuja Tayal et al., 2024)</a></li><li><a href=#4057--129273-ails-ntua-at-semeval-2024-task-9-cracking-brain-teasers-transformer-models-for-lateral-thinking-puzzles-ioannis-panagiotopoulos-et-al-2024>(40/57 | 129/273) AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models for Lateral Thinking Puzzles (Ioannis Panagiotopoulos et al., 2024)</a></li><li><a href=#4157--130273-lite-modeling-environmental-ecosystems-with-multimodal-large-language-models-haoran-li-et-al-2024>(41/57 | 130/273) LITE: Modeling Environmental Ecosystems with Multimodal Large Language Models (Haoran Li et al., 2024)</a></li><li><a href=#4257--131273-evaluating-the-factuality-of-large-language-models-using-large-scale-knowledge-graphs-xiaoze-liu-et-al-2024>(42/57 | 131/273) Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs (Xiaoze Liu et al., 2024)</a></li><li><a href=#4357--132273-open-vocabulary-federated-learning-with-multimodal-prototyping-huimin-zeng-et-al-2024>(43/57 | 132/273) Open-Vocabulary Federated Learning with Multimodal Prototyping (Huimin Zeng et al., 2024)</a></li><li><a href=#4457--133273-enterprise-use-cases-combining-knowledge-graphs-and-natural-language-processing-phillip-schneider-et-al-2024>(44/57 | 133/273) Enterprise Use Cases Combining Knowledge Graphs and Natural Language Processing (Phillip Schneider et al., 2024)</a></li><li><a href=#4557--134273-paireval-open-domain-dialogue-evaluation-with-pairwise-comparison-chaehun-park-et-al-2024>(45/57 | 134/273) PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison (ChaeHun Park et al., 2024)</a></li><li><a href=#4657--135273-creating-emoji-lexica-from-unsupervised-sentiment-analysis-of-their-descriptions-milagros-fernández-gavilanes-et-al-2024>(46/57 | 135/273) Creating emoji lexica from unsupervised sentiment analysis of their descriptions (Milagros Fernández-Gavilanes et al., 2024)</a></li><li><a href=#4757--136273-artificial-intelligence-and-the-spatial-documentation-of-languages-hakam-ghanim-2024>(47/57 | 136/273) Artificial Intelligence and the Spatial Documentation of Languages (Hakam Ghanim, 2024)</a></li><li><a href=#4857--137273-an-image-speaks-a-thousand-words-but-can-everyone-listen-on-translating-images-for-cultural-relevance-simran-khanuja-et-al-2024>(48/57 | 137/273) An image speaks a thousand words, but can everyone listen? On translating images for cultural relevance (Simran Khanuja et al., 2024)</a></li><li><a href=#4957--138273-green-ai-exploring-carbon-footprints-mitigation-strategies-and-trade-offs-in-large-language-model-training-vivian-liu-et-al-2024>(49/57 | 138/273) Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade Offs in Large Language Model Training (Vivian Liu et al., 2024)</a></li><li><a href=#5057--139273-do-llms-find-human-answers-to-fact-driven-questions-perplexing-a-case-study-on-reddit-parker-seegmiller-et-al-2024>(50/57 | 139/273) Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case Study on Reddit (Parker Seegmiller et al., 2024)</a></li><li><a href=#5157--140273-koconovel-annotated-dataset-of-character-coreference-in-korean-novels-kyuhee-kim-et-al-2024>(51/57 | 140/273) KoCoNovel: Annotated Dataset of Character Coreference in Korean Novels (Kyuhee Kim et al., 2024)</a></li><li><a href=#5257--141273-regularized-best-of-n-sampling-to-mitigate-reward-hacking-for-language-model-alignment-yuu-jinnai-et-al-2024>(52/57 | 141/273) Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment (Yuu Jinnai et al., 2024)</a></li><li><a href=#5357--142273-evalverse-unified-and-accessible-library-for-large-language-model-evaluation-jihoo-kim-et-al-2024>(53/57 | 142/273) Evalverse: Unified and Accessible Library for Large Language Model Evaluation (Jihoo Kim et al., 2024)</a></li><li><a href=#5457--143273-tm-trek-at-semeval-2024-task-8-towards-llm-based-automatic-boundary-detection-for-human-machine-mixed-text-xiaoyan-qu-et-al-2024>(54/57 | 143/273) TM-TREK at SemEval-2024 Task 8: Towards LLM-Based Automatic Boundary Detection for Human-Machine Mixed Text (Xiaoyan Qu et al., 2024)</a></li><li><a href=#5557--144273-verifying-claims-about-metaphors-with-large-scale-automatic-metaphor-identification-kotaro-aono-et-al-2024>(55/57 | 144/273) Verifying Claims About Metaphors with Large-Scale Automatic Metaphor Identification (Kotaro Aono et al., 2024)</a></li><li><a href=#5657--145273-constructing-and-expanding-low-resource-and-underrepresented-parallel-datasets-for-indonesian-local-languages-joanito-agili-lopo-et-al-2024>(56/57 | 145/273) Constructing and Expanding Low-Resource and Underrepresented Parallel Datasets for Indonesian Local Languages (Joanito Agili Lopo et al., 2024)</a></li><li><a href=#5757--146273-dialogue-with-robots-proposals-for-broadening-participation-and-research-in-the-slivar-community-casey-kennington-et-al-2024>(57/57 | 146/273) Dialogue with Robots: Proposals for Broadening Participation and Research in the SLIVAR Community (Casey Kennington et al., 2024)</a></li></ul></li><li><a href=#csse-5>cs.SE (5)</a><ul><li><a href=#15--147273-large-language-model-evaluation-via-multi-ai-agents-preliminary-results-zeeshan-rasheed-et-al-2024>(1/5 | 147/273) Large Language Model Evaluation Via Multi AI Agents: Preliminary results (Zeeshan Rasheed et al., 2024)</a></li><li><a href=#25--148273-exploring-and-evaluating-hallucinations-in-llm-powered-code-generation-fang-liu-et-al-2024>(2/5 | 148/273) Exploring and Evaluating Hallucinations in LLM-Powered Code Generation (Fang Liu et al., 2024)</a></li><li><a href=#35--149273-syntactic-robustness-for-llm-based-code-generation-laboni-sarker-et-al-2024>(3/5 | 149/273) Syntactic Robustness for LLM-based Code Generation (Laboni Sarker et al., 2024)</a></li><li><a href=#45--150273-enabling-memory-safety-of-c-programs-using-llms-nausheen-mohammed-et-al-2024>(4/5 | 150/273) Enabling Memory Safety of C Programs using LLMs (Nausheen Mohammed et al., 2024)</a></li><li><a href=#55--151273-aurora-navigating-ui-tarpits-via-automated-neural-screen-understanding-safwat-ali-khan-et-al-2024>(5/5 | 151/273) AURORA: Navigating UI Tarpits via Automated Neural Screen Understanding (Safwat Ali Khan et al., 2024)</a></li></ul></li><li><a href=#csai-3>cs.AI (3)</a><ul><li><a href=#13--152273-isobench-benchmarking-multimodal-foundation-models-on-isomorphic-representations-deqing-fu-et-al-2024>(1/3 | 152/273) IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations (Deqing Fu et al., 2024)</a></li><li><a href=#23--153273-some-orders-are-important-partially-preserving-orders-in-top-quality-planning-michael-katz-et-al-2024>(2/3 | 153/273) Some Orders Are Important: Partially Preserving Orders in Top-Quality Planning (Michael Katz et al., 2024)</a></li><li><a href=#33--154273-mtlight-efficient-multi-task-reinforcement-learning-for-traffic-signal-control-liwen-zhu-et-al-2024>(3/3 | 154/273) MTLight: Efficient Multi-Task Reinforcement Learning for Traffic Signal Control (Liwen Zhu et al., 2024)</a></li></ul></li><li><a href=#cslg-36>cs.LG (36)</a><ul><li><a href=#136--155273-prompt-prompted-mixture-of-experts-for-efficient-llm-generation-harry-dong-et-al-2024>(1/36 | 155/273) Prompt-prompted Mixture of Experts for Efficient LLM Generation (Harry Dong et al., 2024)</a></li><li><a href=#236--156273-new-logarithmic-step-size-for-stochastic-gradient-descent-m-soheil-shamaee-et-al-2024>(2/36 | 156/273) New logarithmic step size for stochastic gradient descent (M. Soheil Shamaee et al., 2024)</a></li><li><a href=#336--157273-incorporating-domain-differential-equations-into-graph-convolutional-networks-to-lower-generalization-discrepancy-yue-sun-et-al-2024>(3/36 | 157/273) Incorporating Domain Differential Equations into Graph Convolutional Networks to Lower Generalization Discrepancy (Yue Sun et al., 2024)</a></li><li><a href=#436--158273-decentralized-collaborative-learning-framework-with-external-privacy-leakage-analysis-tsuyoshi-idé-et-al-2024>(4/36 | 158/273) Decentralized Collaborative Learning Framework with External Privacy Leakage Analysis (Tsuyoshi Idé et al., 2024)</a></li><li><a href=#536--159273-machine-unlearning-for-traditional-models-and-large-language-models-a-short-survey-yi-xu-2024>(5/36 | 159/273) Machine Unlearning for Traditional Models and Large Language Models: A Short Survey (Yi Xu, 2024)</a></li><li><a href=#636--160273-whats-in-your-safe-data-identifying-benign-data-that-breaks-safety-luxi-he-et-al-2024>(6/36 | 160/273) What&rsquo;s in Your &lsquo;Safe&rsquo; Data?: Identifying Benign Data that Breaks Safety (Luxi He et al., 2024)</a></li><li><a href=#736--161273-machine-learning-robustness-a-primer-houssem-ben-braiek-et-al-2024>(7/36 | 161/273) Machine Learning Robustness: A Primer (Houssem Ben Braiek et al., 2024)</a></li><li><a href=#836--162273-lipsum-ft-robust-fine-tuning-of-zero-shot-models-using-random-text-guidance-giung-nam-et-al-2024>(8/36 | 162/273) Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance (Giung Nam et al., 2024)</a></li><li><a href=#936--163273-addressing-heterogeneity-in-federated-load-forecasting-with-personalization-layers-shourya-bose-et-al-2024>(9/36 | 163/273) Addressing Heterogeneity in Federated Load Forecasting with Personalization Layers (Shourya Bose et al., 2024)</a></li><li><a href=#1036--164273-are-large-language-models-superhuman-chemists-adrian-mirza-et-al-2024>(10/36 | 164/273) Are large language models superhuman chemists? (Adrian Mirza et al., 2024)</a></li><li><a href=#1136--165273-is-model-collapse-inevitable-breaking-the-curse-of-recursion-by-accumulating-real-and-synthetic-data-matthias-gerstgrasser-et-al-2024>(11/36 | 165/273) Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data (Matthias Gerstgrasser et al., 2024)</a></li><li><a href=#1236--166273-efficiently-distilling-llms-for-edge-applications-achintya-kundu-et-al-2024>(12/36 | 166/273) Efficiently Distilling LLMs for Edge Applications (Achintya Kundu et al., 2024)</a></li><li><a href=#1336--167273-rethinking-the-relationship-between-recurrent-and-non-recurrent-neural-networks-a-study-in-sparsity-quincy-hershey-et-al-2024>(13/36 | 167/273) Rethinking the Relationship between Recurrent and Non-Recurrent Neural Networks: A Study in Sparsity (Quincy Hershey et al., 2024)</a></li><li><a href=#1436--168273-continual-learning-for-smart-city-a-survey-li-yang-et-al-2024>(14/36 | 168/273) Continual Learning for Smart City: A Survey (Li Yang et al., 2024)</a></li><li><a href=#1536--169273-diffusion-driven-domain-adaptation-for-generating-3d-molecules-haokai-hong-et-al-2024>(15/36 | 169/273) Diffusion-Driven Domain Adaptation for Generating 3D Molecules (Haokai Hong et al., 2024)</a></li><li><a href=#1636--170273-a-survey-on-hypergraph-neural-networks-an-in-depth-and-step-by-step-guide-sunwoo-kim-et-al-2024>(16/36 | 170/273) A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step Guide (Sunwoo Kim et al., 2024)</a></li><li><a href=#1736--171273-securing-social-spaces-harnessing-deep-learning-to-eradicate-cyberbullying-rohan-biswas-et-al-2024>(17/36 | 171/273) Securing Social Spaces: Harnessing Deep Learning to Eradicate Cyberbullying (Rohan Biswas et al., 2024)</a></li><li><a href=#1836--172273-can-llms-get-help-from-other-llms-without-revealing-private-information-florian-hartmann-et-al-2024>(18/36 | 172/273) Can LLMs get help from other LLMs without revealing private information? (Florian Hartmann et al., 2024)</a></li><li><a href=#1936--173273-the-double-edged-sword-of-input-perturbations-to-robust-accurate-fairness-xuran-li-et-al-2024>(19/36 | 173/273) The Double-Edged Sword of Input Perturbations to Robust Accurate Fairness (Xuran Li et al., 2024)</a></li><li><a href=#2036--174273-make-continual-learning-stronger-via-c-flat-ang-bian-et-al-2024>(20/36 | 174/273) Make Continual Learning Stronger via C-Flat (Ang Bian et al., 2024)</a></li><li><a href=#2136--175273-stream-of-search-sos-learning-to-search-in-language-kanishk-gandhi-et-al-2024>(21/36 | 175/273) Stream of Search (SoS): Learning to Search in Language (Kanishk Gandhi et al., 2024)</a></li><li><a href=#2236--176273-caap-class-dependent-automatic-data-augmentation-based-on-adaptive-policies-for-time-series-tien-yu-chang-et-al-2024>(22/36 | 176/273) CAAP: Class-Dependent Automatic Data Augmentation Based On Adaptive Policies For Time Series (Tien-Yu Chang et al., 2024)</a></li><li><a href=#2336--177273-ts-causalnn-learning-temporal-causal-relations-from-non-linear-non-stationary-time-series-data-omar-faruque-et-al-2024>(23/36 | 177/273) TS-CausalNN: Learning Temporal Causal Relations from Non-linear Non-stationary Time Series Data (Omar Faruque et al., 2024)</a></li><li><a href=#2436--178273-novel-node-category-detection-under-subpopulation-shift-hsing-huan-chung-et-al-2024>(24/36 | 178/273) Novel Node Category Detection Under Subpopulation Shift (Hsing-Huan Chung et al., 2024)</a></li><li><a href=#2536--179273-explainable-ai-integrated-feature-engineering-for-wildfire-prediction-di-fan-et-al-2024>(25/36 | 179/273) Explainable AI Integrated Feature Engineering for Wildfire Prediction (Di Fan et al., 2024)</a></li><li><a href=#2636--180273-drive-dual-gradient-based-rapid-iterative-pruning-dhananjay-saikumar-et-al-2024>(26/36 | 180/273) DRIVE: Dual Gradient-Based Rapid Iterative Pruning (Dhananjay Saikumar et al., 2024)</a></li><li><a href=#2736--181273-openchemie-an-information-extraction-toolkit-for-chemistry-literature-vincent-fan-et-al-2024>(27/36 | 181/273) OpenChemIE: An Information Extraction Toolkit For Chemistry Literature (Vincent Fan et al., 2024)</a></li><li><a href=#2836--182273-twin-gpt-digital-twins-for-clinical-trials-via-large-language-model-yue-wang-et-al-2024>(28/36 | 182/273) TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model (Yue Wang et al., 2024)</a></li><li><a href=#2936--183273-information-plane-analysis-visualization-in-deep-learning-via-transfer-entropy-adrian-moldovan-et-al-2024>(29/36 | 183/273) Information Plane Analysis Visualization in Deep Learning via Transfer Entropy (Adrian Moldovan et al., 2024)</a></li><li><a href=#3036--184273-nearly-tight-approximation-guarantees-for-the-improving-multi-armed-bandits-problem-avrim-blum-et-al-2024>(30/36 | 184/273) Nearly-tight Approximation Guarantees for the Improving Multi-Armed Bandits Problem (Avrim Blum et al., 2024)</a></li><li><a href=#3136--185273-sok-a-review-of-differentially-private-linear-models-for-high-dimensional-data-amol-khanna-et-al-2024>(31/36 | 185/273) SoK: A Review of Differentially Private Linear Models For High-Dimensional Data (Amol Khanna et al., 2024)</a></li><li><a href=#3236--186273-energy-model-based-accurate-shapley-value-estimation-for-interpretable-deep-learning-predictive-modelling-cheng-lu-et-al-2024>(32/36 | 186/273) Energy Model-based Accurate Shapley Value Estimation for Interpretable Deep Learning Predictive Modelling (Cheng Lu et al., 2024)</a></li><li><a href=#3336--187273-aetta-label-free-accuracy-estimation-for-test-time-adaptation-taeckyung-lee-et-al-2024>(33/36 | 187/273) AETTA: Label-Free Accuracy Estimation for Test-Time Adaptation (Taeckyung Lee et al., 2024)</a></li><li><a href=#3436--188273-modeling-output-level-task-relatedness-in-multi-task-learning-with-feedback-mechanism-xiangming-xi-et-al-2024>(34/36 | 188/273) Modeling Output-Level Task Relatedness in Multi-Task Learning with Feedback Mechanism (Xiangming Xi et al., 2024)</a></li><li><a href=#3536--189273-do-language-models-plan-ahead-for-future-tokens-wilson-wu-et-al-2024>(35/36 | 189/273) Do language models plan ahead for future tokens? (Wilson Wu et al., 2024)</a></li><li><a href=#3636--190273-interpretable-multi-view-clustering-based-on-anchor-graph-tensor-factorization-jing-li-et-al-2024>(36/36 | 190/273) Interpretable Multi-View Clustering Based on Anchor Graph Tensor Factorization (Jing Li et al., 2024)</a></li></ul></li><li><a href=#eessiv-4>eess.IV (4)</a><ul><li><a href=#14--191273-diffusion-based-zero-shot-medical-image-to-image-translation-for-cross-modality-segmentation-zihao-wang-et-al-2024>(1/4 | 191/273) Diffusion based Zero-shot Medical Image-to-Image Translation for Cross Modality Segmentation (Zihao Wang et al., 2024)</a></li><li><a href=#24--192273-harnessing-data-and-physics-for-deep-learning-phase-recovery-kaiqiang-wang-et-al-2024>(2/4 | 192/273) Harnessing Data and Physics for Deep Learning Phase Recovery (Kaiqiang Wang et al., 2024)</a></li><li><a href=#34--193273-data-efficient-unsupervised-interpolation-without-any-intermediate-frame-for-4d-medical-images-jungeun-kim-et-al-2024>(3/4 | 193/273) Data-Efficient Unsupervised Interpolation Without Any Intermediate Frame for 4D Medical Images (JungEun Kim et al., 2024)</a></li><li><a href=#44--194273-imd4gc-incomplete-multimodal-data-integration-to-advance-precise-treatment-response-prediction-and-survival-analysis-for-gastric-cancer-fengtao-zhou-et-al-2024>(4/4 | 194/273) iMD4GC: Incomplete Multimodal Data Integration to Advance Precise Treatment Response Prediction and Survival Analysis for Gastric Cancer (Fengtao Zhou et al., 2024)</a></li></ul></li><li><a href=#csir-7>cs.IR (7)</a><ul><li><a href=#17--195273-query-performance-prediction-using-relevance-judgments-generated-by-large-language-models-chuan-meng-et-al-2024>(1/7 | 195/273) Query Performance Prediction using Relevance Judgments Generated by Large Language Models (Chuan Meng et al., 2024)</a></li><li><a href=#27--196273-using-chaos-estimator-as-a-stopping-criterion-for-technology-assisted-review-michiel-p-bron-et-al-2024>(2/7 | 196/273) Using Chao&rsquo;s Estimator as a Stopping Criterion for Technology-Assisted Review (Michiel P. Bron et al., 2024)</a></li><li><a href=#37--197273-eeg-svrec-an-eeg-dataset-with-user-multidimensional-affective-engagement-labels-in-short-video-recommendation-shaorun-zhang-et-al-2024>(3/7 | 197/273) EEG-SVRec: An EEG Dataset with User Multidimensional Affective Engagement Labels in Short Video Recommendation (Shaorun Zhang et al., 2024)</a></li><li><a href=#47--198273-higher-education-assessment-practice-in-the-era-of-generative-ai-tools-bayode-ogunleye-et-al-2024>(4/7 | 198/273) Higher education assessment practice in the era of generative AI tools (Bayode Ogunleye et al., 2024)</a></li><li><a href=#57--199273-cross-channel-recommendation-for-multi-channel-retail-yijin-choi-et-al-2024>(5/7 | 199/273) Cross-channel Recommendation for Multi-channel Retail (Yijin Choi et al., 2024)</a></li><li><a href=#67--200273-maximizing-user-experience-with-llmops-driven-personalized-recommendation-systems-chenxi-shi-et-al-2024>(6/7 | 200/273) Maximizing User Experience with LLMOps-Driven Personalized Recommendation Systems (Chenxi Shi et al., 2024)</a></li><li><a href=#77--201273-towards-an-in-depth-comprehension-of-case-relevance-for-better-legal-retrieval-haitao-li-et-al-2024>(7/7 | 201/273) Towards an In-Depth Comprehension of Case Relevance for Better Legal Retrieval (Haitao Li et al., 2024)</a></li></ul></li><li><a href=#cscr-6>cs.CR (6)</a><ul><li><a href=#16--202273-privacy-backdoors-enhancing-membership-inference-through-poisoning-pre-trained-models-yuxin-wen-et-al-2024>(1/6 | 202/273) Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models (Yuxin Wen et al., 2024)</a></li><li><a href=#26--203273-enhancing-reasoning-capacity-of-slm-using-cognitive-enhancement-jonathan-pan-et-al-2024>(2/6 | 203/273) Enhancing Reasoning Capacity of SLM using Cognitive Enhancement (Jonathan Pan et al., 2024)</a></li><li><a href=#36--204273-maglive-near-field-magnetic-sensing-based-voice-liveness-detection-on-smartphones-xiping-sun-et-al-2024>(3/6 | 204/273) MagLive: Near-Field Magnetic Sensing-Based Voice Liveness Detection on Smartphones (Xiping Sun et al., 2024)</a></li><li><a href=#46--205273-poisoning-decentralized-collaborative-recommender-system-and-its-countermeasures-ruiqi-zheng-et-al-2024>(4/6 | 205/273) Poisoning Decentralized Collaborative Recommender System and Its Countermeasures (Ruiqi Zheng et al., 2024)</a></li><li><a href=#56--206273-towards-automated-generation-of-smart-grid-cyber-range-for-cybersecurity-experiments-and-training-daisuke-mashima-et-al-2024>(5/6 | 206/273) Towards Automated Generation of Smart Grid Cyber Range for Cybersecurity Experiments and Training (Daisuke Mashima et al., 2024)</a></li><li><a href=#66--207273-ufid-a-unified-framework-for-input-level-backdoor-detection-on-diffusion-models-zihan-guan-et-al-2024>(6/6 | 207/273) UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models (Zihan Guan et al., 2024)</a></li></ul></li><li><a href=#cshc-5>cs.HC (5)</a><ul><li><a href=#15--208273-a-preliminary-roadmap-for-llms-as-assistants-in-exploring-analyzing-and-visualizing-knowledge-graphs-harry-li-et-al-2024>(1/5 | 208/273) A Preliminary Roadmap for LLMs as Assistants in Exploring, Analyzing, and Visualizing Knowledge Graphs (Harry Li et al., 2024)</a></li><li><a href=#25--209273-how-can-large-language-models-enable-better-socially-assistive-human-robot-interaction-a-brief-survey-zhonghao-shi-et-al-2024>(2/5 | 209/273) How Can Large Language Models Enable Better Socially Assistive Human-Robot Interaction: A Brief Survey (Zhonghao Shi et al., 2024)</a></li><li><a href=#35--210273-towards-a-potential-paradigm-shift-in-health-data-collection-and-analysis-david-josef-herzog-et-al-2024>(3/5 | 210/273) Towards a potential paradigm shift in health data collection and analysis (David Josef Herzog et al., 2024)</a></li><li><a href=#45--211273-delve-into-earths-past-a-visualization-based-exhibit-deployed-across-multiple-museum-contexts-mara-solen-et-al-2024>(4/5 | 211/273) DeLVE into Earth&rsquo;s Past: A Visualization-Based Exhibit Deployed Across Multiple Museum Contexts (Mara Solen et al., 2024)</a></li><li><a href=#55--212273-chat-modeling-natural-language-based-procedural-modeling-of-biological-structures-without-training-donggang-jia-et-al-2024>(5/5 | 212/273) Chat Modeling: Natural Language-based Procedural Modeling of Biological Structures without Training (Donggang Jia et al., 2024)</a></li></ul></li><li><a href=#mathst-2>math.ST (2)</a><ul><li><a href=#12--213273-a-statistical-framework-of-watermarks-for-large-language-models-pivot-detection-efficiency-and-optimal-rules-xiang-li-et-al-2024>(1/2 | 213/273) A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules (Xiang Li et al., 2024)</a></li><li><a href=#22--214273-optimal-ridge-regularization-for-out-of-distribution-prediction-pratik-patil-et-al-2024>(2/2 | 214/273) Optimal Ridge Regularization for Out-of-Distribution Prediction (Pratik Patil et al., 2024)</a></li></ul></li><li><a href=#statml-3>stat.ML (3)</a><ul><li><a href=#13--215273-transfusion-covariate-shift-robust-transfer-learning-for-high-dimensional-regression-zelin-he-et-al-2024>(1/3 | 215/273) TransFusion: Covariate-Shift Robust Transfer Learning for High-Dimensional Regression (Zelin He et al., 2024)</a></li><li><a href=#23--216273-fair-mp-boost-fair-and-interpretable-minipatch-boosting-camille-olivia-little-et-al-2024>(2/3 | 216/273) Fair MP-BOOST: Fair and Interpretable Minipatch Boosting (Camille Olivia Little et al., 2024)</a></li><li><a href=#33--217273-large-scale-non-convex-stochastic-constrained-distributionally-robust-optimization-qi-zhang-et-al-2024>(3/3 | 217/273) Large-Scale Non-convex Stochastic Constrained Distributionally Robust Optimization (Qi Zhang et al., 2024)</a></li></ul></li><li><a href=#csro-12>cs.RO (12)</a><ul><li><a href=#112--218273-quad-query-based-interpretable-neural-motion-planning-for-autonomous-driving-sourav-biswas-et-al-2024>(1/12 | 218/273) QuAD: Query-based Interpretable Neural Motion Planning for Autonomous Driving (Sourav Biswas et al., 2024)</a></li><li><a href=#212--219273-force-evt-a-closer-look-at-robotic-gripper-force-measurement-with-event-based-vision-transformer-qianyu-guo-et-al-2024>(2/12 | 219/273) Force-EvT: A Closer Look at Robotic Gripper Force Measurement with Event-based Vision Transformer (Qianyu Guo et al., 2024)</a></li><li><a href=#312--220273-nvins-robust-visual-inertial-navigation-fused-with-nerf-augmented-camera-pose-regressor-and-uncertainty-quantification-juyeop-han-et-al-2024>(3/12 | 220/273) NVINS: Robust Visual Inertial Navigation Fused with NeRF-augmented Camera Pose Regressor and Uncertainty Quantification (Juyeop Han et al., 2024)</a></li><li><a href=#412--221273-ltl-d-incrementally-optimal-replanning-for-feasible-and-infeasible-tasks-in-linear-temporal-logic-specifications-jiming-ren-et-al-2024>(4/12 | 221/273) LTL-D*: Incrementally Optimal Replanning for Feasible and Infeasible Tasks in Linear Temporal Logic Specifications (Jiming Ren et al., 2024)</a></li><li><a href=#512--222273-visual-inertial-state-estimation-based-on-chebyshev-polynomial-optimization-hongyu-zhang-et-al-2024>(5/12 | 222/273) Visual-inertial state estimation based on Chebyshev polynomial optimization (Hongyu Zhang et al., 2024)</a></li><li><a href=#612--223273-a-center-of-mass-shifting-aerial-manipulation-platform-for-heavy-tool-handling-on-non-horizontal-surfaces-tong-hui-et-al-2024>(6/12 | 223/273) A Center-of-Mass Shifting Aerial Manipulation Platform for Heavy-Tool Handling on Non-Horizontal Surfaces (Tong Hui et al., 2024)</a></li><li><a href=#712--224273-versatile-navigation-under-partial-observability-via-value-guided-diffusion-policy-gengyu-zhang-et-al-2024>(7/12 | 224/273) Versatile Navigation under Partial Observability via Value-guided Diffusion Policy (Gengyu Zhang et al., 2024)</a></li><li><a href=#812--225273-contacthandover-contact-guided-robot-to-human-object-handover-zixi-wang-et-al-2024>(8/12 | 225/273) ContactHandover: Contact-Guided Robot-to-Human Object Handover (Zixi Wang et al., 2024)</a></li><li><a href=#912--226273-entity-centric-reinforcement-learning-for-object-manipulation-from-pixels-dan-haramati-et-al-2024>(9/12 | 226/273) Entity-Centric Reinforcement Learning for Object Manipulation from Pixels (Dan Haramati et al., 2024)</a></li><li><a href=#1012--227273-physreaction-physically-plausible-real-time-humanoid-reaction-synthesis-via-forward-dynamics-guided-4d-imitation-yunze-liu-et-al-2024>(10/12 | 227/273) PhysReaction: Physically Plausible Real-Time Humanoid Reaction Synthesis via Forward Dynamics Guided 4D Imitation (Yunze Liu et al., 2024)</a></li><li><a href=#1112--228273-efficient-motion-planning-for-manipulators-with-control-barrier-function-induced-neural-controller-mingxin-yu-et-al-2024>(11/12 | 228/273) Efficient Motion Planning for Manipulators with Control Barrier Function-Induced Neural Controller (Mingxin Yu et al., 2024)</a></li><li><a href=#1212--229273-scalable-radar-based-its-self-localization-and-occupancy-heat-map-for-traffic-analysis-longfei-han-et-al-2024>(12/12 | 229/273) Scalable Radar-based ITS: Self-localization and Occupancy Heat Map for Traffic Analysis (Longfei Han et al., 2024)</a></li></ul></li><li><a href=#quant-ph-4>quant-ph (4)</a><ul><li><a href=#14--230273-exploring-quantum-enhanced-machine-learning-for-computer-vision-applications-and-insights-on-noisy-intermediate-scale-quantum-devices-purnachandra-mandadapu-2024>(1/4 | 230/273) Exploring Quantum-Enhanced Machine Learning for Computer Vision: Applications and Insights on Noisy Intermediate-Scale Quantum Devices (Purnachandra Mandadapu, 2024)</a></li><li><a href=#24--231273-no-go-theorem-for-probabilistic-one-way-secret-key-distillation-vishal-singh-et-al-2024>(2/4 | 231/273) No-go theorem for probabilistic one-way secret-key distillation (Vishal Singh et al., 2024)</a></li><li><a href=#34--232273-random-circuit-sampling-fourier-expansion-and-statistics-gil-kalai-et-al-2024>(3/4 | 232/273) Random Circuit Sampling: Fourier Expansion and Statistics (Gil Kalai et al., 2024)</a></li><li><a href=#44--233273-parallel-proportional-fusion-of-spiking-quantum-neural-network-for-optimizing-image-classification-zuyu-xu-et-al-2024>(4/4 | 233/273) Parallel Proportional Fusion of Spiking Quantum Neural Network for Optimizing Image Classification (Zuyu Xu et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--234273-a-novel-audio-representation-for-music-genre-identification-in-mir-navin-kamuni-et-al-2024>(1/2 | 234/273) A Novel Audio Representation for Music Genre Identification in MIR (Navin Kamuni et al., 2024)</a></li><li><a href=#22--235273-removing-speaker-information-from-speech-representation-using-variable-length-soft-pooling-injune-hwang-et-al-2024>(2/2 | 235/273) Removing Speaker Information from Speech Representation using Variable-Length Soft Pooling (Injune Hwang et al., 2024)</a></li></ul></li><li><a href=#astro-phim-1>astro-ph.IM (1)</a><ul><li><a href=#11--236273-a-novel-sector-based-algorithm-for-an-optimized-star-galaxy-classification-anumanchi-agastya-sai-ram-likhit-et-al-2024>(1/1 | 236/273) A Novel Sector-Based Algorithm for an Optimized Star-Galaxy Classification (Anumanchi Agastya Sai Ram Likhit et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--237273-utilizing-ai-and-social-media-analytics-to-discover-adverse-side-effects-of-glp-1-receptor-agonists-alon-bartal-et-al-2024>(1/1 | 237/273) Utilizing AI and Social Media Analytics to Discover Adverse Side Effects of GLP-1 Receptor Agonists (Alon Bartal et al., 2024)</a></li></ul></li><li><a href=#csit-7>cs.IT (7)</a><ul><li><a href=#17--238273-joint-beam-scheduling-and-beamforming-design-for-cooperative-positioning-in-multi-beam-leo-satellite-networks-hongtao-xv-et-al-2024>(1/7 | 238/273) Joint Beam Scheduling and Beamforming Design for Cooperative Positioning in Multi-beam LEO Satellite Networks (Hongtao Xv et al., 2024)</a></li><li><a href=#27--239273-distribution-agnostic-database-de-anonymization-under-obfuscation-and-synchronization-errors-serhat-bakirtas-et-al-2024>(2/7 | 239/273) Distribution-Agnostic Database De-Anonymization Under Obfuscation And Synchronization Errors (Serhat Bakirtas et al., 2024)</a></li><li><a href=#37--240273-density-evolution-analysis-of-generalized-low-density-parity-check-codes-under-a-posteriori-probability-decoder-dongxu-chang-et-al-2024>(3/7 | 240/273) Density Evolution Analysis of Generalized Low-density Parity-check Codes under a Posteriori Probability Decoder (Dongxu Chang et al., 2024)</a></li><li><a href=#47--241273-performance-evaluation-of-ris-assisted-spatial-modulation-for-downlink-transmission-xusheng-zhu-et-al-2024>(4/7 | 241/273) Performance Evaluation of RIS-Assisted Spatial Modulation for Downlink Transmission (Xusheng Zhu et al., 2024)</a></li><li><a href=#57--242273-measuring-the-redundancy-of-information-from-a-source-failure-perspective-jesse-milzman-2024>(5/7 | 242/273) Measuring the Redundancy of Information from a Source Failure Perspective (Jesse Milzman, 2024)</a></li><li><a href=#67--243273-rethinking-resource-management-in-edge-learning-a-joint-pre-training-and-fine-tuning-design-paradigm-zhonghao-lyu-et-al-2024>(6/7 | 243/273) Rethinking Resource Management in Edge Learning: A Joint Pre-training and Fine-tuning Design Paradigm (Zhonghao Lyu et al., 2024)</a></li><li><a href=#77--244273-star-ris-aided-secure-mimo-communication-systems-xiequn-dong-et-al-2024>(7/7 | 244/273) STAR-RIS Aided Secure MIMO Communication Systems (Xiequn Dong et al., 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-1>physics.flu-dyn (1)</a><ul><li><a href=#11--245273-numerical-modelling-of-flame-spread-over-thin-circular-ducts-vipin-kumar-et-al-2024>(1/1 | 245/273) Numerical modelling of flame spread over thin circular ducts (Vipin Kumar et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#11--246273-distributed-satellite-terrestrial-cooperative-routing-strategy-based-on-minimum-hop-count-analysis-in-mega-leo-satellite-constellation-xinao-feng-et-al-2024>(1/1 | 246/273) Distributed Satellite-Terrestrial Cooperative Routing Strategy Based on Minimum Hop-Count Analysis in Mega LEO Satellite Constellation (Xin&rsquo;ao Feng et al., 2024)</a></li></ul></li><li><a href=#eesssy-7>eess.SY (7)</a><ul><li><a href=#17--247273-using-dynamic-safety-margins-as-control-barrier-functions-victor-freire-et-al-2024>(1/7 | 247/273) Using Dynamic Safety Margins as Control Barrier Functions (Victor Freire et al., 2024)</a></li><li><a href=#27--248273-foundations-of-cyber-resilience-the-confluence-of-game-control-and-learning-theories-quanyan-zhu-2024>(2/7 | 248/273) Foundations of Cyber Resilience: The Confluence of Game, Control, and Learning Theories (Quanyan Zhu, 2024)</a></li><li><a href=#37--249273-research-on-mechanism-of-voltage-oscillation-caused-by-repeated-lvrt-of-wind-turbine-based-on-switched-system-theory-qiping-lai-et-al-2024>(3/7 | 249/273) Research on Mechanism of Voltage Oscillation Caused by Repeated LVRT of Wind Turbine Based on Switched System Theory (Qiping Lai et al., 2024)</a></li><li><a href=#47--250273-second-order-newton-based-extremum-seeking-for-multivariable-static-maps-azad-ghaffari-et-al-2024>(4/7 | 250/273) Second-Order Newton-Based Extremum Seeking for Multivariable Static Maps (Azad Ghaffari et al., 2024)</a></li><li><a href=#57--251273-finite-sample-frequency-domain-identification-anastasios-tsiamis-et-al-2024>(5/7 | 251/273) Finite Sample Frequency Domain Identification (Anastasios Tsiamis et al., 2024)</a></li><li><a href=#67--252273-performance-triggered-adaptive-model-reduction-for-soil-moisture-estimation-in-precision-irrigation-sarupa-debnath-et-al-2024>(6/7 | 252/273) Performance triggered adaptive model reduction for soil moisture estimation in precision irrigation (Sarupa Debnath et al., 2024)</a></li><li><a href=#77--253273-orchestrating-uavs-for-prioritized-data-harvesting-a-cross-layer-optimization-perspective-bharath-keshavamurthy-et-al-2024>(7/7 | 253/273) Orchestrating UAVs for Prioritized Data Harvesting: A Cross-Layer Optimization Perspective (Bharath Keshavamurthy et al., 2024)</a></li></ul></li><li><a href=#mathna-3>math.NA (3)</a><ul><li><a href=#13--254273-convergence-acceleration-of-favre-averaged-non-linear-harmonic-method-feng-wang-et-al-2024>(1/3 | 254/273) Convergence Acceleration of Favre-Averaged Non-Linear Harmonic Method (Feng Wang et al., 2024)</a></li><li><a href=#23--255273-capturing-shock-waves-by-relaxation-neural-networks-nan-zhou-et-al-2024>(2/3 | 255/273) Capturing Shock Waves by Relaxation Neural Networks (Nan Zhou et al., 2024)</a></li><li><a href=#33--256273-adaptive-hybrid-high-order-method-for-guaranteed-lower-eigenvalue-bounds-carsten-carstensen-et-al-2024>(3/3 | 256/273) Adaptive hybrid high-order method for guaranteed lower eigenvalue bounds (Carsten Carstensen et al., 2024)</a></li></ul></li><li><a href=#mathoc-2>math.OC (2)</a><ul><li><a href=#12--257273-dynamics-and-optimization-in-spatially-distributed-electrical-vehicle-charging-fernando-paganini-et-al-2024>(1/2 | 257/273) Dynamics and Optimization in Spatially Distributed Electrical Vehicle Charging (Fernando Paganini et al., 2024)</a></li><li><a href=#22--258273-multiple-joint-chance-constraints-approximation-for-uncertainty-modeling-in-dispatch-problems-yilin-wen-et-al-2024>(2/2 | 258/273) Multiple Joint Chance Constraints Approximation for Uncertainty Modeling in Dispatch Problems (Yilin Wen et al., 2024)</a></li></ul></li><li><a href=#csma-2>cs.MA (2)</a><ul><li><a href=#12--259273-a-crisp-dm-based-methodology-for-assessing-agent-based-simulation-models-using-process-mining-rob-h-bemthuis-et-al-2024>(1/2 | 259/273) A CRISP-DM-based Methodology for Assessing Agent-based Simulation Models using Process Mining (Rob H. Bemthuis et al., 2024)</a></li><li><a href=#22--260273-gov-rek-governed-reward-engineering-kernels-for-designing-robust-multi-agent-reinforcement-learning-systems-ashish-rana-et-al-2024>(2/2 | 260/273) GOV-REK: Governed Reward Engineering Kernels for Designing Robust Multi-Agent Reinforcement Learning Systems (Ashish Rana et al., 2024)</a></li></ul></li><li><a href=#nlinao-1>nlin.AO (1)</a><ul><li><a href=#11--261273-nonlinear-impulse-pattern-formulation-dynamical-social-and-political-prediction-algorithm-for-city-planning-and-public-participation-rolf-bader-et-al-2024>(1/1 | 261/273) Nonlinear Impulse Pattern Formulation dynamical social and political prediction algorithm for city planning and public participation (Rolf Bader et al., 2024)</a></li></ul></li><li><a href=#eesssp-2>eess.SP (2)</a><ul><li><a href=#12--262273-digital-twins-for-supporting-ai-research-with-autonomous-vehicle-networks-anıl-gürses-et-al-2024>(1/2 | 262/273) Digital Twins for Supporting AI Research with Autonomous Vehicle Networks (Anıl Gürses et al., 2024)</a></li><li><a href=#22--263273-wideband-channel-capacity-maximization-with-beyond-diagonal-ris-reflection-matrices-özlem-tuğfe-demir-et-al-2024>(2/2 | 263/273) Wideband Channel Capacity Maximization With Beyond Diagonal RIS Reflection Matrices (Özlem Tuğfe Demir et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--264273-a-hat-trick-automatically-verifying-representation-invariants-using-symbolic-finite-automata-zhe-zhou-et-al-2024>(1/1 | 264/273) A HAT Trick: Automatically Verifying Representation Invariants Using Symbolic Finite Automata (Zhe Zhou et al., 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--265273-game-theoretic-deep-reinforcement-learning-to-minimize-carbon-emissions-and-energy-costs-for-ai-inference-workloads-in-geo-distributed-data-centers-ninad-hogade-et-al-2024>(1/1 | 265/273) Game-Theoretic Deep Reinforcement Learning to Minimize Carbon Emissions and Energy Costs for AI Inference Workloads in Geo-Distributed Data Centers (Ninad Hogade et al., 2024)</a></li></ul></li><li><a href=#physicsmed-ph-1>physics.med-ph (1)</a><ul><li><a href=#11--266273-prior-frequency-guided-diffusion-model-for-limited-angle-la-cbct-reconstruction-jiacheng-xie-et-al-2024>(1/1 | 266/273) Prior Frequency Guided Diffusion Model for Limited Angle (LA)-CBCT Reconstruction (Jiacheng Xie et al., 2024)</a></li></ul></li><li><a href=#econth-1>econ.TH (1)</a><ul><li><a href=#11--267273-symmetric-mechanisms-for-two-sided-matching-problems-daniela-bubboloni-et-al-2024>(1/1 | 267/273) Symmetric mechanisms for two-sided matching problems (Daniela Bubboloni et al., 2024)</a></li></ul></li><li><a href=#physicssoc-ph-2>physics.soc-ph (2)</a><ul><li><a href=#12--268273-cooperative-evolutionary-pressure-and-diminishing-returns-might-explain-the-fermi-paradox-on-what-super-ais-are-like-daniel-vallstrom-2024>(1/2 | 268/273) Cooperative Evolutionary Pressure and Diminishing Returns Might Explain the Fermi Paradox: On What Super-AIs Are Like (Daniel Vallstrom, 2024)</a></li><li><a href=#22--269273-social-dynamics-of-consumer-response-a-unified-framework-integrating-statistical-physics-and-marketing-dynamics-javier-marin-2024>(2/2 | 269/273) Social Dynamics of Consumer Response: A Unified Framework Integrating Statistical Physics and Marketing Dynamics (Javier Marin, 2024)</a></li></ul></li><li><a href=#cset-1>cs.ET (1)</a><ul><li><a href=#11--270273-mitigating-transient-bullwhip-effects-under-imperfect-demand-forecasts-sarah-h-q-li-et-al-2024>(1/1 | 270/273) Mitigating Transient Bullwhip Effects Under Imperfect Demand Forecasts (Sarah H. Q. Li et al., 2024)</a></li></ul></li><li><a href=#csne-1>cs.NE (1)</a><ul><li><a href=#11--271273-gpu-accelerated-evolutionary-multiobjective-optimization-using-tensorized-rvea-zhenyu-liang-et-al-2024>(1/1 | 271/273) GPU-accelerated Evolutionary Multiobjective Optimization Using Tensorized RVEA (Zhenyu Liang et al., 2024)</a></li></ul></li><li><a href=#csds-1>cs.DS (1)</a><ul><li><a href=#11--272273-on-the-complexity-of-minimizing-energy-consumption-of-partitioning-dag-tasks-wei-liu-et-al-2024>(1/1 | 272/273) On the Complexity of Minimizing Energy Consumption of Partitioning DAG Tasks (Wei Liu et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--273273-data-analytics-for-improving-energy-efficiency-in-short-sea-shipping-mohamed-abuella-et-al-2024>(1/1 | 273/273) Data Analytics for Improving Energy Efficiency in Short Sea Shipping (Mohamed Abuella et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>