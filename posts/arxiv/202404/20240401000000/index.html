<!doctype html><html><head><title>arXiv @ 2024.04.01</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.04.01"><meta property="og:description" content="Primary Categories cs.AI (2) cs.CE (2) cs.CL (42) cs.CR (1) cs.CV (35) cs.CY (1) cs.DC (2) cs.DS (1) cs.GR (1) cs.HC (3) cs.IR (4) cs.IT (2) cs.LG (16) cs.NE (2) cs.RO (14) cs.SD (1) cs.SE (3) eess.SY (3) math.NA (2) math.OC (1) physics.comp-ph (1) physics.med-ph (1) physics.space-ph (1) q-fin.MF (1) stat.ML (3) Keywords keyword cs.CL cs.CV cs.LG cs.RO Adversarial Learning 1 Autoencoder 2 BERT 1 1 BLOOM 1 Bandit Algorithm 1 Benchmarking 14 11 2 1 Black Box 1 Chain-of-thought Prompt 1 Clustering 2 1 1 Continual Learning 3 Contrastive Learning 1 1 1 Convolution 5 1 1 Convolutional Neural Network 1 3 3 Counter-factual 2 Data Augmentation 4 1 Dialogue System 1 Diffusion Model 4 1 Direct Preference Optimization 1 Distribution Shift 2 2 Domain Adaptation 2 Emotion Recognition 1 Fairness 1 Fake News Detection 2 Federated Learning 2 Few-shot 1 1 2 Fine-tuning 13 2 4 Foundation Model 1 1 GPT 5 GPT-3 3 GPT-3."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202404/20240401000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-01T00:00:00+00:00"><meta name=description content="arXiv @ 2024.04.01"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul class=active><li><a class=active href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202404/20240401000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Monday, Apr 1, 2024</p></div><div class=title><h1>arXiv @ 2024.04.01</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#csai-2>cs.AI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#csce-2>cs.CE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#cscl-42>cs.CL (42)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#cscr-1>cs.CR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#cscv-35>cs.CV (35)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#csdc-2>cs.DC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#csds-1>cs.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#cshc-3>cs.HC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#csir-4>cs.IR (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#csit-2>cs.IT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#cslg-16>cs.LG (16)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#csne-2>cs.NE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#csro-14>cs.RO (14)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#cssd-1>cs.SD (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#csse-3>cs.SE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#eesssy-3>eess.SY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#physicscomp-ph-1>physics.comp-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#physicsmed-ph-1>physics.med-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#physicsspace-ph-1>physics.space-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#q-finmf-1>q-fin.MF (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/#statml-3>stat.ML (3)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Adversarial Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td>2</td><td></td><td></td></tr><tr><td>BERT</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>BLOOM</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Benchmarking</td><td>14</td><td>11</td><td>2</td><td>1</td></tr><tr><td>Black Box</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Chain-of-thought Prompt</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td>2</td><td></td><td>1</td><td>1</td></tr><tr><td>Continual Learning</td><td></td><td></td><td>3</td><td></td></tr><tr><td>Contrastive Learning</td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>Convolution</td><td></td><td>5</td><td>1</td><td>1</td></tr><tr><td>Convolutional Neural Network</td><td>1</td><td>3</td><td>3</td><td></td></tr><tr><td>Counter-factual</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Data Augmentation</td><td>4</td><td></td><td>1</td><td></td></tr><tr><td>Dialogue System</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>4</td><td></td><td>1</td></tr><tr><td>Direct Preference Optimization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Distribution Shift</td><td>2</td><td></td><td>2</td><td></td></tr><tr><td>Domain Adaptation</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Emotion Recognition</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fake News Detection</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Few-shot</td><td>1</td><td>1</td><td>2</td><td></td></tr><tr><td>Fine-tuning</td><td>13</td><td>2</td><td>4</td><td></td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>GPT</td><td>5</td><td></td><td></td><td></td></tr><tr><td>GPT-3</td><td>3</td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>2</td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Gemini</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Geometry</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Graph</td><td>3</td><td>4</td><td>3</td><td>1</td></tr><tr><td>Graph Attention Networks</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Neural Network</td><td>2</td><td>1</td><td>1</td><td></td></tr><tr><td>Grounding</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Hate Speech Detection</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td>1</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>6</td><td>1</td><td></td><td>3</td></tr><tr><td>Information Retrieval</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>4</td><td>1</td><td>2</td><td></td></tr><tr><td>Knowledge Graph</td><td>2</td><td></td><td></td><td></td></tr><tr><td>LLaMA</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Label Smoothing</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>41</td><td>4</td><td>4</td><td>4</td></tr><tr><td>Lemmatization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Low-Resource</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Massive Multitask Language Understanding (MMLU)</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Mathematical Reasoning</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Multi-modal</td><td>3</td><td>5</td><td></td><td></td></tr><tr><td>Multiple Instance Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Open-Domain Dialogue</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Opinion Summarization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Out-of-domain</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Perplexity</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Pre-trained Language Model</td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Prompt</td><td>7</td><td>6</td><td>1</td><td>1</td></tr><tr><td>Prompt Learning</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Question Answering</td><td>4</td><td>1</td><td>1</td><td></td></tr><tr><td>Reasoning</td><td>11</td><td>1</td><td></td><td></td></tr><tr><td>Recommendation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td></td><td></td><td>2</td><td>3</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Relation Extraction</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>3</td><td>1</td><td></td><td></td></tr><tr><td>RoBERTa</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Rouge</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Rouge-L</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Self-Distillation</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>3</td><td>2</td><td></td></tr><tr><td>Sentiment Analysis</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td>1</td><td></td><td>5</td></tr><tr><td>Simulator</td><td></td><td>1</td><td></td><td>5</td></tr><tr><td>Stemming</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Summarization</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Supervised Learning</td><td>2</td><td>2</td><td>3</td><td></td></tr><tr><td>Text Classification</td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Text Embedding</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Tokenization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Topic Segmentation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Transformer</td><td>6</td><td>7</td><td>2</td><td></td></tr><tr><td>Unsupervised Learning</td><td>1</td><td>1</td><td>2</td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>4</td><td></td><td>2</td></tr><tr><td>Visual Question Answering</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Word Embedding</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Yolo</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td></td><td></td><td>3</td><td></td></tr><tr><td>human-in-the-loop</td><td></td><td>1</td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-42>cs.CL (42)</h2><h3 id=142--1145-metaie-distilling-a-meta-model-from-llm-for-all-kinds-of-information-extraction-tasks-letian-peng-et-al-2024>(1/42 | 1/145) MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks (Letian Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Letian Peng, Zilong Wang, Feng Yao, Zihan Wang, Jingbo Shang. (2024)<br><strong>MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks</strong><br><button class=copy-to-clipboard title="MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Few-shot, Knowledge Distillation, Knowledge Distillation, Information Retrieval, Named Entity Recognition, Relation Extraction, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00457v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00457v1.pdf filename=2404.00457v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Information</b> <b>extraction</b> (IE) is a fundamental area in natural language processing where <b>prompting</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> even with <b>in-context</b> examples, cannot defeat small LMs tuned on very small IE datasets. We observe that IE tasks, such as <b>named</b> <b>entity</b> <b>recognition</b> and <b>relation</b> <b>extraction,</b> all focus on extracting important <b>information,</b> <b>which</b> can be formalized as a label-to-span matching. In this paper, we propose a novel framework MetaIE to build a small LM as meta-model by learning to extract &ldquo;important <b>information&rdquo;,</b> <b>i.e.,</b> the meta-understanding of IE, so that this meta-model can be adapted to all kind of IE tasks effectively and efficiently. Specifically, MetaIE obtains the small LM via a symbolic <b>distillation</b> from an <b>LLM</b> following the label-to-span scheme. We construct the <b>distillation</b> dataset via sampling sentences from language model pre-training datasets (e.g., OpenWebText in our implementation) and <b>prompting</b> an <b>LLM</b> to identify the typed spans of &ldquo;important <b>information&rdquo;.</b> <b>We</b> evaluate the meta-model under the <b>few-shot</b> adaptation setting. Extensive results on 13 datasets from 6 IE tasks confirm that MetaIE can offer a better starting point for <b>few-shot</b> tuning on IE datasets and outperform other meta-models from (1) vanilla language model pre-training, (2) multi-IE-task pre-training with human annotations, and (3) single-IE-task symbolic <b>distillation</b> from <b>LLM.</b> Moreover, we provide comprehensive analyses of MetaIE, such as the size of the <b>distillation</b> dataset, the meta-model architecture, and the size of the meta-model.</p></p class="citation"></blockquote><h3 id=242--2145-dialectical-alignment-resolving-the-tension-of-3h-and-security-threats-of-llms-shu-yang-et-al-2024>(2/42 | 2/145) Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs (Shu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shu Yang, Jiayuan Su, Han Jiang, Mengdi Li, Keyuan Cheng, Muhammad Asif Ali, Lijie Hu, Di Wang. (2024)<br><strong>Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs</strong><br><button class=copy-to-clipboard title="Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, Reinforcement Learning from Human Feedback, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00486v1.pdf filename=2404.00486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rise of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> ensuring they embody the principles of being helpful, honest, and harmless (3H), known as Human Alignment, becomes crucial. While existing alignment methods like <b>RLHF,</b> DPO, etc., effectively <b>fine-tune</b> <b>LLMs</b> to match preferences in the preference dataset, they often lead <b>LLMs</b> to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for <b>LLMs</b> to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of <b>LLM</b> being attacked by external poisoned data, which poses a significant security risk to <b>LLM</b> system applications such as <b>Retrieval-augmented</b> <b>generation</b> <b>(RAG).</b> To address the challenge, we propose a novel framework: Dialectical Alignment (DA), which (1) utilizes AI feedback to identify optimal strategies for <b>LLMs</b> to navigate inter-context conflicts and context-memory conflicts with different external evidence in context window (i.e., different ratios of poisoned factual contexts); (2) constructs the SFT dataset as well as the preference dataset based on the AI feedback and strategies above; (3) uses the above datasets for <b>LLM</b> alignment to defense poisoned context attack while preserving the effectiveness of <b>in-context</b> knowledge editing. Our experiments show that the dialectical alignment model improves poisoned data attack defense by 20 and does not require any additional <b>prompt</b> engineering or prior declaration of <code>you may be attacked</code> to the <b>LLMs&rsquo;</b> context window.</p></p class="citation"></blockquote><h3 id=342--3145-edinburgh-clinical-nlp-at-semeval-2024-task-2-fine-tune-your-model-unless-you-have-access-to-gpt-4-aryo-pradipta-gema-et-al-2024>(3/42 | 3/145) Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model unless you have access to GPT-4 (Aryo Pradipta Gema et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aryo Pradipta Gema, Giwon Hong, Pasquale Minervini, Luke Daines, Beatrice Alex. (2024)<br><strong>Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model unless you have access to GPT-4</strong><br><button class=copy-to-clipboard title="Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model unless you have access to GPT-4" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, Fine-tuning, GPT, GPT-4, Natural Language Inference, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00484v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00484v1.pdf filename=2404.00484v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The NLI4CT task assesses <b>Natural</b> <b>Language</b> <b>Inference</b> systems in predicting whether hypotheses entail or contradict evidence from Clinical Trial Reports. In this study, we evaluate various <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with multiple strategies, including Chain-of-Thought, <b>In-Context</b> <b>Learning,</b> and Parameter-Efficient <b>Fine-Tuning</b> (PEFT). We propose a PEFT method to improve the consistency of <b>LLMs</b> by merging adapters that were <b>fine-tuned</b> separately using triplet and language modelling objectives. We found that merging the two PEFT adapters improves the F1 score (+0.0346) and consistency (+0.152) of the <b>LLMs.</b> However, our novel methods did not produce more accurate results than <b>GPT-4</b> in terms of faithfulness and consistency. Averaging the three metrics, <b>GPT-4</b> ranks joint-first in the competition with 0.8328. Finally, our contamination analysis with <b>GPT-4</b> indicates that there was no test data leakage.</p></p class="citation"></blockquote><h3 id=442--4145-a-comprehensive-study-on-nlp-data-augmentation-for-hate-speech-detection-legacy-methods-bert-and-llms-md-saroar-jahan-et-al-2024>(4/42 | 4/145) A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs (Md Saroar Jahan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Saroar Jahan, Mourad Oussalah, Djamila Romaissa Beddia, Jhuma kabir Mim, Nabil Arhab. (2024)<br><strong>A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs</strong><br><button class=copy-to-clipboard title="A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Data Augmentation, Supervised Learning, BERT, GPT, GPT-3, Hate Speech Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00303v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00303v1.pdf filename=2404.00303v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The surge of interest in <b>data</b> <b>augmentation</b> within the realm of NLP has been driven by the need to address challenges posed by <b>hate</b> <b>speech</b> <b>domains,</b> the dynamic nature of social media vocabulary, and the demands for <b>large-scale</b> <b>neural</b> <b>networks</b> requiring extensive training <b>data.</b> <b>However,</b> the prevalent use of lexical substitution in <b>data</b> <b>augmentation</b> has raised concerns, as it may inadvertently alter the intended meaning, thereby impacting the efficacy of <b>supervised</b> machine learning models. In pursuit of suitable <b>data</b> <b>augmentation</b> methods, this study explores both established legacy approaches and contemporary practices such as <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM),</b> including <b>GPT</b> in <b>Hate</b> <b>Speech</b> <b>detection.</b> Additionally, we propose an optimized utilization of <b>BERT-based</b> encoder models with contextual cosine similarity filtration, exposing significant limitations in prior synonym substitution methods. Our comparative analysis encompasses five popular augmentation techniques: WordNet and Fast-Text synonym replacement, Back-translation, <b>BERT-mask</b> contextual augmentation, and <b>LLM.</b> Our analysis across five <b>benchmarked</b> datasets revealed that while traditional methods like back-translation show low label alteration rates (0.3-1.5%), and <b>BERT-based</b> contextual synonym replacement offers sentence diversity but at the cost of higher label alteration rates (over 6%). Our proposed <b>BERT-based</b> contextual cosine similarity filtration markedly reduced label alteration to just 0.05%, demonstrating its efficacy in 0.7% higher F1 performance. However, augmenting <b>data</b> <b>with</b> <b>GPT-3</b> not only avoided overfitting with up to sevenfold <b>data</b> <b>increase</b> but also improved embedding space coverage by 15% and classification F1 score by 1.4% over traditional methods, and by 0.8% over our method.</p></p class="citation"></blockquote><h3 id=542--5145-eventground-narrative-reasoning-by-grounding-to-eventuality-centric-knowledge-graphs-cheng-jiayang-et-al-2024>(5/42 | 5/145) EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs (Cheng Jiayang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Jiayang, Lin Qiu, Chunkit Chan, Xin Liu, Yangqiu Song, Zheng Zhang. (2024)<br><strong>EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs</strong><br><button class=copy-to-clipboard title="EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Knowledge Graph, Knowledge Graph, Grounding, Information Retrieval, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00209v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00209v1.pdf filename=2404.00209v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Narrative <b>reasoning</b> relies on the understanding of eventualities in story contexts, which requires a wealth of background world <b>knowledge.</b> <b>To</b> help machines leverage such <b>knowledge,</b> <b>existing</b> solutions can be categorized into two groups. Some focus on implicitly modeling eventuality <b>knowledge</b> <b>by</b> pretraining language models (LMs) with eventuality-aware objectives. However, this approach breaks down <b>knowledge</b> <b>structures</b> and lacks interpretability. Others explicitly collect world <b>knowledge</b> <b>of</b> eventualities into structured eventuality-centric <b>knowledge</b> <b>graphs</b> <b>(KGs).</b> <b>However,</b> existing research on leveraging these <b>knowledge</b> <b>sources</b> for free-texts is limited. In this work, we propose an initial comprehensive framework called EventGround, which aims to tackle the problem of <b>grounding</b> free-texts to eventuality-centric <b>KGs</b> for contextualized narrative <b>reasoning.</b> We identify two critical problems in this direction: the event representation and sparsity problems. We provide simple yet effective parsing and partial <b>information</b> <b>extraction</b> methods to tackle these problems. Experimental results demonstrate that our approach consistently outperforms baseline models when combined with <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> or <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> based <b>graph</b> <b>reasoning</b> <b>models.</b> Our framework, incorporating grounded <b>knowledge,</b> <b>achieves</b> state-of-the-art performance while providing interpretable evidence.</p></p class="citation"></blockquote><h3 id=642--6145-injecting-new-knowledge-into-large-language-models-via-supervised-fine-tuning-nick-mecklenburg-et-al-2024>(6/42 | 6/145) Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning (Nick Mecklenburg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nick Mecklenburg, Yiyou Lin, Xiaoxiao Li, Daniel Holstein, Leonardo Nunes, Sara Malvar, Bruno Silva, Ranveer Chandra, Vijay Aski, Pavan Kumar Reddy Yannam, Tolga Aktas, Todd Hendry. (2024)<br><strong>Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning</strong><br><button class=copy-to-clipboard title="Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Out-of-domain, Supervised Learning, GPT, GPT-4, Domain Adaptation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00213v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00213v2.pdf filename=2404.00213v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown remarkable performance in generating human-like text, proving to be a valuable asset across various applications. However, adapting these models to incorporate new, <b>out-of-domain</b> knowledge remains a challenge, particularly for facts and events that occur after the model&rsquo;s knowledge cutoff date. This paper investigates the effectiveness of <b>Supervised</b> <b>Fine-Tuning</b> (SFT) as a method for knowledge injection in <b>LLMs,</b> specifically focusing on the <b>domain</b> <b>of</b> recent sporting events. We compare different dataset generation strategies &ndash; token-based and fact-based scaling &ndash; to create training data that helps the model learn new information. Our experiments on <b>GPT-4</b> demonstrate that while token-based scaling can lead to improvements in Q&amp;A accuracy, it may not provide uniform coverage of new knowledge. Fact-based scaling, on the other hand, offers a more systematic approach to ensure even coverage across all facts. We present a novel dataset generation process that leads to more effective knowledge ingestion through SFT, and our results show considerable performance improvements in Q&amp;A tasks related to <b>out-of-domain</b> knowledge. This study contributes to the understanding of <b>domain</b> <b>adaptation</b> for <b>LLMs</b> and highlights the potential of SFT in enhancing the factuality of <b>LLM</b> responses in specific knowledge domains.</p></p class="citation"></blockquote><h3 id=742--7145-can-llms-master-math-investigating-large-language-models-on-math-stack-exchange-ankit-satpute-et-al-2024>(7/42 | 7/145) Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange (Ankit Satpute et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ankit Satpute, Noah Giessing, Andre Greiner-Petter, Moritz Schubotz, Olaf Teschke, Akiko Aizawa, Bela Gipp. (2024)<br><strong>Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange</strong><br><button class=copy-to-clipboard title="Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, GPT, GPT-4, Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00344v1.pdf filename=2404.00344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated exceptional capabilities in various natural language tasks, often achieving performances that surpass those of humans. Despite these advancements, the domain of mathematics presents a distinctive challenge, primarily due to its specialized structure and the precision it demands. In this study, we adopted a two-step approach for investigating the proficiency of <b>LLMs</b> in answering <b>mathematical</b> <b>questions.</b> First, we employ the most effective <b>LLMs,</b> as identified by their performance on math question-answer <b>benchmarks,</b> to generate answers to 78 questions from the Math Stack Exchange (MSE). Second, a case analysis is conducted on the <b>LLM</b> that showed the highest performance, focusing on the quality and accuracy of its answers through manual evaluation. We found that <b>GPT-4</b> performs best (nDCG of 0.48 and P@10 of 0.37) amongst existing <b>LLMs</b> <b>fine-tuned</b> for answering mathematics questions and outperforms the current best approach on ArqMATH3 Task1, considering P@10. Our Case analysis indicates that while the <b>GPT-4</b> can generate relevant responses in certain instances, it does not consistently answer all questions accurately. This paper explores the current limitations of <b>LLMs</b> in navigating complex <b>mathematical</b> <b>problem-solving.</b> Through case analysis, we shed light on the gaps in <b>LLM</b> capabilities within mathematics, thereby setting the stage for future research and advancements in AI-driven <b>mathematical</b> <b>reasoning.</b> We make our code and findings publicly available for research: \url{https://github.com/gipplab/LLM-Investig-MathStackExchange}</p></p class="citation"></blockquote><h3 id=842--8145-coda-constrained-generation-based-data-augmentation-for-low-resource-nlp-chandra-kiran-reddy-evuru-et-al-2024>(8/42 | 8/145) CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP (Chandra Kiran Reddy Evuru et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chandra Kiran Reddy Evuru, Sreyan Ghosh, Sonal Kumar, Ramaneswaran S, Utkarsh Tyagi, Dinesh Manocha. (2024)<br><strong>CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP</strong><br><button class=copy-to-clipboard title="CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Data Augmentation, Fine-tuning, Low-Resource, Instruction Following, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00415v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00415v1.pdf filename=2404.00415v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present CoDa (Constrained Generation based <b>Data</b> <b>Augmentation),</b> a controllable, effective, and training-free <b>data</b> <b>augmentation</b> technique for <b>low-resource</b> <b>(data-scarce)</b> <b>NLP.</b> Our approach is based on <b>prompting</b> off-the-shelf <b>instruction-following</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for generating text that satisfies a set of constraints. Precisely, we extract a set of simple constraints from every instance in the <b>low-resource</b> dataset and verbalize them to <b>prompt</b> an <b>LLM</b> to generate novel and diverse training instances. Our findings reveal that synthetic <b>data</b> <b>that</b> follows simple constraints in the downstream dataset act as highly effective augmentations, and CoDa can achieve this without intricate decoding-time constrained generation techniques or <b>fine-tuning</b> with complex algorithms that eventually make the model biased toward the small number of training instances. Additionally, CoDa is the first framework that provides users explicit control over the augmentation generation process, thereby also allowing easy adaptation to several domains. We demonstrate the effectiveness of CoDa across 11 datasets spanning 3 tasks and 3 <b>low-resource</b> settings. CoDa outperforms all our baselines, qualitatively and quantitatively, with improvements of 0.12%-7.19%. Code is available here: <a href=https://github.com/Sreyan88/CoDa>https://github.com/Sreyan88/CoDa</a></p></p class="citation"></blockquote><h3 id=942--9145-dilm-distilling-dataset-into-language-model-for-text-level-dataset-distillation-aru-maekawa-et-al-2024>(9/42 | 9/145) DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation (Aru Maekawa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aru Maekawa, Satoshi Kosugi, Kotaro Funakoshi, Manabu Okumura. (2024)<br><strong>DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation</strong><br><button class=copy-to-clipboard title="DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Knowledge Distillation, Knowledge Distillation, Text Classification, In-context Learning, In-context Learning, Large Language Model, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00264v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00264v1.pdf filename=2404.00264v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dataset <b>distillation</b> aims to compress a training dataset by creating a small number of informative synthetic samples such that neural networks trained on them perform as well as those trained on the original training dataset. Current <b>text</b> <b>dataset</b> <b>distillation</b> methods create each synthetic sample as a sequence of <b>word</b> <b>embeddings</b> instead of a <b>text</b> <b>to</b> apply gradient-based optimization; however, such embedding-level <b>distilled</b> datasets cannot be used for training other models whose <b>word</b> <b>embedding</b> weights are different from the model used for <b>distillation.</b> To address this issue, we propose a novel <b>text</b> <b>dataset</b> <b>distillation</b> approach, called <b>Distilling</b> dataset into Language Model (DiLM), which trains a language model to generate informative synthetic training samples as <b>text</b> <b>data,</b> instead of directly optimizing synthetic samples. We evaluated DiLM on various <b>text</b> <b>classification</b> datasets and showed that <b>distilled</b> synthetic datasets from DiLM outperform those from current coreset selection methods. DiLM achieved remarkable generalization performance in training different types of models and <b>in-context</b> <b>learning</b> of <b>large</b> <b>language</b> <b>models.</b> Our code will be available at <a href=https://github.com/arumaekawa/DiLM>https://github.com/arumaekawa/DiLM</a>.</p></p class="citation"></blockquote><h3 id=1042--10145-small-language-models-learn-enhanced-reasoning-skills-from-medical-textbooks-hyunjae-kim-et-al-2024>(10/42 | 10/145) Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks (Hyunjae Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyunjae Kim, Hyeon Hwang, Jiwoo Lee, Sihyeon Park, Dain Kim, Taewhoo Lee, Chanwoong Yoon, Jiwoong Sohn, Donghee Choi, Jaewoo Kang. (2024)<br><strong>Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks</strong><br><button class=copy-to-clipboard title="Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, GPT, GPT-3, GPT-3.5, Instruction Following, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00376v1.pdf filename=2404.00376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While recent advancements in commercial <b>large</b> <b>language</b> <b>models</b> (LM) have shown promising results in medical tasks, their closed-source nature poses significant privacy and security concerns, hindering their widespread use in the medical field. Despite efforts to create open-source models, their limited parameters often result in insufficient multi-step <b>reasoning</b> capabilities required for solving complex medical problems. To address this, we introduce Meerkat-7B, a novel medical AI system with 7 billion parameters. Meerkat-7B was trained using our new synthetic dataset consisting of high-quality chain-of-thought <b>reasoning</b> paths sourced from 18 medical textbooks, along with diverse <b>instruction-following</b> <b>datasets.</b> Our system achieved remarkable accuracy across seven medical <b>benchmarks,</b> surpassing <b>GPT-3.5</b> by 13.1%, as well as outperforming the previous best 7B models such as MediTron-7B and BioMistral-7B by 13.4% and 9.8%, respectively. Notably, it surpassed the passing threshold of the United States Medical Licensing Examination (USMLE) for the first time for a 7B-parameter model. Additionally, our system offered more detailed free-form responses to clinical queries compared to existing 7B and 13B models, approaching the performance level of <b>GPT-3.5.</b> This significantly narrows the performance gap with <b>large</b> <b>LMs,</b> <b>showcasing</b> its effectiveness in addressing complex medical challenges.</p></p class="citation"></blockquote><h3 id=1142--11145-controllable-and-diverse-data-augmentation-with-large-language-model-for-low-resource-open-domain-dialogue-generation-zhenhua-liu-et-al-2024>(11/42 | 11/145) Controllable and Diverse Data Augmentation with Large Language Model for Low-Resource Open-Domain Dialogue Generation (Zhenhua Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenhua Liu, Tong Zhu, Jianxiang Xiang, Wenliang Chen. (2024)<br><strong>Controllable and Diverse Data Augmentation with Large Language Model for Low-Resource Open-Domain Dialogue Generation</strong><br><button class=copy-to-clipboard title="Controllable and Diverse Data Augmentation with Large Language Model for Low-Resource Open-Domain Dialogue Generation" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Clustering, Data Augmentation, Distribution Shift, Distribution Shift, Low-Resource, Open-Domain Dialogue, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00361v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00361v1.pdf filename=2404.00361v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Data</b> <b>augmentation</b> (DA) is crucial to mitigate model training instability and over-fitting problems in <b>low-resource</b> <b>open-domain</b> <b>dialogue</b> generation. However, traditional DA methods often neglect semantic <b>data</b> <b>diversity,</b> restricting the overall quality. Recently, <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> have been used for DA to generate diversified dialogues. However, they have limited controllability and tend to generate dialogues with a <b>distribution</b> <b>shift</b> compared to the seed dialogues. To maximize the augmentation diversity and address the controllability problem, we propose \textbf{S}ummary-based \textbf{D}ialogue \textbf{A}ugmentation with <b>LLM</b> (SDA). Our approach enhances the controllability of <b>LLM</b> by using dialogue summaries as a planning tool. Based on summaries, SDA can generate high-quality and diverse dialogue <b>data</b> <b>even</b> with a small seed dataset. To evaluate the efficacy of <b>data</b> <b>augmentation</b> methods for <b>open-domain</b> <b>dialogue,</b> we designed a <b>clustering-based</b> metric to characterize the semantic diversity of the augmented dialogue <b>data.</b> <b>The</b> experimental results show that SDA can augment high-quality and semantically diverse dialogues given a small seed dataset and an <b>LLM,</b> and the augmented <b>data</b> <b>can</b> boost the performance of <b>open-domain</b> <b>dialogue</b> models.</p></p class="citation"></blockquote><h3 id=1242--12145-secret-keepers-the-impact-of-llms-on-linguistic-markers-of-personal-traits-zhivar-sourati-et-al-2024>(12/42 | 12/145) Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits (Zhivar Sourati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhivar Sourati, Meltem Ozcan, Colin McDaniel, Alireza Ziabari, Nuan Wen, Ala Tak, Fred Morstatter, Morteza Dehghani. (2024)<br><strong>Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits</strong><br><button class=copy-to-clipboard title="Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: GPT-3, GPT-3.5, Gemini, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00267v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00267v2.pdf filename=2404.00267v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prior research has established associations between individuals&rsquo; language usage and their personal traits; our linguistic patterns reveal information about our personalities, emotional states, and beliefs. However, with the increasing adoption of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> as writing assistants in everyday writing, a critical question emerges: are authors&rsquo; linguistic patterns still predictive of their personal traits when <b>LLMs</b> are involved in the writing process? We investigate the impact of <b>LLMs</b> on the linguistic markers of demographic and psychological traits, specifically examining three <b>LLMs</b> - <b>GPT3.5,</b> <b>Llama</b> 2, and <b>Gemini</b> - across six different traits: gender, age, political affiliation, personality, empathy, and morality. Our findings indicate that although the use of <b>LLMs</b> slightly reduces the predictive power of linguistic patterns over authors&rsquo; personal traits, the significant changes are infrequent, and the use of <b>LLMs</b> does not fully diminish the predictive power of authors&rsquo; linguistic patterns over their personal traits. We also note that some theoretically established lexical-based linguistic markers lose their reliability as predictors when <b>LLMs</b> are used in the writing process. Our findings have important implications for the study of linguistic markers of personal traits in the age of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1342--13145-your-co-workers-matter-evaluating-collaborative-capabilities-of-language-models-in-blocks-world-guande-wu-et-al-2024>(13/42 | 13/145) Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World (Guande Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guande Wu, Chen Zhao, Claudio Silva, He He. (2024)<br><strong>Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World</strong><br><button class=copy-to-clipboard title="Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs.CL<br>Keyword Score: 60<br>Keywords: Grounding, Reasoning, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00246v1.pdf filename=2404.00246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language agents that interact with the world on their own have great potential for automating digital tasks. While <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> agents have made progress in understanding and executing tasks such as textual games and webpage control, many real-world tasks also require collaboration with humans or other <b>LLMs</b> in equal roles, which involves intent understanding, task coordination, and communication. To test <b>LLM&rsquo;s</b> ability to collaborate, we design a blocks-world environment, where two agents, each having unique goals and skills, build a target structure together. To complete the goals, they can act in the world and communicate in natural language. Under this environment, we design increasingly challenging settings to evaluate different collaboration perspectives, from independent to more complex, dependent tasks. We further adopt <b>chain-of-thought</b> <b>prompts</b> that include intermediate <b>reasoning</b> steps to model the partner&rsquo;s state and identify and correct execution errors. Both human-machine and machine-machine experiments show that <b>LLM</b> agents have strong <b>grounding</b> capacities, and our approach significantly improves the evaluation metric.</p></p class="citation"></blockquote><h3 id=1442--14145-trabsa-interpretable-sentiment-analysis-of-tweets-using-attention-based-bilstm-and-twitter-roberta-md-abrar-jahin-et-al-2024>(14/42 | 14/145) TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa (Md Abrar Jahin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Abrar Jahin, Md Sakib Hossain Shovon, M. F. Mridha. (2024)<br><strong>TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa</strong><br><button class=copy-to-clipboard title="TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, RoBERTa, Transformer, Sentiment Analysis, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00297v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00297v1.pdf filename=2404.00297v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Sentiment</b> <b>analysis</b> is crucial for understanding public opinion and consumer behavior. Existing models face challenges with linguistic diversity, generalizability, and explainability. We propose TRABSA, a hybrid framework integrating <b>transformer-based</b> architectures, attention mechanisms, and BiLSTM networks to address this. Leveraging <b>RoBERTa-trained</b> on 124M tweets, we bridge gaps in <b>sentiment</b> <b>analysis</b> <b>benchmarks,</b> ensuring state-of-the-art accuracy. Augmenting datasets with tweets from 32 countries and US states, we compare six <b>word-embedding</b> <b>techniques</b> and three lexicon-based labeling techniques, selecting the best for optimal <b>sentiment</b> <b>analysis.</b> TRABSA outperforms traditional ML and deep learning models with 94% accuracy and significant precision, recall, and F1-score gains. Evaluation across diverse datasets demonstrates consistent superiority and generalizability. SHAP and LIME analyses enhance interpretability, improving confidence in predictions. Our study facilitates pandemic resource management, aiding resource planning, policy formation, and vaccination tactics.</p></p class="citation"></blockquote><h3 id=1542--15145-classification-and-clustering-of-sentence-level-embeddings-of-scientific-articles-generated-by-contrastive-learning-gustavo-bartz-guedes-et-al-2024>(15/42 | 15/145) Classification and Clustering of Sentence-Level Embeddings of Scientific Articles Generated by Contrastive Learning (Gustavo Bartz Guedes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gustavo Bartz Guedes, Ana Estela Antunes da Silva. (2024)<br><strong>Classification and Clustering of Sentence-Level Embeddings of Scientific Articles Generated by Contrastive Learning</strong><br><button class=copy-to-clipboard title="Classification and Clustering of Sentence-Level Embeddings of Scientific Articles Generated by Contrastive Learning" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Clustering, Contrastive Learning, Fine-tuning, Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00224v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00224v1.pdf filename=2404.00224v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scientific articles are long text documents organized into sections, each describing aspects of the research. Analyzing scientific production has become progressively challenging due to the increase in the number of available articles. Within this scenario, our approach consisted of <b>fine-tuning</b> <b>transformer</b> language models to generate sentence-level embeddings from scientific articles, considering the following labels: background, objective, methods, results, and conclusion. We trained our models on three datasets with <b>contrastive</b> <b>learning.</b> Two datasets are from the article&rsquo;s abstracts in the computer science and medical domains. Also, we introduce PMC-Sents-FULL, a novel dataset of sentences extracted from the full texts of medical articles. We compare the <b>fine-tuned</b> and baseline models in <b>clustering</b> and classification tasks to evaluate our approach. On average, <b>clustering</b> agreement measures values were five times higher. For the classification measures, in the best-case scenario, we had an average improvement in F1-micro of 30.73%. Results show that <b>fine-tuning</b> sentence <b>transformers</b> with <b>contrastive</b> <b>learning</b> and using the generated embeddings in downstream tasks is a feasible approach to sentence classification in scientific articles. Our experiment codes are available on GitHub.</p></p class="citation"></blockquote><h3 id=1642--16145-multi-conditional-ranking-with-large-language-models-pouya-pezeshkpour-et-al-2024>(16/42 | 16/145) Multi-Conditional Ranking with Large Language Models (Pouya Pezeshkpour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pouya Pezeshkpour, Estevam Hruschka. (2024)<br><strong>Multi-Conditional Ranking with Large Language Models</strong><br><button class=copy-to-clipboard title="Multi-Conditional Ranking with Large Language Models" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Recommendation, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00211v1.pdf filename=2404.00211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Utilizing <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to rank a set of items has become a common approach in <b>recommendation</b> and retrieval systems. Typically, these systems focus on ordering a substantial number of documents in a monotonic order based on a given query. However, real-world scenarios often present a different challenge: ranking a comparatively smaller set of items, but according to a variety of diverse and occasionally conflicting conditions. In this paper, we define and explore the task of multi-conditional ranking by introducing MCRank, a <b>benchmark</b> tailored for assessing multi-conditional ranking across various item types and conditions. Our analysis of <b>LLMs</b> using MCRank indicates a significant decrease in performance as the number and complexity of items and conditions grow. To overcome this limitation, we propose a novel decomposed <b>reasoning</b> method, consisting of EXtracting and Sorting the conditions, and then Iterativly Ranking the items (EXSIR). Our extensive experiments show that this decomposed <b>reasoning</b> method enhances <b>LLMs&rsquo;</b> performance significantly, achieving up to a 12% improvement over existing <b>LLMs.</b> We also provide a detailed analysis of <b>LLMs</b> performance across various condition categories, and examine the effectiveness of decomposition step. Furthermore, we compare our method with existing approaches such as Chain-of-Thought and an encoder-type ranking model, demonstrating the superiority of our approach and complexity of MCR task. We released our dataset and code.</p></p class="citation"></blockquote><h3 id=1742--17145-configurable-safety-tuning-of-language-models-with-synthetic-preference-data-victor-gallego-2024>(17/42 | 17/145) Configurable Safety Tuning of Language Models with Synthetic Preference Data (Victor Gallego, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor Gallego. (2024)<br><strong>Configurable Safety Tuning of Language Models with Synthetic Preference Data</strong><br><button class=copy-to-clipboard title="Configurable Safety Tuning of Language Models with Synthetic Preference Data" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Direct Preference Optimization, Fine-tuning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00495v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00495v1.pdf filename=2404.00495v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>State-of-the-art language model <b>fine-tuning</b> techniques, such as <b>Direct</b> <b>Preference</b> <b>Optimization</b> (DPO), restrict user control by hard-coding predefined behaviors into the model. To address this, we propose a novel method, Configurable Safety Tuning (CST), that augments DPO using synthetic preference data to facilitate flexible safety configuration of <b>LLMs</b> at inference time. CST overcomes the constraints of vanilla DPO by introducing a system <b>prompt</b> specifying safety configurations, enabling <b>LLM</b> deployers to disable/enable safety preferences based on their need, just changing the system <b>prompt.</b> Our experimental evaluations indicate that CST successfully manages different safety configurations and retains the original functionality of <b>LLMs,</b> showing it is a robust method for configurable deployment. Data and models available at <a href=https://github.com/vicgalle/configurable-safety-tuning>https://github.com/vicgalle/configurable-safety-tuning</a></p></p class="citation"></blockquote><h3 id=1842--18145-planning-and-editing-what-you-retrieve-for-enhanced-tool-learning-tenghao-huang-et-al-2024>(18/42 | 18/145) Planning and Editing What You Retrieve for Enhanced Tool Learning (Tenghao Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tenghao Huang, Dongwon Jung, Muhao Chen. (2024)<br><strong>Planning and Editing What You Retrieve for Enhanced Tool Learning</strong><br><button class=copy-to-clipboard title="Planning and Editing What You Retrieve for Enhanced Tool Learning" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00450v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00450v2.pdf filename=2404.00450v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in integrating external tools with <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have opened new frontiers, with applications in <b>mathematical</b> <b>reasoning,</b> code generators, and smart assistants. However, existing methods, relying on simple one-time retrieval strategies, fall short on effectively and accurately shortlisting relevant tools. This paper introduces a novel PLUTO (Planning, Learning, and Understanding for TOols) approach, encompassing <code>Plan-and-Retrieve (P&amp;R)</code> and <code>Edit-and-Ground (E&amp;G)</code> paradigms. The P&amp;R paradigm consists of a neural retrieval module for shortlisting relevant tools and an <b>LLM-based</b> query planner that decomposes complex queries into actionable tasks, enhancing the effectiveness of tool utilization. The E&amp;G paradigm utilizes <b>LLMs</b> to enrich tool descriptions based on user scenarios, bridging the gap between user queries and tool functionalities. Experiment results demonstrate that these paradigms significantly improve the recall and NDCG in tool retrieval tasks, significantly surpassing current state-of-the-art models.</p></p class="citation"></blockquote><h3 id=1942--19145-aurora-m-the-first-open-source-multilingual-language-model-red-teamed-according-to-the-us-executive-order-taishi-nakamura-et-al-2024>(19/42 | 19/145) Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order (Taishi Nakamura et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taishi Nakamura, Mayank Mishra, Simone Tedeschi, Yekun Chai, Jason T Stillerman, Felix Friedrich, Prateek Yadav, Tanmay Laud, Vu Minh Chien, Terry Yue Zhuo, Diganta Misra, Ben Bogin, Xuan-Son Vu, Marzena Karpinska, Arnav Varma Dantuluri, Wojciech Kusa, Tommaso Furlanello, Rio Yokota, Niklas Muennighoff, Suhas Pai, Tosin Adewumi, Veronika Laippala, Xiaozhe Yao, Adalberto Junior, Alpay Ariyak, Aleksandr Drozd, Jordan Clive, Kshitij Gupta, Liangyu Chen, Qi Sun, Ken Tsui, Noah Persaud, Nour Fahmy, Tianlong Chen, Mohit Bansal, Nicolo Monti, Tai Dang, Ziyang Luo, Tien-Tung Bui, Roberto Navigli, Virendra Mehta, Matthew Blumberg, Victor May, Huu Nguyen, Sampo Pyysalo. (2024)<br><strong>Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order</strong><br><button class=copy-to-clipboard title="Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, BLOOM, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00399v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00399v1.pdf filename=2404.00399v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pretrained</b> <b>language</b> <b>models</b> underpin several AI applications, but their high computational cost for training limits accessibility. Initiatives such as <b>BLOOM</b> and StarCoder aim to democratize access to <b>pretrained</b> <b>models</b> <b>for</b> collaborative community development. However, such existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting, whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually <b>pretrained</b> <b>from</b> <b>StarCoderPlus</b> on 435 billion additional tokens, Aurora-M surpasses 2 trillion tokens in total training token count. It is the first open-source multilingual model <b>fine-tuned</b> on human-reviewed safety instructions, thus aligning its development not only with conventional red-teaming considerations, but also with the specific concerns articulated in the Biden-Harris Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. Aurora-M is rigorously evaluated across various tasks and languages, demonstrating robustness against catastrophic forgetting and outperforming alternatives in multilingual settings, particularly in safety evaluations. To promote responsible open-source <b>LLM</b> development, Aurora-M and its variants are released at <a href=https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407>https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407</a> .</p></p class="citation"></blockquote><h3 id=2042--20145-augmenting-ner-datasets-with-llms-towards-automated-and-refined-annotation-yuji-naraki-et-al-2024>(20/42 | 20/145) Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation (Yuji Naraki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuji Naraki, Ryosuke Yamaki, Yoshikazu Ikeda, Takafumi Horie, Hiroki Naganuma. (2024)<br><strong>Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation</strong><br><button class=copy-to-clipboard title="Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Named Entity Recognition, Named Entity Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01334v1.pdf filename=2404.01334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of Natural Language Processing (NLP), <b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER)</b> is recognized as a critical technology, employed across a wide array of applications. Traditional methodologies for annotating datasets for <b>NER</b> models are challenged by high costs and variations in dataset quality. This research introduces a novel hybrid annotation approach that synergizes human effort with the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> This approach not only aims to ameliorate the noise inherent in manual annotations, such as omissions, thereby enhancing the performance of <b>NER</b> models, but also achieves this in a cost-effective manner. Additionally, by employing a label mixing strategy, it addresses the issue of class imbalance encountered in <b>LLM-based</b> annotations. Through an analysis across multiple datasets, this method has been consistently shown to provide superior performance compared to traditional annotation methods, even under constrained budget conditions. This study illuminates the potential of leveraging <b>LLMs</b> to improve dataset quality, introduces a novel technique to mitigate class imbalances, and demonstrates the feasibility of achieving high-performance <b>NER</b> in a cost-effective way.</p></p class="citation"></blockquote><h3 id=2142--21145-deft-flash-tree-attention-with-io-awareness-for-efficient-tree-search-based-llm-inference-jinwei-yao-et-al-2024>(21/42 | 21/145) DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference (Jinwei Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, Tao Lin. (2024)<br><strong>DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference</strong><br><button class=copy-to-clipboard title="DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Transformer, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00242v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00242v1.pdf filename=2404.00242v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decoding using tree search can greatly enhance the inference quality for <b>transformer-based</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Depending on the guidance signal, it searches for the best path from root to leaf in the tree by forming <b>LLM</b> outputs to improve controllability, <b>reasoning</b> ability, alignment, et cetera. However, current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference. To address this issue, we propose DeFT, an IO-aware tree attention algorithm that maintains memory-efficient attention calculation with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction of memory reads/writes for the KV cache between GPU global memory and on-chip shared memory as much as possible; (2) Attention Calculation: we calculate partial attention of each QKV groups in a fused kernel then apply a Tree-topology-aware Global Reduction strategy to get final attention. Thanks to a reduction in KV cache IO by 3.6-4.5$\times$, along with an additional reduction in IO for $\mathbf{Q} \mathbf{K}^\top$ and Softmax equivalent to 25% of the total KV cache IO, DeFT can achieve a speedup of 1.7-2.4$\times$ in end-to-end latency across two practical <b>reasoning</b> tasks over the SOTA attention algorithms.</p></p class="citation"></blockquote><h3 id=2242--22145-unimeec-towards-unified-multimodal-emotion-recognition-and-emotion-cause-guimin-hu-et-al-2024>(22/42 | 22/145) UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion Cause (Guimin Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guimin Hu, Zhihong Zhu, Daniel Hershcovich, Hasti Seifi, Jiayuan Xie. (2024)<br><strong>UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion Cause</strong><br><button class=copy-to-clipboard title="UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion Cause" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Emotion Recognition, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00403v1.pdf filename=2404.00403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>emotion</b> <b>recognition</b> in conversation (MERC) and <b>multimodal</b> <b>emotion-cause</b> <b>pair</b> extraction (MECPE) has recently garnered significant attention. <b>Emotions</b> <b>are</b> the expression of affect or feelings; responses to specific events, thoughts, or situations are known as <b>emotion</b> <b>causes.</b> Both are like two sides of a coin, collectively describing human behaviors and intents. However, most existing works treat MERC and MECPE as separate tasks, which may result in potential challenges in integrating <b>emotion</b> <b>and</b> cause in real-world applications. In this paper, we propose a Unified <b>Multimodal</b> <b>Emotion</b> <b>recognition</b> and <b>Emotion-Cause</b> <b>analysis</b> framework (UniMEEC) to explore the causality and complementarity between <b>emotion</b> <b>and</b> <b>emotion</b> <b>cause.</b> Concretely, UniMEEC reformulates the MERC and MECPE tasks as two mask prediction problems, enhancing the interaction between <b>emotion</b> <b>and</b> cause. Meanwhile, UniMEEC shares the <b>prompt</b> <b>learning</b> among modalities for probing modality-specific knowledge from the Pre-trained model. Furthermore, we propose a task-specific hierarchical context aggregation to control the information flow to the task. Experiment results on four public <b>benchmark</b> datasets verify the model performance on MERC and MECPE tasks and achieve consistent improvements compared with state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=2342--23145-multi-hop-question-answering-under-temporal-knowledge-editing-keyuan-cheng-et-al-2024>(23/42 | 23/145) Multi-hop Question Answering under Temporal Knowledge Editing (Keyuan Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keyuan Cheng, Gang Lin, Haoyang Fei, Yuxuan zhai, Lu Yu, Muhammad Asif Ali, Lijie Hu, Di Wang. (2024)<br><strong>Multi-hop Question Answering under Temporal Knowledge Editing</strong><br><button class=copy-to-clipboard title="Multi-hop Question Answering under Temporal Knowledge Editing" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 36<br>Keywords: Graph, Benchmarking, Question Answering, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00492v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00492v1.pdf filename=2404.00492v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-hop <b>question</b> <b>answering</b> (MQA) under knowledge editing (KE) has garnered significant attention in the era of <b>large</b> <b>language</b> <b>models.</b> However, existing models for MQA under KE exhibit poor performance when dealing with <b>questions</b> <b>containing</b> explicit temporal contexts. To address this limitation, we propose a novel framework, namely TEMPoral knowLEdge augmented Multi-hop <b>Question</b> <b>Answering</b> (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a time-aware <b>graph</b> (TAG) to store edit knowledge in a structured manner. Then, through our proposed inference path, structural retrieval, and joint <b>reasoning</b> stages, TEMPLE-MQA effectively discerns temporal contexts within the <b>question</b> <b>query.</b> Experiments on <b>benchmark</b> datasets demonstrate that TEMPLE-MQA significantly outperforms baseline models. Additionally, we contribute a new dataset, namely TKEMQA, which serves as the inaugural <b>benchmark</b> tailored specifically for MQA with temporal scopes.</p></p class="citation"></blockquote><h3 id=2442--24145-prompt-saw-leveraging-relation-aware-graphs-for-textual-prompt-compression-muhammad-asif-ali-et-al-2024>(24/42 | 24/145) PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression (Muhammad Asif Ali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang. (2024)<br><strong>PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression</strong><br><button class=copy-to-clipboard title="PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 36<br>Keywords: Graph, Benchmarking, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00489v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00489v1.pdf filename=2404.00489v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown exceptional abilities for multiple different natural language processing tasks. While <b>prompting</b> is a crucial tool for <b>LLM</b> inference, we observe that there is a significant cost associated with exceedingly lengthy <b>prompts.</b> Existing attempts to compress lengthy <b>prompts</b> lead to sub-standard results in terms of readability and interpretability of the compressed <b>prompt,</b> with a detrimental impact on <b>prompt</b> utility. To address this, we propose <b>PROMPT-SAW:</b> <b>Prompt</b> compresSion via Relation AWare <b>graphs,</b> an effective strategy for <b>prompt</b> compression over task-agnostic and task-aware <b>prompts.</b> <b>PROMPT-SAW</b> uses the <b>prompt&rsquo;s</b> textual information to build a <b>graph,</b> later extracts key information elements in the <b>graph</b> to come up with the compressed <b>prompt.</b> We also propose GSM8K-AUG, i.e., an extended version of the existing GSM8k <b>benchmark</b> for task-agnostic <b>prompts</b> in order to provide a comprehensive evaluation platform. Experimental evaluation using <b>benchmark</b> datasets shows that <b>prompts</b> compressed by <b>PROMPT-SAW</b> are not only better in terms of readability, but they also outperform the best-performing baseline models by up to 14.3 and 13.7 respectively for task-aware and task-agnostic settings while compressing the original <b>prompt</b> text by 33.0 and 56.7.</p></p class="citation"></blockquote><h3 id=2542--25145-finefake-a-knowledge-enriched-dataset-for-fine-grained-multi-domain-fake-news-detecction-ziyi-zhou-et-al-2024>(25/42 | 25/145) FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain Fake News Detecction (Ziyi Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyi Zhou, Xiaoming Zhang, Litian Zhang, Jiacheng Liu, Xi Zhang, Chaozhuo Li. (2024)<br><strong>FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain Fake News Detecction</strong><br><button class=copy-to-clipboard title="FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain Fake News Detecction" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-MM, cs.CL<br>Keyword Score: 36<br>Keywords: Benchmarking, Multi-modal, Fake News Detection, Domain Adaptation, Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01336v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01336v1.pdf filename=2404.01336v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>benchmarks</b> for <b>fake</b> <b>news</b> <b>detection</b> have significantly contributed to the advancement of models in assessing the authenticity of news content. However, these <b>benchmarks</b> typically focus solely on news pertaining to a single semantic topic or originating from a single platform, thereby failing to capture the diversity of multi-domain news in real scenarios. In order to understand <b>fake</b> <b>news</b> <b>across</b> various <b>domains,</b> <b>the</b> external knowledge and fine-grained annotations are indispensable to provide precise evidence and uncover the diverse underlying strategies for fabrication, which are also ignored by existing <b>benchmarks.</b> To address this gap, we introduce a novel multi-domain knowledge-enhanced <b>benchmark</b> with fine-grained annotations, named \textbf{FineFake}. FineFake encompasses 16,909 data samples spanning six semantic topics and eight platforms. Each news item is enriched with <b>multi-modal</b> content, potential social context, semi-manually verified common knowledge, and fine-grained annotations that surpass conventional binary labels. Furthermore, we formulate three challenging tasks based on FineFake and propose a knowledge-enhanced <b>domain</b> <b>adaptation</b> network. Extensive experiments are conducted on FineFake under various scenarios, providing accurate and reliable <b>benchmarks</b> for future endeavors. The entire FineFake project is publicly accessible as an open-source repository at \url{https://github.com/Accuser907/FineFake}.</p></p class="citation"></blockquote><h3 id=2642--26145-numerologic-number-encoding-for-enhanced-llms-numerical-reasoning-eli-schwartz-et-al-2024>(26/42 | 26/145) NumeroLogic: Number Encoding for Enhanced LLMs&rsquo; Numerical Reasoning (Eli Schwartz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eli Schwartz, Leshem Choshen, Joseph Shtok, Sivan Doveh, Leonid Karlinsky, Assaf Arbelle. (2024)<br><strong>NumeroLogic: Number Encoding for Enhanced LLMs&rsquo; Numerical Reasoning</strong><br><button class=copy-to-clipboard title="NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Massive Multitask Language Understanding (MMLU), Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00459v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00459v1.pdf filename=2404.00459v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of &ldquo;42&rdquo;, we suggest using &ldquo;{2:42}&rdquo; as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the <b>reasoning</b> process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstrate NumeroLogic applicability to general natural language modeling, improving language understanding performance in the <b>MMLU</b> <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=2742--27145-how-robust-are-the-tabular-qa-models-for-scientific-tables-a-study-using-customized-dataset-akash-ghosh-et-al-2024>(27/42 | 27/145) How Robust are the Tabular QA Models for Scientific Tables? A Study using Customized Dataset (Akash Ghosh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akash Ghosh, B Venkata Sahith, Niloy Ganguly, Pawan Goyal, Mayank Singh. (2024)<br><strong>How Robust are the Tabular QA Models for Scientific Tables? A Study using Customized Dataset</strong><br><button class=copy-to-clipboard title="How Robust are the Tabular QA Models for Scientific Tables? A Study using Customized Dataset" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Question Answering, Question Answering, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00401v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00401v1.pdf filename=2404.00401v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Question-answering</b> <b>(QA)</b> on hybrid scientific tabular and textual data deals with scientific information, and relies on complex numerical <b>reasoning.</b> In recent years, while tabular <b>QA</b> has seen rapid progress, understanding their robustness on scientific information is lacking due to absence of any <b>benchmark</b> dataset. To investigate the robustness of the existing state-of-the-art <b>QA</b> models on scientific hybrid tabular data, we propose a new dataset, &ldquo;SciTabQA&rdquo;, consisting of 822 <b>question-answer</b> <b>pairs</b> from scientific tables and their descriptions. With the help of this dataset, we assess the state-of-the-art Tabular <b>QA</b> models based on their ability (i) to use heterogeneous information requiring both structured data (table) and unstructured data (text) and (ii) to perform complex scientific <b>reasoning</b> tasks. In essence, we check the capability of the models to interpret scientific tables and text. Our experiments show that &ldquo;SciTabQA&rdquo; is an innovative dataset to study <b>question-answering</b> <b>over</b> scientific heterogeneous data. We <b>benchmark</b> three state-of-the-art Tabular <b>QA</b> models, and find that the best F1 score is only 0.462.</p></p class="citation"></blockquote><h3 id=2842--28145-addressing-both-statistical-and-causal-gender-fairness-in-nlp-models-hannah-chen-et-al-2024>(28/42 | 28/145) Addressing Both Statistical and Causal Gender Fairness in NLP Models (Hannah Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hannah Chen, Yangfeng Ji, David Evans. (2024)<br><strong>Addressing Both Statistical and Causal Gender Fairness in NLP Models</strong><br><button class=copy-to-clipboard title="Addressing Both Statistical and Causal Gender Fairness in NLP Models" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Counter-factual, Data Augmentation, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00463v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00463v1.pdf filename=2404.00463v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Statistical <b>fairness</b> stipulates equivalent outcomes for every protected group, whereas causal <b>fairness</b> prescribes that a model makes the same prediction for an individual regardless of their protected characteristics. <b>Counterfactual</b> <b>data</b> <b>augmentation</b> (CDA) is effective for reducing bias in NLP models, yet models trained with CDA are often evaluated only on metrics that are closely tied to the causal <b>fairness</b> notion; similarly, sampling-based methods designed to promote statistical <b>fairness</b> are rarely evaluated for causal <b>fairness.</b> In this work, we evaluate both statistical and causal debiasing methods for gender bias in NLP models, and find that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics. We demonstrate that combinations of statistical and causal debiasing techniques are able to reduce bias measured through both types of metrics.</p></p class="citation"></blockquote><h3 id=2942--29145-automatic-detection-of-relevant-information-predictions-and-forecasts-in-financial-news-through-topic-modelling-with-latent-dirichlet-allocation-silvia-garcía-méndez-et-al-2024>(29/42 | 29/145) Automatic detection of relevant information, predictions and forecasts in financial news through topic modelling with Latent Dirichlet Allocation (Silvia García-Méndez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Silvia García-Méndez, Francisco de Arriba-Pérez, Ana Barros-Vila, Francisco J. González-Castaño, Enrique Costa-Montenegro. (2024)<br><strong>Automatic detection of relevant information, predictions and forecasts in financial news through topic modelling with Latent Dirichlet Allocation</strong><br><button class=copy-to-clipboard title="Automatic detection of relevant information, predictions and forecasts in financial news through topic modelling with Latent Dirichlet Allocation" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CE, cs-CL, cs-IR, cs-LG, cs.CL, q-fin-ST<br>Keyword Score: 30<br>Keywords: Topic Segmentation, Rouge, Rouge-L<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01338v1.pdf filename=2404.01338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Financial news items are unstructured sources of information that can be mined to extract knowledge for market screening applications. Manual extraction of relevant information from the continuous stream of finance-related news is cumbersome and beyond the skills of many investors, who, at most, can follow a few sources and authors. Accordingly, we focus on the analysis of financial news to identify relevant text and, within that text, forecasts and predictions. We propose a novel Natural Language Processing (NLP) system to assist investors in the detection of relevant financial events in unstructured textual sources by considering both relevance and temporality at the discursive level. Firstly, we segment the text to group together closely related text. Secondly, we apply co-reference resolution to discover internal dependencies within segments. Finally, we perform relevant <b>topic</b> <b>modelling</b> with Latent Dirichlet Allocation (LDA) to separate relevant from less relevant text and then analyse the relevant text using a Machine Learning-oriented temporal approach to identify predictions and speculative statements. We created an experimental data set composed of 2,158 financial news items that were manually labelled by NLP researchers to evaluate our solution. The <b>ROUGE-L</b> values for the identification of relevant text and predictions/forecasts were 0.662 and 0.982, respectively. To our knowledge, this is the first work to jointly consider relevance and temporality at the discursive level. It contributes to the transfer of human associative discourse capabilities to expert systems through the combination of multi-paragraph <b>topic</b> <b>segmentation</b> and co-reference resolution to separate author expression patterns, <b>topic</b> <b>modelling</b> with LDA to detect relevant text, and discursive temporality analysis to identify forecasts and predictions within this text.</p></p class="citation"></blockquote><h3 id=3042--30145-an-analysis-of-bpe-vocabulary-trimming-in-neural-machine-translation-marco-cognetta-et-al-2024>(30/42 | 30/145) An Analysis of BPE Vocabulary Trimming in Neural Machine Translation (Marco Cognetta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Cognetta, Tatsuya Hiraoka, Naoaki Okazaki, Rico Sennrich, Yuval Pinter. (2024)<br><strong>An Analysis of BPE Vocabulary Trimming in Neural Machine Translation</strong><br><button class=copy-to-clipboard title="An Analysis of BPE Vocabulary Trimming in Neural Machine Translation" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Neural Machine Translation, Neural Machine Translation, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00397v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00397v1.pdf filename=2404.00397v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explore threshold vocabulary trimming in Byte-Pair Encoding subword <b>tokenization,</b> a postprocessing step that replaces rare subwords with their component subwords. The technique is available in popular <b>tokenization</b> libraries but has not been subjected to rigorous scientific scrutiny. While the removal of rare subwords is suggested as best practice in <b>machine</b> <b>translation</b> implementations, both as a means to reduce model size and for improving model performance through robustness, our experiments indicate that, across a large space of hyperparameter settings, vocabulary trimming fails to improve performance, and is even prone to incurring heavy degradation.</p></p class="citation"></blockquote><h3 id=3142--31145-jetsons-at-finnlp-2024-towards-understanding-the-esg-impact-of-a-news-article-using-transformer-based-models-parag-pravin-dakle-et-al-2024>(31/42 | 31/145) Jetsons at FinNLP 2024: Towards Understanding the ESG Impact of a News Article using Transformer-based Models (Parag Pravin Dakle et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Parag Pravin Dakle, Alolika Gon, Sihan Zha, Liang Wang, SaiKrishna Rallabandi, Preethi Raghavan. (2024)<br><strong>Jetsons at FinNLP 2024: Towards Understanding the ESG Impact of a News Article using Transformer-based Models</strong><br><button class=copy-to-clipboard title="Jetsons at FinNLP 2024: Towards Understanding the ESG Impact of a News Article using Transformer-based Models" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00386v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00386v1.pdf filename=2404.00386v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we describe the different approaches explored by the Jetsons team for the Multi-Lingual ESG Impact Duration Inference (ML-ESG-3) shared task. The shared task focuses on predicting the duration and type of the ESG impact of a news article. The shared task dataset consists of 2,059 news titles and articles in English, French, Korean, and Japanese languages. For the impact duration classification task, we <b>fine-tuned</b> XLM-RoBERTa with a custom <b>fine-tuning</b> strategy and using self-training and DeBERTa-v3 using only English translations. These models individually ranked first on the leaderboard for Korean and Japanese and in an ensemble for the English language, respectively. For the impact type classification task, our XLM-RoBERTa model <b>fine-tuned</b> using a custom <b>fine-tuning</b> strategy ranked first for the English language.</p></p class="citation"></blockquote><h3 id=3242--32145-rationale-based-opinion-summarization-haoyuan-li-et-al-2024>(32/42 | 32/145) Rationale-based Opinion Summarization (Haoyuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyuan Li, Snigdha Chaturvedi. (2024)<br><strong>Rationale-based Opinion Summarization</strong><br><button class=copy-to-clipboard title="Rationale-based Opinion Summarization" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Unsupervised Learning, Opinion Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00217v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00217v1.pdf filename=2404.00217v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Opinion</b> <b>summarization</b> aims to generate concise summaries that present popular <b>opinions</b> <b>of</b> a large group of reviews. However, these summaries can be too generic and lack supporting details. To address these issues, we propose a new paradigm for summarizing reviews, rationale-based <b>opinion</b> <b>summarization.</b> Rationale-based <b>opinion</b> <b>summaries</b> output the representative <b>opinions</b> <b>as</b> well as one or more corresponding rationales. To extract good rationales, we define four desirable properties: relatedness, specificity, popularity, and diversity and present a Gibbs-sampling-based method to extract rationales. Overall, we propose RATION, an <b>unsupervised</b> extractive system that has two components: an <b>Opinion</b> <b>Extractor</b> (to extract representative <b>opinions)</b> <b>and</b> Rationales Extractor (to extract corresponding rationales). We conduct automatic and human evaluations to show that rationales extracted by RATION have the proposed properties and its summaries are more useful than conventional summaries. The implementation of our work is available at <a href=https://github.com/leehaoyuan/RATION>https://github.com/leehaoyuan/RATION</a>.</p></p class="citation"></blockquote><h3 id=3342--33145-cross-lingual-named-entity-corpus-for-slavic-languages-jakub-piskorski-et-al-2024>(33/42 | 33/145) Cross-lingual Named Entity Corpus for Slavic Languages (Jakub Piskorski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jakub Piskorski, Michał Marcińczuk, Roman Yangarber. (2024)<br><strong>Cross-lingual Named Entity Corpus for Slavic Languages</strong><br><button class=copy-to-clipboard title="Cross-lingual Named Entity Corpus for Slavic Languages" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Transformer, Lemmatization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00482v1.pdf filename=2404.00482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a corpus manually annotated with named entities for six Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017-2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5 017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits - single topic out and cross topics. For each split, we set <b>benchmarks</b> using a <b>transformer-based</b> neural network architecture with the pre-trained multilingual models - XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity <b>lemmatization</b> and linking.</p></p class="citation"></blockquote><h3 id=3442--34145-is-factuality-decoding-a-free-lunch-for-llms-evaluation-on-knowledge-editing-benchmark-baolong-bi-et-al-2024>(34/42 | 34/145) Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark (Baolong Bi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Xueqi Cheng. (2024)<br><strong>Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark</strong><br><button class=copy-to-clipboard title="Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00216v1.pdf filename=2404.00216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid development of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> enables them to convey factual knowledge in a more human-like fashion. Extensive efforts have been made to reduce factual hallucinations by modifying <b>LLMs</b> with factuality decoding. However, they also pose risks of hindering knowledge updates, as they make models overly confident in known facts. In this work, we first revisite the current factuality decoding methods and verified their effectiveness in enhancing factual accuracy. Subsequently, we conduct further evaluation of several strong factuality decoding methods on the knowledge editing <b>benchmark.</b> All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3%. This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of preserving the flexibility for knowledge editing. Therefore, our work suggests that research into factual alignment should simultaneously focus on the effectiveness of knowledge editing.</p></p class="citation"></blockquote><h3 id=3542--35145-conceptual-and-unbiased-reasoning-in-language-models-ben-zhou-et-al-2024>(35/42 | 35/145) Conceptual and Unbiased Reasoning in Language Models (Ben Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ben Zhou, Hongming Zhang, Sihao Chen, Dian Yu, Hongwei Wang, Baolin Peng, Dan Roth, Dong Yu. (2024)<br><strong>Conceptual and Unbiased Reasoning in Language Models</strong><br><button class=copy-to-clipboard title="Conceptual and Unbiased Reasoning in Language Models" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00205v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00205v1.pdf filename=2404.00205v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conceptual <b>reasoning,</b> the ability to reason in abstract and high-level perspectives, is key to generalization in human cognition. However, limited study has been done on <b>large</b> <b>language</b> <b>models&rsquo;</b> capability to perform conceptual <b>reasoning.</b> In this work, we bridge this gap and propose a novel conceptualization framework that forces models to perform conceptual <b>reasoning</b> on abstract questions and generate solutions in a verifiable symbolic space. Using this framework as an analytical tool, we show that existing <b>large</b> <b>language</b> <b>models</b> fall short on conceptual <b>reasoning,</b> dropping 9% to 28% on various <b>benchmarks</b> compared to direct inference methods. We then discuss how models can improve since high-level abstract <b>reasoning</b> is key to unbiased and generalizable decision-making. We propose two techniques to add trustworthy induction signals by generating familiar questions with similar underlying <b>reasoning</b> paths and asking models to perform self-refinement. Experiments show that our proposed techniques improve models&rsquo; conceptual <b>reasoning</b> performance by 8% to 11%, achieving a more robust <b>reasoning</b> system that relies less on inductive biases.</p></p class="citation"></blockquote><h3 id=3642--36145-noise-aware-training-of-layout-aware-language-models-ritesh-sarkhel-et-al-2024>(36/42 | 36/145) Noise-Aware Training of Layout-Aware Language Models (Ritesh Sarkhel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ritesh Sarkhel, Xiaoqi Ren, Lauro Beltrao Costa, Guolong Su, Vincent Perot, Yanan Xie, Emmanouil Koukoumidis, Arnab Nandi. (2024)<br><strong>Noise-Aware Training of Layout-Aware Language Models</strong><br><button class=copy-to-clipboard title="Noise-Aware Training of Layout-Aware Language Models" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00488v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00488v1.pdf filename=2404.00488v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A visually rich document (VRD) utilizes visual features along with linguistic cues to disseminate information. Training a custom extractor that identifies named entities from a document requires a large number of instances of the target document type annotated at textual and visual modalities. This is an expensive bottleneck in enterprise scenarios, where we want to train custom extractors for thousands of different document types in a scalable way. Pre-training an extractor model on unlabeled instances of the target document type, followed by a <b>fine-tuning</b> step on human-labeled instances does not work in these scenarios, as it surpasses the maximum allowable training time allocated for the extractor. We address this scenario by proposing a Noise-Aware Training method or NAT in this paper. Instead of acquiring expensive human-labeled documents, NAT utilizes weakly labeled documents to train an extractor in a scalable way. To avoid degradation in the model&rsquo;s quality due to noisy, weakly labeled samples, NAT estimates the confidence of each training sample and incorporates it as uncertainty measure during training. We train multiple state-of-the-art extractor models using NAT. Experiments on a number of publicly available and in-house datasets show that NAT-trained models are not only robust in performance &ndash; it outperforms a <b>transfer-learning</b> <b>baseline</b> by up to 6% in terms of macro-F1 score, but it is also more label-efficient &ndash; it reduces the amount of human-effort required to obtain comparable performance by up to 73%.</p></p class="citation"></blockquote><h3 id=3742--37145-the-shape-of-word-embeddings-recognizing-language-phylogenies-through-topological-data-analysis-ondřej-draganov-et-al-2024>(37/42 | 37/145) The Shape of Word Embeddings: Recognizing Language Phylogenies through Topological Data Analysis (Ondřej Draganov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ondřej Draganov, Steven Skiena. (2024)<br><strong>The Shape of Word Embeddings: Recognizing Language Phylogenies through Topological Data Analysis</strong><br><button class=copy-to-clipboard title="The Shape of Word Embeddings: Recognizing Language Phylogenies through Topological Data Analysis" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL, math-AT<br>Keyword Score: 10<br>Keywords: Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00500v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00500v1.pdf filename=2404.00500v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Word</b> <b>embeddings</b> represent language vocabularies as clouds of $d$-dimensional points. We investigate how information is conveyed by the general shape of these clouds, outside of representing the semantic meaning of each token. Specifically, we use the notion of persistent homology from topological data analysis (TDA) to measure the distances between language pairs from the shape of their unlabeled embeddings. We use these distance matrices to construct language phylogenetic trees over 81 Indo-European languages. Careful evaluation shows that our reconstructed trees exhibit strong similarities to the reference tree.</p></p class="citation"></blockquote><h3 id=3842--38145-docmaster-a-unified-platform-for-annotation-training--inference-in-document-question-answering-alex-nguyen-et-al-2024>(38/42 | 38/145) DOCMASTER: A Unified Platform for Annotation, Training, & Inference in Document Question-Answering (Alex Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alex Nguyen, Zilong Wang, Jingbo Shang, Dheeraj Mekala. (2024)<br><strong>DOCMASTER: A Unified Platform for Annotation, Training, & Inference in Document Question-Answering</strong><br><button class=copy-to-clipboard title="DOCMASTER: A Unified Platform for Annotation, Training, & Inference in Document Question-Answering" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00439v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00439v1.pdf filename=2404.00439v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The application of natural language processing models to PDF documents is pivotal for various business applications yet the challenge of training models for this purpose persists in businesses due to specific hurdles. These include the complexity of working with PDF formats that necessitate parsing text and layout information for curating training data and the lack of privacy-preserving annotation tools. This paper introduces DOCMASTER, a unified platform designed for annotating PDF documents, model training, and inference, tailored to document <b>question-answering.</b> <b>The</b> annotation interface enables users to input <b>questions</b> <b>and</b> highlight text spans within the PDF file as answers, saving layout information and text spans accordingly. Furthermore, DOCMASTER supports both state-of-the-art layout-aware and text models for comprehensive training purposes. Importantly, as annotations, training, and inference occur on-device, it also safeguards privacy. The platform has been instrumental in driving several research prototypes concerning document analysis such as the AI assistant utilized by University of California San Diego&rsquo;s (UCSD) International Services and Engagement Office (ISEO) for processing a substantial volume of PDF documents.</p></p class="citation"></blockquote><h3 id=3942--39145-automatic-explanation-of-the-classification-of-spanish-legal-judgments-in-jurisdiction-dependent-law-categories-with-tree-estimators-jaime-gonzález-gonzález-et-al-2024>(39/42 | 39/145) Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent law categories with tree estimators (Jaime González-González et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaime González-González, Francisco de Arriba-Pérez, Silvia García-Méndez, Andrea Busto-Castiñeira, Francisco J. González-Castaño. (2024)<br><strong>Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent law categories with tree estimators</strong><br><button class=copy-to-clipboard title="Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent law categories with tree estimators" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00437v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00437v1.pdf filename=2404.00437v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic legal <b>text</b> <b>classification</b> systems have been proposed in the literature to address knowledge extraction from judgments and detect their aspects. However, most of these systems are black boxes even when their models are interpretable. This may raise concerns about their trustworthiness. Accordingly, this work contributes with a system combining Natural Language Processing (NLP) with Machine Learning (ML) to classify legal <b>texts</b> <b>in</b> an explainable manner. We analyze the features involved in the decision and the threshold bifurcation values of the decision paths of tree structures and present this information to the users in natural language. This is the first work on automatic analysis of legal <b>texts</b> <b>combining</b> NLP and ML along with Explainable Artificial Intelligence techniques to automatically make the models&rsquo; decisions understandable to end users. Furthermore, legal experts have validated our solution, and this knowledge has also been incorporated into the explanation process as &ldquo;expert-in-the-loop&rdquo; dictionaries. Experimental results on an annotated data set in law categories by jurisdiction demonstrate that our system yields competitive classification performance, with accuracy values well above 90%, and that its automatic explanations are easily understandable even to non-expert users.</p></p class="citation"></blockquote><h3 id=4042--40145-detection-of-temporality-at-discourse-level-on-financial-news-by-combining-natural-language-processing-and-machine-learning-silvia-garcía-méndez-et-al-2024>(40/42 | 40/145) Detection of Temporality at Discourse Level on Financial News by Combining Natural Language Processing and Machine Learning (Silvia García-Méndez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Silvia García-Méndez, Francisco de Arriba-Pérez, Ana Barros-Vila, Francisco J. González-Castaño. (2024)<br><strong>Detection of Temporality at Discourse Level on Financial News by Combining Natural Language Processing and Machine Learning</strong><br><button class=copy-to-clipboard title="Detection of Temporality at Discourse Level on Financial News by Combining Natural Language Processing and Machine Learning" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CE, cs-CL, cs-IR, cs-LG, cs.CL, q-fin-ST<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01337v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01337v1.pdf filename=2404.01337v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Finance-related news such as Bloomberg News, <b>CNN</b> Business and Forbes are valuable sources of real data for market screening systems. In news, an expert shares opinions beyond plain technical analyses that include context such as political, sociological and cultural factors. In the same text, the expert often discusses the performance of different assets. Some key statements are mere descriptions of past events while others are predictions. Therefore, understanding the temporality of the key statements in a text is essential to separate context information from valuable predictions. We propose a novel system to detect the temporality of finance-related news at discourse level that combines Natural Language Processing and Machine Learning techniques, and exploits sophisticated features such as syntactic and semantic dependencies. More specifically, we seek to extract the dominant tenses of the main statements, which may be either explicit or implicit. We have tested our system on a labelled dataset of finance-related news annotated by researchers with knowledge in the field. Experimental results reveal a high detection precision compared to an alternative rule-based baseline approach. Ultimately, this research contributes to the state-of-the-art of market screening by identifying predictive knowledge for financial decision making.</p></p class="citation"></blockquote><h3 id=4142--41145-taco----twitter-arguments-from-conversations-marc-feger-et-al-2024>(41/42 | 41/145) TACO &ndash; Twitter Arguments from COnversations (Marc Feger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marc Feger, Stefan Dietze. (2024)<br><strong>TACO &ndash; Twitter Arguments from COnversations</strong><br><button class=copy-to-clipboard title="TACO -- Twitter Arguments from COnversations" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00406v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00406v1.pdf filename=2404.00406v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Twitter has emerged as a global hub for engaging in online conversations and as a research corpus for various disciplines that have recognized the significance of its user-generated content. Argument mining is an important analytical task for processing and understanding online discourse. Specifically, it aims to identify the structural elements of arguments, denoted as information and inference. These elements, however, are not static and may require context within the conversation they are in, yet there is a lack of data and annotation frameworks addressing this dynamic aspect on Twitter. We contribute TACO, the first dataset of Twitter Arguments utilizing 1,814 tweets covering 200 entire conversations spanning six heterogeneous topics annotated with an agreement of 0.718 Krippendorff&rsquo;s alpha among six experts. Second, we provide our annotation framework, incorporating definitions from the Cambridge Dictionary, to define and identify argument components on Twitter. Our <b>transformer-based</b> classifier achieves an 85.06% macro F1 baseline score in detecting arguments. Moreover, our data reveals that Twitter users tend to engage in discussions involving informed inferences and information. TACO serves multiple purposes, such as training tweet classifiers to manage tweets based on inference and information elements, while also providing valuable insights into the conversational reply patterns of tweets.</p></p class="citation"></blockquote><h3 id=4242--42145-causal-inference-for-human-language-model-collaboration-bohan-zhang-et-al-2024>(42/42 | 42/145) Causal Inference for Human-Language Model Collaboration (Bohan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bohan Zhang, Yixin Wang, Paramveer S. Dhillon. (2024)<br><strong>Causal Inference for Human-Language Model Collaboration</strong><br><button class=copy-to-clipboard title="Causal Inference for Human-Language Model Collaboration" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00207v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00207v1.pdf filename=2404.00207v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we examine the collaborative dynamics between humans and language models (LMs), where the interactions typically involve LMs proposing text segments and humans editing or responding to these proposals. Productive engagement with LMs in such scenarios necessitates that humans discern effective text-based interaction strategies, such as editing and response styles, from historical human-LM interactions. This objective is inherently causal, driven by the <b>counterfactual</b> `what-if&rsquo; question: how would the outcome of collaboration change if humans employed a different text editing/refinement strategy? A key challenge in answering this causal inference question is formulating an appropriate causal estimand: the conventional average treatment effect (ATE) estimand is inapplicable to text-based treatments due to their high dimensionality. To address this concern, we introduce a new causal estimand &ndash; Incremental Stylistic Effect (ISE) &ndash; which characterizes the average impact of infinitesimally shifting a text towards a specific style, such as increasing formality. We establish the conditions for the non-parametric identification of ISE. Building on this, we develop CausalCollab, an algorithm designed to estimate the ISE of various interaction strategies in dynamic human-LM collaborations. Our empirical investigations across three distinct human-LM collaboration scenarios reveal that CausalCollab effectively reduces confounding and significantly improves <b>counterfactual</b> estimation over a set of competitive baselines.</p></p class="citation"></blockquote><h2 id=cslg-16>cs.LG (16)</h2><h3 id=116--43145-tg-nas-leveraging-zero-cost-proxies-with-transformer-and-graph-convolution-networks-for-efficient-neural-architecture-search-ye-qiao-et-al-2024>(1/16 | 43/145) TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph Convolution Networks for Efficient Neural Architecture Search (Ye Qiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ye Qiao, Haocheng Xu, Sitao Huang. (2024)<br><strong>TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph Convolution Networks for Efficient Neural Architecture Search</strong><br><button class=copy-to-clipboard title="TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph Convolution Networks for Efficient Neural Architecture Search" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 76<br>Keywords: Graph Convolutional Network, Graph, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Convolutional Neural Network, Zero-shot, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00271v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00271v1.pdf filename=2404.00271v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural architecture search (NAS) is an effective method for discovering new <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> architectures. However, existing approaches often require time-consuming training or intensive sampling and evaluations. <b>Zero-shot</b> NAS aims to create training-free proxies for architecture performance prediction. However, existing proxies have suboptimal performance, and are often outperformed by simple metrics such as model parameter counts or the number of floating-point operations. Besides, existing model-based proxies cannot be generalized to new search spaces with unseen new types of operators without golden accuracy truth. A universally optimal proxy remains elusive. We introduce TG-NAS, a novel model-based universal proxy that leverages a <b>transformer-based</b> operator embedding generator and a <b>graph</b> <b>convolution</b> <b>network</b> <b>(GCN)</b> to predict architecture performance. This approach guides neural architecture search across any given search space without the need of retraining. Distinct from other model-based predictor subroutines, TG-NAS itself acts as a zero-cost (ZC) proxy, guiding architecture search with advantages in terms of data independence, cost-effectiveness, and consistency across diverse search spaces. Our experiments showcase its advantages over existing proxies across various NAS <b>benchmarks,</b> suggesting its potential as a foundational element for efficient architecture search. TG-NAS achieves up to 300X improvements in search efficiency compared to previous SOTA ZC proxy methods. Notably, it discovers competitive models with 93.75% CIFAR-10 accuracy on the NAS-Bench-201 space and 74.5% ImageNet top-1 accuracy on the DARTS space.</p></p class="citation"></blockquote><h3 id=216--44145-linguistic-calibration-of-language-models-neil-band-et-al-2024>(2/16 | 44/145) Linguistic Calibration of Language Models (Neil Band et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Neil Band, Xuechen Li, Tengyu Ma, Tatsunori Hashimoto. (2024)<br><strong>Linguistic Calibration of Language Models</strong><br><button class=copy-to-clipboard title="Linguistic Calibration of Language Models" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG, stat-ML<br>Keyword Score: 70<br>Keywords: Distribution Shift, Distribution Shift, Fine-tuning, Fine-tuning, Reinforcement Learning, Supervised Learning, LLaMA, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00474v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00474v1.pdf filename=2404.00474v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce text with calibrated confidence statements. Through the lens of decision-making, we formalize linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a <b>supervised</b> <b>finetuning</b> step bootstraps an LM to emit long-form generations with confidence statements such as &ldquo;I estimate a 30% chance of&mldr;&rdquo; or &ldquo;I am certain that&mldr;&rdquo;, followed by a <b>reinforcement</b> <b>learning</b> step which rewards generations that enable a user to provide calibrated answers to related <b>questions.</b> <b>We</b> linguistically calibrate <b>Llama</b> 2 7B and find in automated and human evaluations of long-form generations that it is significantly more calibrated than strong <b>finetuned</b> factuality baselines with comparable accuracy. These findings generalize under <b>distribution</b> <b>shift</b> on <b>question-answering</b> <b>and</b> under a significant task shift to person biography generation. Our results demonstrate that long-form generations may be calibrated end-to-end by constructing an objective in the space of the predictions that users make in downstream decision-making.</p></p class="citation"></blockquote><h3 id=316--45145-shortcuts-arising-from-contrast-effective-and-covert-clean-label-attacks-in-prompt-based-learning-xiaopeng-xie-et-al-2024>(3/16 | 45/145) Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning (Xiaopeng Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaopeng Xie, Ming Yan, Xiwen Zhou, Chenlong Zhao, Suli Wang, Yong Zhang, Joey Tianyi Zhou. (2024)<br><strong>Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning</strong><br><button class=copy-to-clipboard title="Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T50, I-2-7, cs-AI, cs-CL, cs-CR, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Data Augmentation, Few-shot, Text Classification, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00461v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00461v1.pdf filename=2404.00461v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt-based</b> learning paradigm has demonstrated remarkable efficacy in enhancing the adaptability of <b>pretrained</b> <b>language</b> <b>models</b> <b>(PLMs),</b> particularly in <b>few-shot</b> scenarios. However, this learning paradigm has been shown to be vulnerable to backdoor attacks. The current clean-label attack, employing a specific <b>prompt</b> as a trigger, can achieve success without the need for external triggers and ensure correct labeling of poisoned samples, which is more stealthy compared to the poisoned-label attack, but on the other hand, it faces significant issues with false activations and poses greater challenges, necessitating a higher rate of poisoning. Using conventional negative <b>data</b> <b>augmentation</b> methods, we discovered that it is challenging to trade off between effectiveness and stealthiness in a clean-label setting. In addressing this issue, we are inspired by the notion that a backdoor acts as a shortcut and posit that this shortcut stems from the contrast between the trigger and the <b>data</b> <b>utilized</b> for poisoning. In this study, we propose a method named Contrastive Shortcut Injection (CSI), by leveraging activation values, integrates trigger design and <b>data</b> <b>selection</b> strategies to craft stronger shortcut features. With extensive experiments on full-shot and <b>few-shot</b> <b>text</b> <b>classification</b> tasks, we empirically validate CSI&rsquo;s high effectiveness and high stealthiness at low poisoning rates. Notably, we found that the two approaches play leading roles in full-shot and <b>few-shot</b> settings, respectively.</p></p class="citation"></blockquote><h3 id=416--46145-quarot-outlier-free-4-bit-inference-in-rotated-llms-saleh-ashkboos-et-al-2024>(4/16 | 46/145) QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs (Saleh Ashkboos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, James Hensman. (2024)<br><strong>QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs</strong><br><button class=copy-to-clipboard title="QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Quantization, Quantization, Zero-shot, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00456v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00456v1.pdf filename=2404.00456v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce QuaRot, a new <b>Quantization</b> scheme based on Rotations, which is able to <b>quantize</b> <b>LLMs</b> end-to-end, including all weights, activations, and KV cache in 4 bits. QuaRot rotates <b>LLMs</b> in a way that removes outliers from the hidden state without changing the output, making <b>quantization</b> easier. This computational invariance is applied to the hidden state (residual) of the <b>LLM,</b> as well as to the activations of the feed-forward components, aspects of the attention mechanism and to the KV cache. The result is a <b>quantized</b> model where all matrix multiplications are performed in 4-bits, without any channels identified for retention in higher precision. Our <b>quantized</b> LLaMa2-70B model has losses of at most 0.29 WikiText-2 <b>perplexity</b> and retains 99% of the <b>zero-shot</b> performance. Code is available at: <a href=https://github.com/spcl/QuaRot>https://github.com/spcl/QuaRot</a>.</p></p class="citation"></blockquote><h3 id=516--47145-heterogeneous-contrastive-learning-for-foundation-models-and-beyond-lecheng-zheng-et-al-2024>(5/16 | 47/145) Heterogeneous Contrastive Learning for Foundation Models and Beyond (Lecheng Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lecheng Zheng, Baoyu Jing, Zihao Li, Hanghang Tong, Jingrui He. (2024)<br><strong>Heterogeneous Contrastive Learning for Foundation Models and Beyond</strong><br><button class=copy-to-clipboard title="Heterogeneous Contrastive Learning for Foundation Models and Beyond" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Contrastive Learning, Fine-tuning, Foundation Model, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00225v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00225v1.pdf filename=2404.00225v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the era of big data and Artificial Intelligence, an emerging paradigm is to utilize <b>contrastive</b> <b>self-supervised</b> <b>learning</b> to model large-scale heterogeneous data. Many existing <b>foundation</b> <b>models</b> benefit from the generalization capability of <b>contrastive</b> <b>self-supervised</b> <b>learning</b> by learning compact and high-quality representations without relying on any label information. Amidst the explosive advancements in <b>foundation</b> <b>models</b> across multiple domains, including natural language processing and computer vision, a thorough survey on heterogeneous <b>contrastive</b> <b>learning</b> for the <b>foundation</b> <b>model</b> is urgently needed. In response, this survey critically evaluates the current landscape of heterogeneous <b>contrastive</b> <b>learning</b> for <b>foundation</b> <b>models,</b> highlighting the open challenges and future trends of <b>contrastive</b> <b>learning.</b> In particular, we first present how the recent advanced <b>contrastive</b> <b>learning-based</b> methods deal with view heterogeneity and how <b>contrastive</b> <b>learning</b> is applied to train and <b>fine-tune</b> the multi-view <b>foundation</b> <b>models.</b> Then, we move to <b>contrastive</b> <b>learning</b> methods for task heterogeneity, including pretraining tasks and downstream tasks, and show how different tasks are combined with <b>contrastive</b> <b>learning</b> loss for different purposes. Finally, we conclude this survey by discussing the open challenges and shedding light on the future directions of <b>contrastive</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=616--48145-zero-shot-safety-prediction-for-autonomous-robots-with-foundation-world-models-zhenjiang-mao-et-al-2024>(6/16 | 48/145) Zero-shot Safety Prediction for Autonomous Robots with Foundation World Models (Zhenjiang Mao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenjiang Mao, Siqi Dai, Yuang Geng, Ivan Ruchkin. (2024)<br><strong>Zero-shot Safety Prediction for Autonomous Robots with Foundation World Models</strong><br><button class=copy-to-clipboard title="Zero-shot Safety Prediction for Autonomous Robots with Foundation World Models" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-RO, cs.LG<br>Keyword Score: 43<br>Keywords: Benchmarking, Supervised Learning, Supervised Learning, Zero-shot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00462v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00462v2.pdf filename=2404.00462v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A world model creates a surrogate world to train a controller and predict safety violations by learning the internal dynamic model of systems. However, the existing world models rely solely on statistical learning of how observations change in response to actions, lacking precise quantification of how accurate the surrogate dynamics are, which poses a significant challenge in safety-critical systems. To address this challenge, we propose foundation world models that embed observations into meaningful and causally latent representations. This enables the surrogate dynamics to directly predict causal future states by leveraging a training-free <b>large</b> <b>language</b> <b>model.</b> In two common <b>benchmarks,</b> this novel model outperforms standard world models in the safety prediction task and has a performance comparable to <b>supervised</b> <b>learning</b> despite not using any data. We evaluate its performance with a more specialized and system-relevant metric by comparing estimated states instead of aggregating observation-wide error.</p></p class="citation"></blockquote><h3 id=716--49145-leveraging-pre-trained-and-transformer-derived-embeddings-from-ehrs-to-characterize-heterogeneity-across-alzheimers-disease-and-related-dementias-matthew-west-et-al-2024>(7/16 | 49/145) Leveraging Pre-trained and Transformer-derived Embeddings from EHRs to Characterize Heterogeneity Across Alzheimer&rsquo;s Disease and Related Dementias (Matthew West et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthew West, Colin Magdamo, Lily Cheng, Yingnan He, Sudeshna Das. (2024)<br><strong>Leveraging Pre-trained and Transformer-derived Embeddings from EHRs to Characterize Heterogeneity Across Alzheimer&rsquo;s Disease and Related Dementias</strong><br><button class=copy-to-clipboard title="Leveraging Pre-trained and Transformer-derived Embeddings from EHRs to Characterize Heterogeneity Across Alzheimer's Disease and Related Dementias" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Unsupervised Learning, Unsupervised Learning, BERT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00464v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00464v1.pdf filename=2404.00464v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Alzheimer&rsquo;s disease is a progressive, debilitating neurodegenerative disease that affects 50 million people globally. Despite this substantial health burden, available treatments for the disease are limited and its fundamental causes remain poorly understood. Previous work has suggested the existence of clinically-meaningful sub-types, which it is suggested may correspond to distinct etiologies, disease courses, and ultimately appropriate treatments. Here, we use <b>unsupervised</b> <b>learning</b> techniques on electronic health records (EHRs) from a cohort of memory disorder patients to characterise heterogeneity in this disease population. Pre-trained embeddings for medical codes as well as <b>transformer-derived</b> Clinical <b>BERT</b> embeddings of free text are used to encode patient EHRs. We identify the existence of sub-populations on the basis of comorbidities and shared textual features, and discuss their clinical significance.</p></p class="citation"></blockquote><h3 id=816--50145-orchestrate-latent-expertise-advancing-online-continual-learning-with-multi-level-supervision-and-reverse-self-distillation-hongwei-yan-et-al-2024>(8/16 | 50/145) Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation (HongWei Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>HongWei Yan, Liyuan Wang, Kaisheng Ma, Yi Zhong. (2024)<br><strong>Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation</strong><br><button class=copy-to-clipboard title="Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Continual Learning, Knowledge Distillation, Knowledge Distillation, Self-Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00417v1.pdf filename=2404.00417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To accommodate real-world dynamics, artificial intelligence systems need to cope with sequentially arriving content in an online manner. Beyond regular <b>Continual</b> <b>Learning</b> (CL) attempting to address catastrophic forgetting with offline training of each task, Online <b>Continual</b> <b>Learning</b> (OCL) is a more challenging yet realistic setting that performs CL in a one-pass data stream. Current OCL methods primarily rely on memory replay of old training samples. However, a notable gap from CL to OCL stems from the additional overfitting-underfitting dilemma associated with the use of rehearsal buffers: the inadequate learning of new training samples (underfitting) and the repeated learning of a few old training samples (overfitting). To this end, we introduce a novel approach, Multi-level Online Sequential Experts (MOSE), which cultivates the model as stacked sub-experts, integrating multi-level supervision and reverse <b>self-distillation.</b> Supervision signals across multiple stages facilitate appropriate convergence of the new task while gathering various strengths from experts by <b>knowledge</b> <b>distillation</b> mitigates the performance decline of old tasks. MOSE demonstrates remarkable efficacy in learning new samples and preserving past <b>knowledge</b> <b>through</b> multi-level experts, thereby significantly advancing OCL performance over state-of-the-art baselines (e.g., up to 7.3% on Split CIFAR-100 and 6.1% on Split Tiny-ImageNet).</p></p class="citation"></blockquote><h3 id=916--51145-survey-on-large-language-model-enhanced-reinforcement-learning-concept-taxonomy-and-methods-yuji-cao-et-al-2024>(9/16 | 51/145) Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods (Yuji Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Guolong Liu, Gaoqi Liang, Junhua Zhao, Yun Li. (2024)<br><strong>Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods</strong><br><button class=copy-to-clipboard title="Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs-RO, cs.LG<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00282v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00282v1.pdf filename=2404.00282v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With extensive pre-trained knowledge and high-level general capabilities, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> emerge as a promising avenue to augment <b>reinforcement</b> <b>learning</b> (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and <b>summarize</b> its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize <b>LLMs&rsquo;</b> functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we <b>summarize</b> the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospective opportunities and challenges of the $\textit{LLM-enhanced RL}$ are discussed.</p></p class="citation"></blockquote><h3 id=1016--52145-continual-learning-for-autonomous-robots-a-prototype-based-approach-elvin-hajizada-et-al-2024>(10/16 | 52/145) Continual Learning for Autonomous Robots: A Prototype-based Approach (Elvin Hajizada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elvin Hajizada, Balachandran Swaminathan, Yulia Sandamirskaya. (2024)<br><strong>Continual Learning for Autonomous Robots: A Prototype-based Approach</strong><br><button class=copy-to-clipboard title="Continual Learning for Autonomous Robots: A Prototype-based Approach" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs-RO, cs.LG<br>Keyword Score: 20<br>Keywords: Continual Learning, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00418v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00418v1.pdf filename=2404.00418v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans and animals learn throughout their lives from limited amounts of sensed data, both with and without supervision. Autonomous, intelligent robots of the future are often expected to do the same. The existing <b>continual</b> <b>learning</b> (CL) methods are usually not directly applicable to robotic settings: they typically require buffering and a balanced replay of training data. A <b>few-shot</b> online <b>continual</b> <b>learning</b> (FS-OCL) setting has been proposed to address more realistic scenarios where robots must learn from a non-repeated sparse data stream. To enable truly autonomous life-long learning, an additional challenge of detecting novelties and learning new items without supervision needs to be addressed. We address this challenge with our new prototype-based approach called Continually Learning Prototypes (CLP). In addition to being capable of FS-OCL learning, CLP also detects novel objects and learns them without supervision. To mitigate forgetting, CLP utilizes a novel metaplasticity mechanism that adapts the learning rate individually per prototype. CLP is rehearsal-free, hence does not require a memory buffer, and is compatible with neuromorphic hardware, characterized by ultra-low power consumption, real-time processing abilities, and on-chip learning. Indeed, we have open-sourced a simple version of CLP in the neuromorphic software framework Lava, targetting Intel&rsquo;s neuromorphic chip Loihi 2. We evaluate CLP on a robotic vision dataset, OpenLORIS. In a low-instance FS-OCL scenario, CLP shows state-of-the-art results. In the open world, CLP detects novelties with superior precision and recall and learns features of the detected novel classes without supervision, achieving a strong baseline of 99% base class and 65%/76% (5-shot/10-shot) novel class accuracy.</p></p class="citation"></blockquote><h3 id=1116--53145-from-learning-to-analytics-improving-model-efficacy-with-goal-directed-client-selection-jingwen-tong-et-al-2024>(11/16 | 53/145) From Learning to Analytics: Improving Model Efficacy with Goal-Directed Client Selection (Jingwen Tong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingwen Tong, Zhenzhen Chen, Liqun Fu, Jun Zhang, Zhu Han. (2024)<br><strong>From Learning to Analytics: Improving Model Efficacy with Goal-Directed Client Selection</strong><br><button class=copy-to-clipboard title="From Learning to Analytics: Improving Model Efficacy with Goal-Directed Client Selection" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 14J60, I-2-7, cs-LG, cs.LG, eess-SP<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00371v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00371v1.pdf filename=2404.00371v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) is an appealing paradigm for learning a global model among distributed clients while preserving data privacy. Driven by the demand for high-quality user experiences, evaluating the well-trained global model after the FL process is crucial. In this paper, we propose a closed-loop model analytics framework that allows for effective evaluation of the trained global model using clients&rsquo; local data. To address the challenges posed by system and data heterogeneities in the FL process, we study a goal-directed client selection problem based on the model analytics framework by selecting a subset of clients for the model training. This problem is formulated as a stochastic multi-armed <b>bandit</b> (SMAB) problem. We first put forth a quick initial upper confidence bound (Quick-Init UCB) algorithm to solve this SMAB problem under the <b>federated</b> <b>analytics</b> (FA) framework. Then, we further propose a belief propagation-based UCB (BP-UCB) algorithm under the democratized analytics (DA) framework. Moreover, we derive two regret upper bounds for the proposed algorithms, which increase logarithmically over the time horizon. The numerical results demonstrate that the proposed algorithms achieve nearly optimal performance, with a gap of less than 1.44% and 3.12% under the FA and DA frameworks, respectively.</p></p class="citation"></blockquote><h3 id=1216--54145-inflora-interference-free-low-rank-adaptation-for-continual-learning-yan-shuo-liang-et-al-2024>(12/16 | 54/145) InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning (Yan-Shuo Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan-Shuo Liang, Wu-Jun Li. (2024)<br><strong>InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning</strong><br><button class=copy-to-clipboard title="InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Continual Learning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00228v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00228v3.pdf filename=2404.00228v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>learning</b> requires the model to learn multiple tasks sequentially. In <b>continual</b> <b>learning,</b> the model should possess the ability to maintain its performance on old tasks (stability) and the ability to adapt to new tasks continuously (plasticity). Recently, parameter-efficient <b>fine-tuning</b> (PEFT), which involves freezing a pre-trained model and injecting a small number of learnable parameters to adapt to downstream tasks, has gained increasing popularity in <b>continual</b> <b>learning.</b> Although existing <b>continual</b> <b>learning</b> methods based on PEFT have demonstrated superior performance compared to those not based on PEFT, most of them do not consider how to eliminate the interference of the new task on the old tasks, which inhibits the model from making a good trade-off between stability and plasticity. In this work, we propose a new PEFT method, called interference-free low-rank adaptation (InfLoRA), for <b>continual</b> <b>learning.</b> InfLoRA injects a small number of parameters to reparameterize the pre-trained weights and shows that <b>fine-tuning</b> these injected parameters is equivalent to <b>fine-tuning</b> the pre-trained weights within a subspace. Furthermore, InfLoRA designs this subspace to eliminate the interference of the new task on the old tasks, making a good trade-off between stability and plasticity. Experimental results show that InfLoRA outperforms existing state-of-the-art <b>continual</b> <b>learning</b> methods on multiple datasets.</p></p class="citation"></blockquote><h3 id=1316--55145-de-hnn-an-effective-neural-model-for-circuit-netlist-representation-zhishang-luo-et-al-2024>(13/16 | 55/145) DE-HNN: An effective neural model for Circuit Netlist representation (Zhishang Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhishang Luo, Truong Son Hy, Puoya Tabaghi, Donghyeon Koh, Michael Defferrard, Elahe Rezaei, Ryan Carey, Rhett Davis, Rajeev Jain, Yusu Wang. (2024)<br><strong>DE-HNN: An effective neural model for Circuit Netlist representation</strong><br><button class=copy-to-clipboard title="DE-HNN: An effective neural model for Circuit Netlist representation" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AR, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00477v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00477v1.pdf filename=2404.00477v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The run-time for optimization tools used in chip design has grown with the complexity of designs to the point where it can take several days to go through one design cycle which has become a bottleneck. Designers want fast tools that can quickly give feedback on a design. Using the input and output data of the tools from past designs, one can attempt to build a machine learning model that predicts the outcome of a design in significantly shorter time than running the tool. The accuracy of such models is affected by the representation of the design data, which is usually a netlist that describes the elements of the digital circuit and how they are connected. <b>Graph</b> <b>representations</b> <b>for</b> the netlist together with <b>graph</b> <b>neural</b> <b>networks</b> have been investigated for such models. However, the characteristics of netlists pose several challenges for existing <b>graph</b> <b>learning</b> <b>frameworks,</b> due to the large number of nodes and the importance of long-range interactions between nodes. To address these challenges, we represent the netlist as a directed hypergraph and propose a Directional Equivariant Hypergraph Neural Network (DE-HNN) for the effective learning of (directed) hypergraphs. Theoretically, we show that our DE-HNN can universally approximate any node or hyperedge based function that satisfies certain permutation equivariant and invariant properties natural for directed hypergraphs. We compare the proposed DE-HNN with several State-of-the-art (SOTA) machine learning models for (hyper)graphs and netlists, and show that the DE-HNN significantly outperforms them in predicting the outcome of optimized place-and-route tools directly from the input netlists. Our source code and the netlists data used are publicly available at <a href=https://github.com/YusuLab/chips.git>https://github.com/YusuLab/chips.git</a></p></p class="citation"></blockquote><h3 id=1416--56145-clustering-for-protein-representation-learning-ruijie-quan-et-al-2024>(14/16 | 56/145) Clustering for Protein Representation Learning (Ruijie Quan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruijie Quan, Wenguan Wang, Fan Ma, Hehe Fan, Yi Yang. (2024)<br><strong>Clustering for Protein Representation Learning</strong><br><button class=copy-to-clipboard title="Clustering for Protein Representation Learning" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CE, cs-LG, cs.LG, q-bio-BM, q-bio-QM<br>Keyword Score: 11<br>Keywords: Graph, Clustering, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00254v1.pdf filename=2404.00254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Protein <b>representation</b> <b>learning</b> is a challenging task that aims to capture the structure and function of proteins from their amino acid sequences. Previous methods largely ignored the fact that not all amino acids are equally important for protein folding and activity. In this article, we propose a neural <b>clustering</b> framework that can automatically discover the critical components of a protein by considering both its primary and tertiary structure information. Our framework treats a protein as a <b>graph,</b> where each node represents an amino acid and each edge represents a spatial or sequential connection between amino acids. We then apply an iterative <b>clustering</b> strategy to group the nodes into clusters based on their 1D and 3D positions and assign scores to each cluster. We select the highest-scoring clusters and use their medoid nodes for the next iteration of <b>clustering,</b> until we obtain a hierarchical and informative <b>representation</b> <b>of</b> the protein. We evaluate on four protein-related tasks: protein fold classification, enzyme reaction classification, gene ontology term prediction, and enzyme commission number prediction. Experimental results demonstrate that our method achieves state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=1516--57145-computation-and-communication-efficient-lightweighting-vertical-federated-learning-heqiang-wang-et-al-2024>(15/16 | 57/145) Computation and Communication Efficient Lightweighting Vertical Federated Learning (Heqiang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Heqiang Wang, Jieming Bian, Lei Wang. (2024)<br><strong>Computation and Communication Efficient Lightweighting Vertical Federated Learning</strong><br><button class=copy-to-clipboard title="Computation and Communication Efficient Lightweighting Vertical Federated Learning" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00466v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00466v1.pdf filename=2404.00466v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The exploration of computational and communication efficiency within <b>Federated</b> <b>Learning</b> (FL) has emerged as a prominent and crucial field of study. While most existing efforts to enhance these efficiencies have focused on Horizontal FL, the distinct processes and model structures of Vertical FL preclude the direct application of Horizontal FL-based techniques. In response, we introduce the concept of Lightweight Vertical <b>Federated</b> <b>Learning</b> (LVFL), targeting both computational and communication efficiencies. This approach involves separate lightweighting strategies for the feature model, to improve computational efficiency, and for feature embedding, to enhance communication efficiency. Moreover, we establish a convergence bound for our LVFL algorithm, which accounts for both communication and computational lightweighting ratios. Our evaluation of the algorithm on a image classification dataset reveals that LVFL significantly alleviates computational and communication demands while preserving robust learning performance. This work effectively addresses the gaps in communication and computational efficiency within Vertical FL.</p></p class="citation"></blockquote><h3 id=1616--58145-generative-ai-for-architectural-design-a-literature-review-chengyuan-li-et-al-2024>(16/16 | 58/145) Generative AI for Architectural Design: A Literature Review (Chengyuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengyuan Li, Tianyu Zhang, Xusheng Du, Ye Zhang, Haoran Xie. (2024)<br><strong>Generative AI for Architectural Design: A Literature Review</strong><br><button class=copy-to-clipboard title="Generative AI for Architectural Design: A Literature Review" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01335v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01335v1.pdf filename=2404.01335v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>Artificial</b> Intelligence (AI) has pioneered new methodological paradigms in architectural design, significantly expanding the innovative potential and efficiency of the design process. This paper explores the extensive applications of <b>generative</b> <b>AI</b> technologies in architectural design, a trend that has benefited from the rapid development of deep <b>generative</b> <b>models.</b> This article provides a comprehensive review of the basic principles of <b>generative</b> <b>AI</b> and large-scale models and highlights the applications in the generation of 2D images, videos, and 3D models. In addition, by reviewing the latest literature from 2020, this paper scrutinizes the impact of <b>generative</b> <b>AI</b> technologies at different stages of architectural design, from generating initial architectural 3D forms to producing final architectural imagery. The marked trend of research growth indicates an increasing inclination within the architectural design community towards embracing <b>generative</b> <b>AI,</b> thereby catalyzing a shared enthusiasm for research. These research cases and methodologies have not only proven to enhance efficiency and innovation significantly but have also posed challenges to the conventional boundaries of architectural creativity. Finally, we point out new directions for design innovation and articulate fresh trajectories for applying <b>generative</b> <b>AI</b> in the architectural domain. This article provides the first comprehensive literature review about <b>generative</b> <b>AI</b> for architectural design, and we believe this work can facilitate more research work on this significant topic in architecture.</p></p class="citation"></blockquote><h2 id=cscv-35>cs.CV (35)</h2><h3 id=135--59145-design-as-desired-utilizing-visual-question-answering-for-multimodal-pre-training-tongkun-su-et-al-2024>(1/35 | 59/145) Design as Desired: Utilizing Visual Question Answering for Multimodal Pre-training (Tongkun Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tongkun Su, Jun Li, Xi Zhang, Haibo Jin, Hao Chen, Qiong Wang, Faqin Lv, Baoliang Zhao, Yin Hu. (2024)<br><strong>Design as Desired: Utilizing Visual Question Answering for Multimodal Pre-training</strong><br><button class=copy-to-clipboard title="Design as Desired: Utilizing Visual Question Answering for Multimodal Pre-training" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 66<br>Keywords: Contrastive Learning, Multi-modal, Multi-modal, Transformer, Question Answering, Visual Question Answering, Visual Question Answering, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00226v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00226v1.pdf filename=2404.00226v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> pre-training demonstrates its potential in the medical domain, which learns medical <b>visual</b> <b>representations</b> <b>from</b> paired medical reports. However, many pre-training tasks require extra annotations from clinicians, and most of them fail to explicitly guide the model to learn the desired features of different pathologies. To the best of our knowledge, we are the first to utilize <b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA)</b> for <b>multimodal</b> pre-training to guide the framework focusing on targeted pathological features. In this work, we leverage descriptions in medical reports to design multi-granular <b>question-answer</b> <b>pairs</b> associated with different diseases, which assist the framework in pre-training without requiring extra annotations from experts. We also propose a novel pre-training framework with a quasi-textual feature <b>transformer,</b> a module designed to transform <b>visual</b> <b>features</b> <b>into</b> a quasi-textual space closer to the textual domain via a <b>contrastive</b> <b>learning</b> strategy. This narrows the <b>vision-language</b> gap and facilitates modality alignment. Our framework is applied to four downstream tasks: report generation, classification, segmentation, and detection across five datasets. Extensive experiments demonstrate the superiority of our framework compared to other state-of-the-art methods. Our code will be released upon acceptance.</p></p class="citation"></blockquote><h3 id=235--60145-clip-driven-outliers-synthesis-for-few-shot-ood-detection-hao-sun-et-al-2024>(2/35 | 60/145) CLIP-driven Outliers Synthesis for few-shot OOD detection (Hao Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Sun, Rundong He, Zhongyi Han, Zhicong Lin, Yongshun Gong, Yilong Yin. (2024)<br><strong>CLIP-driven Outliers Synthesis for few-shot OOD detection</strong><br><button class=copy-to-clipboard title="CLIP-driven Outliers Synthesis for few-shot OOD detection" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 63<br>Keywords: Benchmarking, Convolution, Few-shot, Out-of-distribution, Prompt, Prompt Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00323v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00323v1.pdf filename=2404.00323v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> OOD detection focuses on recognizing <b>out-of-distribution</b> (OOD) images that belong to classes unseen during training, with the use of only a small number of labeled in-distribution (ID) images. Up to now, a mainstream strategy is based on large-scale <b>vision-language</b> models, such as CLIP. However, these methods overlook a crucial issue: the lack of reliable OOD supervision information, which can lead to biased boundaries between in-distribution (ID) and OOD. To tackle this problem, we propose CLIP-driven Outliers Synthesis~(CLIP-OS). Firstly, CLIP-OS enhances patch-level features&rsquo; perception by newly proposed patch uniform <b>convolution,</b> and adaptively obtains the proportion of ID-relevant information by employing CLIP-surgery-discrepancy, thus achieving separation between ID-relevant and ID-irrelevant. Next, CLIP-OS synthesizes reliable OOD data by mixing up ID-relevant features from different classes to provide OOD supervision information. Afterward, CLIP-OS leverages synthetic OOD samples by unknown-aware <b>prompt</b> <b>learning</b> to enhance the separability of ID and OOD. Extensive experiments across multiple <b>benchmarks</b> demonstrate that CLIP-OS achieves superior <b>few-shot</b> OOD detection capability.</p></p class="citation"></blockquote><h3 id=335--61145-hsimamba-hyperpsectral-imaging-efficient-feature-learning-with-bidirectional-state-space-for-classification-judy-x-yang-et-al-2024>(3/35 | 61/145) HSIMamba: Hyperpsectral Imaging Efficient Feature Learning with Bidirectional State Space for Classification (Judy X Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Judy X Yang, Jun Zhou, Jing Wang, Hui Tian, Alan Wee Chung Liew. (2024)<br><strong>HSIMamba: Hyperpsectral Imaging Efficient Feature Learning with Bidirectional State Space for Classification</strong><br><button class=copy-to-clipboard title="HSIMamba: Hyperpsectral Imaging Efficient Feature Learning with Bidirectional State Space for Classification" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: F-2-2, I-2-7, cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00272v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00272v1.pdf filename=2404.00272v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Classifying hyperspectral images is a difficult task in remote sensing, due to their complex high-dimensional data. To address this challenge, we propose HSIMamba, a novel framework that uses bidirectional reversed <b>convolutional</b> <b>neural</b> <b>network</b> pathways to extract spectral features more efficiently. Additionally, it incorporates a specialized block for spatial analysis. Our approach combines the operational efficiency of <b>CNNs</b> with the dynamic feature extraction capability of attention mechanisms found in <b>Transformers.</b> However, it avoids the associated high computational demands. HSIMamba is designed to process data bidirectionally, significantly enhancing the extraction of spectral features and integrating them with spatial information for comprehensive analysis. This approach improves classification accuracy beyond current <b>benchmarks</b> and addresses computational inefficiencies encountered with advanced models like <b>Transformers.</b> HSIMamba were tested against three widely recognized datasets Houston 2013, Indian Pines, and Pavia University and demonstrated exceptional performance, surpassing existing state-of-the-art models in HSI classification. This method highlights the methodological innovation of HSIMamba and its practical implications, which are particularly valuable in contexts where computational resources are limited. HSIMamba redefines the standards of efficiency and accuracy in HSI classification, thereby enhancing the capabilities of remote sensing applications. Hyperspectral imaging has become a crucial tool for environmental surveillance, agriculture, and other critical areas that require detailed analysis of the Earth surface. Please see our code in HSIMamba for more details.</p></p class="citation"></blockquote><h3 id=435--62145-ttd-text-tag-self-distillation-enhancing-image-text-alignment-in-clip-to-alleviate-single-tag-bias-sanghyun-jo-et-al-2024>(4/35 | 62/145) TTD: Text-Tag Self-Distillation Enhancing Image-Text Alignment in CLIP to Alleviate Single Tag Bias (Sanghyun Jo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanghyun Jo, Soohyun Ryu, Sungyub Kim, Eunho Yang, Kyungsu Kim. (2024)<br><strong>TTD: Text-Tag Self-Distillation Enhancing Image-Text Alignment in CLIP to Alleviate Single Tag Bias</strong><br><button class=copy-to-clipboard title="TTD: Text-Tag Self-Distillation Enhancing Image-Text Alignment in CLIP to Alleviate Single Tag Bias" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Fine-tuning, Self-Distillation, Image2text, Stemming, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00384v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00384v1.pdf filename=2404.00384v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We identify a critical bias in contemporary CLIP-based models, which we denote as \textit{single tag bias}. This bias manifests as a disproportionate focus on a singular tag (word) while neglecting other pertinent tags, <b>stemming</b> from CLIP&rsquo;s <b>text</b> <b>embeddings</b> that prioritize one specific tag in <b>image-text</b> relationships. When deconstructing <b>text</b> <b>into</b> individual tags, only one tag tends to have high relevancy with CLIP&rsquo;s image embedding, leading to an imbalanced tag relevancy. This results in an uneven alignment among multiple tags present in the <b>text.</b> <b>To</b> tackle this challenge, we introduce a novel two-step <b>fine-tuning</b> approach. First, our method leverages the similarity between tags and their nearest pixels for scoring, enabling the extraction of image-relevant tags from the <b>text.</b> <b>Second,</b> we present a <b>self-distillation</b> strategy aimed at aligning the combined masks from extracted tags with the <b>text-derived</b> <b>mask.</b> This approach mitigates the single tag bias, thereby significantly improving the alignment of CLIP&rsquo;s model without necessitating additional data or supervision. Our technique demonstrates model-agnostic improvements in multi-tag classification and segmentation tasks, surpassing competing methods that rely on external resources. Code is available at <a href=https://github.com/shjo-april/TTD>https://github.com/shjo-april/TTD</a>.</p></p class="citation"></blockquote><h3 id=535--63145-reusable-architecture-growth-for-continual-stereo-matching-chenghao-zhang-et-al-2024>(5/35 | 63/145) Reusable Architecture Growth for Continual Stereo Matching (Chenghao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenghao Zhang, Gaofeng Meng, Bin Fan, Kun Tian, Zhaoxiang Zhang, Shiming Xiang, Chunhong Pan. (2024)<br><strong>Reusable Architecture Growth for Continual Stereo Matching</strong><br><button class=copy-to-clipboard title="Reusable Architecture Growth for Continual Stereo Matching" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Retrieval-Augmented Generation, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00360v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00360v1.pdf filename=2404.00360v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The remarkable performance of recent stereo depth estimation models benefits from the successful use of <b>convolutional</b> <b>neural</b> <b>networks</b> to regress dense disparity. Akin to most tasks, this needs gathering training data that covers a number of heterogeneous scenes at deployment time. However, training samples are typically acquired continuously in practical applications, making the capability to learn new scenes continually even more crucial. For this purpose, we propose to perform continual stereo matching where a model is tasked to 1) continually learn new scenes, 2) overcome forgetting previously learned scenes, and 3) continuously predict disparities at inference. We achieve this goal by introducing a Reusable Architecture Growth <b>(RAG)</b> framework. <b>RAG</b> leverages task-specific neural unit search and architecture growth to learn new scenes continually in both <b>supervised</b> and <b>self-supervised</b> manners. It can maintain high reusability during growth by reusing previous units while obtaining good performance. Additionally, we present a Scene Router module to adaptively select the scene-specific architecture path at inference. Comprehensive experiments on numerous datasets show that our framework performs impressively in various weather, road, and city circumstances and surpasses the state-of-the-art methods in more challenging cross-dataset settings. Further experiments also demonstrate the adaptability of our method to unseen scenes, which can facilitate end-to-end stereo architecture learning and practical deployment.</p></p class="citation"></blockquote><h3 id=635--64145-do-vision-language-models-understand-compound-nouns-sonal-kumar-et-al-2024>(6/35 | 64/145) Do Vision-Language Models Understand Compound Nouns? (Sonal Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sonal Kumar, Sreyan Ghosh, S Sakshi, Utkarsh Tyagi, Dinesh Manocha. (2024)<br><strong>Do Vision-Language Models Understand Compound Nouns?</strong><br><button class=copy-to-clipboard title="Do Vision-Language Models Understand Compound Nouns?" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Text2image, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00419v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00419v1.pdf filename=2404.00419v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-vocabulary <b>vision-language</b> models (VLMs) like CLIP, trained using contrastive loss, have emerged as a promising new paradigm for <b>text-to-image</b> retrieval. However, do VLMs understand compound nouns (CNs) (e.g., lab coat) as well as they understand nouns (e.g., lab)? We curate Compun, a novel <b>benchmark</b> with 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in interpreting CNs. The Compun <b>benchmark</b> challenges a VLM for <b>text-to-image</b> retrieval where, given a text <b>prompt</b> with a CN, the task is to select the correct image that shows the CN among a pair of distractor images that show the constituent nouns that make up the CN. Next, we perform an in-depth analysis to highlight CLIPs&rsquo; limited understanding of certain types of CNs. Finally, we present an alternative framework that moves beyond hand-written templates for text <b>prompts</b> widely used by CLIP-like models. We employ a <b>Large</b> <b>Language</b> <b>Model</b> to generate multiple diverse captions that include the CN as an object in the scene described by the caption. Our proposed method improves CN understanding of CLIP by 8.25% on Compun. Code and <b>benchmark</b> are available at: <a href=https://github.com/sonalkum/Compun>https://github.com/sonalkum/Compun</a></p></p class="citation"></blockquote><h3 id=735--65145-yolooc-yolo-based-open-class-incremental-object-detection-with-novel-class-discovery-qian-wan-et-al-2024>(7/35 | 65/145) YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery (Qian Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qian Wan, Xiang Xiang, Qinhao Zhou. (2024)<br><strong>YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery</strong><br><button class=copy-to-clipboard title="YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 43<br>Keywords: Yolo, Object Detection, Benchmarking, Label Smoothing, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00257v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00257v1.pdf filename=2404.00257v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Because of its use in practice, open-world <b>object</b> <b>detection</b> (OWOD) has gotten a lot of attention recently. The challenge is how can a model detect novel classes and then incrementally learn them without forgetting previously known classes. Previous approaches hinge on strongly-supervised or <b>weakly-supervised</b> novel-class data for novel-class detection, which may not apply to real applications. We construct a new <b>benchmark</b> that novel classes are only encountered at the inference stage. And we propose a new OWOD detector YOLOOC, based on the <b>YOLO</b> architecture yet for the Open-Class setup. We introduce <b>label</b> <b>smoothing</b> to prevent the detector from over-confidently mapping novel classes to known classes and to discover novel classes. Extensive experiments conducted on our more realistic setup demonstrate the effectiveness of our method for discovering novel classes in our new <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=835--66145-st-llm-large-language-models-are-effective-temporal-learners-ruyang-liu-et-al-2024>(8/35 | 66/145) ST-LLM: Large Language Models Are Effective Temporal Learners (Ruyang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, Ge Li. (2024)<br><strong>ST-LLM: Large Language Models Are Effective Temporal Learners</strong><br><button class=copy-to-clipboard title="ST-LLM: Large Language Models Are Effective Temporal Learners" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Dialogue System, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00308v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00308v1.pdf filename=2404.00308v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have showcased impressive capabilities in text comprehension and generation, <b>prompting</b> research efforts towards video <b>LLMs</b> to facilitate human-AI interaction at the video level. However, how to effectively encode and understand videos in video-based <b>dialogue</b> <b>systems</b> remains to be solved. In this paper, we investigate a straightforward yet unexplored question: Can we feed all spatial-temporal tokens into the <b>LLM,</b> thus delegating the task of video sequence modeling to the LLMs? Surprisingly, this simple approach yields significant improvements in video understanding. Based upon this, we propose ST-LLM, an effective video-LLM baseline with Spatial-Temporal sequence modeling inside <b>LLM.</b> Furthermore, to address the overhead and stability issues introduced by uncompressed video tokens within <b>LLMs,</b> we develop a dynamic masking strategy with tailor-made training objectives. For particularly long videos, we have also designed a global-local input module to balance efficiency and effectiveness. Consequently, we harness <b>LLM</b> for proficient spatial-temporal modeling, while upholding efficiency and stability. Extensive experimental results attest to the effectiveness of our method. Through a more concise model and training pipeline, ST-LLM establishes a new state-of-the-art result on VideoChatGPT-Bench and MVBench. Codes have been available at <a href=https://github.com/TencentARC/ST-LLM>https://github.com/TencentARC/ST-LLM</a>.</p></p class="citation"></blockquote><h3 id=935--67145-magritte-manipulative-and-generative-3d-realization-from-image-topview-and-text-takayuki-hara-et-al-2024>(9/35 | 67/145) MaGRITTe: Manipulative and Generative 3D Realization from Image, Topview and Text (Takayuki Hara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Takayuki Hara, Tatsuya Harada. (2024)<br><strong>MaGRITTe: Manipulative and Generative 3D Realization from Image, Topview and Text</strong><br><button class=copy-to-clipboard title="MaGRITTe: Manipulative and Generative 3D Realization from Image, Topview and Text" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00345v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00345v1.pdf filename=2404.00345v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The generation of 3D scenes from user-specified conditions offers a promising avenue for alleviating the production burden in 3D applications. Previous studies required significant effort to realize the desired scene, owing to limited control conditions. We propose a method for controlling and generating 3D scenes under <b>multimodal</b> conditions using partial images, layout information represented in the top view, and text <b>prompts.</b> Combining these conditions to generate a 3D scene involves the following significant difficulties: (1) the creation of large datasets, (2) reflection on the interaction of <b>multimodal</b> conditions, and (3) domain dependence of the layout conditions. We decompose the process of 3D scene generation into 2D image generation from the given conditions and 3D scene generation from 2D images. 2D image generation is achieved by <b>fine-tuning</b> a pretrained <b>text-to-image</b> model with a small artificial dataset of partial images and layouts, and 3D scene generation is achieved by layout-conditioned depth estimation and neural radiance fields (NeRF), thereby avoiding the creation of large datasets. The use of a common representation of spatial information using 360-degree images allows for the consideration of <b>multimodal</b> condition interactions and reduces the domain dependence of the layout control. The experimental results qualitatively and quantitatively demonstrated that the proposed method can generate 3D scenes in diverse domains, from indoor to outdoor, according to <b>multimodal</b> conditions.</p></p class="citation"></blockquote><h3 id=1035--68145-constrained-layout-generation-with-factor-graphs-mohammed-haroon-dupty-et-al-2024>(10/35 | 68/145) Constrained Layout Generation with Factor Graphs (Mohammed Haroon Dupty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammed Haroon Dupty, Yanfei Dong, Sicong Leng, Guoji Fu, Yong Liang Goh, Wei Lu, Wee Sun Lee. (2024)<br><strong>Constrained Layout Generation with Factor Graphs</strong><br><button class=copy-to-clipboard title="Constrained Layout Generation with Factor Graphs" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Message-Passing, Graph, Graph Neural Network, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00385v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00385v1.pdf filename=2404.00385v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the challenge of object-centric layout generation under spatial constraints, seen in multiple domains including floorplan design process. The design process typically involves specifying a set of spatial constraints that include object attributes like size and inter-object relations such as relative positioning. Existing works, which typically represent objects as single nodes, lack the granularity to accurately model complex interactions between objects. For instance, often only certain parts of an object, like a room&rsquo;s right wall, interact with adjacent objects. To address this gap, we introduce a factor <b>graph</b> <b>based</b> <b>approach</b> with four latent variable nodes for each room, and a factor node for each constraint. The factor nodes represent dependencies among the variables to which they are connected, effectively capturing constraints that are potentially of a higher order. We then develop <b>message-passing</b> on the bipartite <b>graph,</b> <b>forming</b> <b>a</b> factor <b>graph</b> <b>neural</b> <b>network</b> that is trained to produce a floorplan that aligns with the desired requirements. Our approach is simple and generates layouts faithful to the user requirements, demonstrated by a large improvement in IOU scores over existing methods. Additionally, our approach, being inferential and accurate, is well-suited to the practical <b>human-in-the-loop</b> design process where specifications evolve iteratively, offering a practical and powerful tool for AI-guided design.</p></p class="citation"></blockquote><h3 id=1135--69145-dhr-dual-features-driven-hierarchical-rebalancing-in-inter--and-intra-class-regions-for-weakly-supervised-semantic-segmentation-sanghyun-jo-et-al-2024>(11/35 | 69/145) DHR: Dual Features-Driven Hierarchical Rebalancing in Inter- and Intra-Class Regions for Weakly-Supervised Semantic Segmentation (Sanghyun Jo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanghyun Jo, Fei Pan, In-Jae Yu, Kyungsu Kim. (2024)<br><strong>DHR: Dual Features-Driven Hierarchical Rebalancing in Inter- and Intra-Class Regions for Weakly-Supervised Semantic Segmentation</strong><br><button class=copy-to-clipboard title="DHR: Dual Features-Driven Hierarchical Rebalancing in Inter- and Intra-Class Regions for Weakly-Supervised Semantic Segmentation" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Supervised Learning, Unsupervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00380v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00380v1.pdf filename=2404.00380v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Weakly-supervised</b> semantic segmentation (WSS) ensures high-quality segmentation with limited data and excels when employed as input seed masks for large-scale vision models such as Segment Anything. However, WSS faces challenges related to minor classes since those are overlooked in images with adjacent multiple classes, a limitation originating from the overfitting of traditional expansion methods like Random Walk. We first address this by employing <b>unsupervised</b> and <b>weakly-supervised</b> feature maps instead of conventional methodologies, allowing for hierarchical mask enhancement. This method distinctly categorizes higher-level classes and subsequently separates their associated lower-level classes, ensuring all classes are correctly restored in the mask without losing minor ones. Our approach, validated through extensive experimentation, significantly improves WSS across five <b>benchmarks</b> (VOC: 79.8%, COCO: 53.9%, Context: 49.0%, ADE: 32.9%, Stuff: 37.4%), reducing the gap with fully <b>supervised</b> methods by over 84% on the VOC validation set. Code is available at <a href=https://github.com/shjo-april/DHR>https://github.com/shjo-april/DHR</a>.</p></p class="citation"></blockquote><h3 id=1235--70145-bayesian-exploration-of-pre-trained-models-for-low-shot-image-classification-yibo-miao-et-al-2024>(12/35 | 70/145) Bayesian Exploration of Pre-trained Models for Low-shot Image Classification (Yibo Miao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yibo Miao, Yu Lei, Feng Zhou, Zhijie Deng. (2024)<br><strong>Bayesian Exploration of Pre-trained Models for Low-shot Image Classification</strong><br><button class=copy-to-clipboard title="Bayesian Exploration of Pre-trained Models for Low-shot Image Classification" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Out-of-distribution, Probabilistic Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00312v1.pdf filename=2404.00312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low-shot image classification is a fundamental task in computer vision, and the emergence of large-scale <b>vision-language</b> models such as CLIP has greatly advanced the forefront of research in this field. However, most existing CLIP-based methods lack the flexibility to effectively incorporate other pre-trained models that encompass knowledge distinct from CLIP. To bridge the gap, this work proposes a simple and effective <b>probabilistic</b> <b>model</b> ensemble framework based on Gaussian processes, which have previously demonstrated remarkable efficacy in processing small data. We achieve the integration of prior knowledge by specifying the mean function with CLIP and the kernel function with an ensemble of deep kernels built upon various pre-trained models. By regressing the classification label directly, our framework enables analytical inference, straightforward uncertainty quantification, and principled hyper-parameter tuning. Through extensive experiments on standard <b>benchmarks,</b> we demonstrate that our method consistently outperforms competitive ensemble baselines regarding predictive performance. Additionally, we assess the robustness of our method and the quality of the yielded uncertainty estimates on <b>out-of-distribution</b> datasets. We also illustrate that our method, despite relying on label regression, still enjoys superior model calibration compared to most deterministic baselines.</p></p class="citation"></blockquote><h3 id=1335--71145-seeing-the-unseen-a-frequency-prompt-guided-transformer-for-image-restoration-shihao-zhou-et-al-2024>(13/35 | 71/145) Seeing the Unseen: A Frequency Prompt Guided Transformer for Image Restoration (Shihao Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shihao Zhou, Jinshan Pan, Jinglei Shi, Duosheng Chen, Lishen Qu, Jufeng Yang. (2024)<br><strong>Seeing the Unseen: A Frequency Prompt Guided Transformer for Image Restoration</strong><br><button class=copy-to-clipboard title="Seeing the Unseen: A Frequency Prompt Guided Transformer for Image Restoration" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Graph Attention Networks, Benchmarking, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00288v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00288v1.pdf filename=2404.00288v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How to explore useful features from images as <b>prompts</b> to guide the deep image restoration models is an effective way to solve image restoration. In contrast to mining spatial relations within images as <b>prompt,</b> which leads to characteristics of different frequencies being neglected and further remaining subtle or undetectable artifacts in the restored image, we develop a Frequency <b>Prompting</b> image restoration method, dubbed FPro, which can effectively provide <b>prompt</b> components from a frequency perspective to guild the restoration model address these differences. Specifically, we first decompose input features into separate frequency parts via dynamically learned filters, where we introduce a <b>gating</b> mechanism for suppressing the less informative elements within the kernels. To propagate useful frequency information as <b>prompt,</b> we then propose a dual <b>prompt</b> block, consisting of a low-frequency <b>prompt</b> modulator (LPM) and a high-frequency <b>prompt</b> modulator (HPM), to handle signals from different bands respectively. Each modulator contains a generation process to incorporate <b>prompting</b> components into the extracted frequency maps, and a modulation part that modifies the <b>prompt</b> feature with the guidance of the decoder features. Experimental results on commonly used <b>benchmarks</b> have demonstrated the favorable performance of our pipeline against SOTA methods on 5 image restoration tasks, including deraining, deraindrop, demoir'eing, deblurring, and dehazing. The source code and pre-trained models will be available at <a href=https://github.com/joshyZhou/FPro>https://github.com/joshyZhou/FPro</a>.</p></p class="citation"></blockquote><h3 id=1435--72145-towards-variable-and-coordinated-holistic-co-speech-motion-generation-yifei-liu-et-al-2024>(14/35 | 72/145) Towards Variable and Coordinated Holistic Co-Speech Motion Generation (Yifei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Liu, Qiong Cao, Yandong Wen, Huaiguang Jiang, Changxing Ding. (2024)<br><strong>Towards Variable and Coordinated Holistic Co-Speech Motion Generation</strong><br><button class=copy-to-clipboard title="Towards Variable and Coordinated Holistic Co-Speech Motion Generation" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Autoencoder, Quantization, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00368v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00368v1.pdf filename=2404.00368v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the problem of generating lifelike holistic co-speech motions for 3D avatars, focusing on two key aspects: variability and coordination. Variability allows the avatar to exhibit a wide range of motions even with similar speech content, while coordination ensures a harmonious alignment among facial expressions, hand gestures, and body poses. We aim to achieve both with ProbTalk, a unified probabilistic framework designed to jointly model facial, hand, and body movements in speech. ProbTalk builds on the <b>variational</b> <b>autoencoder</b> (VAE) architecture and incorporates three core designs. First, we introduce product <b>quantization</b> (PQ) to the VAE, which enriches the representation of complex holistic motion. Second, we devise a novel non-autoregressive model that embeds 2D positional encoding into the product-quantized representation, thereby preserving essential structure information of the PQ codes. Last, we employ a secondary stage to refine the preliminary prediction, further sharpening the high-frequency details. Coupling these three designs enables ProbTalk to generate natural and diverse holistic co-speech motions, outperforming several state-of-the-art methods in qualitative and quantitative evaluations, particularly in terms of realism. Our code and model will be released for research purposes at <a href=https://feifeifeiliu.github.io/probtalk/>https://feifeifeiliu.github.io/probtalk/</a>.</p></p class="citation"></blockquote><h3 id=1535--73145-look-around-before-you-leap-high-frequency-injected-transformer-for-image-restoration-shihao-zhou-et-al-2024>(15/35 | 73/145) Look-Around Before You Leap: High-Frequency Injected Transformer for Image Restoration (Shihao Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shihao Zhou, Duosheng Chen, Jinshan Pan, Jufeng Yang. (2024)<br><strong>Look-Around Before You Leap: High-Frequency Injected Transformer for Image Restoration</strong><br><button class=copy-to-clipboard title="Look-Around Before You Leap: High-Frequency Injected Transformer for Image Restoration" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00279v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00279v1.pdf filename=2404.00279v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> approaches have achieved superior performance in image restoration, since they can model long-term dependencies well. However, the limitation in capturing local information restricts their capacity to remove degradations. While existing approaches attempt to mitigate this issue by incorporating <b>convolutional</b> operations, the core component in <b>Transformer,</b> i.e., <b>self-attention,</b> which serves as a low-pass filter, could unintentionally dilute or even eliminate the acquired local patterns. In this paper, we propose HIT, a simple yet effective High-frequency Injected <b>Transformer</b> for image restoration. Specifically, we design a window-wise injection module (WIM), which incorporates abundant high-frequency details into the feature map, to provide reliable references for restoring high-quality images. We also develop a bidirectional interaction module (BIM) to aggregate features at different scales using a mutually reinforced paradigm, resulting in spatially and contextually improved representations. In addition, we introduce a spatial enhancement unit (SEU) to preserve essential spatial relationships that may be lost due to the computations carried out across channel dimensions in the BIM. Extensive experiments on 9 tasks (real noise, real rain streak, raindrop, motion blur, moir'e, shadow, snow, haze, and low-light condition) demonstrate that HIT with linear computational complexity performs favorably against the state-of-the-art methods. The source code and pre-trained models will be available at <a href=https://github.com/joshyZhou/HIT>https://github.com/joshyZhou/HIT</a>.</p></p class="citation"></blockquote><h3 id=1635--74145-exploiting-self-supervised-constraints-in-image-super-resolution-gang-wu-et-al-2024>(16/35 | 74/145) Exploiting Self-Supervised Constraints in Image Super-Resolution (Gang Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gang Wu, Junjun Jiang, Kui Jiang, Xianming Liu. (2024)<br><strong>Exploiting Self-Supervised Constraints in Image Super-Resolution</strong><br><button class=copy-to-clipboard title="Exploiting Self-Supervised Constraints in Image Super-Resolution" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 23<br>Keywords: Benchmarking, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00260v1.pdf filename=2404.00260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>self-supervised</b> <b>learning,</b> predominantly studied in high-level visual tasks, have been explored in low-level image processing. This paper introduces a novel <b>self-supervised</b> <b>constraint</b> for single image super-resolution, termed SSC-SR. SSC-SR uniquely addresses the divergence in image complexity by employing a dual asymmetric paradigm and a target model updated via exponential moving average to enhance stability. The proposed SSC-SR framework works as a plug-and-play paradigm and can be easily applied to existing SR models. Empirical evaluations reveal that our SSC-SR framework delivers substantial enhancements on a variety of <b>benchmark</b> datasets, achieving an average increase of 0.1 dB over EDSR and 0.06 dB over SwinIR. In addition, extensive ablation studies corroborate the effectiveness of each constituent in our SSC-SR framework. Codes are available at <a href=https://github.com/Aitical/SSCSR>https://github.com/Aitical/SSCSR</a>.</p></p class="citation"></blockquote><h3 id=1735--75145-svgcraft-beyond-single-object-text-to-svg-synthesis-with-comprehensive-canvas-layout-ayan-banerjee-et-al-2024>(17/35 | 75/145) SVGCraft: Beyond Single Object Text-to-SVG Synthesis with Comprehensive Canvas Layout (Ayan Banerjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayan Banerjee, Nityanand Mathur, Josep Lladós, Umapada Pal, Anjan Dutta. (2024)<br><strong>SVGCraft: Beyond Single Object Text-to-SVG Synthesis with Comprehensive Canvas Layout</strong><br><button class=copy-to-clipboard title="SVGCraft: Beyond Single Object Text-to-SVG Synthesis with Comprehensive Canvas Layout" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00412v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00412v1.pdf filename=2404.00412v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating VectorArt from text <b>prompts</b> is a challenging vision task, requiring diverse yet realistic depictions of the seen as well as unseen entities. However, existing research has been mostly limited to the generation of single objects, rather than comprehensive scenes comprising multiple elements. In response, this work introduces SVGCraft, a novel end-to-end framework for the creation of vector graphics depicting entire scenes from textual descriptions. Utilizing a pre-trained <b>LLM</b> for layout generation from text <b>prompts,</b> this framework introduces a technique for producing masked latents in specified bounding boxes for accurate object placement. It introduces a fusion mechanism for integrating attention maps and employs a diffusion U-Net for coherent composition, speeding up the drawing process. The resulting SVG is optimized using a pre-trained encoder and LPIPS loss with opacity modulation to maximize similarity. Additionally, this work explores the potential of primitive shapes in facilitating canvas completion in constrained environments. Through both qualitative and quantitative assessments, SVGCraft is demonstrated to surpass prior works in abstraction, recognizability, and detail, as evidenced by its performance metrics (CLIP-T: 0.4563, Cosine Similarity: 0.6342, Confusion: 0.66, Aesthetic: 6.7832). The code will be available at <a href=https://github.com/ayanban011/SVGCraft>https://github.com/ayanban011/SVGCraft</a>.</p></p class="citation"></blockquote><h3 id=1835--76145-spread-your-wings-a-radial-strip-transformer-for-image-deblurring-duosheng-chen-et-al-2024>(18/35 | 76/145) Spread Your Wings: A Radial Strip Transformer for Image Deblurring (Duosheng Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Duosheng Chen, Shihao Zhou, Jinshan Pan, Jinglei Shi, Lishen Qu, Jufeng Yang. (2024)<br><strong>Spread Your Wings: A Radial Strip Transformer for Image Deblurring</strong><br><button class=copy-to-clipboard title="Spread Your Wings: A Radial Strip Transformer for Image Deblurring" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00358v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00358v1.pdf filename=2404.00358v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exploring motion information is important for the motion deblurring task. Recent the window-based <b>transformer</b> approaches have achieved decent performance in image deblurring. Note that the motion causing blurry results is usually composed of translation and rotation movements and the window-shift operation in the Cartesian coordinate system by the window-based <b>transformer</b> approaches only directly explores translation motion in orthogonal directions. Thus, these methods have the limitation of modeling the rotation part. To alleviate this problem, we introduce the polar coordinate-based <b>transformer,</b> which has the angles and distance to explore rotation motion and translation information together. In this paper, we propose a Radial Strip <b>Transformer</b> (RST), which is a <b>transformer-based</b> architecture that restores the blur images in a polar coordinate system instead of a Cartesian one. RST contains a dynamic radial embedding module (DRE) to extract the shallow feature by a radial deformable <b>convolution.</b> We design a polar mask layer to generate the offsets for the deformable <b>convolution,</b> which can reshape the <b>convolution</b> kernel along the radius to better capture the rotation motion information. Furthermore, we proposed a radial strip attention solver (RSAS) as deep feature extraction, where the relationship of windows is organized by azimuth and radius. This attention module contains radial strip windows to reweight image features in the polar coordinate, which preserves more useful information in rotation and translation motion together for better recovering the sharp images. Experimental results on six synthesis and real-world datasets prove that our method performs favorably against other SOTA methods for the image deblurring task.</p></p class="citation"></blockquote><h3 id=1935--77145-learing-trimaps-via-clicks-for-image-matting-chenyi-zhang-et-al-2024>(19/35 | 77/145) Learing Trimaps via Clicks for Image Matting (Chenyi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenyi Zhang, Yihan Hu, Henghui Ding, Humphrey Shi, Yao Zhao, Yunchao Wei. (2024)<br><strong>Learing Trimaps via Clicks for Image Matting</strong><br><button class=copy-to-clipboard title="Learing Trimaps via Clicks for Image Matting" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00335v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00335v1.pdf filename=2404.00335v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite significant advancements in image matting, existing models heavily depend on manually-drawn trimaps for accurate results in natural image scenarios. However, the process of obtaining trimaps is time-consuming, lacking user-friendliness and device compatibility. This reliance greatly limits the practical application of all trimap-based matting methods. To address this issue, we introduce Click2Trimap, an interactive model capable of predicting high-quality trimaps and alpha mattes with minimal user click inputs. Through analyzing real users&rsquo; behavioral logic and characteristics of trimaps, we successfully propose a powerful iterative three-class training strategy and a dedicated <b>simulation</b> function, making Click2Trimap exhibit versatility across various scenarios. Quantitative and qualitative assessments on synthetic and real-world matting datasets demonstrate Click2Trimap&rsquo;s superior performance compared to all existing trimap-free matting methods. Especially, in the user study, Click2Trimap achieves high-quality trimap and matting predictions in just an average of 5 seconds per image, demonstrating its substantial practical value in real-world applications.</p></p class="citation"></blockquote><h3 id=2035--78145-grid-diffusion-models-for-text-to-video-generation-taegyeong-lee-et-al-2024>(20/35 | 78/145) Grid Diffusion Models for Text-to-Video Generation (Taegyeong Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taegyeong Lee, Soyeong Kwon, Taehwan Kim. (2024)<br><strong>Grid Diffusion Models for Text-to-Video Generation</strong><br><button class=copy-to-clipboard title="Grid Diffusion Models for Text-to-Video Generation" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00234v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00234v1.pdf filename=2404.00234v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in the <b>diffusion</b> <b>models</b> have significantly improved <b>text-to-image</b> generation. However, generating videos from text is a more challenging task than generating images from text, due to the much larger dataset and higher computational cost required. Most existing video generation methods use either a 3D U-Net architecture that considers the temporal dimension or autoregressive generation. These methods require large datasets and are limited in terms of computational costs compared to <b>text-to-image</b> generation. To tackle these challenges, we propose a simple but effective novel grid <b>diffusion</b> <b>for</b> text-to-video generation without temporal dimension in architecture and a large text-video paired dataset. We can generate a high-quality video using a fixed amount of GPU memory regardless of the number of frames by representing the video as a grid image. Additionally, since our method reduces the dimensions of the video to the dimensions of the image, various image-based methods can be applied to videos, such as text-guided video manipulation from image manipulation. Our proposed method outperforms the existing methods in both quantitative and qualitative evaluations, demonstrating the suitability of our model for real-world video generation.</p></p class="citation"></blockquote><h3 id=2135--79145-stba-towards-evaluating-the-robustness-of-dnns-for-query-limited-black-box-scenario-renyang-liu-et-al-2024>(21/35 | 79/145) STBA: Towards Evaluating the Robustness of DNNs for Query-Limited Black-box Scenario (Renyang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renyang Liu, Kwok-Yan Lam, Wei Zhou, Sixing Wu, Jun Zhao, Dongting Hu, Mingming Gong. (2024)<br><strong>STBA: Towards Evaluating the Robustness of DNNs for Query-Limited Black-box Scenario</strong><br><button class=copy-to-clipboard title="STBA: Towards Evaluating the Robustness of DNNs for Query-Limited Black-box Scenario" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 15<br>Keywords: Adversarial Learning, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00362v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00362v1.pdf filename=2404.00362v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many attack techniques have been proposed to explore the vulnerability of DNNs and further help to improve their robustness. Despite the significant progress made recently, existing <b>black-box</b> <b>attack</b> methods still suffer from unsatisfactory performance due to the vast number of queries needed to optimize desired perturbations. Besides, the other critical challenge is that <b>adversarial</b> <b>examples</b> built in a noise-adding manner are abnormal and struggle to successfully attack robust models, whose robustness is enhanced by <b>adversarial</b> <b>training</b> against small perturbations. There is no doubt that these two issues mentioned above will significantly increase the risk of exposure and result in a failure to dig deeply into the vulnerability of DNNs. Hence, it is necessary to evaluate DNNs&rsquo; fragility sufficiently under query-limited settings in a non-additional way. In this paper, we propose the Spatial Transform <b>Black-box</b> <b>Attack</b> (STBA), a novel framework to craft formidable <b>adversarial</b> <b>examples</b> in the query-limited scenario. Specifically, STBA introduces a flow field to the high-frequency part of clean images to generate <b>adversarial</b> <b>examples</b> and adopts the following two processes to enhance their naturalness and significantly improve the query efficiency: a) we apply an estimated flow field to the high-frequency part of clean images to generate <b>adversarial</b> <b>examples</b> instead of introducing external noise to the benign image, and b) we leverage an efficient gradient estimation method based on a batch of samples to optimize such an ideal flow field under query-limited settings. Compared to existing score-based <b>black-box</b> <b>baselines,</b> extensive experiments indicated that STBA could effectively improve the imperceptibility of the <b>adversarial</b> <b>examples</b> and remarkably boost the attack success rate under query-limited settings.</p></p class="citation"></blockquote><h3 id=2235--80145-rethinking-attention-based-multiple-instance-learning-for-whole-slide-pathological-image-classification-an-instance-attribute-viewpoint-linghan-cai-et-al-2024>(22/35 | 80/145) Rethinking Attention-Based Multiple Instance Learning for Whole-Slide Pathological Image Classification: An Instance Attribute Viewpoint (Linghan Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linghan Cai, Shenjin Huang, Ye Zhang, Jinpeng Lu, Yongbing Zhang. (2024)<br><strong>Rethinking Attention-Based Multiple Instance Learning for Whole-Slide Pathological Image Classification: An Instance Attribute Viewpoint</strong><br><button class=copy-to-clipboard title="Rethinking Attention-Based Multiple Instance Learning for Whole-Slide Pathological Image Classification: An Instance Attribute Viewpoint" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Multiple Instance Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00351v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00351v1.pdf filename=2404.00351v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multiple</b> <b>instance</b> <b>learning</b> (MIL) is a robust paradigm for whole-slide pathological image (WSI) analysis, processing gigapixel-resolution images with slide-level labels. As pioneering efforts, attention-based MIL (ABMIL) and its variants are increasingly becoming popular due to the characteristics of simultaneously handling clinical diagnosis and tumor localization. However, the attention mechanism exhibits limitations in discriminating between instances, which often misclassifies tissues and potentially impairs MIL performance. This paper proposes an Attribute-Driven MIL (AttriMIL) framework to address these issues. Concretely, we dissect the calculation process of ABMIL and present an attribute scoring mechanism that measures the contribution of each instance to bag prediction effectively, quantifying instance attributes. Based on attribute quantification, we develop a spatial attribute constraint and an attribute ranking constraint to model instance correlations within and across slides, respectively. These constraints encourage the network to capture the spatial correlation and semantic similarity of instances, improving the ability of AttriMIL to distinguish tissue types and identify challenging instances. Additionally, AttriMIL employs a histopathology adaptive backbone that maximizes the pre-trained model&rsquo;s feature extraction capability for collecting pathological features. Extensive experiments on three public <b>benchmarks</b> demonstrate that our AttriMIL outperforms existing state-of-the-art frameworks across <b>multiple</b> <b>evaluation</b> <b>metrics.</b> The implementation code is available at <a href=https://github.com/MedCAI/AttriMIL>https://github.com/MedCAI/AttriMIL</a>.</p></p class="citation"></blockquote><h3 id=2335--81145-image-to-image-matching-via-foundation-models-a-new-perspective-for-open-vocabulary-semantic-segmentation-yuan-wang-et-al-2024>(23/35 | 81/145) Image-to-Image Matching via Foundation Models: A New Perspective for Open-Vocabulary Semantic Segmentation (Yuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Wang, Rui Sun, Naisong Luo, Yuwen Pan, Tianzhu Zhang. (2024)<br><strong>Image-to-Image Matching via Foundation Models: A New Perspective for Open-Vocabulary Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Image-to-Image Matching via Foundation Models: A New Perspective for Open-Vocabulary Semantic Segmentation" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00262v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00262v1.pdf filename=2404.00262v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-vocabulary semantic segmentation (OVS) aims to segment images of arbitrary categories specified by class labels or captions. However, most previous best-performing methods, whether pixel grouping methods or region recognition methods, suffer from false matches between image features and category labels. We attribute this to the natural gap between the textual features and visual features. In this work, we rethink how to mitigate false matches from the perspective of image-to-image matching and propose a novel relation-aware intra-modal matching (RIM) framework for OVS based on visual <b>foundation</b> <b>models.</b> RIM achieves robust region classification by firstly constructing diverse image-modal reference features and then matching them with region features based on relation-aware ranking distribution. The proposed RIM enjoys several merits. First, the intra-modal reference features are better aligned, circumventing potential ambiguities that may arise in cross-modal matching. Second, the ranking-based matching process harnesses the structure information implicit in the inter-class relationships, making it more robust than comparing individually. Extensive experiments on three <b>benchmarks</b> demonstrate that RIM outperforms previous state-of-the-art methods by large margins, obtaining a lead of more than 10% in mIoU on PASCAL VOC <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=2435--82145-denoising-monte-carlo-renders-with-diffusion-models-vaibhav-vavilala-et-al-2024>(24/35 | 82/145) Denoising Monte Carlo Renders With Diffusion Models (Vaibhav Vavilala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vaibhav Vavilala, Rahul Vasanth, David Forsyth. (2024)<br><strong>Denoising Monte Carlo Renders With Diffusion Models</strong><br><button class=copy-to-clipboard title="Denoising Monte Carlo Renders With Diffusion Models" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00491v1.pdf filename=2404.00491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Physically-based renderings contain Monte-Carlo noise, with variance that increases as the number of rays per pixel decreases. This noise, while zero-mean for good modern renderers, can have heavy tails (most notably, for scenes containing specular or refractive objects). Learned methods for restoring low fidelity renders are highly developed, because suppressing render noise means one can save compute and use fast renders with few rays per pixel. We demonstrate that a <b>diffusion</b> <b>model</b> can denoise low fidelity renders successfully. Furthermore, our method can be conditioned on a variety of natural render information, and this conditioning helps performance. Quantitative experiments show that our method is competitive with SOTA across a range of sampling rates, but current metrics slightly favor competitor methods. Qualitative examination of the reconstructions suggests that the metrics themselves may not be reliable. The image prior applied by a <b>diffusion</b> <b>method</b> strongly favors reconstructions that are &ldquo;like&rdquo; real images &ndash; so have straight shadow boundaries, curved specularities, no &ldquo;fireflies&rdquo; and the like &ndash; and metrics do not account for this. We show numerous examples where methods preferred by current metrics produce qualitatively weaker reconstructions than ours.</p></p class="citation"></blockquote><h3 id=2535--83145-diffhuman-probabilistic-photorealistic-3d-reconstruction-of-humans-akash-sengupta-et-al-2024>(25/35 | 83/145) DiffHuman: Probabilistic Photorealistic 3D Reconstruction of Humans (Akash Sengupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akash Sengupta, Thiemo Alldieck, Nikos Kolotouros, Enric Corona, Andrei Zanfir, Cristian Sminchisescu. (2024)<br><strong>DiffHuman: Probabilistic Photorealistic 3D Reconstruction of Humans</strong><br><button class=copy-to-clipboard title="DiffHuman: Probabilistic Photorealistic 3D Reconstruction of Humans" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00485v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00485v1.pdf filename=2404.00485v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present DiffHuman, a probabilistic method for photorealistic 3D human reconstruction from a single RGB image. Despite the ill-posed nature of this problem, most methods are deterministic and output a single solution, often resulting in a lack of geometric detail and blurriness in unseen or uncertain regions. In contrast, DiffHuman predicts a probability distribution over 3D reconstructions conditioned on an input 2D image, which allows us to sample multiple detailed 3D avatars that are consistent with the image. DiffHuman is implemented as a conditional <b>diffusion</b> <b>model</b> that denoises pixel-aligned 2D observations of an underlying 3D shape representation. During inference, we may sample 3D avatars by iteratively denoising 2D renders of the predicted 3D representation. Furthermore, we introduce a generator neural network that approximates rendering with considerably reduced runtime (55x speed up), resulting in a novel dual-branch <b>diffusion</b> <b>framework.</b> Our experiments show that DiffHuman can produce diverse and detailed reconstructions for the parts of the person that are unseen or uncertain in the input image, while remaining competitive with the state-of-the-art when reconstructing visible surfaces.</p></p class="citation"></blockquote><h3 id=2635--84145-sgdformer-one-stage-transformer-based-architecture-for-cross-spectral-stereo-image-guided-denoising-runmin-zhang-et-al-2024>(26/35 | 84/145) SGDFormer: One-stage Transformer-based Architecture for Cross-Spectral Stereo Image Guided Denoising (Runmin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runmin Zhang, Zhu Yu, Zehua Sheng, Jiacheng Ying, Si-Yuan Cao, Shu-Jie Chen, Bailin Yang, Junwei Li, Hui-Liang Shen. (2024)<br><strong>SGDFormer: One-stage Transformer-based Architecture for Cross-Spectral Stereo Image Guided Denoising</strong><br><button class=copy-to-clipboard title="SGDFormer: One-stage Transformer-based Architecture for Cross-Spectral Stereo Image Guided Denoising" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00349v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00349v1.pdf filename=2404.00349v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-spectral image guided denoising has shown its great potential in recovering clean images with rich details, such as using the near-infrared image to guide the denoising process of the visible one. To obtain such image pairs, a feasible and economical way is to employ a stereo system, which is widely used on mobile devices. Current works attempt to generate an aligned guidance image to handle the disparity between two images. However, due to occlusion, spectral differences and noise degradation, the aligned guidance image generally exists ghosting and artifacts, leading to an unsatisfactory denoised result. To address this issue, we propose a one-stage <b>transformer-based</b> architecture, named SGDFormer, for cross-spectral Stereo image Guided Denoising. The architecture integrates the correspondence modeling and feature fusion of stereo images into a unified network. Our <b>transformer</b> block contains a noise-robust cross-attention (NRCA) module and a spatially variant feature fusion (SVFF) module. The NRCA module captures the long-range correspondence of two images in a coarse-to-fine manner to alleviate the interference of noise. The SVFF module further enhances salient structures and suppresses harmful artifacts through dynamically selecting useful information. Thanks to the above design, our SGDFormer can restore artifact-free images with fine structures, and achieves state-of-the-art performance on various datasets. Additionally, our SGDFormer can be extended to handle other unaligned cross-model guided restoration tasks such as guided depth super-resolution.</p></p class="citation"></blockquote><h3 id=2735--85145-monocular-identity-conditioned-facial-reflectance-reconstruction-xingyu-ren-et-al-2024>(27/35 | 85/145) Monocular Identity-Conditioned Facial Reflectance Reconstruction (Xingyu Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyu Ren, Jiankang Deng, Yuhao Cheng, Jia Guo, Chao Ma, Yichao Yan, Wenhan Zhu, Xiaokang Yang. (2024)<br><strong>Monocular Identity-Conditioned Facial Reflectance Reconstruction</strong><br><button class=copy-to-clipboard title="Monocular Identity-Conditioned Facial Reflectance Reconstruction" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00301v1.pdf filename=2404.00301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent 3D face reconstruction methods have made remarkable advancements, yet there remain huge challenges in monocular high-quality facial reflectance reconstruction. Existing methods rely on a large amount of light-stage captured data to learn facial reflectance models. However, the lack of subject diversity poses challenges in achieving good generalization and widespread applicability. In this paper, we learn the reflectance prior in image space rather than UV space and present a framework named ID2Reflectance. Our framework can directly estimate the reflectance maps of a single image while using limited reflectance data for training. Our key insight is that reflectance data shares facial structures with RGB faces, which enables obtaining expressive facial prior from inexpensive RGB data thus reducing the dependency on reflectance data. We first learn a high-quality prior for facial reflectance. Specifically, we pretrain multi-domain facial feature codebooks and design a codebook fusion method to align the reflectance and RGB domains. Then, we propose an identity-conditioned swapping module that injects facial identity from the target image into the pre-trained <b>autoencoder</b> to modify the identity of the source reflectance image. Finally, we stitch multi-view swapped reflectance images to obtain renderable assets. Extensive experiments demonstrate that our method exhibits excellent generalization capability and achieves state-of-the-art facial reflectance reconstruction results for in-the-wild faces. Our project page is <a href=https://xingyuren.github.io/id2reflectance/>https://xingyuren.github.io/id2reflectance/</a>.</p></p class="citation"></blockquote><h3 id=2835--86145-lake-red-camouflaged-images-generation-by-latent-background-knowledge-retrieval-augmented-diffusion-pancheng-zhao-et-al-2024>(28/35 | 86/145) LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge Retrieval-Augmented Diffusion (Pancheng Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pancheng Zhao, Peng Xu, Pengda Qin, Deng-Ping Fan, Zhicheng Zhang, Guoli Jia, Bowen Zhou, Jufeng Yang. (2024)<br><strong>LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge Retrieval-Augmented Diffusion</strong><br><button class=copy-to-clipboard title="LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge Retrieval-Augmented Diffusion" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00292v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00292v1.pdf filename=2404.00292v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Camouflaged vision perception is an important vision task with numerous practical applications. Due to the expensive collection and labeling costs, this community struggles with a major bottleneck that the species category of its datasets is limited to a small number of object species. However, the existing camouflaged generation methods require specifying the background manually, thus failing to extend the camouflaged sample diversity in a low-cost manner. In this paper, we propose a Latent Background Knowledge Retrieval-Augmented Diffusion (LAKE-RED) for camouflaged image generation. To our knowledge, our contributions mainly include: (1) For the first time, we propose a camouflaged generation paradigm that does not need to receive any background inputs. (2) Our LAKE-RED is the first knowledge retrieval-augmented method with interpretability for camouflaged generation, in which we propose an idea that knowledge retrieval and <b>reasoning</b> enhancement are separated explicitly, to alleviate the task-specific challenges. Moreover, our method is not restricted to specific foreground targets or backgrounds, offering a potential for extending camouflaged vision perception to more diverse domains. (3) Experimental results demonstrate that our method outperforms the existing approaches, generating more realistic camouflage images.</p></p class="citation"></blockquote><h3 id=2935--87145-long-tailed-recognition-on-binary-networks-by-calibrating-a-pre-trained-model-jihun-kim-et-al-2024>(29/35 | 87/145) Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained Model (Jihun Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jihun Kim, Dahyun Kim, Hyungrok Jung, Taeil Oh, Jonghyun Choi. (2024)<br><strong>Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained Model</strong><br><button class=copy-to-clipboard title="Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained Model" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00285v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00285v1.pdf filename=2404.00285v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deploying deep models in real-world scenarios entails a number of challenges, including computational efficiency and real-world (e.g., long-tailed) data distributions. We address the combined challenge of learning long-tailed distributions using highly resource-efficient binary neural networks as backbones. Specifically, we propose a calibrate-and-distill framework that uses off-the-shelf pretrained full-precision models trained on balanced datasets to use as teachers for <b>distillation</b> when learning binary networks on long-tailed datasets. To better generalize to various datasets, we further propose a novel adversarial balancing among the terms in the objective function and an efficient multiresolution learning scheme. We conducted the largest empirical study in the literature using 15 datasets, including newly derived long-tailed datasets from existing balanced datasets, and show that our proposed method outperforms prior art by large margins (>14.33% on average).</p></p class="citation"></blockquote><h3 id=3035--88145-ipod-implicit-field-learning-with-point-diffusion-for-generalizable-3d-object-reconstruction-from-single-rgb-d-images-yushuang-wu-et-al-2024>(30/35 | 88/145) IPoD: Implicit Field Learning with Point Diffusion for Generalizable 3D Object Reconstruction from Single RGB-D Images (Yushuang Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yushuang Wu, Luyue Shi, Junhao Cai, Weihao Yuan, Lingteng Qiu, Zilong Dong, Liefeng Bo, Shuguang Cui, Xiaoguang Han. (2024)<br><strong>IPoD: Implicit Field Learning with Point Diffusion for Generalizable 3D Object Reconstruction from Single RGB-D Images</strong><br><button class=copy-to-clipboard title="IPoD: Implicit Field Learning with Point Diffusion for Generalizable 3D Object Reconstruction from Single RGB-D Images" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00269v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00269v1.pdf filename=2404.00269v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generalizable 3D object reconstruction from single-view RGB-D images remains a challenging task, particularly with real-world data. Current state-of-the-art methods develop <b>Transformer-based</b> implicit field learning, necessitating an intensive learning paradigm that requires dense query-supervision uniformly sampled throughout the entire space. We propose a novel approach, IPoD, which harmonizes implicit field learning with point diffusion. This approach treats the query points for implicit field learning as a noisy point cloud for iterative denoising, allowing for their dynamic adaptation to the target object shape. Such adaptive query points harness diffusion learning&rsquo;s capability for coarse shape recovery and also enhances the implicit representation&rsquo;s ability to delineate finer details. Besides, an additional self-conditioning mechanism is designed to use implicit predictions as the guidance of diffusion learning, leading to a cooperative system. Experiments conducted on the CO3D-v2 dataset affirm the superiority of IPoD, achieving 7.8% improvement in F-score and 28.6% in Chamfer distance over existing methods. The generalizability of IPoD is also demonstrated on the MVImgNet dataset. Our project page is at <a href=https://yushuang-wu.github.io/IPoD>https://yushuang-wu.github.io/IPoD</a>.</p></p class="citation"></blockquote><h3 id=3135--89145-latent-watermark-inject-and-detect-watermarks-in-latent-diffusion-space-zheling-meng-et-al-2024>(31/35 | 89/145) Latent Watermark: Inject and Detect Watermarks in Latent Diffusion Space (Zheling Meng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheling Meng, Bo Peng, Jing Dong. (2024)<br><strong>Latent Watermark: Inject and Detect Watermarks in Latent Diffusion Space</strong><br><button class=copy-to-clipboard title="Latent Watermark: Inject and Detect Watermarks in Latent Diffusion Space" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00230v1.pdf filename=2404.00230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Watermarking is a tool for actively identifying and attributing the images generated by latent <b>diffusion</b> <b>models.</b> Existing methods face the dilemma of watermark robustness and image quality. The reason for this dilemma is that watermark detection is performed in pixel space, implying an intrinsic link between image quality and watermark robustness. In this paper, we highlight that an effective solution to the problem is to both inject and detect watermarks in latent space, and propose Latent Watermark (LW) with a progressive training strategy. Experiments show that compared to the recently proposed methods such as StegaStamp, StableSignature, RoSteALS and TreeRing, LW not only surpasses them in terms of robustness but also offers superior image quality. When we inject 64-bit messages, LW can achieve an identification performance close to 100% and an attribution performance above 97% under 9 single-attack scenarios and one all-attack scenario. Our code will be available on GitHub.</p></p class="citation"></blockquote><h3 id=3235--90145-scenegraphloc-cross-modal-coarse-visual-localization-on-3d-scene-graphs-yang-miao-et-al-2024>(32/35 | 90/145) SceneGraphLoc: Cross-Modal Coarse Visual Localization on 3D Scene Graphs (Yang Miao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Miao, Francis Engelmann, Olga Vysotska, Federico Tombari, Marc Pollefeys, Dániel Béla Baráth. (2024)<br><strong>SceneGraphLoc: Cross-Modal Coarse Visual Localization on 3D Scene Graphs</strong><br><button class=copy-to-clipboard title="SceneGraphLoc: Cross-Modal Coarse Visual Localization on 3D Scene Graphs" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Graph, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00469v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00469v1.pdf filename=2404.00469v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel problem, i.e., the localization of an input image within a <b>multi-modal</b> reference map represented by a database of 3D scene <b>graphs.</b> These <b>graphs</b> comprise multiple modalities, including object-level point clouds, images, attributes, and relationships between objects, offering a lightweight and efficient alternative to conventional methods that rely on extensive image databases. Given the available modalities, the proposed method SceneGraphLoc learns a fixed-sized embedding for each node (i.e., representing an object instance) in the scene <b>graph,</b> enabling effective matching with the objects visible in the input query image. This strategy significantly outperforms other cross-modal methods, even without incorporating images into the map embeddings. When images are leveraged, SceneGraphLoc achieves performance close to that of state-of-the-art techniques depending on large image databases, while requiring three orders-of-magnitude less storage and operating orders-of-magnitude faster. The code will be made public.</p></p class="citation"></blockquote><h3 id=3335--91145-multiway-point-cloud-mosaicking-with-diffusion-and-global-optimization-shengze-jin-et-al-2024>(33/35 | 91/145) Multiway Point Cloud Mosaicking with Diffusion and Global Optimization (Shengze Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengze Jin, Iro Armeni, Marc Pollefeys, Daniel Barath. (2024)<br><strong>Multiway Point Cloud Mosaicking with Diffusion and Global Optimization</strong><br><button class=copy-to-clipboard title="Multiway Point Cloud Mosaicking with Diffusion and Global Optimization" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00429v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00429v1.pdf filename=2404.00429v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel framework for multiway point cloud mosaicking (named Wednesday), designed to co-align sets of partially overlapping point clouds &ndash; typically obtained from 3D scanners or moving RGB-D cameras &ndash; into a unified coordinate system. At the core of our approach is ODIN, a learned pairwise registration algorithm that iteratively identifies overlaps and refines attention scores, employing a diffusion-based process for denoising pairwise correlation matrices to enhance matching accuracy. Further steps include constructing a pose <b>graph</b> from all point clouds, performing rotation averaging, a novel robust algorithm for re-estimating translations optimally in terms of consensus maximization and translation optimization. Finally, the point cloud rotations and positions are optimized jointly by a diffusion-based approach. Tested on four diverse, large-scale datasets, our method achieves state-of-the-art pairwise and multiway registration results by a large margin on all <b>benchmarks.</b> Our code and models are available at <a href=https://github.com/jinsz/Multiway-Point-Cloud-Mosaicking-with-Diffusion-and-Global-Optimization>https://github.com/jinsz/Multiway-Point-Cloud-Mosaicking-with-Diffusion-and-Global-Optimization</a>.</p></p class="citation"></blockquote><h3 id=3435--92145-attention-based-shape-deformation-networks-for-artifact-free-geometry-reconstruction-of-lumbar-spine-from-mr-images-linchen-qian-et-al-2024>(34/35 | 92/145) Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images (Linchen Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linchen Qian, Jiasong Chen, Linhai Ma, Timur Urakov, Weiyong Gu, Liang Liang. (2024)<br><strong>Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images</strong><br><button class=copy-to-clipboard title="Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00231v1.pdf filename=2404.00231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lumbar disc degeneration, a progressive structural wear and tear of lumbar intervertebral disc, is regarded as an essential role on low back pain, a significant global health concern. Automated lumbar spine <b>geometry</b> reconstruction from MR images will enable fast measurement of medical parameters to evaluate the lumbar status, in order to determine a suitable treatment. Existing image segmentation-based techniques often generate erroneous segments or unstructured point clouds, unsuitable for medical parameter measurement. In this work, we present TransDeformer: a novel attention-based deep learning approach that reconstructs the contours of the lumbar spine with high spatial accuracy and mesh correspondence across patients, and we also present a variant of TransDeformer for error estimation. Specially, we devise new attention modules with a new attention formula, which integrates image features and tokenized contour features to predict the displacements of the points on a shape template without the need for image segmentation. The deformed template reveals the lumbar spine <b>geometry</b> in the input image. We develop a multi-stage training strategy to enhance model robustness with respect to template initialization. Experiment results show that our TransDeformer generates artifact-free <b>geometry</b> outputs, and its variant predicts the error of a reconstructed <b>geometry.</b> Our code is available at <a href=https://github.com/linchenq/TransDeformer-Mesh>https://github.com/linchenq/TransDeformer-Mesh</a>.</p></p class="citation"></blockquote><h3 id=3535--93145-instrument-tissue-interaction-detection-framework-for-surgical-video-understanding-wenjun-lin-et-al-2024>(35/35 | 93/145) Instrument-tissue Interaction Detection Framework for Surgical Video Understanding (Wenjun Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenjun Lin, Yan Hu, Huazhu Fu, Mingming Yang, Chin-Boon Chng, Ryo Kawasaki, Cheekong Chui, Jiang Liu. (2024)<br><strong>Instrument-tissue Interaction Detection Framework for Surgical Video Understanding</strong><br><button class=copy-to-clipboard title="Instrument-tissue Interaction Detection Framework for Surgical Video Understanding" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00322v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00322v1.pdf filename=2404.00322v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Instrument-tissue interaction detection task, which helps understand surgical activities, is vital for constructing computer-assisted surgery systems but with many challenges. Firstly, most models represent instrument-tissue interaction in a coarse-grained way which only focuses on classification and lacks the ability to automatically detect instruments and tissues. Secondly, existing works do not fully consider relations between intra- and inter-frame of instruments and tissues. In the paper, we propose to represent instrument-tissue interaction as &lt;instrument class, instrument bounding box, tissue class, tissue bounding box, action class> quintuple and present an Instrument-Tissue Interaction Detection Network (ITIDNet) to detect the quintuple for surgery videos understanding. Specifically, we propose a Snippet Consecutive Feature (SCF) Layer to enhance features by modeling relationships of proposals in the current frame using global context information in the video snippet. We also propose a Spatial Corresponding Attention (SCA) Layer to incorporate features of proposals between adjacent frames through spatial encoding. To reason relationships between instruments and tissues, a Temporal <b>Graph</b> (TG) Layer is proposed with intra-frame connections to exploit relationships between instruments and tissues in the same frame and inter-frame connections to model the temporal information for the same instance. For evaluation, we build a cataract surgery video (PhacoQ) dataset and a cholecystectomy surgery video (CholecQ) dataset. Experimental results demonstrate the promising performance of our model, which outperforms other state-of-the-art models on both datasets.</p></p class="citation"></blockquote><h2 id=csse-3>cs.SE (3)</h2><h3 id=13--94145-an-empirical-study-of-automated-vulnerability-localization-with-large-language-models-jian-zhang-et-al-2024>(1/3 | 94/145) An Empirical Study of Automated Vulnerability Localization with Large Language Models (Jian Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jian Zhang, Chong Wang, Anran Li, Weisong Sun, Cen Zhang, Wei Ma, Yang Liu. (2024)<br><strong>An Empirical Study of Automated Vulnerability Localization with Large Language Models</strong><br><button class=copy-to-clipboard title="An Empirical Study of Automated Vulnerability Localization with Large Language Models" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CR, cs-SE, cs.SE<br>Keyword Score: 60<br>Keywords: Fine-tuning, Zero-shot, ChatGPT, Large Language Model, Large Language Model, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00287v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00287v1.pdf filename=2404.00287v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, Automated Vulnerability Localization (AVL) has attracted much attention, aiming to facilitate diagnosis by pinpointing the lines of code responsible for discovered vulnerabilities. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown potential in various domains, yet their effectiveness in vulnerability localization remains underexplored. In this work, we perform the first comprehensive study of <b>LLMs</b> for AVL. Our investigation encompasses 10+ leading <b>LLMs</b> suitable for code analysis, including <b>ChatGPT</b> and various open-source models, across three architectural types: encoder-only, encoder-decoder, and decoder-only, with model sizes ranging from 60M to 16B parameters. We explore the efficacy of these <b>LLMs</b> using 4 distinct paradigms: <b>zero-shot</b> <b>learning,</b> one-shot learning, discriminative <b>fine-tuning,</b> and generative <b>fine-tuning.</b> Our evaluation framework is applied to the BigVul-based dataset for C/C++, and an additional dataset comprising smart contract vulnerabilities. The results demonstrate that discriminative <b>fine-tuning</b> of <b>LLMs</b> can significantly outperform existing learning-based methods for AVL, while other paradigms prove less effective or unexpectedly ineffective for the task. We also identify challenges related to input length and unidirectional context in <b>fine-tuning</b> processes for encoders and decoders. We then introduce two remedial strategies: the sliding window and the right-forward embedding, both of which substantially enhance performance. Furthermore, our findings highlight certain generalization capabilities of <b>LLMs</b> across Common Weakness Enumerations (CWEs) and different projects, indicating a promising pathway toward their practical application in vulnerability localization.</p></p class="citation"></blockquote><h3 id=23--95145-a-survey-of-using-large-language-models-for-generating-infrastructure-as-code-kalahasti-ganesh-srivatsa-et-al-2024>(2/3 | 95/145) A Survey of using Large Language Models for Generating Infrastructure as Code (Kalahasti Ganesh Srivatsa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kalahasti Ganesh Srivatsa, Sabyasachi Mukhopadhyay, Ganesh Katrapati, Manish Shrivastava. (2024)<br><strong>A Survey of using Large Language Models for Generating Infrastructure as Code</strong><br><button class=copy-to-clipboard title="A Survey of using Large Language Models for Generating Infrastructure as Code" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00227v1.pdf filename=2404.00227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Infrastructure as <b>Code</b> <b>(IaC)</b> is a revolutionary approach which has gained significant prominence in the Industry. IaC manages and provisions IT infrastructure using machine-readable <b>code</b> <b>by</b> enabling automation, consistency across the environments, reproducibility, version control, error reduction and enhancement in scalability. However, IaC orchestration is often a painstaking effort which requires specialised skills as well as a lot of manual effort. Automation of IaC is a necessity in the present conditions of the Industry and in this survey, we study the feasibility of applying <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> to address this problem. <b>LLMs</b> are <b>large</b> <b>neural</b> <b>network-based</b> models which have demonstrated significant language processing abilities and shown to be capable of following a range of instructions within a broad scope. Recently, they have also been adapted for <b>code</b> <b>understanding</b> and generation tasks successfully, which makes them a promising choice for the automatic generation of IaC configurations. In this survey, we delve into the details of IaC, usage of IaC in different platforms, their challenges, <b>LLMs</b> in terms of <b>code-generation</b> <b>aspects</b> and the importance of <b>LLMs</b> in IaC along with our own experiments. Finally, we conclude by presenting the challenges in this area and highlighting the scope for future research.</p></p class="citation"></blockquote><h3 id=33--96145-learning-service-selection-decision-making-behaviors-during-scientific-workflow-development-xihao-xie-et-al-2024>(3/3 | 96/145) Learning Service Selection Decision Making Behaviors During Scientific Workflow Development (Xihao Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xihao Xie, Jia Zhang, Rahul Ramachandran, Tsengdar J. Lee, Seungwon Lee. (2024)<br><strong>Learning Service Selection Decision Making Behaviors During Scientific Workflow Development</strong><br><button class=copy-to-clipboard title="Learning Service Selection Decision Making Behaviors During Scientific Workflow Development" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-LG, cs-SE, cs.SE<br>Keyword Score: 18<br>Keywords: Graph, Knowledge Graph, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00420v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00420v1.pdf filename=2404.00420v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Increasingly, more software services have been published onto the Internet, making it a big challenge to recommend services in the process of a scientific workflow composition. In this paper, a novel context-aware approach is proposed to recommending next services in a workflow development process, through learning service representation and service selection decision making behaviors from workflow provenance. Inspired by natural language sentence generation, the composition process of a scientific workflow is formalized as a step-wise procedure within the context of the goal of workflow, and the problem of next service <b>recommendation</b> is mapped to next word prediction. Historical service dependencies are first extracted from scientific workflow provenance to build a <b>knowledge</b> <b>graph.</b> Service sequences are then generated based on diverse composition path generation strategies. Afterwards, the generated corpus of composition paths are leveraged to study previous decision making strategies. Such a trained goal-oriented next service prediction model will be used to recommend top K candidate services during workflow composition process. Extensive experiments on a real-word repository have demonstrated the effectiveness of this approach.</p></p class="citation"></blockquote><h2 id=csir-4>cs.IR (4)</h2><h3 id=14--97145-aligning-large-language-models-with-recommendation-knowledge-yuwei-cao-et-al-2024>(1/4 | 97/145) Aligning Large Language Models with Recommendation Knowledge (Yuwei Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuwei Cao, Nikhil Mehta, Xinyang Yi, Raghunandan Keshavan, Lukasz Heldt, Lichan Hong, Ed H. Chi, Maheswaran Sathiamoorthy. (2024)<br><strong>Aligning Large Language Models with Recommendation Knowledge</strong><br><button class=copy-to-clipboard title="Aligning Large Language Models with Recommendation Knowledge" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 60<br>Keywords: Fine-tuning, Recommendation, Recommender System, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00245v1.pdf filename=2404.00245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have recently been used as backbones for <b>recommender</b> <b>systems.</b> However, their performance often lags behind conventional methods in standard tasks like retrieval. We attribute this to a mismatch between <b>LLMs&rsquo;</b> knowledge and the knowledge crucial for effective <b>recommendations.</b> While <b>LLMs</b> excel at natural language <b>reasoning,</b> they cannot model complex user-item interactions inherent in <b>recommendation</b> tasks. We propose bridging the knowledge gap and equipping <b>LLMs</b> with <b>recommendation-specific</b> knowledge to address this. Operations such as Masked Item Modeling (MIM) and Bayesian Personalized Ranking (BPR) have found success in conventional <b>recommender</b> <b>systems.</b> Inspired by this, we simulate these operations through natural language to generate auxiliary-task data samples that encode item correlations and user preferences. <b>Fine-tuning</b> <b>LLMs</b> on such auxiliary-task data samples and incorporating more informative <b>recommendation-task</b> data samples facilitates the injection of <b>recommendation-specific</b> knowledge into <b>LLMs.</b> Extensive experiments across retrieval, ranking, and rating prediction tasks on <b>LLMs</b> such as FLAN-T5-Base and FLAN-T5-XL show the effectiveness of our technique in domains such as Amazon Toys & Games, Beauty, and Sports & Outdoors. Notably, our method outperforms conventional and <b>LLM-based</b> baselines, including the current SOTA, by significant margins in retrieval, showcasing its potential for enhancing <b>recommendation</b> quality.</p></p class="citation"></blockquote><h3 id=24--98145-a-unified-framework-for-adaptive-representation-enhancement-and-inversed-learning-in-cross-domain-recommendation-luankang-zhang-et-al-2024>(2/4 | 98/145) A Unified Framework for Adaptive Representation Enhancement and Inversed Learning in Cross-Domain Recommendation (Luankang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luankang Zhang, Hao Wang, Suojuan Zhang, Mingjia Yin, Yongqiang Han, Jiaqing Zhang, Defu Lian, Enhong Chen. (2024)<br><strong>A Unified Framework for Adaptive Representation Enhancement and Inversed Learning in Cross-Domain Recommendation</strong><br><button class=copy-to-clipboard title="A Unified Framework for Adaptive Representation Enhancement and Inversed Learning in Cross-Domain Recommendation" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 38<br>Keywords: Graph, Convolution, Recommendation, Representation Learning, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00268v1.pdf filename=2404.00268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-domain <b>recommendation</b> (CDR), aiming to extract and transfer knowledge across domains, has attracted wide attention for its efficacy in addressing data sparsity and cold-start problems. Despite significant advances in <b>representation</b> <b>disentanglement</b> to capture diverse user preferences, existing methods usually neglect <b>representation</b> <b>enhancement</b> and lack rigorous decoupling constraints, thereby limiting the transfer of relevant information. To this end, we propose a Unified Framework for Adaptive <b>Representation</b> <b>Enhancement</b> and Inversed Learning in Cross-Domain <b>Recommendation</b> (AREIL). Specifically, we first divide user embeddings into domain-shared and domain-specific components to disentangle mixed user preferences. Then, we incorporate intra-domain and inter-domain information to adaptively enhance the ability of user <b>representations.</b> <b>In</b> particular, we propose a <b>graph</b> <b>convolution</b> module to capture high-order information, and a <b>self-attention</b> module to reveal inter-domain correlations and accomplish adaptive fusion. Next, we adopt domain classifiers and gradient reversal layers to achieve inversed <b>representation</b> <b>learning</b> in a unified framework. Finally, we employ a cross-entropy loss for measuring <b>recommendation</b> performance and jointly optimize the entire framework via multi-task learning. Extensive experiments on multiple datasets validate the substantial improvement in the <b>recommendation</b> performance of AREIL. Moreover, ablation studies and <b>representation</b> <b>visualizations</b> further illustrate the effectiveness of adaptive enhancement and inversed learning in CDR.</p></p class="citation"></blockquote><h3 id=34--99145-a-simple-yet-effective-approach-for-diversified-session-based-recommendation-qing-yin-et-al-2024>(3/4 | 99/145) A Simple Yet Effective Approach for Diversified Session-Based Recommendation (Qing Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qing Yin, Hui Fang, Zhu Sun, Yew-Soon Ong. (2024)<br><strong>A Simple Yet Effective Approach for Diversified Session-Based Recommendation</strong><br><button class=copy-to-clipboard title="A Simple Yet Effective Approach for Diversified Session-Based Recommendation" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00261v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00261v1.pdf filename=2404.00261v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Session-based <b>recommender</b> <b>systems</b> (SBRSs) have become extremely popular in view of the core capability of capturing short-term and dynamic user preferences. However, most SBRSs primarily maximize <b>recommendation</b> accuracy but ignore user minor preferences, thus leading to filter bubbles in the long run. Only a handful of works, being devoted to improving diversity, depend on unique model designs and calibrated loss functions, which cannot be easily adapted to existing accuracy-oriented SBRSs. It is thus worthwhile to come up with a simple yet effective design that can be used as a plugin to facilitate existing SBRSs on generating a more diversified list in the meantime preserving the <b>recommendation</b> accuracy. In this case, we propose an end-to-end framework applied for every existing representative (accuracy-oriented) SBRS, called diversified category-aware attentive SBRS (DCA-SBRS), to boost the performance on <b>recommendation</b> diversity. It consists of two novel designs: a model-agnostic diversity-oriented loss function, and a non-invasive category-aware attention mechanism. Extensive experiments on three datasets showcase that our framework helps existing SBRSs achieve extraordinary performance in terms of <b>recommendation</b> diversity and comprehensive performance, without significantly deteriorating <b>recommendation</b> accuracy compared to state-of-the-art accuracy-oriented SBRSs.</p></p class="citation"></blockquote><h3 id=44--100145-enhancing-content-based-recommendation-via-large-language-model-wentao-xu-et-al-2024>(4/4 | 100/145) Enhancing Content-based Recommendation via Large Language Model (Wentao Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wentao Xu, Qianqian Xie, Shuo Yang, Jiangxia Cao, Shuchao Pang. (2024)<br><strong>Enhancing Content-based Recommendation via Large Language Model</strong><br><button class=copy-to-clipboard title="Enhancing Content-based Recommendation via Large Language Model" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00236v1.pdf filename=2404.00236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In real-world applications, users express different behaviors when they interact with different items, including implicit click/like interactions, and explicit comments/reviews interactions. Nevertheless, almost all recommender works are focused on how to describe user preferences by the implicit click/like interactions, to find the synergy of people. For the content-based explicit comments/reviews interactions, some works attempt to utilize them to mine the semantic knowledge to enhance recommender models. However, they still neglect the following two points: (1) The content semantic is a universal world knowledge; how do we extract the multi-aspect semantic information to empower different domains? (2) The user/item ID feature is a fundamental element for recommender models; how do we align the ID and content semantic feature space? In this paper, we propose a `plugin&rsquo; semantic knowledge transferring method \textbf{LoID}, which includes two major components: (1) LoRA-based <b>large</b> <b>language</b> <b>model</b> pretraining to extract multi-aspect semantic information; (2) ID-based contrastive objective to align their feature spaces. We conduct extensive experiments with SOTA baselines on real-world datasets, the detailed results demonstrating significant improvements of our method LoID.</p></p class="citation"></blockquote><h2 id=physicsspace-ph-1>physics.space-ph (1)</h2><h3 id=11--101145-language-models-are-spacecraft-operators-victor-rodriguez-fernandez-et-al-2024>(1/1 | 101/145) Language Models are Spacecraft Operators (Victor Rodriguez-Fernandez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor Rodriguez-Fernandez, Alejandro Carrasco, Jason Cheng, Eli Scharf, Peng Mun Siew, Richard Linares. (2024)<br><strong>Language Models are Spacecraft Operators</strong><br><button class=copy-to-clipboard title="Language Models are Spacecraft Operators" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.space-ph<br>Categories: cs-AI, cs-LG, physics-space-ph, physics.space-ph<br>Keyword Score: 50<br>Keywords: Few-shot, Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00413v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00413v1.pdf filename=2404.00413v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent trends are emerging in the use of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> as autonomous agents that take actions based on the content of the user text <b>prompts.</b> We intend to apply these concepts to the field of Guidance, Navigation, and Control in space, enabling <b>LLMs</b> to have a significant role in the decision-making process for autonomous satellite operations. As a first step towards this goal, we have developed a pure <b>LLM-based</b> solution for the Kerbal Space Program Differential Games (KSPDG) challenge, a public software design competition where participants create autonomous agents for maneuvering satellites involved in non-cooperative space operations, running on the KSP game engine. Our approach leverages <b>prompt</b> engineering, <b>few-shot</b> <b>prompting,</b> and <b>fine-tuning</b> techniques to create an effective <b>LLM-based</b> agent that ranked 2nd in the competition. To the best of our knowledge, this work pioneers the integration of <b>LLM</b> agents into space research. Code is available at <a href=https://github.com/ARCLab-MIT/kspdg>https://github.com/ARCLab-MIT/kspdg</a>.</p></p class="citation"></blockquote><h2 id=csro-14>cs.RO (14)</h2><h3 id=114--102145-exploring-unseen-environments-with-robots-using-large-language-and-vision-models-through-a-procedurally-generated-3d-scene-representation-arjun-p-s-et-al-2024>(1/14 | 102/145) Exploring Unseen Environments with Robots using Large Language and Vision Models through a Procedurally Generated 3D Scene Representation (Arjun P S et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arjun P S, Andrew Melnik, Gora Chand Nandi. (2024)<br><strong>Exploring Unseen Environments with Robots using Large Language and Vision Models through a Procedurally Generated 3D Scene Representation</strong><br><button class=copy-to-clipboard title="Exploring Unseen Environments with Robots using Large Language and Vision Models through a Procedurally Generated 3D Scene Representation" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: In-context Learning, In-context Learning, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00318v1.pdf filename=2404.00318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in Generative Artificial Intelligence, particularly in the realm of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and <b>Large</b> <b>Vision</b> <b>Language</b> Models (LVLMs), have enabled the prospect of leveraging cognitive planners within robotic systems. This work focuses on solving the object goal navigation problem by mimicking human cognition to attend, perceive and store task specific information and generate plans with the same. We introduce a comprehensive framework capable of exploring an unfamiliar environment in search of an object by leveraging the capabilities of <b>Large</b> <b>Language</b> <b>Models(LLMs)</b> and <b>Large</b> <b>Vision</b> <b>Language</b> Models (LVLMs) in understanding the underlying semantics of our world. A challenging task in using <b>LLMs</b> to generate high level sub-goals is to efficiently represent the environment around the robot. We propose to use a 3D scene modular representation, with semantically rich descriptions of the object, to provide the <b>LLM</b> with task relevant information. But providing the <b>LLM</b> with a mass of contextual information (rich 3D scene semantic representation), can lead to redundant and inefficient plans. We propose to use an <b>LLM</b> based pruner that leverages the capabilities of <b>in-context</b> <b>learning</b> to prune out irrelevant goal specific information.</p></p class="citation"></blockquote><h3 id=214--103145-efficient-automatic-tuning-for-data-driven-model-predictive-control-via-meta-learning-baoyu-li-et-al-2024>(2/14 | 103/145) Efficient Automatic Tuning for Data-driven Model Predictive Control via Meta-Learning (Baoyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baoyu Li, William Edwards, Kris Hauser. (2024)<br><strong>Efficient Automatic Tuning for Data-driven Model Predictive Control via Meta-Learning</strong><br><button class=copy-to-clipboard title="Efficient Automatic Tuning for Data-driven Model Predictive Control via Meta-Learning" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Benchmarking, Meta Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00232v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00232v1.pdf filename=2404.00232v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AutoMPC is a Python package that automates and optimizes data-driven model predictive control. However, it can be computationally expensive and unstable when exploring large search spaces using pure Bayesian Optimization (BO). To address these issues, this paper proposes to employ a <b>meta-learning</b> <b>approach</b> called Portfolio that improves AutoMPC&rsquo;s efficiency and stability by warmstarting BO. Portfolio optimizes initial designs for BO using a diverse set of configurations from previous tasks and stabilizes the tuning process by fixing initial configurations instead of selecting them randomly. Experimental results demonstrate that Portfolio outperforms the pure BO in finding desirable solutions for AutoMPC within limited computational resources on 11 nonlinear control <b>simulation</b> <b>benchmarks</b> and 1 physical underwater soft robot dataset.</p></p class="citation"></blockquote><h3 id=314--104145-thin-shell-object-manipulations-with-differentiable-physics-simulations-yian-wang-et-al-2024>(3/14 | 104/145) Thin-Shell Object Manipulations With Differentiable Physics Simulations (Yian Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yian Wang, Juntian Zheng, Zhehuan Chen, Zhou Xian, Gu Zhang, Chao Liu, Chuang Gan. (2024)<br><strong>Thin-Shell Object Manipulations With Differentiable Physics Simulations</strong><br><button class=copy-to-clipboard title="Thin-Shell Object Manipulations With Differentiable Physics Simulations" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00451v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00451v1.pdf filename=2404.00451v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we aim to teach robots to manipulate various thin-shell materials. Prior works studying thin-shell object manipulation mostly rely on heuristic policies or learn policies from real-world video demonstrations, and only focus on limited material types and tasks (e.g., cloth unfolding). However, these approaches face significant challenges when extended to a wider variety of thin-shell materials and a diverse range of tasks. While virtual <b>simulations</b> are shown to be effective in diverse robot skill learning and evaluation, prior thin-shell <b>simulation</b> environments only support a subset of thin-shell materials, which also limits their supported range of tasks. We introduce ThinShellLab - a fully differentiable <b>simulation</b> platform tailored for robotic interactions with diverse thin-shell materials possessing varying material properties, enabling flexible thin-shell manipulation skill learning and evaluation. Our experiments suggest that manipulating thin-shell objects presents several unique challenges: 1) thin-shell manipulation relies heavily on frictional forces due to the objects&rsquo; co-dimensional nature, 2) the materials being manipulated are highly sensitive to minimal variations in interaction actions, and 3) the constant and frequent alteration in contact pairs makes trajectory optimization methods susceptible to local optima, and neither standard <b>reinforcement</b> <b>learning</b> algorithms nor trajectory optimization methods (either gradient-based or gradient-free) are able to solve the tasks alone. To overcome these challenges, we present an optimization scheme that couples sampling-based trajectory optimization and gradient-based optimization, boosting both learning efficiency and converged performance across various proposed tasks. In addition, the differentiable nature of our platform facilitates a smooth sim-to-real transition.</p></p class="citation"></blockquote><h3 id=414--105145-accurate-cutting-point-estimation-for-robotic-lychee-harvesting-through-geometry-aware-learning-gengming-zhang-et-al-2024>(4/14 | 105/145) Accurate Cutting-point Estimation for Robotic Lychee Harvesting through Geometry-aware Learning (Gengming Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gengming Zhang, Hao Cao, Kewei Hu, Yaoqiang Pan, Yuqin Deng, Hongjun Wang, Hanwen Kang. (2024)<br><strong>Accurate Cutting-point Estimation for Robotic Lychee Harvesting through Geometry-aware Learning</strong><br><button class=copy-to-clipboard title="Accurate Cutting-point Estimation for Robotic Lychee Harvesting through Geometry-aware Learning" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 25<br>Keywords: Object Detection, Convolution, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00364v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00364v1.pdf filename=2404.00364v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurately identifying lychee-picking points in unstructured orchard environments and obtaining their coordinate locations is critical to the success of lychee-picking robots. However, traditional two-dimensional (2D) image-based <b>object</b> <b>detection</b> methods often struggle due to the complex geometric structures of branches, leaves and fruits, leading to incorrect determination of lychee picking points. In this study, we propose a Fcaf3d-lychee network model specifically designed for the accurate localisation of lychee picking points. Point cloud data of lychee picking points in natural environments are acquired using Microsoft&rsquo;s Azure Kinect DK time-of-flight (TOF) camera through multi-view stitching. We augment the Fully <b>Convolutional</b> Anchor-Free 3D <b>Object</b> <b>Detection</b> (Fcaf3d) model with a squeeze-and-excitation(SE) module, which exploits human visual attention mechanisms for improved feature extraction of lychee picking points. The trained network model is evaluated on a test set of lychee-picking locations and achieves an impressive F1 score of 88.57%, significantly outperforming existing models. Subsequent three-dimensional (3D) position detection of picking points in real lychee orchard environments yields high accuracy, even under varying degrees of occlusion. Localisation errors of lychee picking points are within 1.5 cm in all directions, demonstrating the robustness and generality of the model.</p></p class="citation"></blockquote><h3 id=514--106145-commonsense-scene-graph-based-target-localization-for-object-search-wenqi-ge-et-al-2024>(5/14 | 106/145) Commonsense Scene Graph-based Target Localization for Object Search (Wenqi Ge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenqi Ge, Chao Tang, Hong Zhang. (2024)<br><strong>Commonsense Scene Graph-based Target Localization for Object Search</strong><br><button class=copy-to-clipboard title="Commonsense Scene Graph-based Target Localization for Object Search" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Graph, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00343v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00343v1.pdf filename=2404.00343v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Object search is a fundamental skill for household robots, yet the core problem lies in the robot&rsquo;s ability to locate the target object accurately. The dynamic nature of household environments, characterized by the arbitrary placement of daily objects by users, makes it challenging to perform target localization. To efficiently locate the target object, the robot needs to be equipped with knowledge at both the object and room level. However, existing approaches rely solely on one type of knowledge, leading to unsatisfactory object localization performance and, consequently, inefficient object search processes. To address this problem, we propose a commonsense scene <b>graph-based</b> target localization, CSG-TL, to enhance target object search in the household environment. Given the pre-built map with stationary items, the robot models the room-level knowledge with object-level commonsense knowledge generated by a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to a commonsense scene <b>graph</b> (CSG), supporting both types of knowledge for CSG-TL. To demonstrate the superiority of CSG-TL on target localization, extensive experiments are performed on the real-world ScanNet dataset and the AI2THOR simulator. Moreover, we have extended CSG-TL to an object search framework, CSG-OS, validated in both simulated and real-world environments. Code and videos are available at <a href=https://sites.google.com/view/csg-os>https://sites.google.com/view/csg-os</a>.</p></p class="citation"></blockquote><h3 id=614--107145-ude-based-dynamic-motion-force-control-of-mobile-manipulators-songqun-gao-et-al-2024>(6/14 | 107/145) UDE-based Dynamic Motion Force Control of Mobile Manipulators (Songqun Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Songqun Gao, Wendi Ding, Qinyuan Ren, Ben M. Chen. (2024)<br><strong>UDE-based Dynamic Motion Force Control of Mobile Manipulators</strong><br><button class=copy-to-clipboard title="UDE-based Dynamic Motion Force Control of Mobile Manipulators" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00443v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00443v1.pdf filename=2404.00443v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mobile manipulators are known for their superior mobility over manipulators on fixed bases, offering promising applications in smart industry and housekeeping scenarios. However, the dynamic coupling nature between the mobile base and the manipulator presents challenges for the physical interactive tasks of the mobile manipulator. Current methods suffer from complex modeling processes and poor transferability. To address this, this article presents a novel dynamic model of the manipulator on the mobile base that requires only the manipulator dynamics and the kinematic information of the mobile base. In addition, embedding the dynamic model, an uncertainty and disturbance estimator-based (UDE-based) dynamic motion/force control scheme is proposed for the mobile manipulator, which compensates for the dynamic coupling and other unmodeled uncertainties. Passivity and stability analyses justify the proposed control law. <b>Simulation</b> and experimental results on our mobile manipulator platform demonstrate the feasibility and effectiveness of our proposed methodology.</p></p class="citation"></blockquote><h3 id=714--108145-cbf-based-motion-planning-for-socially-responsible-robot-navigation-guaranteeing-stl-specification-andrea-ruo-et-al-2024>(7/14 | 108/145) CBF-Based Motion Planning for Socially Responsible Robot Navigation Guaranteeing STL Specification (Andrea Ruo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Ruo, Lorenzo Sabattini, Valeria Villani. (2024)<br><strong>CBF-Based Motion Planning for Socially Responsible Robot Navigation Guaranteeing STL Specification</strong><br><button class=copy-to-clipboard title="CBF-Based Motion Planning for Socially Responsible Robot Navigation Guaranteeing STL Specification" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00356v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00356v1.pdf filename=2404.00356v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of control engineering, the connection between Signal Temporal Logic (STL) and time-varying Control Barrier Functions (CBF) has attracted considerable attention. CBFs have demonstrated notable success in ensuring the safety of critical applications by imposing constraints on system states, while STL allows for precisely specifying spatio-temporal constraints on the behavior of robotic systems. Leveraging these methodologies, this paper addresses the safety-critical navigation problem, in Socially Responsible Navigation (SRN) context, presenting a CBF-based STL motion planning methodology. This methodology enables task completion at any time within a specified time interval considering a dynamic system subject to velocity constraints. The proposed approach involves real-time computation of a smooth CBF, with the computation of a dynamically adjusted parameter based on the available path space and the maximum allowable velocity. A <b>simulation</b> study is conducted to validate the methodology, ensuring safety in the presence of static and dynamic obstacles and demonstrating its compliance with spatio-temporal constraints under non-linear velocity constraints.</p></p class="citation"></blockquote><h3 id=814--109145-cbf-based-stl-motion-planning-for-social-navigation-in-crowded-environment-andrea-ruo-et-al-2024>(8/14 | 109/145) CBF-Based STL Motion Planning for Social Navigation in Crowded Environment (Andrea Ruo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Ruo, Lorenzo Sabattini, Valeria Villani. (2024)<br><strong>CBF-Based STL Motion Planning for Social Navigation in Crowded Environment</strong><br><button class=copy-to-clipboard title="CBF-Based STL Motion Planning for Social Navigation in Crowded Environment" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00353v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00353v1.pdf filename=2404.00353v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A motion planning methodology based on the combination of Control Barrier Functions (CBF) and Signal Temporal Logic (STL) is employed in this paper. This methodology allows task completion at any point within a specified time interval, considering a dynamic system subject to velocity constraints. In this work, we apply this approach into the context of Socially Responsible Navigation (SRN), introducing a rotation constraint. This constraint is designed to maintain the user within the robot&rsquo;s field of view (FOV), enhancing human-robot interaction with the concept of side-by-side human-robot companion. This angular constraint offers the possibility to customize social navigation to specific needs, thereby enabling safe SRN. Its validation is carried out through <b>simulations</b> demonstrating the system&rsquo;s effectiveness in adhering to spatio-temporal constraints, including those related to robot velocity, rotation, and the presence of static and dynamic obstacles.</p></p class="citation"></blockquote><h3 id=914--110145-socially-aware-robot-navigation-through-scoring-using-vision-language-models-daeun-song-et-al-2024>(9/14 | 110/145) Socially Aware Robot Navigation through Scoring Using Vision-Language Models (Daeun Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daeun Song, Jing Liang, Amirreza Payandeh, Xuesu Xiao, Dinesh Manocha. (2024)<br><strong>Socially Aware Robot Navigation through Scoring Using Vision-Language Models</strong><br><button class=copy-to-clipboard title="Socially Aware Robot Navigation through Scoring Using Vision-Language Models" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00210v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00210v1.pdf filename=2404.00210v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose VLM-Social-Nav, a novel <b>Vision-Language</b> Model (VLM) based navigation approach to compute a robot&rsquo;s trajectory in human-centered environments. Our goal is to make real-time decisions on robot actions that are socially compliant with human expectations. We utilize a perception model to detect important social entities and <b>prompt</b> a VLM to generate guidance for socially compliant robot behavior. VLM-Social-Nav uses a VLM-based scoring module that computes a cost term that ensures socially appropriate and effective robot actions generated by the underlying planner. Our overall approach reduces reliance on large datasets (for training) and enhances adaptability in decision-making. In practice, it results in improved socially compliant navigation in human-shared environments. We demonstrate and evaluate our system in four different real-world social navigation scenarios with a Turtlebot robot. We observe at least 36.37% improvement in average success rate and 20.00% improvement in average collision rate in the four social navigation scenarios. The user study score shows that VLM-Social-Nav generates the most socially compliant navigation behavior.</p></p class="citation"></blockquote><h3 id=1014--111145-designing-robot-identity-the-role-of-voice-clothing-and-task-on-robot-gender-perception-nathaniel-s-dennler-et-al-2024>(10/14 | 111/145) Designing Robot Identity: The Role of Voice, Clothing, and Task on Robot Gender Perception (Nathaniel S. Dennler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nathaniel S. Dennler, Mina Kian, Stefanos Nikolaidis, Maja Matarić. (2024)<br><strong>Designing Robot Identity: The Role of Voice, Clothing, and Task on Robot Gender Perception</strong><br><button class=copy-to-clipboard title="Designing Robot Identity: The Role of Voice, Clothing, and Task on Robot Gender Perception" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00494v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00494v1.pdf filename=2404.00494v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Perceptions of gender are a significant aspect of human-human interaction, and gender has wide-reaching social implications for robots deployed in contexts where they are expected to interact with humans. This work explored two flexible modalities for communicating gender in robots&ndash;voice and appearance&ndash;and we studied their individual and combined influences on a robot&rsquo;s perceived gender. We evaluated the perception of a robot&rsquo;s gender through three video-based studies. First, we conducted a study (n=65) on the gender perception of robot voices by varying speaker identity and pitch. Second, we conducted a study (n=93) on the gender perception of robot clothing designed for two different tasks. Finally, building on the results of the first two studies, we completed a large integrative video-based study (n=273) involving two human-robot interaction tasks. We found that voice and clothing can be used to reliably establish a robot&rsquo;s perceived gender, and that combining these two modalities can have different effects on the robot&rsquo;s perceived gender. Taken together, these results inform the design of robot voices and clothing as individual and interacting components in the perceptions of robot gender.</p></p class="citation"></blockquote><h3 id=1114--112145-deep-reinforcement-learning-in-autonomous-car-path-planning-and-control-a-survey-yiyang-chen-et-al-2024>(11/14 | 112/145) Deep Reinforcement Learning in Autonomous Car Path Planning and Control: A Survey (Yiyang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiyang Chen, Chao Ji, Yunrui Cai, Tong Yan, Bo Su. (2024)<br><strong>Deep Reinforcement Learning in Autonomous Car Path Planning and Control: A Survey</strong><br><button class=copy-to-clipboard title="Deep Reinforcement Learning in Autonomous Car Path Planning and Control: A Survey" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00340v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00340v1.pdf filename=2404.00340v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Combining data-driven applications with control systems plays a key role in recent Autonomous Car research. This thesis offers a structured review of the latest literature on Deep <b>Reinforcement</b> <b>Learning</b> (DRL) within the realm of autonomous vehicle Path Planning and Control. It collects a series of DRL methodologies and algorithms and their applications in the field, focusing notably on their roles in trajectory planning and dynamic control. In this review, we delve into the application outcomes of DRL technologies in this domain. By summarizing these literatures, we highlight potential challenges, aiming to offer insights that might aid researchers engaged in related fields.</p></p class="citation"></blockquote><h3 id=1214--113145-joint-pedestrian-trajectory-prediction-through-posterior-sampling-haotian-lin-et-al-2024>(12/14 | 113/145) Joint Pedestrian Trajectory Prediction through Posterior Sampling (Haotian Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haotian Lin, Yixiao Wang, Mingxiao Huo, Chensheng Peng, Zhiyuan Liu, Masayoshi Tomizuka. (2024)<br><strong>Joint Pedestrian Trajectory Prediction through Posterior Sampling</strong><br><button class=copy-to-clipboard title="Joint Pedestrian Trajectory Prediction through Posterior Sampling" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00237v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00237v1.pdf filename=2404.00237v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Joint pedestrian trajectory prediction has long grappled with the inherent unpredictability of human behaviors. Recent investigations employing variants of conditional <b>diffusion</b> <b>models</b> in trajectory prediction have exhibited notable success. Nevertheless, the heavy dependence on accurate historical data results in their vulnerability to noise disturbances and data incompleteness. To improve the robustness and reliability, we introduce the Guided Full Trajectory Diffuser (GFTD), a novel <b>diffusion</b> <b>model</b> framework that captures the joint full (historical and future) trajectory distribution. By learning from the full trajectory, GFTD can recover the noisy and missing data, hence improving the robustness. In addition, GFTD can adapt to data imperfections without additional training requirements, leveraging posterior sampling for reliable prediction and controllable generation. Our approach not only simplifies the prediction process but also enhances generalizability in scenarios with noise and incomplete inputs. Through rigorous experimental evaluation, GFTD exhibits superior performance in both trajectory prediction and controllable generation.</p></p class="citation"></blockquote><h3 id=1314--114145-a-ppo-based-drl-auto-tuning-nonlinear-pid-drone-controller-for-robust-autonomous-flights-junyang-zhang-et-al-2024>(13/14 | 114/145) A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights (Junyang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyang Zhang, Cristian Emanuel Ocampo Rivera, Kyle Tyni, Steven Nguyen. (2024)<br><strong>A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights</strong><br><button class=copy-to-clipboard title="A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00204v1.pdf filename=2404.00204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This project aims to revolutionize drone flight control by implementing a nonlinear Deep <b>Reinforcement</b> <b>Learning</b> (DRL) agent as a replacement for traditional linear Proportional Integral Derivative (PID) controllers. The primary objective is to seamlessly transition drones between manual and autonomous modes, enhancing responsiveness and stability. We utilize the Proximal Policy Optimization (PPO) <b>reinforcement</b> <b>learning</b> strategy within the Gazebo simulator to train the DRL agent. Adding a $20,000 indoor Vicon tracking system offers &lt;1mm positioning accuracy, which significantly improves autonomous flight precision. To navigate the drone in the shortest collision-free trajectory, we also build a 3 dimensional A* path planner and implement it into the real flight successfully.</p></p class="citation"></blockquote><h3 id=1414--115145-self-corrective-sensor-fusion-for-drone-positioning-in-indoor-facilities-francisco-javier-gonzález-castaño-et-al-2024>(14/14 | 115/145) Self-Corrective Sensor Fusion for Drone Positioning in Indoor Facilities (Francisco Javier González-Castaño et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francisco Javier González-Castaño, Felipe Gil-Castiñeira, David Rodríguez-Pereira, José Ángel Regueiro-Janeiro, Silvia García-Méndez, David Candal-Ventureira. (2024)<br><strong>Self-Corrective Sensor Fusion for Drone Positioning in Indoor Facilities</strong><br><button class=copy-to-clipboard title="Self-Corrective Sensor Fusion for Drone Positioning in Indoor Facilities" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO, eess-SP<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00426v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00426v1.pdf filename=2404.00426v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Drones may be more advantageous than fixed cameras for quality control applications in industrial facilities, since they can be redeployed dynamically and adjusted to production planning. The practical scenario that has motivated this paper, image acquisition with drones in a car manufacturing plant, requires drone positioning accuracy in the order of 5 cm. During repetitive manufacturing processes, it is assumed that quality control imaging drones will follow highly deterministic periodic paths, stop at predefined points to take images and send them to image recognition servers. Therefore, by relying on prior knowledge about production chain schedules, it is possible to optimize the positioning technologies for the drones to stay at all times within the boundaries of their flight plans, which will be composed of stopping points and the paths in between. This involves mitigating issues such as temporary blocking of line-of-sight between the drone and any existing radio beacons; sensor data noise; and the loss of visual references. We present a self-corrective solution for this purpose. It corrects visual odometer readings based on filtered and clustered Ultra-Wide Band (UWB) data, as an alternative to direct Kalman fusion. The approach combines the advantages of these technologies when at least one of them works properly at any measurement spot. It has three method components: independent Kalman filtering, data association by means of stream <b>clustering</b> and mutual correction of sensor readings based on the generation of cumulative correction vectors. The approach is inspired by the observation that UWB positioning works reasonably well at static spots whereas visual odometer measurements reflect straight displacements correctly but can underestimate their length. Our experimental results demonstrate the advantages of the approach in the application scenario over Kalman fusion.</p></p class="citation"></blockquote><h2 id=cscr-1>cs.CR (1)</h2><h3 id=11--116145-privacy-backdoors-stealing-data-with-corrupted-pretrained-models-shanglun-feng-et-al-2024>(1/1 | 116/145) Privacy Backdoors: Stealing Data with Corrupted Pretrained Models (Shanglun Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shanglun Feng, Florian Tramèr. (2024)<br><strong>Privacy Backdoors: Stealing Data with Corrupted Pretrained Models</strong><br><button class=copy-to-clipboard title="Privacy Backdoors: Stealing Data with Corrupted Pretrained Models" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Transformer, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00473v1.pdf filename=2404.00473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Practitioners commonly download pretrained machine learning models from open repositories and <b>finetune</b> them to fit specific applications. We show that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model&rsquo;s weights, an attacker can fully compromise the privacy of the <b>finetuning</b> data. We show how to build privacy backdoors for a variety of models, including <b>transformers,</b> which enable an attacker to reconstruct individual <b>finetuning</b> samples, with a guaranteed success! We further show that backdoored models allow for tight privacy attacks on models trained with <b>differential</b> <b>privacy</b> (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, our work highlights a crucial and overlooked supply chain attack on machine learning privacy.</p></p class="citation"></blockquote><h2 id=cshc-3>cs.HC (3)</h2><h3 id=13--117145-a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration-jie-gao-et-al-2024>(1/3 | 117/145) A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration (Jie Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Gao, Simret Araya Gebreegziabher, Kenny Tsu Wei Choo, Toby Jia-Jun Li, Simon Tangi Perrault, Thomas W. Malone. (2024)<br><strong>A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration</strong><br><button class=copy-to-clipboard title="A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: ChatGPT, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00405v1.pdf filename=2404.00405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With <b>ChatGPT&rsquo;s</b> release, conversational <b>prompting</b> has become the most popular form of human-LLM interaction. However, its effectiveness is limited for more complex tasks involving <b>reasoning,</b> creativity, and iteration. Through a systematic analysis of HCI papers published since 2021, we identified four key phases in the human-LLM interaction flow - planning, facilitating, iterating, and testing - to precisely understand the dynamics of this process. Additionally, we have developed a taxonomy of four primary interaction modes: Mode 1: Standard <b>Prompting,</b> Mode 2: User Interface, Mode 3: Context-based, and Mode 4: Agent Facilitator. This taxonomy was further enriched using the &ldquo;5W1H&rdquo; guideline method, which involved a detailed examination of definitions, participant roles (Who), the phases that happened (When), human objectives and <b>LLM</b> abilities (What), and the mechanics of each interaction mode (How). We anticipate this taxonomy will contribute to the future design and evaluation of human-LLM interaction.</p></p class="citation"></blockquote><h3 id=23--118145-contextual-ai-journaling-integrating-llm-and-time-series-behavioral-sensing-technology-to-promote-self-reflection-and-well-being-using-the-mindscape-app-subigya-nepal-et-al-2024>(2/3 | 118/145) Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App (Subigya Nepal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Subigya Nepal, Arvind Pillai, William Campbell, Talie Massachi, Eunsol Soul Choi, Orson Xu, Joanna Kuc, Jeremy Huckins, Jason Holden, Colin Depp, Nicholas Jacobson, Mary Czerwinski, Eric Granholm, Andrew T. Campbell. (2024)<br><strong>Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App</strong><br><button class=copy-to-clipboard title="Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: H-5-0; H-5-3; H-5-m; J-0, cs-AI, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00487v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00487v1.pdf filename=2404.00487v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>MindScape aims to study the benefits of integrating time series behavioral patterns (e.g., conversational engagement, sleep, location) with <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to create a new form of contextual AI journaling, promoting self-reflection and well-being. We argue that integrating behavioral sensing in <b>LLMs</b> will likely lead to a new frontier in AI. In this Late-Breaking Work paper, we discuss the MindScape contextual journal App design that uses <b>LLMs</b> and behavioral sensing to generate contextual and personalized journaling <b>prompts</b> crafted to encourage self-reflection and emotional development. We also discuss the MindScape study of college students based on a preliminary user study and our upcoming study to assess the effectiveness of contextual AI journaling in promoting better well-being on college campuses. MindScape represents a new application class that embeds behavioral intelligence in AI.</p></p class="citation"></blockquote><h3 id=33--119145-enhancing-empathy-in-virtual-reality-an-embodied-approach-to-mindset-modulation-seoyeon-bae-et-al-2024>(3/3 | 119/145) Enhancing Empathy in Virtual Reality: An Embodied Approach to Mindset Modulation (Seoyeon Bae et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seoyeon Bae, Yoon Kyung Lee, Jungcheol Lee, Jaeheon Kim, Haeseong Jeon, Seung-Hwan Lim, Byung-Cheol Kim, Sowon Hahn. (2024)<br><strong>Enhancing Empathy in Virtual Reality: An Embodied Approach to Mindset Modulation</strong><br><button class=copy-to-clipboard title="Enhancing Empathy in Virtual Reality: An Embodied Approach to Mindset Modulation" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00300v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00300v1.pdf filename=2404.00300v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A growth mindset has shown promising outcomes for increasing empathy ability. However, stimulating a growth mindset in VR-based empathy interventions is under-explored. In the present study, we implemented prosocial VR content, Our Neighbor Hero, focusing on embodying a virtual character to modulate players&rsquo; mindsets. The virtual body served as a stepping stone, enabling players to identify with the character and cultivate a growth mindset as they followed mission instructions. We considered several implementation factors to assist players in positioning within the VR experience, including positive feedback, content difficulty, background lighting, and <b>multimodal</b> feedback. We conducted an experiment to investigate the intervention&rsquo;s effectiveness in increasing empathy. Our findings revealed that the VR content and mindset training encouraged participants to improve their growth mindsets and empathic motives. This VR content was developed for college students to enhance their empathy and teamwork skills. It has the potential to improve collaboration in organizational and community environments.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--120145-sa-lsplsequence-aware-long--and-short--term-preference-learning-for-next-poi-recommendation-bin-wang-et-al-2024>(1/1 | 120/145) SA-LSPL:Sequence-Aware Long- and Short- Term Preference Learning for next POI recommendation (Bin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bin Wang, Yan Zhang, Yan Ma, Yaohui Jin, Yanyan Xu. (2024)<br><strong>SA-LSPL:Sequence-Aware Long- and Short- Term Preference Learning for next POI recommendation</strong><br><button class=copy-to-clipboard title="SA-LSPL:Sequence-Aware Long- and Short- Term Preference Learning for next POI recommendation" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 33<br>Keywords: Multi-modal, Recommendation, Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00367v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00367v1.pdf filename=2404.00367v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The next Point of Interest (POI) <b>recommendation</b> aims to recommend the next POI for users at a specific time. As users&rsquo; check-in records can be viewed as a long sequence, methods based on <b>Recurrent</b> <b>Neural</b> <b>Networks</b> <b>(RNNs)</b> have recently shown good applicability to this task. However, existing methods often struggle to fully explore the spatio-temporal correlations and dependencies at the sequence level, and don&rsquo;t take full consideration for various factors influencing users&rsquo; preferences. To address these issues, we propose a novel approach called Sequence-Aware Long- and Short-Term Preference Learning (SA-LSPL) for next-POI <b>recommendation.</b> We combine various information features to effectively model users&rsquo; long-term preferences. Specifically, our proposed model uses a <b>multi-modal</b> embedding module to embed diverse check-in details, taking into account both user&rsquo;s personalized preferences and social influences comprehensively. Additionally, we consider explicit spatio-temporal correlations at the sequence level and implicit sequence dependencies. Furthermore, SA-LSPL learns the spatio-temporal correlations of consecutive and non-consecutive visits in the current check-in sequence, as well as transition dependencies between categories, providing a comprehensive capture of user&rsquo;s short-term preferences. Extensive experiments on two real-world datasets demonstrate the superiority of SA-LSPL over state-of-the-art baseline methods.</p></p class="citation"></blockquote><h2 id=cssd-1>cs.SD (1)</h2><h3 id=11--121145-classification-of-short-segment-pediatric-heart-sounds-based-on-a-transformer-based-convolutional-neural-network-md-hassanuzzaman-et-al-2024>(1/1 | 121/145) Classification of Short Segment Pediatric Heart Sounds Based on a Transformer-Based Convolutional Neural Network (Md Hassanuzzaman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Hassanuzzaman, Nurul Akhtar Hasan, Mohammad Abdullah Al Mamun, Khawza I Ahmed, Ahsan H Khandoker, Raqibul Mostafa. (2024)<br><strong>Classification of Short Segment Pediatric Heart Sounds Based on a Transformer-Based Convolutional Neural Network</strong><br><button class=copy-to-clipboard title="Classification of Short Segment Pediatric Heart Sounds Based on a Transformer-Based Convolutional Neural Network" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00470v1.pdf filename=2404.00470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Congenital anomalies arising as a result of a defect in the structure of the heart and great vessels are known as congenital heart diseases or CHDs. A PCG can provide essential details about the mechanical conduction system of the heart and point out specific patterns linked to different kinds of CHD. This study aims to investigate the minimum signal duration required for the automatic classification of heart sounds. This study also investigated the optimum signal quality assessment indicator (Root Mean Square of Successive Differences) RMSSD and (Zero Crossings Rate) ZCR value. Mel-frequency cepstral coefficients (MFCCs) based feature is used as an input to build a <b>Transformer-Based</b> residual one-dimensional <b>convolutional</b> <b>neural</b> <b>network,</b> which is then used for classifying the heart sound. The study showed that 0.4 is the ideal threshold for getting suitable signals for the RMSSD and ZCR indicators. Moreover, a minimum signal length of 5s is required for effective heart sound classification. It also shows that a shorter signal (3 s heart sound) does not have enough information to categorize heart sounds accurately, and the longer signal (15 s heart sound) may contain more noise. The best accuracy, 93.69%, is obtained for the 5s signal to distinguish the heart sound.</p></p class="citation"></blockquote><h2 id=q-finmf-1>q-fin.MF (1)</h2><h3 id=11--122145-from-attention-to-profit-quantitative-trading-strategy-based-on-transformer-zhaofeng-zhang-et-al-2024>(1/1 | 122/145) From attention to profit: quantitative trading strategy based on transformer (Zhaofeng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaofeng Zhang, Banghao Chen, Shengxin Zhu, Nicolas Langrené. (2024)<br><strong>From attention to profit: quantitative trading strategy based on transformer</strong><br><button class=copy-to-clipboard title="From attention to profit: quantitative trading strategy based on transformer" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.MF<br>Categories: G-3; J-2, cs-AI, cs-CE, q-fin-MF, q-fin.MF<br>Keyword Score: 30<br>Keywords: Transfer Learning, Transformer, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00424v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00424v1.pdf filename=2404.00424v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In traditional quantitative trading practice, navigating the complicated and dynamic financial market presents a persistent challenge. Former machine learning approaches have struggled to fully capture various market variables, often ignore long-term information and fail to catch up with essential signals that may lead the profit. This paper introduces an enhanced <b>transformer</b> architecture and designs a novel factor based on the model. By <b>transfer</b> <b>learning</b> from <b>sentiment</b> <b>analysis,</b> the proposed model not only exploits its original inherent advantages in capturing long-range dependencies and modelling complex data relationships but is also able to solve tasks with numerical inputs and accurately forecast future returns over a period. This work collects more than 5,000,000 rolling data of 4,601 stocks in the Chinese capital market from 2010 to 2019. The results of this study demonstrated the model&rsquo;s superior performance in predicting stock trends compared with other 100 factor-based quantitative strategies with lower turnover rates and a more robust half-life period. Notably, the model&rsquo;s innovative use <b>transformer</b> to establish factors, in conjunction with market <b>sentiment</b> <b>information,</b> has been shown to enhance the accuracy of trading signals significantly, thereby offering promising implications for the future of quantitative trading strategies.</p></p class="citation"></blockquote><h2 id=csai-2>cs.AI (2)</h2><h3 id=12--123145-instruction-driven-game-engines-on-large-language-models-hongqiu-wu-et-al-2024>(1/2 | 123/145) Instruction-Driven Game Engines on Large Language Models (Hongqiu Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongqiu Wu, Y. Wang, Xingyuan Liu, Hai Zhao, Min Zhang. (2024)<br><strong>Instruction-Driven Game Engines on Large Language Models</strong><br><button class=copy-to-clipboard title="Instruction-Driven Game Engines on Large Language Models" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00276v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00276v2.pdf filename=2404.00276v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Instruction-Driven Game Engine (IDGE) project aims to democratize game development by enabling a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to follow free-form game rules and autonomously generate game-play processes. The IDGE allows users to create games by issuing simple natural language instructions, which significantly lowers the barrier for game development. We approach the learning process for IDGEs as a Next State Prediction task, wherein the model autoregressively predicts in-game states given player actions. It is a challenging task because the computation of in-game states must be precise; otherwise, slight errors could disrupt the game-play. To address this, we train the IDGE in a curriculum manner that progressively increases the model&rsquo;s exposure to complex scenarios. Our initial progress lies in developing an IDGE for Poker, a universally cherished card game. The engine we&rsquo;ve designed not only supports a wide range of poker variants but also allows for high customization of rules through natural language inputs. Furthermore, it also favors rapid prototyping of new games from minimal samples, proposing an innovative paradigm in game development that relies on minimal <b>prompt</b> and data engineering. This work lays the groundwork for future advancements in instruction-driven game creation, potentially transforming how games are designed and played.</p></p class="citation"></blockquote><h3 id=22--124145-advancing-multimodal-data-fusion-in-pain-recognition-a-strategy-leveraging-statistical-correlation-and-human-centered-perspectives-xingrui-gu-et-al-2024>(2/2 | 124/145) Advancing Multimodal Data Fusion in Pain Recognition: A Strategy Leveraging Statistical Correlation and Human-Centered Perspectives (Xingrui Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingrui Gu, Zhixuan Wang, Irisa Jin, Zekun Wu. (2024)<br><strong>Advancing Multimodal Data Fusion in Pain Recognition: A Strategy Leveraging Statistical Correlation and Human-Centered Perspectives</strong><br><button class=copy-to-clipboard title="Advancing Multimodal Data Fusion in Pain Recognition: A Strategy Leveraging Statistical Correlation and Human-Centered Perspectives" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 16<br>Keywords: Explainable AI, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00320v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00320v1.pdf filename=2404.00320v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research tackles the challenge of integrating heterogeneous data for specific behavior recognition within the domain of Pain Recognition, presenting a novel methodology that harmonizes statistical correlations with a human-centered approach. By leveraging a diverse range of deep learning architectures, we highlight the adaptability and efficacy of our approach in improving model performance across various complex scenarios. The novelty of our methodology is the strategic incorporation of statistical relevance weights and the segmentation of modalities from a human-centric perspective, enhancing model precision and providing a <b>explainable</b> <b>analysis</b> of <b>multimodal</b> data. This study surpasses traditional modality fusion techniques by underscoring the role of data diversity and customized modality segmentation in enhancing pain behavior analysis. Introducing a framework that matches each modality with an suited classifier, based on the statistical significance, signals a move towards customized and accurate <b>multimodal</b> fusion strategies. Our contributions extend beyond the field of Pain Recognition by delivering new insights into modality fusion and human-centered computing applications, contributing towards <b>explainable</b> <b>AI</b> and bolstering patient-centric healthcare interventions. Thus, we bridge a significant void in the effective and interpretable fusion of <b>multimodal</b> data, establishing a novel standard for forthcoming inquiries in pain behavior recognition and allied fields.</p></p class="citation"></blockquote><h2 id=eesssy-3>eess.SY (3)</h2><h3 id=13--125145-facilitating-reinforcement-learning-for-process-control-using-transfer-learning-perspectives-runze-lin-et-al-2024>(1/3 | 125/145) Facilitating Reinforcement Learning for Process Control Using Transfer Learning: Perspectives (Runze Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runze Lin, Junghui Chen, Lei Xie, Hongye Su, Biao Huang. (2024)<br><strong>Facilitating Reinforcement Learning for Process Control Using Transfer Learning: Perspectives</strong><br><button class=copy-to-clipboard title="Facilitating Reinforcement Learning for Process Control Using Transfer Learning: Perspectives" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AI, cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Recommendation, Reinforcement Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00247v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00247v1.pdf filename=2404.00247v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper provides insights into deep <b>reinforcement</b> <b>learning</b> (DRL) for process control from the perspective of <b>transfer</b> <b>learning.</b> We analyze the challenges of applying DRL in the field of process industries and the necessity of introducing <b>transfer</b> <b>learning.</b> Furthermore, <b>recommendations</b> and prospects are provided for future research directions on how <b>transfer</b> <b>learning</b> can be integrated with DRL to empower process control.</p></p class="citation"></blockquote><h3 id=23--126145-analysis-of-fairness-promoting-optimization-schemes-of-photovoltaic-curtailments-for-voltage-regulation-in-power-distribution-networks-rahul-k-gupta-et-al-2024>(2/3 | 126/145) Analysis of Fairness-promoting Optimization Schemes of Photovoltaic Curtailments for Voltage Regulation in Power Distribution Networks (Rahul K. Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rahul K. Gupta, Daniel K. Molzahn. (2024)<br><strong>Analysis of Fairness-promoting Optimization Schemes of Photovoltaic Curtailments for Voltage Regulation in Power Distribution Networks</strong><br><button class=copy-to-clipboard title="Analysis of Fairness-promoting Optimization Schemes of Photovoltaic Curtailments for Voltage Regulation in Power Distribution Networks" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 13<br>Keywords: Benchmarking, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00394v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00394v1.pdf filename=2404.00394v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Active power curtailment of photovoltaic (PV) generation is commonly exercised to mitigate over-voltage issues in power distribution networks. However, <b>fairness</b> concerns arise as certain PV plants may experience more significant curtailments than others depending on their locations within the network. Existing literature tackles this issue through <b>fairness-promoting/aware</b> optimization schemes. These schemes can be broadly categorized into two types. The first type maximizes an additional <b>fairness</b> objective along with the main objective of curtailment minimization. The second type is formulated as a feedback controller, where <b>fairness</b> is accounted for by assigning different weights (as feedback) in the curtailment minimization objective for each PV plant based on previous curtailment actions. In this work, we combine these two schemes and provide extensive analyses and comparisons of these two <b>fairness</b> schemes. We compare the performance in terms of <b>fairness</b> and net curtailments for several <b>benchmark</b> test networks.</p></p class="citation"></blockquote><h3 id=33--127145-on-accessibility-fairness-in-intermodal-autonomous-mobility-on-demand-systems-mauro-salazar-et-al-2024>(3/3 | 127/145) On Accessibility Fairness in Intermodal Autonomous Mobility-on-Demand Systems (Mauro Salazar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mauro Salazar, Sara Betancur Giraldo, Fabio Paparella, Leonardo Pedroso. (2024)<br><strong>On Accessibility Fairness in Intermodal Autonomous Mobility-on-Demand Systems</strong><br><button class=copy-to-clipboard title="On Accessibility Fairness in Intermodal Autonomous Mobility-on-Demand Systems" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00434v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00434v2.pdf filename=2404.00434v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Research on the operation of mobility systems so far has mostly focused on minimizing cost-centered metrics such as average travel time, distance driven, and operational costs. Whilst capturing economic indicators, such metrics do not account for transportation justice aspects. In this paper, we present an optimization model to plan the operation of Intermodal Autonomous Mobility-on-Demand (I-AMoD) systems, where self-driving vehicles provide on-demand mobility jointly with public transit and active modes, with the goal to minimize the accessibility unfairness experienced by the population. Specifically, we first leverage a previously developed network flow model to compute the I-AMoD system operation in a minimum-time manner. Second, we formally define accessibility unfairness, and use it to frame the minimum-accessibility-unfairness problem and cast it as a linear program. We showcase our framework for a real-world case-study in the city of Eindhoven, NL. Our results show that it is possible to reach an operation that is on average fully fair at the cost of a slight travel time increase compared to a minimum-travel-time solution. Thereby we observe that the accessibility <b>fairness</b> of individual paths is, on average, worse than the average values obtained from flows, setting the stage for a discussion on the definition of accessibility <b>fairness</b> itself.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--128145-ccwsim-an-efficient-and-fast-wavelet-based-ccsim-for-categorical-characterization-of-large-scale-mojtaba-bavandsavadkoohi-et-al-2024>(1/1 | 128/145) CCWSIM: An Efficient and Fast Wavelet-Based CCSIM for Categorical Characterization of Large-Scale (Mojtaba Bavandsavadkoohi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mojtaba Bavandsavadkoohi, Erwan Gloaguen, Behzad Tokhmechi, Alireza Arab-Amiri, Bernard Giroux. (2024)<br><strong>CCWSIM: An Efficient and Fast Wavelet-Based CCSIM for Categorical Characterization of Large-Scale</strong><br><button class=copy-to-clipboard title="CCWSIM: An Efficient and Fast Wavelet-Based CCSIM for Categorical Characterization of Large-Scale" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00441v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00441v1.pdf filename=2404.00441v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the last couple of decades, there has been a surge in various approaches to multiple-point statistics <b>simulation,</b> commonly referred to as MPS. These methods have aimed to improve several critical aspects of realism in the results, including spatial continuity, conditioning, stochasticity, and computational efficiency. Nevertheless, achieving a simultaneous enhancement of these crucial factors has presented challenges to researchers. In the approach that we propose, CCSIM is combined with the Discrete Wavelet Transform (DWT) to address some of these concerns. The primary step in the method involves the computation of the DWT for both the Training Image (TI) and a region shared with previously simulated grids at a specific level of wavelet decomposition. Then, the degree of similarity between the wavelet approximation coefficients is measured using a Cross-Correlation Function (CCF). These approximation coefficients offer a compressed representation of the pattern while capturing its primary variations and essential characteristics, thereby expediting the search for the best-matched pattern. Once the best-matched pattern in the wavelet approximation coefficients is identified, the original pattern can be perfectly reconstructed by integrating the DWT detail coefficients through an Inverse-DWT transformation. Experiments conducted across diverse categorical TIs demonstrate <b>simulations</b> comparable to multi-scale CCSIM (MS-CCSIM), accompanied by an enhancement in facies connectivity and pattern reproduction. The source code implementations are available at <a href=https://github.com/MBS1984/CCWSIM>https://github.com/MBS1984/CCWSIM</a>.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--129145-learning-truly-monotone-operators-with-applications-to-nonlinear-inverse-problems-younes-belkouchi-et-al-2024>(1/1 | 129/145) Learning truly monotone operators with applications to nonlinear inverse problems (Younes Belkouchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Younes Belkouchi, Jean-Christophe Pesquet, Audrey Repetti, Hugues Talbot. (2024)<br><strong>Learning truly monotone operators with applications to nonlinear inverse problems</strong><br><button class=copy-to-clipboard title="Learning truly monotone operators with applications to nonlinear inverse problems" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, cs-NA, math-NA, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00390v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00390v1.pdf filename=2404.00390v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article introduces a novel approach to learning monotone neural networks through a newly defined penalization loss. The proposed method is particularly effective in solving classes of variational problems, specifically monotone inclusion problems, commonly encountered in image processing tasks. The Forward-Backward-Forward (FBF) algorithm is employed to address these problems, offering a solution even when the Lipschitz constant of the neural network is unknown. Notably, the FBF algorithm provides convergence guarantees under the condition that the learned operator is monotone. Building on plug-and-play methodologies, our objective is to apply these newly learned operators to solving non-linear inverse problems. To achieve this, we initially formulate the problem as a variational inclusion problem. Subsequently, we train a monotone neural network to approximate an operator that may not inherently be monotone. Leveraging the FBF algorithm, we then show <b>simulation</b> examples where the non-linear inverse problem is successfully solved.</p></p class="citation"></blockquote><h2 id=physicscomp-ph-1>physics.comp-ph (1)</h2><h3 id=11--130145-predicting-the-statistical-error-of-analog-particle-tracing-monte-carlo-vince-maes-et-al-2024>(1/1 | 130/145) Predicting the statistical error of analog particle tracing Monte Carlo (Vince Maes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vince Maes, Ignace Bossuyt, Hannes Vandecasteele, Wouter Dekeyser, Julian Koellermeier, Martine Baelmans, Giovanni Samaey. (2024)<br><strong>Predicting the statistical error of analog particle tracing Monte Carlo</strong><br><button class=copy-to-clipboard title="Predicting the statistical error of analog particle tracing Monte Carlo" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.comp-ph<br>Categories: cs-NA, math-NA, physics-comp-ph, physics.comp-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00315v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00315v1.pdf filename=2404.00315v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large particle systems are often described by high-dimensional (linear) kinetic equations that are simulated using Monte Carlo methods for which the asymptotic convergence rate is independent of the dimensionality. Even though the asymptotic convergence rate is known, predicting the actual value of the statistical error remains a challenging problem. In this paper, we show how the statistical error of an analog particle tracing Monte Carlo method can be calculated (expensive) and predicted a priori (cheap) when estimating quantities of interest (QoI) on a histogram. We consider two types of QoI estimators: point estimators for which each particle provides one independent contribution to the QoI estimates, and analog estimators for which each particle provides multiple correlated contributions to the QoI estimates. The developed statistical error predictors can be applied to other QoI estimators and nonanalog <b>simulation</b> routines as well. The error analysis is based on interpreting the number of particle visits to a histogram bin as the result of a (correlated) binomial experiment. The resulting expressions can be used to optimize (non)analog particle tracing Monte Carlo methods and hybrid <b>simulation</b> methods involving a Monte Carlo component, as well as to select an optimal particle tracing Monte Carlo method from several available options. Additionally, the cheap statistical error predictors can be used to determine a priori the number of particles N that is needed to reach a desired accuracy. We illustrate the theory using a linear kinetic equation describing neutral particles in the plasma edge of a fusion device and show numerical results. The code used to perform the numerical experiments is openly available.</p></p class="citation"></blockquote><h2 id=csit-2>cs.IT (2)</h2><h3 id=12--131145-model-driven-deep-learning-for-distributed-detection-with-binary-quantization-wei-guo-et-al-2024>(1/2 | 131/145) Model-Driven Deep Learning for Distributed Detection with Binary Quantization (Wei Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Guo, Meng He, Chuan Huang, Hengtao He, Shenghui Song, Jun Zhang, Khaled B. Letaief. (2024)<br><strong>Model-Driven Deep Learning for Distributed Detection with Binary Quantization</strong><br><button class=copy-to-clipboard title="Model-Driven Deep Learning for Distributed Detection with Binary Quantization" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00309v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00309v1.pdf filename=2404.00309v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Within the realm of rapidly advancing wireless sensor networks (WSNs), distributed detection assumes a significant role in various practical applications. However, critical challenge lies in maintaining robust detection performance while operating within the constraints of limited bandwidth and energy resources. This paper introduces a novel approach that combines model-driven deep learning (DL) with binary <b>quantization</b> to strike a balance between communication overhead and detection performance in WSNs. We begin by establishing the lower bound of detection error probability for distributed detection using the maximum a posteriori (MAP) criterion. Furthermore, we prove the global optimality of employing identical local quantizers across sensors, thereby maximizing the corresponding Chernoff information. Subsequently, the paper derives the minimum MAP detection error probability (MAPDEP) by inplementing identical binary probabilistic quantizers across the sensors. Moreover, the paper establishes the equivalence between utilizing all <b>quantized</b> data and their average as input to the detector at the fusion center (FC). In particular, we derive the Kullback-Leibler (KL) divergence, which measures the difference between the true posterior probability and output of the proposed detector. Leveraging the MAPDEP and KL divergence as loss functions, the paper proposes model-driven DL method to separately train the probability controller module in the quantizer and the detector module at the FC. Numerical results validate the convergence and effectiveness of the proposed method, which achieves near-optimal performance with reduced complexity for Gaussian hypothesis testing.</p></p class="citation"></blockquote><h3 id=22--132145-environment-aware-codebook-for-ris-assisted-mu-miso-communications-implementation-and-performance-analysis-zhiheng-yu-et-al-2024>(2/2 | 132/145) Environment-Aware Codebook for RIS-Assisted MU-MISO Communications: Implementation and Performance Analysis (Zhiheng Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiheng Yu, Jiancheng An, Lu Gan, Chau Yuen. (2024)<br><strong>Environment-Aware Codebook for RIS-Assisted MU-MISO Communications: Implementation and Performance Analysis</strong><br><button class=copy-to-clipboard title="Environment-Aware Codebook for RIS-Assisted MU-MISO Communications: Implementation and Performance Analysis" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00265v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00265v1.pdf filename=2404.00265v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconfigurable intelligent surface (RIS) provides a new electromagnetic response control solution, which can reshape the characteristics of wireless channels. In this paper, we propose a novel environment-aware codebook protocol for RIS-assisted multi-user multiple-input single-output (MU-MISO) systems. Specifically, we first introduce a channel training protocol which consists of off-line and on-line stages. Secondly, we propose an environment-aware codebook generation scheme, which utilizes the statistical channel state information and alternating optimization method to generate codewords offline. Then, in the on-line stage, we use these pre-designed codewords to configure the RIS, and the optimal codeword resulting in the highest sum rate is adopted for assisting in the downlink data transmission. Thirdly, we analyze the theoretical performance of the proposed protocol considering the channel estimation errors. Finally, numerical <b>simulations</b> are provided to verify our theoretical analysis and the performance of the proposed scheme.</p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=12--133145-numerical-simulations-for-fractional-differential-equations-of-higher-order-and-a-wright-type-transformation-m-nacianceno-et-al-2024>(1/2 | 133/145) Numerical Simulations for Fractional Differential Equations of Higher Order and a Wright-Type Transformation (M. Nacianceno et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>M. Nacianceno, T. Oraby, H. Rodrigo, Y. Sepulveda, J. Sifuentes, E. Suazo, T. Stuck, J. Williams. (2024)<br><strong>Numerical Simulations for Fractional Differential Equations of Higher Order and a Wright-Type Transformation</strong><br><button class=copy-to-clipboard title="Numerical Simulations for Fractional Differential Equations of Higher Order and a Wright-Type Transformation" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-MP, math-NA, math-ph, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00248v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00248v1.pdf filename=2404.00248v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, a new relationship is established between the solutions of higher fractional differential equations and a Wright-type transformation. Solutions could be interpreted as expected values of functions in a random time process. As applications, we solve the fractional beam equation, fractional electric circuits with special functions as external sources, and derive dAlemberts formula for the fractional wave equation. Due to this relationship, we present two methods for simulating solutions of fractional differential equations. The two approaches use the interpretation of the Caputo derivative of a function as a Wright-type transformation of the higher derivative of the function. In the first approach, we use the Runge-Kutta method of hybrid orders 4 and 5 to solve ordinary differential equations combined with the Monte Carlo integration to conduct the Wrighttype transformation. The second method uses a feedforward neural network to simulate the fractional differential equation.</p></p class="citation"></blockquote><h3 id=22--134145-geometric-mean-for-t-positive-definite-tensors-and-associated-riemannian-geometry-jeong-hoon-ju-et-al-2024>(2/2 | 134/145) Geometric mean for T-positive definite tensors and associated Riemannian geometry (Jeong-Hoon Ju et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeong-Hoon Ju, Taehyeong Kim, Yeongrak Kim, Hayoung Choi. (2024)<br><strong>Geometric mean for T-positive definite tensors and associated Riemannian geometry</strong><br><button class=copy-to-clipboard title="Geometric mean for T-positive definite tensors and associated Riemannian geometry" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 15A69, 15A72, 15B48, 47A64, 53A45, cs-NA, math-DG, math-FA, math-NA, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00255v1.pdf filename=2404.00255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we generalize the geometric mean of two positive definite matrices to that of third-order tensors using the notion of T-product. Specifically, we define the geometric mean of two T-positive definite tensors and verify several properties that &ldquo;mean&rdquo; should satisfy including the idempotence and the commutative property, and so on. Moreover, it is shown that the geometric mean is a unique T-positive definite solution of an algebraic Riccati tensor equation and can be expressed as solutions of algebraic Riccati matrix equations. In addition, we investigate the Riemannian manifold associated with the geometric mean for T-positive definite tensors, considering it as a totally geodesic embedded submanifold of the Riemannian manifold associated with the case of matrices. It is particularly shown that the geometric mean of two T-positive definite tensors is the midpoint of a unique geodesic joining the tensors, and the manifold is a Cartan-Hadamard-Riemannian manifold.</p></p class="citation"></blockquote><h2 id=statml-3>stat.ML (3)</h2><h3 id=13--135145-functional-edged-network-modeling-haijie-xu-et-al-2024>(1/3 | 135/145) Functional-Edged Network Modeling (Haijie Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haijie Xu, Chen Zhang. (2024)<br><strong>Functional-Edged Network Modeling</strong><br><button class=copy-to-clipboard title="Functional-Edged Network Modeling" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00218v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00218v1.pdf filename=2404.00218v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contrasts with existing works which all consider nodes as functions and use edges to represent the relationships between different functions. We target at network modeling whose edges are functional data and transform the adjacency matrix into a functional adjacency tensor, introducing an additional dimension dedicated to function representation. Tucker functional decomposition is used for the functional adjacency tensor, and to further consider the community between nodes, we regularize the basis matrices to be symmetrical. Furthermore, to deal with irregular observations of the functional edges, we conduct model inference to solve a tensor completion problem. It is optimized by a Riemann conjugate gradient descent method. Besides these, we also derive several theorems to show the desirable properties of the functional edged network model. Finally, we evaluate the efficacy of our proposed model using <b>simulation</b> data and real metro system data from Hong Kong and Singapore.</p></p class="citation"></blockquote><h3 id=23--136145-convolutional-bayesian-filtering-wenhan-cao-et-al-2024>(2/3 | 136/145) Convolutional Bayesian Filtering (Wenhan Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenhan Cao, Shiqi Liu, Chang Liu, Zeyu He, Stephen S. -T. Yau, Shengbo Eben Li. (2024)<br><strong>Convolutional Bayesian Filtering</strong><br><button class=copy-to-clipboard title="Convolutional Bayesian Filtering" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, cs-SY, eess-SY, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00481v1.pdf filename=2404.00481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bayesian filtering serves as the mainstream framework of state estimation in dynamic systems. Its standard version utilizes total probability rule and Bayes&rsquo; law alternatively, where how to define and compute conditional probability is critical to state distribution inference. Previously, the conditional probability is assumed to be exactly known, which represents a measure of the occurrence probability of one event, given the second event. In this paper, we find that by adding an additional event that stipulates an inequality condition, we can transform the conditional probability into a special integration that is analogous to <b>convolution.</b> Based on this transformation, we show that both transition probability and output probability can be generalized to <b>convolutional</b> forms, resulting in a more general filtering framework that we call <b>convolutional</b> Bayesian filtering. This new framework encompasses standard Bayesian filtering as a special case when the distance metric of the inequality condition is selected as Dirac delta function. It also allows for a more nuanced consideration of model mismatch by choosing different types of inequality conditions. For instance, when the distance metric is defined in a distributional sense, the transition probability and output probability can be approximated by simply rescaling them into fractional powers. Under this framework, a robust version of Kalman filter can be constructed by only altering the noise covariance matrix, while maintaining the conjugate nature of Gaussian distributions. Finally, we exemplify the effectiveness of our approach by reshaping classic filtering algorithms into <b>convolutional</b> versions, including Kalman filter, extended Kalman filter, unscented Kalman filter and particle filter.</p></p class="citation"></blockquote><h3 id=33--137145-partially-observable-sequential-change-point-detection-for-autocorrelated-data-via-upper-confidence-region-haijie-xu-et-al-2024>(3/3 | 137/145) Partially-Observable Sequential Change-Point Detection for Autocorrelated Data via Upper Confidence Region (Haijie Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haijie Xu, Xiaochen Xian, Chen Zhang, Kaibo Liu. (2024)<br><strong>Partially-Observable Sequential Change-Point Detection for Autocorrelated Data via Upper Confidence Region</strong><br><button class=copy-to-clipboard title="Partially-Observable Sequential Change-Point Detection for Autocorrelated Data via Upper Confidence Region" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00220v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00220v1.pdf filename=2404.00220v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sequential change point detection for multivariate autocorrelated data is a very common problem in practice. However, when the sensing resources are limited, only a subset of variables from the multivariate system can be observed at each sensing time point. This raises the problem of partially observable multi-sensor sequential change point detection. For it, we propose a detection scheme called adaptive upper confidence region with state space model (AUCRSS). It models multivariate time series via a state space model (SSM), and uses an adaptive sampling policy for efficient change point detection and localization. A partially-observable Kalman filter algorithm is developed for online inference of SSM, and accordingly, a change point detection scheme based on a generalized likelihood ratio test is developed. How its detection power relates to the adaptive sampling strategy is analyzed. Meanwhile, by treating the detection power as a reward, its connection with the online combinatorial multi-armed <b>bandit</b> (CMAB) problem is formulated and an adaptive upper confidence region algorithm is proposed for adaptive sampling policy design. Theoretical analysis of the asymptotic average detection delay is performed, and thorough numerical studies with synthetic data and real-world data are conducted to demonstrate the effectiveness of our method.</p></p class="citation"></blockquote><h2 id=physicsmed-ph-1>physics.med-ph (1)</h2><h3 id=11--138145-score-based-diffusion-models-for-photoacoustic-tomography-image-reconstruction-sreemanti-dey-et-al-2024>(1/1 | 138/145) Score-Based Diffusion Models for Photoacoustic Tomography Image Reconstruction (Sreemanti Dey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sreemanti Dey, Snigdha Saha, Berthy T. Feng, Manxiu Cui, Laure Delisle, Oscar Leong, Lihong V. Wang, Katherine L. Bouman. (2024)<br><strong>Score-Based Diffusion Models for Photoacoustic Tomography Image Reconstruction</strong><br><button class=copy-to-clipboard title="Score-Based Diffusion Models for Photoacoustic Tomography Image Reconstruction" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.med-ph<br>Categories: cs-CV, cs-LG, eess-IV, physics-med-ph, physics.med-ph<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00471v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00471v1.pdf filename=2404.00471v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Photoacoustic tomography (PAT) is a rapidly-evolving medical imaging modality that combines optical absorption contrast with ultrasound imaging depth. One challenge in PAT is image reconstruction with inadequate acoustic signals due to limited sensor coverage or due to the density of the transducer array. Such cases call for solving an ill-posed inverse reconstruction problem. In this work, we use score-based <b>diffusion</b> <b>models</b> to solve the inverse problem of reconstructing an image from limited PAT measurements. The proposed approach allows us to incorporate an expressive prior learned by a <b>diffusion</b> <b>model</b> on simulated vessel structures while still being robust to varying transducer sparsity conditions.</p></p class="citation"></blockquote><h2 id=csdc-2>cs.DC (2)</h2><h3 id=12--139145-communication-efficient-distributed-training-with-distributed-lion-bo-liu-et-al-2024>(1/2 | 139/145) Communication Efficient Distributed Training with Distributed Lion (Bo Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Liu, Lemeng Wu, Lizhang Chen, Kaizhao Liang, Jiaxu Zhu, Chen Liang, Raghuraman Krishnamoorthi, Qiang Liu. (2024)<br><strong>Communication Efficient Distributed Training with Distributed Lion</strong><br><button class=copy-to-clipboard title="Communication Efficient Distributed Training with Distributed Lion" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-AI, cs-DC, cs-LG, cs.DC, math-OC, stat-ML<br>Keyword Score: 10<br>Keywords: Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00438v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00438v1.pdf filename=2404.00438v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Lion optimizer has been a promising competitor with the AdamW for training large AI models, with advantages on memory, computation, and sample efficiency. In this paper, we introduce Distributed Lion, an innovative adaptation of Lion for distributed training environments. Leveraging the sign operator in Lion, our Distributed Lion only requires communicating binary or lower-precision vectors between workers to the center server, significantly reducing the communication cost. Our theoretical analysis confirms Distributed Lion&rsquo;s convergence properties. Empirical results demonstrate its robustness across a range of tasks, worker counts, and batch sizes, on both vision and language problems. Notably, Distributed Lion attains comparable performance to standard Lion or AdamW optimizers applied on aggregated gradients, but with significantly reduced communication bandwidth. This feature is particularly advantageous for training large models. In addition, we also demonstrate that Distributed Lion presents a more favorable performance-bandwidth balance compared to existing efficient distributed methods such as deep gradient compression and ternary gradients.</p></p class="citation"></blockquote><h3 id=22--140145-engineering-a-workload-balanced-push-relabel-algorithm-for-massive-graphs-on-gpus-chou-ying-hsieh-et-al-2024>(2/2 | 140/145) Engineering A Workload-balanced Push-Relabel Algorithm for Massive Graphs on GPUs (Chou-Ying Hsieh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chou-Ying Hsieh, Po-Chieh Lin, Sy-Yen Kuo. (2024)<br><strong>Engineering A Workload-balanced Push-Relabel Algorithm for Massive Graphs on GPUs</strong><br><button class=copy-to-clipboard title="Engineering A Workload-balanced Push-Relabel Algorithm for Massive Graphs on GPUs" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-DS, cs.DC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00270v1.pdf filename=2404.00270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The push-relabel algorithm is an efficient algorithm that solves the maximum flow/ minimum cut problems of its affinity to parallelization. As the size of <b>graphs</b> grows exponentially, researchers have used Graphics Processing Units (GPUs) to accelerate the computation of the push-relabel algorithm further. However, prior works need to handle the significant memory consumption to represent a massive residual <b>graph.</b> In addition, the nature of their algorithms has inherently imbalanced workload distribution on GPUs. This paper first identifies the two challenges with the memory and computational models. Based on the analysis of these models, we propose a workload-balanced push-relabel algorithm (WBPR) with two enhanced compressed sparse representations (CSR) and a vertex-centric approach. The enhanced CSR significantly reduces memory consumption, while the vertex-centric approach alleviates the workload imbalance and improves the utilization of the GPU. In the experiment, our approach reduces the memory consumption from O(V^2) to O(V + E). Moreover, we can achieve up to 7.31x and 2.29x runtime speedup compared to the state-of-the-art on real-world <b>graphs</b> in maximum flow and bipartite matching tasks, respectively. Our code will be open-sourced for further research on accelerating the push-relabel algorithm.</p></p class="citation"></blockquote><h2 id=csne-2>cs.NE (2)</h2><h3 id=12--141145-spikingjet-enhancing-fault-injection-for-fully-and-convolutional-spiking-neural-networks-anil-bayram-gogebakan-et-al-2024>(1/2 | 141/145) SpikingJET: Enhancing Fault Injection for Fully and Convolutional Spiking Neural Networks (Anil Bayram Gogebakan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anil Bayram Gogebakan, Enrico Magliano, Alessio Carpegna, Annachiara Ruospo, Alessandro Savino, Stefano Di Carlo. (2024)<br><strong>SpikingJET: Enhancing Fault Injection for Fully and Convolutional Spiking Neural Networks</strong><br><button class=copy-to-clipboard title="SpikingJET: Enhancing Fault Injection for Fully and Convolutional Spiking Neural Networks" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: I-2, cs-AI, cs-NE, cs.NE<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00383v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00383v1.pdf filename=2404.00383v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As artificial neural networks become increasingly integrated into safety-critical systems such as autonomous vehicles, devices for medical diagnosis, and industrial automation, ensuring their reliability in the face of random hardware faults becomes paramount. This paper introduces SpikingJET, a novel fault injector designed specifically for fully connected and <b>convolutional</b> Spiking Neural Networks (SNNs). Our work underscores the critical need to evaluate the resilience of SNNs to hardware faults, considering their growing prominence in real-world applications. SpikingJET provides a comprehensive platform for assessing the resilience of SNNs by inducing errors and injecting faults into critical components such as synaptic weights, neuron model parameters, internal states, and activation functions. This paper demonstrates the effectiveness of Spiking-JET through extensive software-level experiments on various SNN architectures, revealing insights into their vulnerability and resilience to hardware faults. Moreover, highlighting the importance of fault resilience in SNNs contributes to the ongoing effort to enhance the reliability and safety of Neural Network (NN)-powered systems in diverse domains.</p></p class="citation"></blockquote><h3 id=22--142145-discrete-natural-evolution-strategies-ahmad-ayaz-amin-2024>(2/2 | 142/145) Discrete Natural Evolution Strategies (Ahmad Ayaz Amin, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmad Ayaz Amin. (2024)<br><strong>Discrete Natural Evolution Strategies</strong><br><button class=copy-to-clipboard title="Discrete Natural Evolution Strategies" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE, math-OC<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00208v1.pdf filename=2404.00208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Natural evolution strategies are a class of approximate-gradient <b>black-box</b> <b>optimizers</b> that have been successfully used for continuous parameter spaces. In this paper, we derive NES algorithms for discrete parameter spaces and demonstrate their effectiveness in tasks involving discrete parameters.</p></p class="citation"></blockquote><h2 id=csce-2>cs.CE (2)</h2><h3 id=12--143145-leveraging-intelligent-recommender-system-as-a-first-step-resilience-measure----a-data-driven-supply-chain-disruption-response-framework-yang-hu-2024>(1/2 | 143/145) Leveraging Intelligent Recommender system as a first step resilience measure &ndash; A data-driven supply chain disruption response framework (Yang Hu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Hu. (2024)<br><strong>Leveraging Intelligent Recommender system as a first step resilience measure &ndash; A data-driven supply chain disruption response framework</strong><br><button class=copy-to-clipboard title="Leveraging Intelligent Recommender system as a first step resilience measure -- A data-driven supply chain disruption response framework" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-AI, cs-CE, cs.CE<br>Keyword Score: 10<br>Keywords: Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00306v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00306v1.pdf filename=2404.00306v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interests in the value of digital technologies for its potential uses to increase supply chain resilience (SCRes) are increasing in light to the industry 4.0 and the global pandemic. Utilization of <b>Recommender</b> <b>systems</b> (RS) as a supply chain (SC) resilience measure is neglected although RS is a capable tool to enhance SC resilience from a reactive aspect. To address this problem, this research proposed a novel data-driven supply chain disruption response framework based on the intelligent <b>recommender</b> <b>system</b> techniques and validated the conceptual model through a practical use case. Results show that our framework can be implemented as an effective SC disruption mitigation measure in the very first response phrase and help SC participants get better reaction performance after the SC disruption.</p></p class="citation"></blockquote><h3 id=22--144145-a-stackelberg-regret-minimizing-framework-for-online-learning-in-newsvendor-pricing-games-larkin-liu-et-al-2024>(2/2 | 144/145) A Stackelberg Regret Minimizing Framework for Online Learning in Newsvendor Pricing Games (Larkin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Larkin Liu, Yuming Rong. (2024)<br><strong>A Stackelberg Regret Minimizing Framework for Online Learning in Newsvendor Pricing Games</strong><br><button class=copy-to-clipboard title="A Stackelberg Regret Minimizing Framework for Online Learning in Newsvendor Pricing Games" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs-MA, cs.CE<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00203v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00203v1.pdf filename=2404.00203v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the application of online learning in a Stackelberg game pertaining to a system with two learning agents in a dyadic exchange network, consisting of a supplier and retailer, specifically where the parameters of the demand function are unknown. In this game, the supplier is the first-moving leader, and must determine the optimal wholesale price of the product. Subsequently, the retailer who is the follower, must determine both the optimal procurement amount and selling price of the product. In the perfect information setting, this is known as the classical price-setting Newsvendor problem, and we prove the existence of a unique Stackelberg equilibrium when extending this to a two-player pricing game. In the framework of online learning, the parameters of the reward function for both the follower and leader must be learned, under the assumption that the follower will best respond with optimism under uncertainty. A novel algorithm based on contextual linear <b>bandits</b> with a measurable uncertainty set is used to provide a confidence bound on the parameters of the stochastic demand. Consequently, optimal finite time regret bounds on the Stackelberg regret, along with convergence guarantees to an approximate Stackelberg equilibrium, are provided.</p></p class="citation"></blockquote><h2 id=csds-1>cs.DS (1)</h2><h3 id=11--145145-circular-arc-graphs-and-the-helly-property-jan-derbisz-et-al-2024>(1/1 | 145/145) Circular-arc graphs and the Helly property (Jan Derbisz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan Derbisz, Tomasz Krawczyk. (2024)<br><strong>Circular-arc graphs and the Helly property</strong><br><button class=copy-to-clipboard title="Circular-arc graphs and the Helly property" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00416v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00416v1.pdf filename=2404.00416v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we investigate some problems related to the Helly properties of circular-arc <b>graphs,</b> which are defined as intersection <b>graphs</b> of arcs of a fixed circle. As such, circular-arc <b>graphs</b> are among the simplest classes of intersection <b>graphs</b> whose models might not satisfy the Helly property. In particular, some cliques of a circular-arc <b>graph</b> might be Helly in some but not all arc intersection models of the <b>graph.</b> Our first result is an alternative proof of a theorem by Lin and Szwarcfiter which asserts that for every circular-arc <b>graph</b> $G$ either every normalized model of $G$ satisfies the Helly property or no normalized model of $G$ satisfies this property. Further, we study the Helly properties of a single clique of a circular-arc <b>graph</b> $G$. We divide the cliques of $G$ into three types: a clique $C$ of $G$ is always-Helly/always-non-Helly/ambiguous if $C$ is Helly in every/no/(some but not all) normalized model of $G$. We provide a combinatorial description for the cliques of each type, and based on it, we devise a polynomial time algorithm which determines the type of a given clique. Finally, we study the Helly Cliques problem, in which we are given an $n$-vertex circular-arc <b>graph</b> $G$ and some of its cliques $C_1, \ldots, C_k$ and we ask if there is an arc intersection model of $G$ in which all the cliques $C_1, \ldots, C_k$ satisfy the Helly property. We show that: (1) the Helly Cliques problem admits a $2^{O(k\log{k})}n^{O(1)}$-time algorithm (that is, it is FPT when parametrized by the number of cliques given in the input), (2) assuming Exponential Time Hypothesis (ETH), the Helly Cliques problem cannot be solved in time $2^{o(k)}n^{O(1)}$, (3) the Helly Cliques problem admits a polynomial kernel of size $O(k^6)$. All our results use a data structure, called a PQM-tree, which maintains all normalized models of a circular-arc <b>graph</b> $G$.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.31</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.04.02</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-42>cs.CL (42)</a><ul><li><a href=#142--1145-metaie-distilling-a-meta-model-from-llm-for-all-kinds-of-information-extraction-tasks-letian-peng-et-al-2024>(1/42 | 1/145) MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks (Letian Peng et al., 2024)</a></li><li><a href=#242--2145-dialectical-alignment-resolving-the-tension-of-3h-and-security-threats-of-llms-shu-yang-et-al-2024>(2/42 | 2/145) Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs (Shu Yang et al., 2024)</a></li><li><a href=#342--3145-edinburgh-clinical-nlp-at-semeval-2024-task-2-fine-tune-your-model-unless-you-have-access-to-gpt-4-aryo-pradipta-gema-et-al-2024>(3/42 | 3/145) Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model unless you have access to GPT-4 (Aryo Pradipta Gema et al., 2024)</a></li><li><a href=#442--4145-a-comprehensive-study-on-nlp-data-augmentation-for-hate-speech-detection-legacy-methods-bert-and-llms-md-saroar-jahan-et-al-2024>(4/42 | 4/145) A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs (Md Saroar Jahan et al., 2024)</a></li><li><a href=#542--5145-eventground-narrative-reasoning-by-grounding-to-eventuality-centric-knowledge-graphs-cheng-jiayang-et-al-2024>(5/42 | 5/145) EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs (Cheng Jiayang et al., 2024)</a></li><li><a href=#642--6145-injecting-new-knowledge-into-large-language-models-via-supervised-fine-tuning-nick-mecklenburg-et-al-2024>(6/42 | 6/145) Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning (Nick Mecklenburg et al., 2024)</a></li><li><a href=#742--7145-can-llms-master-math-investigating-large-language-models-on-math-stack-exchange-ankit-satpute-et-al-2024>(7/42 | 7/145) Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange (Ankit Satpute et al., 2024)</a></li><li><a href=#842--8145-coda-constrained-generation-based-data-augmentation-for-low-resource-nlp-chandra-kiran-reddy-evuru-et-al-2024>(8/42 | 8/145) CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP (Chandra Kiran Reddy Evuru et al., 2024)</a></li><li><a href=#942--9145-dilm-distilling-dataset-into-language-model-for-text-level-dataset-distillation-aru-maekawa-et-al-2024>(9/42 | 9/145) DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation (Aru Maekawa et al., 2024)</a></li><li><a href=#1042--10145-small-language-models-learn-enhanced-reasoning-skills-from-medical-textbooks-hyunjae-kim-et-al-2024>(10/42 | 10/145) Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks (Hyunjae Kim et al., 2024)</a></li><li><a href=#1142--11145-controllable-and-diverse-data-augmentation-with-large-language-model-for-low-resource-open-domain-dialogue-generation-zhenhua-liu-et-al-2024>(11/42 | 11/145) Controllable and Diverse Data Augmentation with Large Language Model for Low-Resource Open-Domain Dialogue Generation (Zhenhua Liu et al., 2024)</a></li><li><a href=#1242--12145-secret-keepers-the-impact-of-llms-on-linguistic-markers-of-personal-traits-zhivar-sourati-et-al-2024>(12/42 | 12/145) Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits (Zhivar Sourati et al., 2024)</a></li><li><a href=#1342--13145-your-co-workers-matter-evaluating-collaborative-capabilities-of-language-models-in-blocks-world-guande-wu-et-al-2024>(13/42 | 13/145) Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World (Guande Wu et al., 2024)</a></li><li><a href=#1442--14145-trabsa-interpretable-sentiment-analysis-of-tweets-using-attention-based-bilstm-and-twitter-roberta-md-abrar-jahin-et-al-2024>(14/42 | 14/145) TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa (Md Abrar Jahin et al., 2024)</a></li><li><a href=#1542--15145-classification-and-clustering-of-sentence-level-embeddings-of-scientific-articles-generated-by-contrastive-learning-gustavo-bartz-guedes-et-al-2024>(15/42 | 15/145) Classification and Clustering of Sentence-Level Embeddings of Scientific Articles Generated by Contrastive Learning (Gustavo Bartz Guedes et al., 2024)</a></li><li><a href=#1642--16145-multi-conditional-ranking-with-large-language-models-pouya-pezeshkpour-et-al-2024>(16/42 | 16/145) Multi-Conditional Ranking with Large Language Models (Pouya Pezeshkpour et al., 2024)</a></li><li><a href=#1742--17145-configurable-safety-tuning-of-language-models-with-synthetic-preference-data-victor-gallego-2024>(17/42 | 17/145) Configurable Safety Tuning of Language Models with Synthetic Preference Data (Victor Gallego, 2024)</a></li><li><a href=#1842--18145-planning-and-editing-what-you-retrieve-for-enhanced-tool-learning-tenghao-huang-et-al-2024>(18/42 | 18/145) Planning and Editing What You Retrieve for Enhanced Tool Learning (Tenghao Huang et al., 2024)</a></li><li><a href=#1942--19145-aurora-m-the-first-open-source-multilingual-language-model-red-teamed-according-to-the-us-executive-order-taishi-nakamura-et-al-2024>(19/42 | 19/145) Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order (Taishi Nakamura et al., 2024)</a></li><li><a href=#2042--20145-augmenting-ner-datasets-with-llms-towards-automated-and-refined-annotation-yuji-naraki-et-al-2024>(20/42 | 20/145) Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation (Yuji Naraki et al., 2024)</a></li><li><a href=#2142--21145-deft-flash-tree-attention-with-io-awareness-for-efficient-tree-search-based-llm-inference-jinwei-yao-et-al-2024>(21/42 | 21/145) DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference (Jinwei Yao et al., 2024)</a></li><li><a href=#2242--22145-unimeec-towards-unified-multimodal-emotion-recognition-and-emotion-cause-guimin-hu-et-al-2024>(22/42 | 22/145) UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion Cause (Guimin Hu et al., 2024)</a></li><li><a href=#2342--23145-multi-hop-question-answering-under-temporal-knowledge-editing-keyuan-cheng-et-al-2024>(23/42 | 23/145) Multi-hop Question Answering under Temporal Knowledge Editing (Keyuan Cheng et al., 2024)</a></li><li><a href=#2442--24145-prompt-saw-leveraging-relation-aware-graphs-for-textual-prompt-compression-muhammad-asif-ali-et-al-2024>(24/42 | 24/145) PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression (Muhammad Asif Ali et al., 2024)</a></li><li><a href=#2542--25145-finefake-a-knowledge-enriched-dataset-for-fine-grained-multi-domain-fake-news-detecction-ziyi-zhou-et-al-2024>(25/42 | 25/145) FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain Fake News Detecction (Ziyi Zhou et al., 2024)</a></li><li><a href=#2642--26145-numerologic-number-encoding-for-enhanced-llms-numerical-reasoning-eli-schwartz-et-al-2024>(26/42 | 26/145) NumeroLogic: Number Encoding for Enhanced LLMs&rsquo; Numerical Reasoning (Eli Schwartz et al., 2024)</a></li><li><a href=#2742--27145-how-robust-are-the-tabular-qa-models-for-scientific-tables-a-study-using-customized-dataset-akash-ghosh-et-al-2024>(27/42 | 27/145) How Robust are the Tabular QA Models for Scientific Tables? A Study using Customized Dataset (Akash Ghosh et al., 2024)</a></li><li><a href=#2842--28145-addressing-both-statistical-and-causal-gender-fairness-in-nlp-models-hannah-chen-et-al-2024>(28/42 | 28/145) Addressing Both Statistical and Causal Gender Fairness in NLP Models (Hannah Chen et al., 2024)</a></li><li><a href=#2942--29145-automatic-detection-of-relevant-information-predictions-and-forecasts-in-financial-news-through-topic-modelling-with-latent-dirichlet-allocation-silvia-garcía-méndez-et-al-2024>(29/42 | 29/145) Automatic detection of relevant information, predictions and forecasts in financial news through topic modelling with Latent Dirichlet Allocation (Silvia García-Méndez et al., 2024)</a></li><li><a href=#3042--30145-an-analysis-of-bpe-vocabulary-trimming-in-neural-machine-translation-marco-cognetta-et-al-2024>(30/42 | 30/145) An Analysis of BPE Vocabulary Trimming in Neural Machine Translation (Marco Cognetta et al., 2024)</a></li><li><a href=#3142--31145-jetsons-at-finnlp-2024-towards-understanding-the-esg-impact-of-a-news-article-using-transformer-based-models-parag-pravin-dakle-et-al-2024>(31/42 | 31/145) Jetsons at FinNLP 2024: Towards Understanding the ESG Impact of a News Article using Transformer-based Models (Parag Pravin Dakle et al., 2024)</a></li><li><a href=#3242--32145-rationale-based-opinion-summarization-haoyuan-li-et-al-2024>(32/42 | 32/145) Rationale-based Opinion Summarization (Haoyuan Li et al., 2024)</a></li><li><a href=#3342--33145-cross-lingual-named-entity-corpus-for-slavic-languages-jakub-piskorski-et-al-2024>(33/42 | 33/145) Cross-lingual Named Entity Corpus for Slavic Languages (Jakub Piskorski et al., 2024)</a></li><li><a href=#3442--34145-is-factuality-decoding-a-free-lunch-for-llms-evaluation-on-knowledge-editing-benchmark-baolong-bi-et-al-2024>(34/42 | 34/145) Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark (Baolong Bi et al., 2024)</a></li><li><a href=#3542--35145-conceptual-and-unbiased-reasoning-in-language-models-ben-zhou-et-al-2024>(35/42 | 35/145) Conceptual and Unbiased Reasoning in Language Models (Ben Zhou et al., 2024)</a></li><li><a href=#3642--36145-noise-aware-training-of-layout-aware-language-models-ritesh-sarkhel-et-al-2024>(36/42 | 36/145) Noise-Aware Training of Layout-Aware Language Models (Ritesh Sarkhel et al., 2024)</a></li><li><a href=#3742--37145-the-shape-of-word-embeddings-recognizing-language-phylogenies-through-topological-data-analysis-ondřej-draganov-et-al-2024>(37/42 | 37/145) The Shape of Word Embeddings: Recognizing Language Phylogenies through Topological Data Analysis (Ondřej Draganov et al., 2024)</a></li><li><a href=#3842--38145-docmaster-a-unified-platform-for-annotation-training--inference-in-document-question-answering-alex-nguyen-et-al-2024>(38/42 | 38/145) DOCMASTER: A Unified Platform for Annotation, Training, & Inference in Document Question-Answering (Alex Nguyen et al., 2024)</a></li><li><a href=#3942--39145-automatic-explanation-of-the-classification-of-spanish-legal-judgments-in-jurisdiction-dependent-law-categories-with-tree-estimators-jaime-gonzález-gonzález-et-al-2024>(39/42 | 39/145) Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent law categories with tree estimators (Jaime González-González et al., 2024)</a></li><li><a href=#4042--40145-detection-of-temporality-at-discourse-level-on-financial-news-by-combining-natural-language-processing-and-machine-learning-silvia-garcía-méndez-et-al-2024>(40/42 | 40/145) Detection of Temporality at Discourse Level on Financial News by Combining Natural Language Processing and Machine Learning (Silvia García-Méndez et al., 2024)</a></li><li><a href=#4142--41145-taco----twitter-arguments-from-conversations-marc-feger-et-al-2024>(41/42 | 41/145) TACO &ndash; Twitter Arguments from COnversations (Marc Feger et al., 2024)</a></li><li><a href=#4242--42145-causal-inference-for-human-language-model-collaboration-bohan-zhang-et-al-2024>(42/42 | 42/145) Causal Inference for Human-Language Model Collaboration (Bohan Zhang et al., 2024)</a></li></ul></li><li><a href=#cslg-16>cs.LG (16)</a><ul><li><a href=#116--43145-tg-nas-leveraging-zero-cost-proxies-with-transformer-and-graph-convolution-networks-for-efficient-neural-architecture-search-ye-qiao-et-al-2024>(1/16 | 43/145) TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph Convolution Networks for Efficient Neural Architecture Search (Ye Qiao et al., 2024)</a></li><li><a href=#216--44145-linguistic-calibration-of-language-models-neil-band-et-al-2024>(2/16 | 44/145) Linguistic Calibration of Language Models (Neil Band et al., 2024)</a></li><li><a href=#316--45145-shortcuts-arising-from-contrast-effective-and-covert-clean-label-attacks-in-prompt-based-learning-xiaopeng-xie-et-al-2024>(3/16 | 45/145) Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning (Xiaopeng Xie et al., 2024)</a></li><li><a href=#416--46145-quarot-outlier-free-4-bit-inference-in-rotated-llms-saleh-ashkboos-et-al-2024>(4/16 | 46/145) QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs (Saleh Ashkboos et al., 2024)</a></li><li><a href=#516--47145-heterogeneous-contrastive-learning-for-foundation-models-and-beyond-lecheng-zheng-et-al-2024>(5/16 | 47/145) Heterogeneous Contrastive Learning for Foundation Models and Beyond (Lecheng Zheng et al., 2024)</a></li><li><a href=#616--48145-zero-shot-safety-prediction-for-autonomous-robots-with-foundation-world-models-zhenjiang-mao-et-al-2024>(6/16 | 48/145) Zero-shot Safety Prediction for Autonomous Robots with Foundation World Models (Zhenjiang Mao et al., 2024)</a></li><li><a href=#716--49145-leveraging-pre-trained-and-transformer-derived-embeddings-from-ehrs-to-characterize-heterogeneity-across-alzheimers-disease-and-related-dementias-matthew-west-et-al-2024>(7/16 | 49/145) Leveraging Pre-trained and Transformer-derived Embeddings from EHRs to Characterize Heterogeneity Across Alzheimer&rsquo;s Disease and Related Dementias (Matthew West et al., 2024)</a></li><li><a href=#816--50145-orchestrate-latent-expertise-advancing-online-continual-learning-with-multi-level-supervision-and-reverse-self-distillation-hongwei-yan-et-al-2024>(8/16 | 50/145) Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation (HongWei Yan et al., 2024)</a></li><li><a href=#916--51145-survey-on-large-language-model-enhanced-reinforcement-learning-concept-taxonomy-and-methods-yuji-cao-et-al-2024>(9/16 | 51/145) Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods (Yuji Cao et al., 2024)</a></li><li><a href=#1016--52145-continual-learning-for-autonomous-robots-a-prototype-based-approach-elvin-hajizada-et-al-2024>(10/16 | 52/145) Continual Learning for Autonomous Robots: A Prototype-based Approach (Elvin Hajizada et al., 2024)</a></li><li><a href=#1116--53145-from-learning-to-analytics-improving-model-efficacy-with-goal-directed-client-selection-jingwen-tong-et-al-2024>(11/16 | 53/145) From Learning to Analytics: Improving Model Efficacy with Goal-Directed Client Selection (Jingwen Tong et al., 2024)</a></li><li><a href=#1216--54145-inflora-interference-free-low-rank-adaptation-for-continual-learning-yan-shuo-liang-et-al-2024>(12/16 | 54/145) InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning (Yan-Shuo Liang et al., 2024)</a></li><li><a href=#1316--55145-de-hnn-an-effective-neural-model-for-circuit-netlist-representation-zhishang-luo-et-al-2024>(13/16 | 55/145) DE-HNN: An effective neural model for Circuit Netlist representation (Zhishang Luo et al., 2024)</a></li><li><a href=#1416--56145-clustering-for-protein-representation-learning-ruijie-quan-et-al-2024>(14/16 | 56/145) Clustering for Protein Representation Learning (Ruijie Quan et al., 2024)</a></li><li><a href=#1516--57145-computation-and-communication-efficient-lightweighting-vertical-federated-learning-heqiang-wang-et-al-2024>(15/16 | 57/145) Computation and Communication Efficient Lightweighting Vertical Federated Learning (Heqiang Wang et al., 2024)</a></li><li><a href=#1616--58145-generative-ai-for-architectural-design-a-literature-review-chengyuan-li-et-al-2024>(16/16 | 58/145) Generative AI for Architectural Design: A Literature Review (Chengyuan Li et al., 2024)</a></li></ul></li><li><a href=#cscv-35>cs.CV (35)</a><ul><li><a href=#135--59145-design-as-desired-utilizing-visual-question-answering-for-multimodal-pre-training-tongkun-su-et-al-2024>(1/35 | 59/145) Design as Desired: Utilizing Visual Question Answering for Multimodal Pre-training (Tongkun Su et al., 2024)</a></li><li><a href=#235--60145-clip-driven-outliers-synthesis-for-few-shot-ood-detection-hao-sun-et-al-2024>(2/35 | 60/145) CLIP-driven Outliers Synthesis for few-shot OOD detection (Hao Sun et al., 2024)</a></li><li><a href=#335--61145-hsimamba-hyperpsectral-imaging-efficient-feature-learning-with-bidirectional-state-space-for-classification-judy-x-yang-et-al-2024>(3/35 | 61/145) HSIMamba: Hyperpsectral Imaging Efficient Feature Learning with Bidirectional State Space for Classification (Judy X Yang et al., 2024)</a></li><li><a href=#435--62145-ttd-text-tag-self-distillation-enhancing-image-text-alignment-in-clip-to-alleviate-single-tag-bias-sanghyun-jo-et-al-2024>(4/35 | 62/145) TTD: Text-Tag Self-Distillation Enhancing Image-Text Alignment in CLIP to Alleviate Single Tag Bias (Sanghyun Jo et al., 2024)</a></li><li><a href=#535--63145-reusable-architecture-growth-for-continual-stereo-matching-chenghao-zhang-et-al-2024>(5/35 | 63/145) Reusable Architecture Growth for Continual Stereo Matching (Chenghao Zhang et al., 2024)</a></li><li><a href=#635--64145-do-vision-language-models-understand-compound-nouns-sonal-kumar-et-al-2024>(6/35 | 64/145) Do Vision-Language Models Understand Compound Nouns? (Sonal Kumar et al., 2024)</a></li><li><a href=#735--65145-yolooc-yolo-based-open-class-incremental-object-detection-with-novel-class-discovery-qian-wan-et-al-2024>(7/35 | 65/145) YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery (Qian Wan et al., 2024)</a></li><li><a href=#835--66145-st-llm-large-language-models-are-effective-temporal-learners-ruyang-liu-et-al-2024>(8/35 | 66/145) ST-LLM: Large Language Models Are Effective Temporal Learners (Ruyang Liu et al., 2024)</a></li><li><a href=#935--67145-magritte-manipulative-and-generative-3d-realization-from-image-topview-and-text-takayuki-hara-et-al-2024>(9/35 | 67/145) MaGRITTe: Manipulative and Generative 3D Realization from Image, Topview and Text (Takayuki Hara et al., 2024)</a></li><li><a href=#1035--68145-constrained-layout-generation-with-factor-graphs-mohammed-haroon-dupty-et-al-2024>(10/35 | 68/145) Constrained Layout Generation with Factor Graphs (Mohammed Haroon Dupty et al., 2024)</a></li><li><a href=#1135--69145-dhr-dual-features-driven-hierarchical-rebalancing-in-inter--and-intra-class-regions-for-weakly-supervised-semantic-segmentation-sanghyun-jo-et-al-2024>(11/35 | 69/145) DHR: Dual Features-Driven Hierarchical Rebalancing in Inter- and Intra-Class Regions for Weakly-Supervised Semantic Segmentation (Sanghyun Jo et al., 2024)</a></li><li><a href=#1235--70145-bayesian-exploration-of-pre-trained-models-for-low-shot-image-classification-yibo-miao-et-al-2024>(12/35 | 70/145) Bayesian Exploration of Pre-trained Models for Low-shot Image Classification (Yibo Miao et al., 2024)</a></li><li><a href=#1335--71145-seeing-the-unseen-a-frequency-prompt-guided-transformer-for-image-restoration-shihao-zhou-et-al-2024>(13/35 | 71/145) Seeing the Unseen: A Frequency Prompt Guided Transformer for Image Restoration (Shihao Zhou et al., 2024)</a></li><li><a href=#1435--72145-towards-variable-and-coordinated-holistic-co-speech-motion-generation-yifei-liu-et-al-2024>(14/35 | 72/145) Towards Variable and Coordinated Holistic Co-Speech Motion Generation (Yifei Liu et al., 2024)</a></li><li><a href=#1535--73145-look-around-before-you-leap-high-frequency-injected-transformer-for-image-restoration-shihao-zhou-et-al-2024>(15/35 | 73/145) Look-Around Before You Leap: High-Frequency Injected Transformer for Image Restoration (Shihao Zhou et al., 2024)</a></li><li><a href=#1635--74145-exploiting-self-supervised-constraints-in-image-super-resolution-gang-wu-et-al-2024>(16/35 | 74/145) Exploiting Self-Supervised Constraints in Image Super-Resolution (Gang Wu et al., 2024)</a></li><li><a href=#1735--75145-svgcraft-beyond-single-object-text-to-svg-synthesis-with-comprehensive-canvas-layout-ayan-banerjee-et-al-2024>(17/35 | 75/145) SVGCraft: Beyond Single Object Text-to-SVG Synthesis with Comprehensive Canvas Layout (Ayan Banerjee et al., 2024)</a></li><li><a href=#1835--76145-spread-your-wings-a-radial-strip-transformer-for-image-deblurring-duosheng-chen-et-al-2024>(18/35 | 76/145) Spread Your Wings: A Radial Strip Transformer for Image Deblurring (Duosheng Chen et al., 2024)</a></li><li><a href=#1935--77145-learing-trimaps-via-clicks-for-image-matting-chenyi-zhang-et-al-2024>(19/35 | 77/145) Learing Trimaps via Clicks for Image Matting (Chenyi Zhang et al., 2024)</a></li><li><a href=#2035--78145-grid-diffusion-models-for-text-to-video-generation-taegyeong-lee-et-al-2024>(20/35 | 78/145) Grid Diffusion Models for Text-to-Video Generation (Taegyeong Lee et al., 2024)</a></li><li><a href=#2135--79145-stba-towards-evaluating-the-robustness-of-dnns-for-query-limited-black-box-scenario-renyang-liu-et-al-2024>(21/35 | 79/145) STBA: Towards Evaluating the Robustness of DNNs for Query-Limited Black-box Scenario (Renyang Liu et al., 2024)</a></li><li><a href=#2235--80145-rethinking-attention-based-multiple-instance-learning-for-whole-slide-pathological-image-classification-an-instance-attribute-viewpoint-linghan-cai-et-al-2024>(22/35 | 80/145) Rethinking Attention-Based Multiple Instance Learning for Whole-Slide Pathological Image Classification: An Instance Attribute Viewpoint (Linghan Cai et al., 2024)</a></li><li><a href=#2335--81145-image-to-image-matching-via-foundation-models-a-new-perspective-for-open-vocabulary-semantic-segmentation-yuan-wang-et-al-2024>(23/35 | 81/145) Image-to-Image Matching via Foundation Models: A New Perspective for Open-Vocabulary Semantic Segmentation (Yuan Wang et al., 2024)</a></li><li><a href=#2435--82145-denoising-monte-carlo-renders-with-diffusion-models-vaibhav-vavilala-et-al-2024>(24/35 | 82/145) Denoising Monte Carlo Renders With Diffusion Models (Vaibhav Vavilala et al., 2024)</a></li><li><a href=#2535--83145-diffhuman-probabilistic-photorealistic-3d-reconstruction-of-humans-akash-sengupta-et-al-2024>(25/35 | 83/145) DiffHuman: Probabilistic Photorealistic 3D Reconstruction of Humans (Akash Sengupta et al., 2024)</a></li><li><a href=#2635--84145-sgdformer-one-stage-transformer-based-architecture-for-cross-spectral-stereo-image-guided-denoising-runmin-zhang-et-al-2024>(26/35 | 84/145) SGDFormer: One-stage Transformer-based Architecture for Cross-Spectral Stereo Image Guided Denoising (Runmin Zhang et al., 2024)</a></li><li><a href=#2735--85145-monocular-identity-conditioned-facial-reflectance-reconstruction-xingyu-ren-et-al-2024>(27/35 | 85/145) Monocular Identity-Conditioned Facial Reflectance Reconstruction (Xingyu Ren et al., 2024)</a></li><li><a href=#2835--86145-lake-red-camouflaged-images-generation-by-latent-background-knowledge-retrieval-augmented-diffusion-pancheng-zhao-et-al-2024>(28/35 | 86/145) LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge Retrieval-Augmented Diffusion (Pancheng Zhao et al., 2024)</a></li><li><a href=#2935--87145-long-tailed-recognition-on-binary-networks-by-calibrating-a-pre-trained-model-jihun-kim-et-al-2024>(29/35 | 87/145) Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained Model (Jihun Kim et al., 2024)</a></li><li><a href=#3035--88145-ipod-implicit-field-learning-with-point-diffusion-for-generalizable-3d-object-reconstruction-from-single-rgb-d-images-yushuang-wu-et-al-2024>(30/35 | 88/145) IPoD: Implicit Field Learning with Point Diffusion for Generalizable 3D Object Reconstruction from Single RGB-D Images (Yushuang Wu et al., 2024)</a></li><li><a href=#3135--89145-latent-watermark-inject-and-detect-watermarks-in-latent-diffusion-space-zheling-meng-et-al-2024>(31/35 | 89/145) Latent Watermark: Inject and Detect Watermarks in Latent Diffusion Space (Zheling Meng et al., 2024)</a></li><li><a href=#3235--90145-scenegraphloc-cross-modal-coarse-visual-localization-on-3d-scene-graphs-yang-miao-et-al-2024>(32/35 | 90/145) SceneGraphLoc: Cross-Modal Coarse Visual Localization on 3D Scene Graphs (Yang Miao et al., 2024)</a></li><li><a href=#3335--91145-multiway-point-cloud-mosaicking-with-diffusion-and-global-optimization-shengze-jin-et-al-2024>(33/35 | 91/145) Multiway Point Cloud Mosaicking with Diffusion and Global Optimization (Shengze Jin et al., 2024)</a></li><li><a href=#3435--92145-attention-based-shape-deformation-networks-for-artifact-free-geometry-reconstruction-of-lumbar-spine-from-mr-images-linchen-qian-et-al-2024>(34/35 | 92/145) Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images (Linchen Qian et al., 2024)</a></li><li><a href=#3535--93145-instrument-tissue-interaction-detection-framework-for-surgical-video-understanding-wenjun-lin-et-al-2024>(35/35 | 93/145) Instrument-tissue Interaction Detection Framework for Surgical Video Understanding (Wenjun Lin et al., 2024)</a></li></ul></li><li><a href=#csse-3>cs.SE (3)</a><ul><li><a href=#13--94145-an-empirical-study-of-automated-vulnerability-localization-with-large-language-models-jian-zhang-et-al-2024>(1/3 | 94/145) An Empirical Study of Automated Vulnerability Localization with Large Language Models (Jian Zhang et al., 2024)</a></li><li><a href=#23--95145-a-survey-of-using-large-language-models-for-generating-infrastructure-as-code-kalahasti-ganesh-srivatsa-et-al-2024>(2/3 | 95/145) A Survey of using Large Language Models for Generating Infrastructure as Code (Kalahasti Ganesh Srivatsa et al., 2024)</a></li><li><a href=#33--96145-learning-service-selection-decision-making-behaviors-during-scientific-workflow-development-xihao-xie-et-al-2024>(3/3 | 96/145) Learning Service Selection Decision Making Behaviors During Scientific Workflow Development (Xihao Xie et al., 2024)</a></li></ul></li><li><a href=#csir-4>cs.IR (4)</a><ul><li><a href=#14--97145-aligning-large-language-models-with-recommendation-knowledge-yuwei-cao-et-al-2024>(1/4 | 97/145) Aligning Large Language Models with Recommendation Knowledge (Yuwei Cao et al., 2024)</a></li><li><a href=#24--98145-a-unified-framework-for-adaptive-representation-enhancement-and-inversed-learning-in-cross-domain-recommendation-luankang-zhang-et-al-2024>(2/4 | 98/145) A Unified Framework for Adaptive Representation Enhancement and Inversed Learning in Cross-Domain Recommendation (Luankang Zhang et al., 2024)</a></li><li><a href=#34--99145-a-simple-yet-effective-approach-for-diversified-session-based-recommendation-qing-yin-et-al-2024>(3/4 | 99/145) A Simple Yet Effective Approach for Diversified Session-Based Recommendation (Qing Yin et al., 2024)</a></li><li><a href=#44--100145-enhancing-content-based-recommendation-via-large-language-model-wentao-xu-et-al-2024>(4/4 | 100/145) Enhancing Content-based Recommendation via Large Language Model (Wentao Xu et al., 2024)</a></li></ul></li><li><a href=#physicsspace-ph-1>physics.space-ph (1)</a><ul><li><a href=#11--101145-language-models-are-spacecraft-operators-victor-rodriguez-fernandez-et-al-2024>(1/1 | 101/145) Language Models are Spacecraft Operators (Victor Rodriguez-Fernandez et al., 2024)</a></li></ul></li><li><a href=#csro-14>cs.RO (14)</a><ul><li><a href=#114--102145-exploring-unseen-environments-with-robots-using-large-language-and-vision-models-through-a-procedurally-generated-3d-scene-representation-arjun-p-s-et-al-2024>(1/14 | 102/145) Exploring Unseen Environments with Robots using Large Language and Vision Models through a Procedurally Generated 3D Scene Representation (Arjun P S et al., 2024)</a></li><li><a href=#214--103145-efficient-automatic-tuning-for-data-driven-model-predictive-control-via-meta-learning-baoyu-li-et-al-2024>(2/14 | 103/145) Efficient Automatic Tuning for Data-driven Model Predictive Control via Meta-Learning (Baoyu Li et al., 2024)</a></li><li><a href=#314--104145-thin-shell-object-manipulations-with-differentiable-physics-simulations-yian-wang-et-al-2024>(3/14 | 104/145) Thin-Shell Object Manipulations With Differentiable Physics Simulations (Yian Wang et al., 2024)</a></li><li><a href=#414--105145-accurate-cutting-point-estimation-for-robotic-lychee-harvesting-through-geometry-aware-learning-gengming-zhang-et-al-2024>(4/14 | 105/145) Accurate Cutting-point Estimation for Robotic Lychee Harvesting through Geometry-aware Learning (Gengming Zhang et al., 2024)</a></li><li><a href=#514--106145-commonsense-scene-graph-based-target-localization-for-object-search-wenqi-ge-et-al-2024>(5/14 | 106/145) Commonsense Scene Graph-based Target Localization for Object Search (Wenqi Ge et al., 2024)</a></li><li><a href=#614--107145-ude-based-dynamic-motion-force-control-of-mobile-manipulators-songqun-gao-et-al-2024>(6/14 | 107/145) UDE-based Dynamic Motion Force Control of Mobile Manipulators (Songqun Gao et al., 2024)</a></li><li><a href=#714--108145-cbf-based-motion-planning-for-socially-responsible-robot-navigation-guaranteeing-stl-specification-andrea-ruo-et-al-2024>(7/14 | 108/145) CBF-Based Motion Planning for Socially Responsible Robot Navigation Guaranteeing STL Specification (Andrea Ruo et al., 2024)</a></li><li><a href=#814--109145-cbf-based-stl-motion-planning-for-social-navigation-in-crowded-environment-andrea-ruo-et-al-2024>(8/14 | 109/145) CBF-Based STL Motion Planning for Social Navigation in Crowded Environment (Andrea Ruo et al., 2024)</a></li><li><a href=#914--110145-socially-aware-robot-navigation-through-scoring-using-vision-language-models-daeun-song-et-al-2024>(9/14 | 110/145) Socially Aware Robot Navigation through Scoring Using Vision-Language Models (Daeun Song et al., 2024)</a></li><li><a href=#1014--111145-designing-robot-identity-the-role-of-voice-clothing-and-task-on-robot-gender-perception-nathaniel-s-dennler-et-al-2024>(10/14 | 111/145) Designing Robot Identity: The Role of Voice, Clothing, and Task on Robot Gender Perception (Nathaniel S. Dennler et al., 2024)</a></li><li><a href=#1114--112145-deep-reinforcement-learning-in-autonomous-car-path-planning-and-control-a-survey-yiyang-chen-et-al-2024>(11/14 | 112/145) Deep Reinforcement Learning in Autonomous Car Path Planning and Control: A Survey (Yiyang Chen et al., 2024)</a></li><li><a href=#1214--113145-joint-pedestrian-trajectory-prediction-through-posterior-sampling-haotian-lin-et-al-2024>(12/14 | 113/145) Joint Pedestrian Trajectory Prediction through Posterior Sampling (Haotian Lin et al., 2024)</a></li><li><a href=#1314--114145-a-ppo-based-drl-auto-tuning-nonlinear-pid-drone-controller-for-robust-autonomous-flights-junyang-zhang-et-al-2024>(13/14 | 114/145) A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights (Junyang Zhang et al., 2024)</a></li><li><a href=#1414--115145-self-corrective-sensor-fusion-for-drone-positioning-in-indoor-facilities-francisco-javier-gonzález-castaño-et-al-2024>(14/14 | 115/145) Self-Corrective Sensor Fusion for Drone Positioning in Indoor Facilities (Francisco Javier González-Castaño et al., 2024)</a></li></ul></li><li><a href=#cscr-1>cs.CR (1)</a><ul><li><a href=#11--116145-privacy-backdoors-stealing-data-with-corrupted-pretrained-models-shanglun-feng-et-al-2024>(1/1 | 116/145) Privacy Backdoors: Stealing Data with Corrupted Pretrained Models (Shanglun Feng et al., 2024)</a></li></ul></li><li><a href=#cshc-3>cs.HC (3)</a><ul><li><a href=#13--117145-a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration-jie-gao-et-al-2024>(1/3 | 117/145) A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration (Jie Gao et al., 2024)</a></li><li><a href=#23--118145-contextual-ai-journaling-integrating-llm-and-time-series-behavioral-sensing-technology-to-promote-self-reflection-and-well-being-using-the-mindscape-app-subigya-nepal-et-al-2024>(2/3 | 118/145) Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App (Subigya Nepal et al., 2024)</a></li><li><a href=#33--119145-enhancing-empathy-in-virtual-reality-an-embodied-approach-to-mindset-modulation-seoyeon-bae-et-al-2024>(3/3 | 119/145) Enhancing Empathy in Virtual Reality: An Embodied Approach to Mindset Modulation (Seoyeon Bae et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--120145-sa-lsplsequence-aware-long--and-short--term-preference-learning-for-next-poi-recommendation-bin-wang-et-al-2024>(1/1 | 120/145) SA-LSPL:Sequence-Aware Long- and Short- Term Preference Learning for next POI recommendation (Bin Wang et al., 2024)</a></li></ul></li><li><a href=#cssd-1>cs.SD (1)</a><ul><li><a href=#11--121145-classification-of-short-segment-pediatric-heart-sounds-based-on-a-transformer-based-convolutional-neural-network-md-hassanuzzaman-et-al-2024>(1/1 | 121/145) Classification of Short Segment Pediatric Heart Sounds Based on a Transformer-Based Convolutional Neural Network (Md Hassanuzzaman et al., 2024)</a></li></ul></li><li><a href=#q-finmf-1>q-fin.MF (1)</a><ul><li><a href=#11--122145-from-attention-to-profit-quantitative-trading-strategy-based-on-transformer-zhaofeng-zhang-et-al-2024>(1/1 | 122/145) From attention to profit: quantitative trading strategy based on transformer (Zhaofeng Zhang et al., 2024)</a></li></ul></li><li><a href=#csai-2>cs.AI (2)</a><ul><li><a href=#12--123145-instruction-driven-game-engines-on-large-language-models-hongqiu-wu-et-al-2024>(1/2 | 123/145) Instruction-Driven Game Engines on Large Language Models (Hongqiu Wu et al., 2024)</a></li><li><a href=#22--124145-advancing-multimodal-data-fusion-in-pain-recognition-a-strategy-leveraging-statistical-correlation-and-human-centered-perspectives-xingrui-gu-et-al-2024>(2/2 | 124/145) Advancing Multimodal Data Fusion in Pain Recognition: A Strategy Leveraging Statistical Correlation and Human-Centered Perspectives (Xingrui Gu et al., 2024)</a></li></ul></li><li><a href=#eesssy-3>eess.SY (3)</a><ul><li><a href=#13--125145-facilitating-reinforcement-learning-for-process-control-using-transfer-learning-perspectives-runze-lin-et-al-2024>(1/3 | 125/145) Facilitating Reinforcement Learning for Process Control Using Transfer Learning: Perspectives (Runze Lin et al., 2024)</a></li><li><a href=#23--126145-analysis-of-fairness-promoting-optimization-schemes-of-photovoltaic-curtailments-for-voltage-regulation-in-power-distribution-networks-rahul-k-gupta-et-al-2024>(2/3 | 126/145) Analysis of Fairness-promoting Optimization Schemes of Photovoltaic Curtailments for Voltage Regulation in Power Distribution Networks (Rahul K. Gupta et al., 2024)</a></li><li><a href=#33--127145-on-accessibility-fairness-in-intermodal-autonomous-mobility-on-demand-systems-mauro-salazar-et-al-2024>(3/3 | 127/145) On Accessibility Fairness in Intermodal Autonomous Mobility-on-Demand Systems (Mauro Salazar et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--128145-ccwsim-an-efficient-and-fast-wavelet-based-ccsim-for-categorical-characterization-of-large-scale-mojtaba-bavandsavadkoohi-et-al-2024>(1/1 | 128/145) CCWSIM: An Efficient and Fast Wavelet-Based CCSIM for Categorical Characterization of Large-Scale (Mojtaba Bavandsavadkoohi et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--129145-learning-truly-monotone-operators-with-applications-to-nonlinear-inverse-problems-younes-belkouchi-et-al-2024>(1/1 | 129/145) Learning truly monotone operators with applications to nonlinear inverse problems (Younes Belkouchi et al., 2024)</a></li></ul></li><li><a href=#physicscomp-ph-1>physics.comp-ph (1)</a><ul><li><a href=#11--130145-predicting-the-statistical-error-of-analog-particle-tracing-monte-carlo-vince-maes-et-al-2024>(1/1 | 130/145) Predicting the statistical error of analog particle tracing Monte Carlo (Vince Maes et al., 2024)</a></li></ul></li><li><a href=#csit-2>cs.IT (2)</a><ul><li><a href=#12--131145-model-driven-deep-learning-for-distributed-detection-with-binary-quantization-wei-guo-et-al-2024>(1/2 | 131/145) Model-Driven Deep Learning for Distributed Detection with Binary Quantization (Wei Guo et al., 2024)</a></li><li><a href=#22--132145-environment-aware-codebook-for-ris-assisted-mu-miso-communications-implementation-and-performance-analysis-zhiheng-yu-et-al-2024>(2/2 | 132/145) Environment-Aware Codebook for RIS-Assisted MU-MISO Communications: Implementation and Performance Analysis (Zhiheng Yu et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#12--133145-numerical-simulations-for-fractional-differential-equations-of-higher-order-and-a-wright-type-transformation-m-nacianceno-et-al-2024>(1/2 | 133/145) Numerical Simulations for Fractional Differential Equations of Higher Order and a Wright-Type Transformation (M. Nacianceno et al., 2024)</a></li><li><a href=#22--134145-geometric-mean-for-t-positive-definite-tensors-and-associated-riemannian-geometry-jeong-hoon-ju-et-al-2024>(2/2 | 134/145) Geometric mean for T-positive definite tensors and associated Riemannian geometry (Jeong-Hoon Ju et al., 2024)</a></li></ul></li><li><a href=#statml-3>stat.ML (3)</a><ul><li><a href=#13--135145-functional-edged-network-modeling-haijie-xu-et-al-2024>(1/3 | 135/145) Functional-Edged Network Modeling (Haijie Xu et al., 2024)</a></li><li><a href=#23--136145-convolutional-bayesian-filtering-wenhan-cao-et-al-2024>(2/3 | 136/145) Convolutional Bayesian Filtering (Wenhan Cao et al., 2024)</a></li><li><a href=#33--137145-partially-observable-sequential-change-point-detection-for-autocorrelated-data-via-upper-confidence-region-haijie-xu-et-al-2024>(3/3 | 137/145) Partially-Observable Sequential Change-Point Detection for Autocorrelated Data via Upper Confidence Region (Haijie Xu et al., 2024)</a></li></ul></li><li><a href=#physicsmed-ph-1>physics.med-ph (1)</a><ul><li><a href=#11--138145-score-based-diffusion-models-for-photoacoustic-tomography-image-reconstruction-sreemanti-dey-et-al-2024>(1/1 | 138/145) Score-Based Diffusion Models for Photoacoustic Tomography Image Reconstruction (Sreemanti Dey et al., 2024)</a></li></ul></li><li><a href=#csdc-2>cs.DC (2)</a><ul><li><a href=#12--139145-communication-efficient-distributed-training-with-distributed-lion-bo-liu-et-al-2024>(1/2 | 139/145) Communication Efficient Distributed Training with Distributed Lion (Bo Liu et al., 2024)</a></li><li><a href=#22--140145-engineering-a-workload-balanced-push-relabel-algorithm-for-massive-graphs-on-gpus-chou-ying-hsieh-et-al-2024>(2/2 | 140/145) Engineering A Workload-balanced Push-Relabel Algorithm for Massive Graphs on GPUs (Chou-Ying Hsieh et al., 2024)</a></li></ul></li><li><a href=#csne-2>cs.NE (2)</a><ul><li><a href=#12--141145-spikingjet-enhancing-fault-injection-for-fully-and-convolutional-spiking-neural-networks-anil-bayram-gogebakan-et-al-2024>(1/2 | 141/145) SpikingJET: Enhancing Fault Injection for Fully and Convolutional Spiking Neural Networks (Anil Bayram Gogebakan et al., 2024)</a></li><li><a href=#22--142145-discrete-natural-evolution-strategies-ahmad-ayaz-amin-2024>(2/2 | 142/145) Discrete Natural Evolution Strategies (Ahmad Ayaz Amin, 2024)</a></li></ul></li><li><a href=#csce-2>cs.CE (2)</a><ul><li><a href=#12--143145-leveraging-intelligent-recommender-system-as-a-first-step-resilience-measure----a-data-driven-supply-chain-disruption-response-framework-yang-hu-2024>(1/2 | 143/145) Leveraging Intelligent Recommender system as a first step resilience measure &ndash; A data-driven supply chain disruption response framework (Yang Hu, 2024)</a></li><li><a href=#22--144145-a-stackelberg-regret-minimizing-framework-for-online-learning-in-newsvendor-pricing-games-larkin-liu-et-al-2024>(2/2 | 144/145) A Stackelberg Regret Minimizing Framework for Online Learning in Newsvendor Pricing Games (Larkin Liu et al., 2024)</a></li></ul></li><li><a href=#csds-1>cs.DS (1)</a><ul><li><a href=#11--145145-circular-arc-graphs-and-the-helly-property-jan-derbisz-et-al-2024>(1/1 | 145/145) Circular-arc graphs and the Helly property (Jan Derbisz et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>