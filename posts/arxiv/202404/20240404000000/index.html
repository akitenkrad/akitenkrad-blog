<!doctype html><html><head><title>arXiv @ 2024.04.04</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.04.04"><meta property="og:description" content="Primary Categories astro-ph.IM (1) cs.AI (4) cs.AR (3) cs.CE (2) cs.CL (69) cs.CR (10) cs.CV (82) cs.DB (2) cs.DC (2) cs.DL (2) cs.DM (1) cs.DS (1) cs.FL (1) cs.GR (1) cs.HC (7) cs.IR (6) cs.IT (3) cs.LG (48) cs.MA (3) cs.NE (3) cs.NI (6) cs.RO (19) cs.SD (4) cs.SE (7) cs.SI (2) eess.AS (2) eess.IV (7) eess.SP (2) eess.SY (9) math.AC (1) math.CO (1) math.NA (5) math.OC (1) physics.bio-ph (1) physics."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202404/20240404000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-04T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-04T00:00:00+00:00"><meta name=description content="arXiv @ 2024.04.04"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240408000000/ title="arXiv @ 2024.04.08">arXiv @ 2024.04.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/ title="arXiv @ 2024.04.09">arXiv @ 2024.04.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240410000000/ title="arXiv @ 2024.04.10">arXiv @ 2024.04.10</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202404/20240404000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Thursday, Apr 4, 2024</p></div><div class=title><h1>arXiv @ 2024.04.04</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#astro-phim-1>astro-ph.IM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#csai-4>cs.AI (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#csar-3>cs.AR (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#csce-2>cs.CE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#cscl-69>cs.CL (69)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#cscr-10>cs.CR (10)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#cscv-82>cs.CV (82)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#csdb-2>cs.DB (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#csdc-2>cs.DC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#csdl-2>cs.DL (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#csdm-1>cs.DM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#csds-1>cs.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#csfl-1>cs.FL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#cshc-7>cs.HC (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#csir-6>cs.IR (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#csit-3>cs.IT (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#cslg-48>cs.LG (48)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#csma-3>cs.MA (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#csne-3>cs.NE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#csni-6>cs.NI (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#csro-19>cs.RO (19)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#cssd-4>cs.SD (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#csse-7>cs.SE (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#cssi-2>cs.SI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#eessas-2>eess.AS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#eessiv-7>eess.IV (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#eesssp-2>eess.SP (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#eesssy-9>eess.SY (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#mathac-1>math.AC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#mathna-5>math.NA (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#physicsbio-ph-1>physics.bio-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#physicsflu-dyn-1>physics.flu-dyn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#physicsmed-ph-1>physics.med-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#quant-ph-1>quant-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#statme-1>stat.ME (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/#statml-5>stat.ML (5)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CR</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Active Learning</td><td>2</td><td></td><td></td><td>1</td><td>2</td></tr><tr><td>Adversarial Attack</td><td>2</td><td>1</td><td>2</td><td>2</td><td></td></tr><tr><td>Adversarial Learning</td><td>2</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Autoencoder</td><td>1</td><td></td><td>5</td><td>1</td><td>1</td></tr><tr><td>Automatic Speech Recognition</td><td>4</td><td></td><td>2</td><td>3</td><td></td></tr><tr><td>BART</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>BERT</td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>Bard</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td>19</td><td>2</td><td>24</td><td>11</td><td>2</td></tr><tr><td>Black Box</td><td>2</td><td>1</td><td>3</td><td>1</td><td></td></tr><tr><td>ChatGPT</td><td>3</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Claude</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td>1</td><td></td><td>1</td><td>4</td><td></td></tr><tr><td>Common-sense Reasoning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Content Detection</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td></td><td>5</td><td>1</td><td></td></tr><tr><td>ControlNet</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Convolution</td><td>1</td><td></td><td>8</td><td>4</td><td></td></tr><tr><td>Convolutional Neural Network</td><td>1</td><td></td><td>11</td><td>3</td><td></td></tr><tr><td>Coreference Resolution</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Counter-factual</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Data Augmentation</td><td>1</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Differential Privacy</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>1</td><td>9</td><td>2</td><td>1</td></tr><tr><td>Disambiguation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Distribution Shift</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Event Detection</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Fact Verification</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Few-shot</td><td>3</td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Few-shot Learning</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Fine-tuning</td><td>15</td><td></td><td>5</td><td>8</td><td>1</td></tr><tr><td>Foundation Model</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>GPT</td><td>15</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>GPT-2</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-3</td><td>6</td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>5</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>8</td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-4 turbo</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Gemini</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td>2</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Geometry</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Graph</td><td>5</td><td>1</td><td>4</td><td>8</td><td>2</td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>1</td><td>6</td><td></td></tr><tr><td>Grounding</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Hate Speech Detection</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>In-context Learning</td><td>15</td><td>2</td><td></td><td></td><td>2</td></tr><tr><td>Information Retrieval</td><td>2</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Instruction Following</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Keyword Extraction</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>3</td><td></td><td>10</td><td></td><td>2</td></tr><tr><td>Knowledge Graph</td><td>3</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Knowledge Transfer</td><td>1</td><td></td><td>2</td><td></td><td></td></tr><tr><td>LLaMA</td><td>1</td><td>2</td><td></td><td></td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>63</td><td>7</td><td>3</td><td>2</td><td>5</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Low-Resource</td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>Machine Unlearning</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Meta Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Multi-modal</td><td>8</td><td></td><td>15</td><td>6</td><td>4</td></tr><tr><td>Named Entity Recognition</td><td>3</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td>5</td><td></td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>11</td><td></td><td></td></tr><tr><td>Open-Domain Question Answering</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Opinion Summarization</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Out-of-domain</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>PaLM</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Perplexity</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>7</td><td></td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Prompt</td><td>11</td><td>4</td><td>10</td><td>2</td><td></td></tr><tr><td>Prompt Learning</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td>1</td><td>3</td><td>1</td><td></td></tr><tr><td>Question Answering</td><td>9</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Reasoning</td><td>8</td><td></td><td>3</td><td>2</td><td>1</td></tr><tr><td>Recommendation</td><td>1</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Reinforcement Learning</td><td></td><td></td><td>2</td><td>5</td><td>4</td></tr><tr><td>Relation Extraction</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>7</td><td></td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>Self-Distillation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td>1</td><td></td><td>6</td><td></td><td>1</td></tr><tr><td>Self-supervised Pre-training</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Semantic Parsing</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Sentence Embedding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>1</td><td></td><td>1</td><td>4</td><td>5</td></tr><tr><td>Simulator</td><td>1</td><td></td><td>1</td><td>4</td><td>5</td></tr><tr><td>Stemming</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Style Transfer</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Summarization</td><td>5</td><td></td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td></td><td></td><td>9</td><td>3</td><td></td></tr><tr><td>Temporal Knowledge Graph</td><td>2</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Text Classification</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Text Generation</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Summarization</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Understanding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Topic Model</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Topic Modeling</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td></td><td></td><td>2</td><td>1</td></tr><tr><td>Transformer</td><td>1</td><td></td><td>16</td><td>6</td><td></td></tr><tr><td>Unsupervised Learning</td><td>1</td><td></td><td>8</td><td>4</td><td>2</td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td>2</td><td></td><td>1</td></tr><tr><td>Video-and-Language</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>6</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td></td><td>13</td><td>1</td><td>3</td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Word Embedding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Yolo</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Zero Trust</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td>2</td><td></td><td>4</td><td>1</td><td>1</td></tr><tr><td>Zero-shot Learning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>falcon</td><td>1</td><td></td><td></td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-69>cs.CL (69)</h2><h3 id=169--1327-deconstructing-in-context-learning-understanding-prompts-via-corruption-namrata-shivagunde-et-al-2024>(1/69 | 1/327) Deconstructing In-Context Learning: Understanding Prompts via Corruption (Namrata Shivagunde et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Namrata Shivagunde, Vladislav Lialin, Sherin Muckatira, Anna Rumshisky. (2024)<br><strong>Deconstructing In-Context Learning: Understanding Prompts via Corruption</strong><br><button class=copy-to-clipboard title="Deconstructing In-Context Learning: Understanding Prompts via Corruption" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 125<br>Keywords: Black Box, Few-shot, Bard, ChatGPT, Claude, GPT, GPT-3, PaLM, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02054v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02054v1.pdf filename=2404.02054v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to &ldquo;learn in context&rdquo; based on the provided <b>prompt</b> has led to an explosive growth in their use, culminating in the proliferation of AI assistants such as <b>ChatGPT,</b> <b>Claude,</b> and <b>Bard.</b> These AI assistants are known to be robust to minor <b>prompt</b> modifications, mostly due to alignment techniques that use human feedback. In contrast, the underlying pre-trained <b>LLMs</b> they use as a backbone are known to be brittle in this respect. Building high-quality backbone models remains a core challenge, and a common approach to assessing their quality is to conduct <b>few-shot</b> evaluation. Such evaluation is notorious for being highly sensitive to minor <b>prompt</b> modifications, as well as the choice of specific <b>in-context</b> <b>examples.</b> Prior work has examined how modifying different elements of the <b>prompt</b> can affect model performance. However, these earlier studies tended to concentrate on a limited number of specific <b>prompt</b> attributes and often produced contradictory results. Additionally, previous research either focused on models with fewer than 15 billion parameters or exclusively examined <b>black-box</b> <b>models</b> like <b>GPT-3</b> or <b>PaLM,</b> making replication challenging. In the present study, we decompose the entire <b>prompt</b> into four components: task description, demonstration inputs, labels, and inline instructions provided for each demonstration. We investigate the effects of structural and semantic corruptions of these elements on model performance. We study models ranging from 1.5B to 70B in size, using ten datasets covering classification and generation tasks. We find that repeating text within the <b>prompt</b> boosts model performance, and bigger models ($\geq$30B) are more sensitive to the semantics of the <b>prompt.</b> Finally, we observe that adding task and inline instructions to the demonstrations enhances model performance even when the instructions are semantically corrupted.</p></p class="citation"></blockquote><h3 id=269--2327-toward-informal-language-processing-knowledge-of-slang-in-large-language-models-zhewei-sun-et-al-2024>(2/69 | 2/327) Toward Informal Language Processing: Knowledge of Slang in Large Language Models (Zhewei Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhewei Sun, Qian Hu, Rahul Gupta, Richard Zemel, Yang Xu. (2024)<br><strong>Toward Informal Language Processing: Knowledge of Slang in Large Language Models</strong><br><button class=copy-to-clipboard title="Toward Informal Language Processing: Knowledge of Slang in Large Language Models" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 103<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Zero-shot, BERT, GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02323v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02323v1.pdf filename=2404.02323v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancement in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has offered a strong potential for natural language systems to process informal language. A representative form of informal language is slang, used commonly in daily conversations and online social media. To date, slang has not been comprehensively evaluated in <b>LLMs</b> due partly to the absence of a carefully designed and publicly accessible <b>benchmark.</b> Using movie subtitles, we construct a dataset that supports evaluation on a diverse set of tasks pertaining to automatic processing of slang. For both evaluation and <b>finetuning,</b> we show the effectiveness of our dataset on two core applications: 1) slang detection, and 2) identification of regional and historical sources of slang from natural sentences. We also show how our dataset can be used to probe the output distributions of <b>LLMs</b> for interpretive insights. We find that while <b>LLMs</b> such as <b>GPT-4</b> achieve good performance in a <b>zero-shot</b> setting, smaller <b>BERT-like</b> models <b>finetuned</b> on our dataset achieve comparable performance. Furthermore, we show that our dataset enables <b>finetuning</b> of <b>LLMs</b> such as <b>GPT-3.5</b> that achieve substantially better performance than strong <b>zero-shot</b> baselines. Our work offers a comprehensive evaluation and a high-quality <b>benchmark</b> on English slang based on the OpenSubtitles corpus, serving both as a publicly accessible resource and a platform for applying tools for informal language processing.</p></p class="citation"></blockquote><h3 id=369--3327-comparative-study-of-domain-driven-terms-extraction-using-large-language-models-sandeep-chataut-et-al-2024>(3/69 | 3/327) Comparative Study of Domain Driven Terms Extraction Using Large Language Models (Sandeep Chataut et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sandeep Chataut, Tuyen Do, Bichar Dip Shrestha Gurung, Shiva Aryal, Anup Khanal, Carol Lushbough, Etienne Gnimpieba. (2024)<br><strong>Comparative Study of Domain Driven Terms Extraction Using Large Language Models</strong><br><button class=copy-to-clipboard title="Comparative Study of Domain Driven Terms Extraction Using Large Language Models" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: GPT, GPT-3, GPT-3.5, falcon, Information Retrieval, Keyword Extraction, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02330v1.pdf filename=2404.02330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Keywords</b> <b>play</b> a crucial role in bridging the gap between human understanding and machine processing of textual data. They are essential to data enrichment because they form the basis for detailed annotations that provide a more insightful and in-depth view of the underlying data. Keyword/domain driven term extraction is a pivotal task in natural language processing, facilitating <b>information</b> <b>retrieval,</b> document <b>summarization,</b> and content categorization. This review focuses on <b>keyword</b> <b>extraction</b> methods, emphasizing the use of three major <b>Large</b> <b>Language</b> <b>Models(LLMs):</b> Llama2-7B, <b>GPT-3.5,</b> and <b>Falcon-7B.</b> We employed a custom Python package to interface with these <b>LLMs,</b> simplifying <b>keyword</b> <b>extraction.</b> Our study, utilizing the Inspec and PubMed datasets, evaluates the performance of these models. The Jaccard similarity index was used for assessment, yielding scores of 0.64 (Inspec) and 0.21 (PubMed) for <b>GPT-3.5,</b> 0.40 and 0.17 for Llama2-7B, and 0.23 and 0.12 for <b>Falcon-7B.</b> This paper underlines the role of <b>prompt</b> engineering in <b>LLMs</b> for better <b>keyword</b> <b>extraction</b> and discusses the impact of hallucination in <b>LLMs</b> on result evaluation. It also sheds light on the challenges in using <b>LLMs</b> for <b>keyword</b> <b>extraction,</b> including model complexity, resource demands, and optimization techniques.</p></p class="citation"></blockquote><h3 id=469--4327-sgsh-stimulate-large-language-models-with-skeleton-heuristics-for-knowledge-base-question-generation-shasha-guo-et-al-2024>(4/69 | 4/327) SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation (Shasha Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shasha Guo, Lizi Liao, Jing Zhang, Yanling Wang, Cuiping Li, Hong Chen. (2024)<br><strong>SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation</strong><br><button class=copy-to-clipboard title="SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: BART, ChatGPT, GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01923v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01923v1.pdf filename=2404.01923v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge base question generation (KBQG) aims to generate natural language questions from a set of triplet facts extracted from KB. Existing methods have significantly boosted the performance of KBQG via <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> thanks to the richly endowed semantic knowledge. With the advance of pre-training techniques, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> (e.g., <b>GPT-3.5)</b> undoubtedly possess much more semantic knowledge. Therefore, how to effectively organize and exploit the abundant knowledge for KBQG becomes the focus of our study. In this work, we propose SGSH&ndash;a simple and effective framework to Stimulate <b>GPT-3.5</b> with Skeleton Heuristics to enhance KBQG. The framework incorporates &ldquo;skeleton heuristics&rdquo;, which provides more fine-grained guidance associated with each input to stimulate <b>LLMs</b> to generate optimal questions, encompassing essential elements like the question phrase and the auxiliary verb.More specifically, we devise an automatic data construction strategy leveraging <b>ChatGPT</b> to construct a skeleton training dataset, based on which we employ a soft <b>prompting</b> approach to train a <b>BART</b> model dedicated to generating the skeleton associated with each input. Subsequently, skeleton heuristics are encoded into the <b>prompt</b> to incentivize <b>GPT-3.5</b> to generate desired questions. Extensive experiments demonstrate that SGSH derives the new state-of-the-art performance on the KBQG tasks.</p></p class="citation"></blockquote><h3 id=569--5327-class-incremental-few-shot-event-detection-kailin-zhao-et-al-2024>(5/69 | 5/327) Class-Incremental Few-Shot Event Detection (Kailin Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kailin Zhao, Xiaolong Jin, Long Bai, Jiafeng Guo, Xueqi Cheng. (2024)<br><strong>Class-Incremental Few-Shot Event Detection</strong><br><button class=copy-to-clipboard title="Class-Incremental Few-Shot Event Detection" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 91<br>Keywords: Graph, Benchmarking, Few-shot, Few-shot Learning, Knowledge Distillation, Knowledge Distillation, Knowledge Graph, Event Detection, Information Retrieval, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01767v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01767v1.pdf filename=2404.01767v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Event</b> <b>detection</b> is one of the fundamental tasks in <b>information</b> <b>extraction</b> and <b>knowledge</b> <b>graph.</b> However, a realistic <b>event</b> <b>detection</b> system often needs to deal with new <b>event</b> <b>classes</b> constantly. These new classes usually have only a few labeled instances as it is time-consuming and labor-intensive to annotate a large number of unlabeled instances. Therefore, this paper proposes a new task, called class-incremental <b>few-shot</b> <b>event</b> <b>detection.</b> Nevertheless, this task faces two problems, i.e., old <b>knowledge</b> <b>forgetting</b> and new class overfitting. To solve these problems, this paper further presents a novel <b>knowledge</b> <b>distillation</b> and <b>prompt</b> <b>learning</b> based method, called <b>Prompt-KD.</b> <b>Specifically,</b> to handle the forgetting problem about old <b>knowledge,</b> <b>Prompt-KD</b> <b>develops</b> an attention based multi-teacher <b>knowledge</b> <b>distillation</b> framework, where the ancestor teacher model pre-trained on base classes is reused in all learning sessions, and the father teacher model derives the current student model via adaptation. On the other hand, in order to cope with the <b>few-shot</b> <b>learning</b> scenario and alleviate the corresponding new class overfitting problem, <b>Prompt-KD</b> <b>is</b> also equipped with a <b>prompt</b> <b>learning</b> mechanism. Extensive experiments on two <b>benchmark</b> datasets, i.e., FewEvent and MAVEN, demonstrate the superior performance of <b>Prompt-KD.</b></p></p class="citation"></blockquote><h3 id=669--6327-self-improvement-programming-for-temporal-knowledge-graph-question-answering-zhuo-chen-et-al-2024>(6/69 | 6/327) Self-Improvement Programming for Temporal Knowledge Graph Question Answering (Zhuo Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuo Chen, Zhao Zhang, Zixuan Li, Fei Wang, Yutao Zeng, Xiaolong Jin, Yongjun Xu. (2024)<br><strong>Self-Improvement Programming for Temporal Knowledge Graph Question Answering</strong><br><button class=copy-to-clipboard title="Self-Improvement Programming for Temporal Knowledge Graph Question Answering" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 88<br>Keywords: Graph, Knowledge Graph, Question Answering, Semantic Parsing, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Temporal Knowledge Graph, Temporal Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01720v1.pdf filename=2404.01720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Temporal</b> <b>Knowledge</b> <b>Graph</b> <b>Question</b> <b>Answering</b> (TKGQA) aims to answer <b>questions</b> <b>with</b> <b>temporal</b> <b>intent</b> <b>over</b> <b>Temporal</b> <b>Knowledge</b> <b>Graphs</b> <b>(TKGs).</b> The core challenge of this task lies in understanding the complex <b>semantic</b> <b>information</b> regarding multiple types of time constraints (e.g., before, first) in <b>questions.</b> <b>Existing</b> end-to-end methods implicitly model the time constraints by learning time-aware embeddings of <b>questions</b> <b>and</b> candidate answers, which is far from understanding the <b>question</b> <b>comprehensively.</b> Motivated by <b>semantic-parsing-based</b> <b>approaches</b> that explicitly model constraints in <b>questions</b> <b>by</b> generating logical forms with symbolic operators, we design fundamental <b>temporal</b> <b>operators</b> <b>for</b> time constraints and introduce a novel self-improvement Programming method for TKGQA (Prog-TQA). Specifically, Prog-TQA leverages the <b>in-context</b> <b>learning</b> ability of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to understand the combinatory time constraints in the <b>questions</b> <b>and</b> generate corresponding program drafts with a few examples given. Then, it aligns these drafts to <b>TKGs</b> with the linking module and subsequently executes them to generate the answers. To enhance the ability to understand <b>questions,</b> <b>Prog-TQA</b> is further equipped with a self-improvement strategy to effectively bootstrap <b>LLMs</b> using high-quality self-generated drafts. Extensive experiments demonstrate the superiority of the proposed Prog-TQA on MultiTQ and CronQuestions datasets, especially in the Hits@1 metric.</p></p class="citation"></blockquote><h3 id=769--7327-metal-towards-multilingual-meta-evaluation-rishav-hada-et-al-2024>(7/69 | 7/327) METAL: Towards Multilingual Meta-Evaluation (Rishav Hada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rishav Hada, Varun Gumma, Mohamed Ahmed, Kalika Bali, Sunayana Sitaram. (2024)<br><strong>METAL: Towards Multilingual Meta-Evaluation</strong><br><button class=copy-to-clipboard title="METAL: Towards Multilingual Meta-Evaluation" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, GPT, GPT-3, GPT-3.5, GPT-4, Reasoning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01667v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01667v1.pdf filename=2404.01667v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rising human-like precision of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in numerous tasks, their utilization in a variety of real-world applications is becoming more prevalent. Several studies have shown that <b>LLMs</b> excel on many standard NLP <b>benchmarks.</b> However, it is challenging to evaluate <b>LLMs</b> due to test dataset contamination and the limitations of traditional metrics. Since human evaluations are difficult to collect, there is a growing interest in the community to use <b>LLMs</b> themselves as reference-free evaluators for subjective metrics. However, past work has shown that <b>LLM-based</b> evaluators can exhibit bias and have poor alignment with human judgments. In this study, we propose a framework for an end-to-end assessment of <b>LLMs</b> as evaluators in multilingual scenarios. We create a carefully curated dataset, covering 10 languages containing native speaker judgments for the task of <b>summarization.</b> This dataset is created specifically to evaluate <b>LLM-based</b> evaluators, which we refer to as meta-evaluation (METAL). We compare the performance of <b>LLM-based</b> evaluators created using <b>GPT-3.5-Turbo,</b> <b>GPT-4,</b> and PaLM2. Our results indicate that <b>LLM-based</b> evaluators based on <b>GPT-4</b> perform the best across languages, while <b>GPT-3.5-Turbo</b> performs poorly. Additionally, we perform an analysis of the <b>reasoning</b> provided by <b>LLM-based</b> evaluators and find that it often does not match the <b>reasoning</b> provided by human judges.</p></p class="citation"></blockquote><h3 id=869--8327-improving-retrieval-augmented-open-domain-question-answering-with-vectorized-contexts-zhuo-chen-et-al-2024>(8/69 | 8/327) Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts (Zhuo Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuo Chen, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, Kewei Tu. (2024)<br><strong>Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts</strong><br><button class=copy-to-clipboard title="Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Open-Domain Question Answering, Question Answering, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02022v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02022v1.pdf filename=2404.02022v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the era of <b>large</b> <b>language</b> <b>models,</b> applying techniques such as <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> can better address <b>Open-Domain</b> <b>Question-Answering</b> <b>problems.</b> Due to constraints including model sizes and computing resources, the length of context is often limited, and it becomes challenging to empower the model to cover overlong contexts while answering <b>questions</b> <b>from</b> open domains. This paper proposes a general and convenient method to covering longer contexts in <b>Open-Domain</b> <b>Question-Answering</b> <b>tasks.</b> It leverages a small encoder language model that effectively encodes contexts, and the encoding applies cross-attention with origin inputs. With our method, the origin language models can cover several times longer contexts while keeping the computing requirements close to the baseline. Our experiments demonstrate that after <b>fine-tuning,</b> there is improved performance across two held-in datasets, four held-out datasets, and also in two In Context Learning settings.</p></p class="citation"></blockquote><h3 id=969--9327-a-rationale-centric-counterfactual-data-augmentation-method-for-cross-document-event-coreference-resolution-bowen-ding-et-al-2024>(9/69 | 9/327) A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document Event Coreference Resolution (Bowen Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Ding, Qingkai Min, Shengkun Ma, Yingjie Li, Linyi Yang, Yue Zhang. (2024)<br><strong>A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document Event Coreference Resolution</strong><br><button class=copy-to-clipboard title="A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document Event Coreference Resolution" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 76<br>Keywords: Benchmarking, Clustering, Counter-factual, Data Augmentation, Out-of-domain, Coreference Resolution, Large Language Model, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01921v1.pdf filename=2404.01921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Based on <b>Pre-trained</b> <b>Language</b> <b>Models</b> <b>(PLMs),</b> event <b>coreference</b> <b>resolution</b> (ECR) systems have demonstrated outstanding performance in <b>clustering</b> coreferential events across documents. However, the existing system exhibits an excessive reliance on the `triggers lexical matching&rsquo; spurious pattern in the input mention pair text. We formalize the decision-making process of the baseline ECR system using a Structural Causal Model (SCM), aiming to identify spurious and causal associations (i.e., rationales) within the ECR task. Leveraging the debiasing capability of <b>counterfactual</b> <b>data</b> <b>augmentation,</b> we develop a rationale-centric <b>counterfactual</b> <b>data</b> <b>augmentation</b> method with <b>LLM-in-the-loop.</b> This method is specialized for pairwise input in the ECR system, where we conduct direct interventions on triggers and context to mitigate the spurious association while emphasizing the causation. Our approach achieves state-of-the-art performance on three popular cross-document ECR <b>benchmarks</b> and demonstrates robustness in <b>out-of-domain</b> scenarios.</p></p class="citation"></blockquote><h3 id=1069--10327-long-context-llms-struggle-with-long-in-context-learning-tianle-li-et-al-2024>(10/69 | 10/327) Long-context LLMs Struggle with Long In-context Learning (Tianle Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, Wenhu Chen. (2024)<br><strong>Long-context LLMs Struggle with Long In-context Learning</strong><br><button class=copy-to-clipboard title="Long-context LLMs Struggle with Long In-context Learning" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Few-shot, Reasoning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02060v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02060v2.pdf filename=2404.02060v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have made significant strides in handling long sequences exceeding 32K tokens. However, their performance evaluation has largely been confined to metrics like <b>perplexity</b> and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios. This study introduces a specialized <b>benchmark</b> (LongICLBench) focusing on long <b>in-context</b> <b>learning</b> within the realm of extreme-label classification. We meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input <b>(few-shot</b> demonstration) lengths from 2K to 50K tokens. Our <b>benchmark</b> requires <b>LLMs</b> to comprehend the entire input to recognize the massive label spaces to make correct predictions. We evaluate 13 long-context <b>LLMs</b> on our <b>benchmarks.</b> We find that the long-context <b>LLMs</b> perform relatively well on less challenging tasks with shorter demonstration lengths by effectively utilizing the long context window. However, on the most challenging task Discovery with 174 labels, all the <b>LLMs</b> struggle to understand the task definition, thus reaching a performance close to zero. This suggests a notable gap in current <b>LLM</b> capabilities for processing and understanding long, context-rich sequences. Further analysis revealed a tendency among models to favor predictions for labels presented toward the end of the sequence. Their ability to reason over multiple pieces in the long sequence is yet to be improved. Our study reveals that long context understanding and <b>reasoning</b> is still a challenging task for the existing <b>LLMs.</b> We believe LongICLBench could serve as a more realistic evaluation for the future long-context <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1169--11327-llms-in-the-loop-leveraging-large-language-model-annotations-for-active-learning-in-low-resource-languages-nataliia-kholodna-et-al-2024>(11/69 | 11/327) LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages (Nataliia Kholodna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nataliia Kholodna, Sahib Julka, Mohammad Khodadadi, Muhammed Nurullah Gumus, Michael Granitzer. (2024)<br><strong>LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages</strong><br><button class=copy-to-clipboard title="LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; I-2-6, cs-AI, cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Active Learning, Low-Resource, GPT, GPT-4, GPT-4 turbo, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02261v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02261v1.pdf filename=2404.02261v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Low-resource</b> languages face significant barriers in AI development due to limited linguistic resources and expertise for data labeling, rendering them rare and costly. The scarcity of data and the absence of preexisting tools exacerbate these challenges, especially since these languages may not be adequately represented in various NLP datasets. To address this gap, we propose leveraging the potential of <b>LLMs</b> in the <b>active</b> <b>learning</b> loop for data annotation. Initially, we conduct evaluations to assess inter-annotator agreement and consistency, facilitating the selection of a suitable <b>LLM</b> annotator. The chosen annotator is then integrated into a training loop for a classifier using an <b>active</b> <b>learning</b> paradigm, minimizing the amount of queried data required. Empirical evaluations, notably employing <b>GPT-4-Turbo,</b> <b>demonstrate</b> near-state-of-the-art performance with significantly reduced data requirements, as indicated by estimated potential cost savings of at least 42.45 times compared to human annotation. Our proposed solution shows promising potential to substantially reduce both the monetary and computational costs associated with automation in <b>low-resource</b> settings. By bridging the gap between <b>low-resource</b> languages and AI, this approach fosters broader inclusion and shows the potential to enable automation across diverse linguistic landscapes.</p></p class="citation"></blockquote><h3 id=1269--12327-emergent-abilities-in-reduced-scale-generative-language-models-sherin-muckatira-et-al-2024>(12/69 | 12/327) Emergent Abilities in Reduced-Scale Generative Language Models (Sherin Muckatira et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sherin Muckatira, Vijeta Deshpande, Vladislav Lialin, Anna Rumshisky. (2024)<br><strong>Emergent Abilities in Reduced-Scale Generative Language Models</strong><br><button class=copy-to-clipboard title="Emergent Abilities in Reduced-Scale Generative Language Models" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Zero-shot, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02204v1.pdf filename=2404.02204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> can solve new tasks without task-specific <b>fine-tuning.</b> This ability, also known as <b>in-context</b> <b>learning</b> <b>(ICL),</b> is considered an emergent ability and is primarily seen in <b>large</b> <b>language</b> <b>models</b> with billions of parameters. This study investigates if such emergent properties are strictly tied to model size or can be demonstrated by smaller models trained on reduced-scale data. To explore this, we simplify pre-training data and pre-train 36 causal language models with parameters varying from 1 million to 165 million parameters. We show that models trained on this simplified pre-training data demonstrate enhanced <b>zero-shot</b> <b>capabilities</b> across various tasks in simplified language, achieving performance comparable to that of pre-trained models six times larger on unrestricted language. This suggests that downscaling the language allows <b>zero-shot</b> <b>learning</b> capabilities to emerge in models with limited size. Additionally, we find that these smaller models pre-trained on simplified data demonstrate a power law relationship between the evaluation loss and the three scaling factors: compute, dataset size, and model size.</p></p class="citation"></blockquote><h3 id=1369--13327-hyperclova-x-technical-report-kang-min-yoo-et-al-2024>(13/69 | 13/327) HyperCLOVA X Technical Report (Kang Min Yoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kang Min Yoo, Jaegeun Han, Sookyo In, Heewon Jeon, Jisu Jeong, Jaewook Kang, Hyunwook Kim, Kyung-Min Kim, Munhyong Kim, Sungju Kim, Donghyun Kwak, Hanock Kwak, Se Jung Kwon, Bado Lee, Dongsoo Lee, Gichang Lee, Jooho Lee, Baeseong Park, Seongjin Shin, Joonsang Yu, Seolki Baek, Sumin Byeon, Eungsup Cho, Dooseok Choe, Jeesung Han, Youngkyun Jin, Hyein Jun, Jaeseung Jung, Chanwoong Kim, Jinhong Kim, Jinuk Kim, Dokyeong Lee, Dongwook Park, Jeong Min Sohn, Sujung Han, Jiae Heo, Sungju Hong, Mina Jeon, Hyunhoon Jung, Jungeun Jung, Wangkyo Jung, Chungjoon Kim, Hyeri Kim, Jonghyun Kim, Min Young Kim, Soeun Lee, Joonhee Park, Jieun Shin, Sojin Yang, Jungsoon Yoon, Hwaran Lee, Sanghwan Bae, Jeehwan Cha, Donghoon Ham, Youngki Hong, Yunki Hong, Myunggeun Ji, Yeguk Jin, Chansong Jo, Shinyoung Joo, Seunghwan Jung, Hyomin Kim, Jungwhan Kim, Minkyoung Kim, Minseung Kim, Sungdong Kim, Yonghee Kim, Youngjun Kim, Donghyeon Ko, Dughyun Lee, Jaehong Lee, Jieun Lee, Jongjin Lee, Min Young Lee, Yehbin Lee, Taehong Min, Kiyoon Moon, Jaesun Park, Kyuyon Park, Seunghyun Seo, Gyubin Son, Wonjoon Yoo, Myungin You, Doheon Ahn, Homin Ahn, Joohee Ahn, Seongmin Ahn, Chanwoo An, Hyeryun An, Junho An, Sang-Min An, Boram Byun, Jongho Cha, Minji Chang, Seunggyu Chang, Haesong Cho, Youngdo Cho, Dalnim Choi, Daseul Choi, Hyoseok Choi, Minseong Choi, Sangho Choi, Seongjae Choi, Wooyong Choi, Sewhan Chun, Dong Young Go, Chiheon Ham, Danbi Han, Jaemin Han, Mihak Hong, Moonyoung Hong, Sung Bum Hong, Seongchan Hwang, Eunbin Hyun, Jinbae Im, Jaehyung Jang, Jaeni Jang, Sihyeon Jang, Sungwon Jang, Joonha Jeon, Yujin Jeon, Daun Jeong, Joonhyun Jeong, Kyeongseok Jeong, Mini Jeong, Yeji Jeong, Sol Jin, Hanbyeol Jo, Hanju Jo, Minjung Jo, Lee Jonghyun, Chaeyoon Jung, Hyungsik Jung, Jaeuk Jung, Ju Hwan Jung, Kwangsun Jung, Seungjae Jung, Soonwon Ka, Donghan Kang, Soyoung Kang, Taeho Kil, Areum Kim, Beomyoung Kim, Byeongwook Kim, Daehee Kim, Dong-Gyun Kim, Donggook Kim, Donghyun Kim, Euna Kim, Eunchul Kim, Geewook Kim, Gyu Ri Kim, Hanbyul Kim, Heesu Kim, Isaac Kim, Jeonghoon Kim, Jihye Kim, Joonghoon Kim, Minjae Kim, Minsub Kim, Pil Hwan Kim, Sammy Kim, Seokhun Kim, Seonghyeon Kim, Soojin Kim, Soong Kim, Soyoon Kim, Sunyoung Kim, Taeho Kim, Wonho Kim, Yoonsik Kim, You Jin Kim, Yuri Kim, Beomseok Kwon, Ohsung Kwon, Yoo-Hwan Kwon, Anna Lee, Byungwook Lee, Changho Lee, Daun Lee, Dongjae Lee, Ha-Ram Lee, Hodong Lee, Hwiyeong Lee, Hyunmi Lee, Injae Lee, Jaeung Lee, Jeongsang Lee, Jisoo Lee, Joongjae Lee, Juhan Lee, Jung Hyun Lee, Junghoon Lee, Junwoo Lee, Se Yun Lee, Sujin Lee, Sungjae Lee, Sungwoo Lee, Wonjae Lee, Zoo Hyun Lee, Jong Kun Lim, Kun Lim, Taemin Lim, Yuri Min, Nuri Na, Jeongyeon Nam, Kyeong-Min Nam, Yeonseog Noh, Biro Oh, Hyangnam Oh, Jung-Sik Oh, Solgil Oh, Yeontaek Oh, Boyoun Park, Cheonbok Park, Dongju Park, Hyeonjin Park, Hyun Tae Park, Hyunjung Park, Jihye Park, Jooseok Park, Junghwan Park, Jungsoo Park, Miru Park, Sang Hee Park, Seunghyun Park, Taerim Park, Wonkyeong Park, Hyunjoon Ryu, Jeonghun Ryu, Nahyeon Ryu, Soonshin Seo, Suk Min Seo, Yoonjeong Shim, Kyuyong Shin, Wonkwang Shin, Hyun Sim, Mihyun Sim, Woongseob Sim, Hyejin Soh, Bokyoung Son, Hyunjun Son, Seulah Son, Chi-Yun Song, Chiyoung Song, Ka Yeon Song, Minchul Song, Seungmin Song, Jisung Wang, Matt Yeo, Yonggoo Yeo, Myeong Yeon Yi, Moon Bin Yim, Taehwan Yoo, Youngjoon Yoo, Sungmin Yoon, Young Jin Yoon, Hangyeol Yu, Ui Seon Yu, Xingdong Zuo, Jeongin Bae, Joungeun Bae, Hyunsoo Cho, Seonghyun Cho, Yongjin Cho, Taekyoon Choi, Yera Choi, Jiwan Chung, Zhenghui Han, Byeongho Heo, Euisuk Hong, Taebaek Hwang, Seonyeol Im, Sumin Jegal, Sumin Jeon, Yelim Jeong, Yonghyun Jeong, Can Jiang, Juyong Jiang, Jiho Jin, Ara Jo, Younghyun Jo, Hoyoun Jung, Juyoung Jung, Dae Hee Kim, Ginam Kim, Hangyeol Kim, Heeseung Kim, Hyojin Kim, Hyojun Kim, Hyun-Ah Kim, Jeehye Kim, Jin-Hwa Kim, Jiseon Kim, Jonghak Kim, Jung Yoon Kim, Rak Yeong Kim, Seoyoon Kim, Sewon Kim, Sooyoung Kim, Sukyoung Kim, Taeyong Kim, Naeun Ko, Bonseung Koo, Heeyoung Kwak, Haena Kwon, Youngjin Kwon, Boram Lee, Bruce W. Lee, Dagyeong Lee, Erin Lee, Euijin Lee, Ha Gyeong Lee, Hyojin Lee, Hyunjeong Lee, Jeeyoon Lee, Jeonghyun Lee, Jongheok Lee, Joonhyung Lee, Junhyuk Lee, Mingu Lee, Nayeon Lee, Sangkyu Lee, Se Young Lee, Seulgi Lee, Seung Jin Lee, Suhyeon Lee, Yeonjae Lee, Yesol Lee, Youngbeom Lee, Yujin Lee, Shaodong Li, Tianyu Liu, Seong-Eun Moon, Taehong Moon, Max-Lasse Nihlenramstroem, Wonseok Oh, Yuri Oh, Hongbeen Park, Hyekyung Park, Nohil Park, Sangjin Park, Jiwon Ryu, Miru Ryu, Simo Ryu, Ahreum Seo, Hee Seo, Kangdeok Seo, Jamin Shin, Seungyoun Shin, Heetae Sin, Jiangping Wang, Lei Wang, Ning Xiang, Longxiang Xiao, Jing Xu, Seonyeong Yi, Haanju Yoo, Haneul Yoo, Hwanhee Yoo, Liang Yu, Youngjae Yu, Weijie Yuan, Bo Zeng, Qian Zhou, Kyunghyun Cho, Jung-Woo Ha, Joonsuk Park, Jihyun Hwang, Hyoung Jo Kwon, Soonyong Kwon, Jungyeon Lee, Seungho Lee, Seungho Choi, Sang-Woo Lee, Jung Hwa Lim, Nako Sung. (2024)<br><strong>HyperCLOVA X Technical Report</strong><br><button class=copy-to-clipboard title="HyperCLOVA X Technical Report" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Instruction Following, Neural Machine Translation, Reasoning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01954v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01954v1.pdf filename=2404.01954v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce HyperCLOVA X, a family of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding. HyperCLOVA X was trained on a balanced mix of Korean, English, and code data, followed by <b>instruction-tuning</b> <b>with</b> high-quality human-annotated datasets while abiding by strict safety guidelines reflecting our commitment to responsible AI. The model is evaluated across various <b>benchmarks,</b> including comprehensive <b>reasoning,</b> knowledge, commonsense, factuality, coding, math, chatting, <b>instruction-following,</b> <b>and</b> harmlessness, in both Korean and English. HyperCLOVA X exhibits strong <b>reasoning</b> capabilities in Korean backed by a deep understanding of the language and cultural nuances. Further analysis of the inherent bilingual nature and its extension to multilingualism highlights the model&rsquo;s cross-lingual proficiency and strong generalization ability to untargeted languages, including <b>machine</b> <b>translation</b> between several language pairs and cross-lingual inference tasks. We believe that HyperCLOVA X can provide helpful guidance for regions or countries in developing their sovereign <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1469--14327-octopus-on-device-language-model-for-function-calling-of-software-apis-wei-chen-et-al-2024>(14/69 | 14/327) Octopus: On-device language model for function calling of software APIs (Wei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Chen, Zhiyuan Li, Mingyuan Ma. (2024)<br><strong>Octopus: On-device language model for function calling of software APIs</strong><br><button class=copy-to-clipboard title="Octopus: On-device language model for function calling of software APIs" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SE, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01549v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01549v1.pdf filename=2404.01549v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving domain of artificial intelligence, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> play a crucial role due to their advanced text processing and generation abilities. This study introduces a new strategy aimed at harnessing on-device <b>LLMs</b> in invoking software APIs. We meticulously compile a dataset derived from software API documentation and apply <b>fine-tuning</b> to <b>LLMs</b> with capacities of 2B, 3B and 7B parameters, specifically to enhance their proficiency in software API interactions. Our approach concentrates on refining the models&rsquo; grasp of API structures and syntax, significantly enhancing the accuracy of API function calls. Additionally, we propose \textit{conditional masking} techniques to ensure outputs in the desired formats and reduce error rates while maintaining inference speeds. We also propose a novel <b>benchmark</b> designed to evaluate the effectiveness of <b>LLMs</b> in API interactions, establishing a foundation for subsequent research. Octopus, the <b>fine-tuned</b> model, is proved to have better performance than <b>GPT-4</b> for the software APIs calling. This research aims to advance automated software development and API integration, representing substantial progress in aligning <b>LLM</b> capabilities with the demands of practical software engineering applications.</p></p class="citation"></blockquote><h3 id=1569--15327-patch----psychometrics-assisted-benchmarking-of-large-language-models-a-case-study-of-mathematics-proficiency-qixiang-fang-et-al-2024>(15/69 | 15/327) PATCH &ndash; Psychometrics-AssisTed benCHmarking of Large Language Models: A Case Study of Mathematics Proficiency (Qixiang Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qixiang Fang, Daniel L. Oberski, Dong Nguyen. (2024)<br><strong>PATCH &ndash; Psychometrics-AssisTed benCHmarking of Large Language Models: A Case Study of Mathematics Proficiency</strong><br><button class=copy-to-clipboard title="PATCH -- Psychometrics-AssisTed benCHmarking of Large Language Models: A Case Study of Mathematics Proficiency" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 62<br>Keywords: Benchmarking, Benchmarking, Multi-modal, Multi-modal, GPT, GPT-4, Gemini, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01799v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01799v1.pdf filename=2404.01799v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many existing <b>benchmarks</b> of <b>large</b> <b>(multimodal)</b> <b>language</b> models <b>(LLMs)</b> focus on measuring <b>LLMs&rsquo;</b> academic proficiency, often with also an interest in comparing model performance with human test takers. While these <b>benchmarks</b> have proven key to the development of <b>LLMs,</b> they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?) and unclear human population reference (e.g., To whom can the model be compared?). In response to these challenges, we propose leveraging knowledge from psychometrics - a field dedicated to the measurement of latent variables like academic proficiency - into <b>LLM</b> <b>benchmarking.</b> We make three primary contributions. First, we introduce PATCH: a novel framework for Psychometrics-AssisTed <b>benCHmarking</b> of <b>LLMs.</b> PATCH addresses the aforementioned limitations, presenting a new direction for <b>LLM</b> <b>benchmark</b> research. Second, we implement PATCH by measuring <b>GPT-4</b> and <b>Gemini-Pro-Vision&rsquo;s</b> proficiency in 8th grade mathematics against 56 human populations. We show that adopting a psychometrics-based approach yields evaluation outcomes that diverge from those based on existing <b>benchmarking</b> practices. Third, we release 4 datasets to support measuring and comparing <b>LLM</b> proficiency in grade school mathematics and science against human populations.</p></p class="citation"></blockquote><h3 id=1669--16327-auditing-large-language-models-for-enhanced-text-based-stereotype-detection-and-probing-based-bias-evaluation-zekun-wu-et-al-2024>(16/69 | 16/327) Auditing Large Language Models for Enhanced Text-Based Stereotype Detection and Probing-Based Bias Evaluation (Zekun Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zekun Wu, Sahan Bulathwela, Maria Perez-Ortiz, Adriano Soares Koshiyama. (2024)<br><strong>Auditing Large Language Models for Enhanced Text-Based Stereotype Detection and Probing-Based Bias Evaluation</strong><br><button class=copy-to-clipboard title="Auditing Large Language Models for Enhanced Text-Based Stereotype Detection and Probing-Based Bias Evaluation" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, GPT, Text Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01768v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01768v1.pdf filename=2404.01768v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have significantly increased their presence in human-facing Artificial Intelligence (AI) applications. However, <b>LLMs</b> could reproduce and even exacerbate stereotypical outputs from training data. This work introduces the Multi-Grain Stereotype (MGS) dataset, encompassing 51,867 instances across gender, race, profession, religion, and stereotypical <b>text,</b> <b>collected</b> by fusing multiple previously publicly available stereotype detection datasets. We explore different machine learning approaches aimed at establishing baselines for stereotype detection, and <b>fine-tune</b> several language models of various architectures and model sizes, presenting in this work a series of stereotypes classifier models for English <b>text</b> <b>trained</b> on MGS. To understand whether our stereotype detectors capture relevant features (aligning with human common sense) we utilise a variety of explanainable AI tools, including SHAP, LIME, and BertViz, and analyse a series of example cases discussing the results. Finally, we develop a series of stereotype elicitation <b>prompts</b> and evaluate the presence of stereotypes in <b>text</b> <b>generation</b> tasks with popular <b>LLMs,</b> using one of our best performing previously presented stereotypes detectors. Our experiments yielded several key findings: i) Training stereotype detectors in a multi-dimension setting yields better results than training multiple single-dimension classifiers.ii) The integrated MGS Dataset enhances both the in-dataset and cross-dataset generalisation ability of stereotype detectors compared to using the datasets separately. iii) There is a reduction in stereotypes in the content generated by <b>GPT</b> Family <b>LLMs</b> with newer versions.</p></p class="citation"></blockquote><h3 id=1769--17327-hallucination-diversity-aware-active-learning-for-text-summarization-yu-xia-et-al-2024>(17/69 | 17/327) Hallucination Diversity-Aware Active Learning for Text Summarization (Yu Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Xia, Xu Liu, Tong Yu, Sungchul Kim, Ryan A. Rossi, Anup Rao, Tung Mai, Shuai Li. (2024)<br><strong>Hallucination Diversity-Aware Active Learning for Text Summarization</strong><br><button class=copy-to-clipboard title="Hallucination Diversity-Aware Active Learning for Text Summarization" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Active Learning, Fine-tuning, Text Summarization, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01588v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01588v1.pdf filename=2404.01588v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown propensity to generate hallucinated outputs, i.e., <b>texts</b> <b>that</b> are factually incorrect or unsupported. Existing methods for alleviating hallucinations typically require costly human annotations to identify and correct hallucinations in <b>LLM</b> outputs. Moreover, most of these methods focus on a specific type of hallucination, e.g., entity or token errors, which limits their effectiveness in addressing various types of hallucinations exhibited in <b>LLM</b> outputs. To our best knowledge, in this paper we propose the first <b>active</b> <b>learning</b> framework to alleviate <b>LLM</b> hallucinations, reducing costly human annotations of hallucination needed. By measuring fine-grained hallucinations from errors in semantic frame, discourse and content verifiability in <b>text</b> <b>summarization,</b> we propose HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in <b>active</b> <b>learning</b> for <b>LLM</b> <b>finetuning.</b> Extensive experiments on three datasets and different backbone models demonstrate advantages of our method in effectively and efficiently mitigating <b>LLM</b> hallucinations.</p></p class="citation"></blockquote><h3 id=1869--18327-clapnq-cohesive-long-form-answers-from-passages-in-natural-questions-for-rag-systems-sara-rosenthal-et-al-2024>(18/69 | 18/327) CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems (Sara Rosenthal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sara Rosenthal, Avirup Sil, Radu Florian, Salim Roukos. (2024)<br><strong>CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems</strong><br><button class=copy-to-clipboard title="CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02103v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02103v1.pdf filename=2404.02103v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Retrieval</b> <b>Augmented</b> <b>Generation</b> <b>(RAG)</b> has become a popular application for <b>large</b> <b>language</b> <b>models.</b> It is preferable that successful <b>RAG</b> systems provide accurate answers that are supported by being grounded in a passage without any hallucinations. While considerable work is required for building a full <b>RAG</b> pipeline, being able to <b>benchmark</b> performance is also necessary. We present ClapNQ, a <b>benchmark</b> Long-form <b>Question</b> <b>Answering</b> dataset for the full <b>RAG</b> pipeline. ClapNQ includes long answers with grounded gold passages from Natural <b>Questions</b> <b>(NQ)</b> and a corpus to perform either <b>retrieval,</b> <b>generation,</b> <b>or</b> the full <b>RAG</b> pipeline. The ClapNQ answers are concise, 3x smaller than the full passage, and cohesive, with multiple pieces of the passage that are not contiguous. <b>RAG</b> models must adapt to these properties to be successful at ClapNQ. We present baseline experiments and analysis for ClapNQ that highlight areas where there is still significant room for improvement in grounded <b>RAG.</b> CLAPNQ is publicly available at <a href=https://github.com/primeqa/clapnq>https://github.com/primeqa/clapnq</a></p></p class="citation"></blockquote><h3 id=1969--19327-using-large-language-models-to-understand-telecom-standards-athanasios-karapantelakis-et-al-2024>(19/69 | 19/327) Using Large Language Models to Understand Telecom Standards (Athanasios Karapantelakis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Athanasios Karapantelakis, Mukesh Shakur, Alexandros Nikou, Farnaz Moradi, Christian Orlog, Fitsum Gaim, Henrik Holm, Doumitrou Daniil Nimara, Vincent Huang. (2024)<br><strong>Using Large Language Models to Understand Telecom Standards</strong><br><button class=copy-to-clipboard title="Using Large Language Models to Understand Telecom Standards" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02929v1.pdf filename=2404.02929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Third Generation Partnership Project (3GPP) has successfully introduced standards for global mobility. However, the volume and complexity of these standards has increased over time, thus complicating access to relevant information for vendors and service providers. Use of Generative Artificial Intelligence (AI) and in particular <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> may provide faster access to relevant information. In this paper, we evaluate the capability of state-of-art <b>LLMs</b> to be used as <b>Question</b> <b>Answering</b> <b>(QA)</b> assistants for 3GPP document reference. Our contribution is threefold. First, we provide a <b>benchmark</b> and measuring methods for evaluating performance of <b>LLMs.</b> Second, we do data preprocessing and <b>fine-tuning</b> for one of these <b>LLMs</b> and provide guidelines to increase accuracy of the responses that apply to all <b>LLMs.</b> Third, we provide a model of our own, TeleRoBERTa, that performs on-par with foundation <b>LLMs</b> but with an order of magnitude less number of parameters. Results show that <b>LLMs</b> can be used as a credible reference tool on telecom technical documents, and thus have potential for a number of different applications from troubleshooting and maintenance, to network operations and software product development.</p></p class="citation"></blockquote><h3 id=2069--20327-prompts-as-programs-a-structure-aware-approach-to-efficient-compile-time-prompt-optimization-tobias-schnabel-et-al-2024>(20/69 | 20/327) Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization (Tobias Schnabel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobias Schnabel, Jennifer Neville. (2024)<br><strong>Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization</strong><br><button class=copy-to-clipboard title="Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Retrieval-Augmented Generation, Instruction Tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02319v1.pdf filename=2404.02319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can now handle longer and more complex inputs, which facilitate the use of more elaborate <b>prompts.</b> However, <b>prompts</b> often require some tuning to improve performance for deployment. Recent work has proposed automatic <b>prompt</b> optimization methods, but as <b>prompt</b> complexity and <b>LLM</b> strength increase, many <b>prompt</b> optimization techniques are no longer sufficient and a new approach is needed to optimize {\em meta <b>prompt</b> programs}. To address this, we introduce SAMMO, a framework for {\em compile-time} optimizations of metaprompt programs, which represent <b>prompts</b> as structured objects that allows for a rich set of transformations that can be searched over during optimization. We show that SAMMO generalizes previous methods and improves the performance of complex <b>prompts</b> on (1) <b>instruction</b> <b>tuning,</b> (2) <b>RAG</b> pipeline tuning, and (3) <b>prompt</b> compression, across several different <b>LLMs.</b> We make all code available open-source at <a href=https://github.com/microsoft/sammo>https://github.com/microsoft/sammo</a> .</p></p class="citation"></blockquote><h3 id=2169--21327-exploring-automated-distractor-generation-for-math-multiple-choice-questions-via-large-language-models-wanyong-feng-et-al-2024>(21/69 | 21/327) Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models (Wanyong Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanyong Feng, Jaewook Lee, Hunter McNichols, Alexander Scarlatos, Digory Smith, Simon Woodhead, Nancy Otero Ornelas, Andrew Lan. (2024)<br><strong>Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models</strong><br><button class=copy-to-clipboard title="Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02124v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02124v2.pdf filename=2404.02124v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multiple-choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer, grade, and are a reliable format in assessments and practices. One of the most important aspects of MCQs is the distractors, i.e., incorrect options that are designed to target common errors or misconceptions among real students. To date, the task of crafting high-quality distractors largely remains a labor and time-intensive process for teachers and learning content designers, which has limited scalability. In this work, we study the task of automated distractor generation in the domain of math MCQs and explore a wide variety of <b>large</b> <b>language</b> <b>model</b> <b>(LLM)-based</b> approaches, from <b>in-context</b> <b>learning</b> to <b>fine-tuning.</b> We conduct extensive experiments using a real-world math MCQ dataset and find that although <b>LLMs</b> can generate some mathematically valid distractors, they are less adept at anticipating common errors or misconceptions among real students.</p></p class="citation"></blockquote><h3 id=2269--22327-multiparadetox-extending-text-detoxification-with-parallel-data-to-new-languages-daryna-dementieva-et-al-2024>(22/69 | 22/327) MultiParaDetox: Extending Text Detoxification with Parallel Data to New Languages (Daryna Dementieva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daryna Dementieva, Nikolay Babakov, Alexander Panchenko. (2024)<br><strong>MultiParaDetox: Extending Text Detoxification with Parallel Data to New Languages</strong><br><button class=copy-to-clipboard title="MultiParaDetox: Extending Text Detoxification with Parallel Data to New Languages" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Unsupervised Learning, Style Transfer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02037v1.pdf filename=2404.02037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text detoxification is a textual <b>style</b> <b>transfer</b> (TST) task where a text is paraphrased from a toxic surface form, e.g. featuring rude words, to the neutral register. Recently, text detoxification methods found their applications in various task such as detoxification of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> (Leong et al., 2023; He et al., 2024; Tang et al., 2023) and toxic speech combating in social networks (Deng et al., 2023; Mun et al., 2023; Agarwal et al., 2023). All these applications are extremely important to ensure safe communication in modern digital worlds. However, the previous approaches for parallel text detoxification corpora collection &ndash; ParaDetox (Logacheva et al., 2022) and APPADIA (Atwell et al., 2022) &ndash; were explored only in monolingual setup. In this work, we aim to extend ParaDetox pipeline to multiple languages presenting MultiParaDetox to automate parallel detoxification corpus collection for potentially any language. Then, we experiment with different text detoxification models &ndash; from <b>unsupervised</b> baselines to <b>LLMs</b> and <b>fine-tuned</b> models on the presented parallel corpora &ndash; showing the great benefit of parallel corpus presence to obtain state-of-the-art text detoxification models for any language.</p></p class="citation"></blockquote><h3 id=2369--23327-team-utsa-nlp-at-semeval-2024-task-5-prompt-ensembling-for-argument-reasoning-in-civil-procedures-with-gpt4-dan-schumacher-et-al-2024>(23/69 | 23/327) Team UTSA-NLP at SemEval 2024 Task 5: Prompt Ensembling for Argument Reasoning in Civil Procedures with GPT4 (Dan Schumacher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dan Schumacher, Anthony Rios. (2024)<br><strong>Team UTSA-NLP at SemEval 2024 Task 5: Prompt Ensembling for Argument Reasoning in Civil Procedures with GPT4</strong><br><button class=copy-to-clipboard title="Team UTSA-NLP at SemEval 2024 Task 5: Prompt Ensembling for Argument Reasoning in Civil Procedures with GPT4" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: GPT-4, Reasoning, In-context Learning, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01961v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01961v1.pdf filename=2404.01961v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present our system for the SemEval Task 5, The Legal Argument <b>Reasoning</b> Task in Civil Procedure Challenge. Legal argument <b>reasoning</b> is an essential skill that all law students must master. Moreover, it is important to develop natural language processing solutions that can reason about a question given terse domain-specific contextual information. Our system explores a <b>prompt-based</b> solution using <b>GPT4</b> to reason over legal arguments. We also evaluate an ensemble of <b>prompting</b> strategies, including chain-of-thought <b>reasoning</b> and <b>in-context</b> <b>learning.</b> Overall, our system results in a Macro F1 of .8095 on the validation dataset and .7315 (5th out of 21 teams) on the final test set. Code for this project is available at <a href=https://github.com/danschumac1/CivilPromptReasoningGPT4>https://github.com/danschumac1/CivilPromptReasoningGPT4</a>.</p></p class="citation"></blockquote><h3 id=2469--24327-on-the-role-of-summary-content-units-in-text-summarization-evaluation-marcel-nawrath-et-al-2024>(24/69 | 24/327) On the Role of Summary Content Units in Text Summarization Evaluation (Marcel Nawrath et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcel Nawrath, Agnieszka Nowak, Tristan Ratz, Danilo C. Walenta, Juri Opitz, Leonardo F. R. Ribeiro, João Sedoc, Daniel Deutsch, Simon Mille, Yixin Liu, Lining Zhang, Sebastian Gehrmann, Saad Mahamood, Miruna Clinciu, Khyathi Chandu, Yufang Hou. (2024)<br><strong>On the Role of Summary Content Units in Text Summarization Evaluation</strong><br><button class=copy-to-clipboard title="On the Role of Summary Content Units in Text Summarization Evaluation" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Natural Language Inference, Natural Language Inference, Text Summarization, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01701v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01701v1.pdf filename=2404.01701v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>At the heart of the Pyramid evaluation method for <b>text</b> <b>summarization</b> lie human written summary content units (SCUs). These SCUs are concise sentences that decompose a summary into small facts. Such SCUs can be used to judge the quality of a candidate summary, possibly partially automated via <b>natural</b> <b>language</b> <b>inference</b> <b>(NLI)</b> systems. Interestingly, with the aim to fully automate the Pyramid evaluation, Zhang and Bansal (2021) show that SCUs can be approximated by automatically generated semantic role triplets (STUs). However, several questions currently lack answers, in particular: i) Are there other ways of approximating SCUs that can offer advantages? ii) Under which conditions are SCUs (or their approximations) offering the most value? In this work, we examine two novel strategies to approximate SCUs: generating SCU approximations from AMR meaning representations (SMUs) and from <b>large</b> <b>language</b> <b>models</b> (SGUs), respectively. We find that while STUs and SMUs are competitive, the best approximation quality is achieved by SGUs. We also show through a simple sentence-decomposition baseline (SSUs) that SCUs (and their approximations) offer the most value when ranking short summaries, but may not help as much when ranking systems or longer summaries.</p></p class="citation"></blockquote><h3 id=2569--25327-cmat-a-multi-agent-collaboration-tuning-framework-for-enhancing-small-language-models-xuechen-liang-et-al-2024>(25/69 | 25/327) CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models (Xuechen Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuechen Liang, Meiling Tao, Tianyu Shi, Yiting Xie. (2024)<br><strong>CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models</strong><br><button class=copy-to-clipboard title="CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01663v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01663v2.pdf filename=2404.01663v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.Despite the significant advancements in <b>LLMs,</b> their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset. We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback. This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory. In this research, we propose a new communication agent framework that integrates multi-agent systems with environmental feedback mechanisms, offering a scalable method to explore cooperative behaviors. Notably, our TinyAgent-7B model exhibits performance on par with <b>GPT-3.5,</b> despite having fewer parameters, signifying a substantial improvement in the efficiency and effectiveness of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=2669--26327-helmsman-of-the-masses-evaluate-the-opinion-leadership-of-large-language-models-in-the-werewolf-game-silin-du-et-al-2024>(26/69 | 26/327) Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game (Silin Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Silin Du, Xiaowei Zhang. (2024)<br><strong>Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game</strong><br><button class=copy-to-clipboard title="Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs.CL<br>Keyword Score: 50<br>Keywords: Simulation, Simulator, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01602v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01602v1.pdf filename=2404.01602v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have exhibited memorable strategic behaviors in social deductive games. However, the significance of opinion leadership exhibited by <b>LLM-based</b> agents has been overlooked, which is crucial for practical applications in multi-agent and human-AI interaction settings. Opinion leaders are individuals who have a noticeable impact on the beliefs and behaviors of others within a social group. In this work, we employ the Werewolf game as a <b>simulation</b> platform to assess the opinion leadership of <b>LLMs.</b> The game features the role of the Sheriff, tasked with summarizing arguments and recommending decision options, and therefore serves as a credible proxy for an opinion leader. We develop a framework integrating the Sheriff role and devise two novel metrics for evaluation based on the critical characteristics of opinion leaders. The first metric measures the reliability of the opinion leader, and the second assesses the influence of the opinion leader on other players&rsquo; decisions. We conduct extensive experiments to evaluate <b>LLMs</b> of different scales. In addition, we collect a Werewolf <b>question-answering</b> <b>dataset</b> (WWQA) to assess and enhance <b>LLM&rsquo;s</b> grasp of the game rules, and we also incorporate human participants for further analysis. The results suggest that the Werewolf game is a suitable test bed to evaluate the opinion leadership of <b>LLMs</b> and few <b>LLMs</b> possess the capacity for opinion leadership.</p></p class="citation"></blockquote><h3 id=2769--27327-classifying-cancer-stage-with-open-source-clinical-large-language-models-chia-hsuan-chang-et-al-2024>(27/69 | 27/327) Classifying Cancer Stage with Open-Source Clinical Large Language Models (Chia-Hsuan Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chia-Hsuan Chang, Mary M. Lucas, Grace Lu-Yao, Christopher C. Yang. (2024)<br><strong>Classifying Cancer Stage with Open-Source Clinical Large Language Models</strong><br><button class=copy-to-clipboard title="Classifying Cancer Stage with Open-Source Clinical Large Language Models" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, BERT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01589v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01589v1.pdf filename=2404.01589v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cancer stage classification is important for making treatment and care management plans for oncology patients. Information on staging is often included in unstructured form in clinical, pathology, radiology and other free-text reports in the electronic health record system, requiring extensive work to parse and obtain. To facilitate the extraction of this information, previous NLP approaches rely on labeled training datasets, which are labor-intensive to prepare. In this study, we demonstrate that without any labeled training data, open-source clinical <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can extract pathologic tumor-node-metastasis (pTNM) staging information from real-world pathology reports. Our experiments compare <b>LLMs</b> and a <b>BERT-based</b> model <b>fine-tuned</b> using the labeled data. Our findings suggest that while <b>LLMs</b> still exhibit subpar performance in Tumor (T) classification, with the appropriate adoption of <b>prompting</b> strategies, they can achieve comparable performance on Metastasis (M) classification and improved performance on Node (N) classification.</p></p class="citation"></blockquote><h3 id=2869--28327-scanner-knowledge-enhanced-approach-for-robust-multi-modal-named-entity-recognition-of-unseen-entities-hyunjong-ok-et-al-2024>(28/69 | 28/327) SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities (Hyunjong Ok et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyunjong Ok, Taeho Kil, Sukmin Seo, Jaeho Lee. (2024)<br><strong>SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities</strong><br><button class=copy-to-clipboard title="SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 46<br>Keywords: Benchmarking, Knowledge Distillation, Multi-modal, Self-Distillation, Named Entity Recognition, Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01914v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01914v1.pdf filename=2404.01914v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>named</b> <b>entity</b> <b>recognition</b> <b>(NER)</b> have pushed the boundary of the task to incorporate visual signals, leading to many variants, including <b>multi-modal</b> <b>NER</b> (MNER) or grounded MNER (GMNER). A key challenge to these tasks is that the model should be able to generalize to the entities unseen during the training, and should be able to handle the training samples with noisy annotations. To address this obstacle, we propose SCANNER (Span CANdidate detection and recognition for <b>NER),</b> a model capable of effectively handling all three <b>NER</b> variants. SCANNER is a two-stage structure; we extract entity candidates in the first stage and use it as a query to get knowledge, effectively pulling knowledge from various sources. We can boost our performance by utilizing this entity-centric extracted knowledge to address unseen entities. Furthermore, to tackle the challenges arising from noisy annotations in <b>NER</b> datasets, we introduce a novel <b>self-distillation</b> method, enhancing the robustness and accuracy of our model in processing training data with inherent uncertainties. Our approach demonstrates competitive performance on the <b>NER</b> <b>benchmark</b> and surpasses existing methods on both MNER and GMNER <b>benchmarks.</b> Further analysis shows that the proposed <b>distillation</b> and knowledge utilization methods improve the performance of our model on various <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=2969--29327-humanizing-machine-generated-content-evading-ai-text-detection-through-adversarial-attack-ying-zhou-et-al-2024>(29/69 | 29/327) Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack (Ying Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ying Zhou, Ben He, Le Sun. (2024)<br><strong>Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack</strong><br><button class=copy-to-clipboard title="Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs-LG, cs.CL<br>Keyword Score: 45<br>Keywords: Adversarial Learning, Black Box, Large Language Model, Large Language Model, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01907v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01907v1.pdf filename=2404.01907v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the development of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> detecting whether text is generated by a machine becomes increasingly challenging in the face of malicious use cases like the spread of false information, protection of intellectual property, and prevention of academic plagiarism. While well-trained text detectors have demonstrated promising performance on unseen test data, recent research suggests that these detectors have vulnerabilities when dealing with <b>adversarial</b> <b>attacks</b> such as paraphrasing. In this paper, we propose a framework for a broader class of <b>adversarial</b> <b>attacks,</b> designed to perform minor perturbations in machine-generated content to evade detection. We consider two attack settings: white-box and <b>black-box,</b> <b>and</b> employ <b>adversarial</b> <b>learning</b> in dynamic scenarios to assess the potential enhancement of the current detection model&rsquo;s robustness against such attacks. The empirical results reveal that the current detection models can be compromised in as little as 10 seconds, leading to the misclassification of machine-generated text as human-written content. Furthermore, we explore the prospect of improving the model&rsquo;s robustness over iterative <b>adversarial</b> <b>learning.</b> Although some improvements in model robustness are observed, practical applications still face significant challenges. These findings shed light on the future development of AI-text detectors, emphasizing the need for more accurate and robust detection methods.</p></p class="citation"></blockquote><h3 id=3069--30327-read-improving-relation-extraction-from-an-adversarial-perspective-dawei-li-et-al-2024>(30/69 | 30/327) READ: Improving Relation Extraction from an ADversarial Perspective (Dawei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dawei Li, William Hogan, Jingbo Shang. (2024)<br><strong>READ: Improving Relation Extraction from an ADversarial Perspective</strong><br><button class=copy-to-clipboard title="READ: Improving Relation Extraction from an ADversarial Perspective" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Adversarial Learning, Benchmarking, Low-Resource, Relation Extraction, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02931v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02931v1.pdf filename=2404.02931v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent works in <b>relation</b> <b>extraction</b> (RE) have achieved promising <b>benchmark</b> accuracy; however, our <b>adversarial</b> <b>attack</b> experiments show that these works excessively rely on entities, making their generalization capability questionable. To address this issue, we propose an <b>adversarial</b> <b>training</b> method specifically designed for RE. Our approach introduces both sequence- and token-level perturbations to the sample and uses a separate perturbation vocabulary to improve the search for entity and context perturbations. Furthermore, we introduce a probabilistic strategy for leaving clean tokens in the context during <b>adversarial</b> <b>training.</b> This strategy enables a larger attack budget for entities and coaxes the model to leverage <b>relational</b> <b>patterns</b> embedded in the context. Extensive experiments show that compared to various <b>adversarial</b> <b>training</b> methods, our method significantly improves both the accuracy and robustness of the model. Additionally, experiments on different data availability settings highlight the effectiveness of our method in <b>low-resource</b> scenarios. We also perform in-depth analyses of our proposed method and provide further hints. We will release our code at <a href=https://github.com/David-Li0406/READ>https://github.com/David-Li0406/READ</a>.</p></p class="citation"></blockquote><h3 id=3169--31327-transforming-llms-into-cross-modal-and-cross-lingual-retrieval-systems-frank-palma-gomez-et-al-2024>(31/69 | 31/327) Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems (Frank Palma Gomez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frank Palma Gomez, Ramon Sanabria, Yun-hsuan Sung, Daniel Cer, Siddharth Dalmia, Gustavo Hernandez Abrego. (2024)<br><strong>Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems</strong><br><button class=copy-to-clipboard title="Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs-SD, cs.CL, eess-AS<br>Keyword Score: 43<br>Keywords: Multi-modal, Neural Machine Translation, Large Language Model, Large Language Model, Text Understanding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01616v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01616v2.pdf filename=2404.01616v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are trained on <b>text-only</b> <b>data</b> that go far beyond the languages with paired speech and <b>text</b> <b>data.</b> At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and <b>text</b> <b>in</b> many languages, we propose using <b>LLMs</b> to initialize <b>multi-modal</b> DE retrieval systems. Unlike traditional methods, our system doesn&rsquo;t require speech data during <b>LLM</b> pre-training and can exploit <b>LLM&rsquo;s</b> multilingual <b>text</b> <b>understanding</b> capabilities to match speech and <b>text</b> <b>in</b> languages unseen during retrieval training. Our <b>multi-modal</b> <b>LLM-based</b> retrieval system is capable of matching speech and <b>text</b> <b>in</b> 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these languages. Additionally, our model demonstrates cross-lingual speech and <b>text</b> <b>matching,</b> which is further enhanced by readily available <b>machine</b> <b>translation</b> data.</p></p class="citation"></blockquote><h3 id=3269--32327-two-heads-are-better-than-one-nested-poe-for-robust-defense-against-multi-backdoors-victoria-graf-et-al-2024>(32/69 | 32/327) Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors (Victoria Graf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victoria Graf, Qin Liu, Muhao Chen. (2024)<br><strong>Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors</strong><br><button class=copy-to-clipboard title="Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Hate Speech Detection, Sentiment Analysis, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02356v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02356v1.pdf filename=2404.02356v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data poisoning backdoor attacks can cause undesirable behaviors in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> and defending against them is of increasing importance. Existing defense mechanisms often assume that only one type of trigger is adopted by the attacker, while defending against multiple simultaneous and independent trigger types necessitates general defense frameworks and is relatively unexplored. In this paper, we propose Nested Product of Experts(NPoE) defense framework, which involves a mixture of experts (MoE) as a trigger-only ensemble within the PoE defense framework to simultaneously defend against multiple trigger types. During NPoE training, the main model is trained in an ensemble with a mixture of smaller expert models that learn the features of backdoor triggers. At inference time, only the main model is used. Experimental results on <b>sentiment</b> <b>analysis,</b> <b>hate</b> <b>speech</b> <b>detection,</b> and question classification tasks demonstrate that NPoE effectively defends against a variety of triggers both separately and in trigger mixtures. Due to the versatility of the MoE structure in NPoE, this framework can be further expanded to defend against other attack settings</p></p class="citation"></blockquote><h3 id=3369--33327-ukrainian-texts-classification-exploration-of-cross-lingual-knowledge-transfer-approaches-daryna-dementieva-et-al-2024>(33/69 | 33/327) Ukrainian Texts Classification: Exploration of Cross-lingual Knowledge Transfer Approaches (Daryna Dementieva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daryna Dementieva, Valeriia Khylenko, Georg Groh. (2024)<br><strong>Ukrainian Texts Classification: Exploration of Cross-lingual Knowledge Transfer Approaches</strong><br><button class=copy-to-clipboard title="Ukrainian Texts Classification: Exploration of Cross-lingual Knowledge Transfer Approaches" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Knowledge Transfer, Natural Language Inference, Text Classification, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02043v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02043v1.pdf filename=2404.02043v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the extensive amount of labeled datasets in the NLP <b>text</b> <b>classification</b> field, the persistent imbalance in data availability across various languages remains evident. Ukrainian, in particular, stands as a language that still can benefit from the continued refinement of cross-lingual methodologies. Due to our <b>knowledge,</b> <b>there</b> is a tremendous lack of Ukrainian corpora for typical <b>text</b> <b>classification</b> tasks. In this work, we leverage the state-of-the-art advances in NLP, exploring cross-lingual <b>knowledge</b> <b>transfer</b> methods avoiding manual data curation: large multilingual encoders and translation systems, <b>LLMs,</b> and language adapters. We test the approaches on three <b>text</b> <b>classification</b> tasks &ndash; toxicity classification, formality classification, and <b>natural</b> <b>language</b> <b>inference</b> &ndash; providing the &ldquo;recipe&rdquo; for the optimal setups.</p></p class="citation"></blockquote><h3 id=3469--34327-towards-better-understanding-of-cybercrime-the-role-of-fine-tuned-llms-in-translation-veronica-valeros-et-al-2024>(34/69 | 34/327) Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation (Veronica Valeros et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Veronica Valeros, Anna Širokova, Carlos Catania, Sebastian Garcia. (2024)<br><strong>Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation</strong><br><button class=copy-to-clipboard title="Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01940v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01940v1.pdf filename=2404.01940v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding cybercrime communications is paramount for cybersecurity defence. This often involves translating communications into English for processing, interpreting, and generating timely intelligence. The problem is that translation is hard. Human translation is slow, expensive, and scarce. <b>Machine</b> <b>translation</b> is inaccurate and biased. We propose using <b>fine-tuned</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> to generate translations that can accurately capture the nuances of cybercrime language. We apply our technique to public chats from the NoName057(16) Russian-speaking hacktivist group. Our results show that our <b>fine-tuned</b> <b>LLM</b> model is better, faster, more accurate, and able to capture nuances of the language. Our method shows it is possible to achieve high-fidelity translations and significantly reduce costs by a factor ranging from 430 to 23,000 compared to a human translator.</p></p class="citation"></blockquote><h3 id=3569--35327-indoculture-exploring-geographically-influenced-cultural-commonsense-reasoning-across-eleven-indonesian-provinces-fajri-koto-et-al-2024>(35/69 | 35/327) IndoCulture: Exploring Geographically-Influenced Cultural Commonsense Reasoning Across Eleven Indonesian Provinces (Fajri Koto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fajri Koto, Rahmad Mahendra, Nurul Aisyah, Timothy Baldwin. (2024)<br><strong>IndoCulture: Exploring Geographically-Influenced Cultural Commonsense Reasoning Across Eleven Indonesian Provinces</strong><br><button class=copy-to-clipboard title="IndoCulture: Exploring Geographically-Influenced Cultural Commonsense Reasoning Across Eleven Indonesian Provinces" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Common-sense Reasoning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01854v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01854v1.pdf filename=2404.01854v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although <b>commonsense</b> <b>reasoning</b> is greatly shaped by cultural and geographical factors, previous studies on language models have predominantly centered on English cultures, potentially resulting in an Anglocentric bias. In this paper, we introduce IndoCulture, aimed at understanding the influence of geographical factors on language model <b>reasoning</b> ability, with a specific emphasis on the diverse cultures found within eleven Indonesian provinces. In contrast to prior works that relied on templates (Yin et al., 2022) and online scrapping (Fung et al., 2024), we created IndoCulture by asking local people to manually develop the context and plausible options based on predefined topics. Evaluations of 23 language models reveal several insights: (1) even the best open-source model struggles with an accuracy of 53.2%, (2) models often provide more accurate predictions for specific provinces, such as Bali and West Java, and (3) the inclusion of location contexts enhances performance, especially in larger models like <b>GPT-4,</b> emphasizing the significance of geographical context in <b>commonsense</b> <b>reasoning.</b></p></p class="citation"></blockquote><h3 id=3669--36327-generative-ai-based-text-generation-methods-using-pre-trained-gpt-2-model-rohit-pandey-et-al-2024>(36/69 | 36/327) Generative AI-Based Text Generation Methods Using Pre-Trained GPT-2 Model (Rohit Pandey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohit Pandey, Hetvi Waghela, Sneha Rakshit, Aparna Rangari, Anjali Singh, Rahul Kumar, Ratnadeep Ghosal, Jaydip Sen. (2024)<br><strong>Generative AI-Based Text Generation Methods Using Pre-Trained GPT-2 Model</strong><br><button class=copy-to-clipboard title="Generative AI-Based Text Generation Methods Using Pre-Trained GPT-2 Model" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Generative AI, GPT, GPT-2, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01786v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01786v1.pdf filename=2404.01786v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work delved into the realm of automatic <b>text</b> <b>generation,</b> exploring a variety of techniques ranging from traditional deterministic approaches to more modern stochastic methods. Through analysis of greedy search, beam search, top-k sampling, top-p sampling, contrastive searching, and locally typical searching, this work has provided valuable insights into the strengths, weaknesses, and potential applications of each method. Each <b>text-generating</b> <b>method</b> is evaluated using several standard metrics and a comparative study has been made on the performance of the approaches. Finally, some future directions of research in the field of automatic <b>text</b> <b>generation</b> are also identified.</p></p class="citation"></blockquote><h3 id=3769--37327-octopus-v2-on-device-language-model-for-super-agent-wei-chen-et-al-2024>(37/69 | 37/327) Octopus v2: On-device language model for super agent (Wei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Chen, Zhiyuan Li. (2024)<br><strong>Octopus v2: On-device language model for super agent</strong><br><button class=copy-to-clipboard title="Octopus v2: On-device language model for super agent" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Retrieval-Augmented Generation, GPT, GPT-4, LLaMA<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01744v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01744v3.pdf filename=2404.01744v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models have shown effectiveness in a variety of software applications, particularly in tasks related to automatic workflow. These models possess the crucial ability to call functions, which is essential in creating AI agents. Despite the high performance of large-scale language models in cloud environments, they are often associated with concerns over privacy and cost. Current on-device models for function calling face issues with latency and accuracy. Our research presents a new method that empowers an on-device model with 2 billion parameters to surpass the performance of <b>GPT-4</b> in both accuracy and latency, and decrease the context length by 95%. When compared to <b>Llama-7B</b> with a <b>RAG-based</b> function calling mechanism, our method enhances latency by 35-fold. This method reduces the latency to levels deemed suitable for deployment across a variety of edge devices in production environments, aligning with the performance requisites for real-world applications.</p></p class="citation"></blockquote><h3 id=3869--38327-evaluating-large-language-models-using-contrast-sets-an-experimental-approach-manish-sanwal-2024>(38/69 | 38/327) Evaluating Large Language Models Using Contrast Sets: An Experimental Approach (Manish Sanwal, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manish Sanwal. (2024)<br><strong>Evaluating Large Language Models Using Contrast Sets: An Experimental Approach</strong><br><button class=copy-to-clipboard title="Evaluating Large Language Models Using Contrast Sets: An Experimental Approach" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Natural Language Inference, Natural Language Inference, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01569v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01569v1.pdf filename=2404.01569v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the domain of <b>Natural</b> <b>Language</b> <b>Inference</b> <b>(NLI),</b> especially in tasks involving the classification of multiple input texts, the Cross-Entropy Loss metric is widely employed as a standard for error measurement. However, this metric falls short in effectively evaluating a model&rsquo;s capacity to understand language entailments. In this study, we introduce an innovative technique for generating a contrast set for the Stanford <b>Natural</b> <b>Language</b> <b>Inference</b> (SNLI) dataset. Our strategy involves the automated substitution of verbs, adverbs, and adjectives with their synonyms to preserve the original meaning of sentences. This method aims to assess whether a model&rsquo;s performance is based on genuine language comprehension or simply on pattern recognition. We conducted our analysis using the ELECTRA-small model. The model achieved an accuracy of 89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5% on our contrast set, indicating a substantial 17% decline. This outcome led us to conduct a detailed examination of the model&rsquo;s learning behaviors. Following this, we improved the model&rsquo;s resilience by <b>fine-tuning</b> it with a contrast-enhanced training dataset specifically designed for SNLI, which increased its accuracy to 85.5% on the contrast sets. Our findings highlight the importance of incorporating diverse linguistic expressions into datasets for <b>NLI</b> tasks. We hope that our research will encourage the creation of more inclusive datasets, thereby contributing to the development of <b>NLI</b> models that are both more sophisticated and effective.</p></p class="citation"></blockquote><h3 id=3969--39327-ginopic-topic-modeling-with-graph-isomorphism-network-suman-adhya-et-al-2024>(39/69 | 39/327) GINopic: Topic Modeling with Graph Isomorphism Network (Suman Adhya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suman Adhya, Debarshi Kumar Sanyal. (2024)<br><strong>GINopic: Topic Modeling with Graph Isomorphism Network</strong><br><button class=copy-to-clipboard title="GINopic: Topic Modeling with Graph Isomorphism Network" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 36<br>Keywords: Graph, Benchmarking, Topic Model, BERT, Topic Modeling<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02115v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02115v1.pdf filename=2404.02115v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Topic</b> <b>modeling</b> is a widely used approach for analyzing and exploring large document collections. Recent research efforts have incorporated pre-trained contextualized language models, such as <b>BERT</b> embeddings, into <b>topic</b> <b>modeling.</b> However, they often neglect the intrinsic informational value conveyed by mutual dependencies between words. In this study, we introduce GINopic, a <b>topic</b> <b>modeling</b> framework based on <b>graph</b> isomorphism networks to capture the correlation between words. By conducting intrinsic (quantitative as well as qualitative) and extrinsic evaluations on diverse <b>benchmark</b> datasets, we demonstrate the effectiveness of GINopic compared to existing <b>topic</b> <b>models</b> and highlight its potential for advancing <b>topic</b> <b>modeling.</b></p></p class="citation"></blockquote><h3 id=4069--40327-africa-centric-self-supervised-pre-training-for-multilingual-speech-representation-in-a-sub-saharan-context-antoine-caubrière-et-al-2024>(40/69 | 40/327) Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context (Antoine Caubrière et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antoine Caubrière, Elodie Gauthier. (2024)<br><strong>Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context</strong><br><button class=copy-to-clipboard title="Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs-SD, cs.CL, eess-AS<br>Keyword Score: 33<br>Keywords: Benchmarking, Self-supervised Learning, Self-supervised Pre-training, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02000v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02000v2.pdf filename=2404.02000v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the first <b>self-supervised</b> <b>multilingual</b> speech model trained exclusively on African speech. The model learned from nearly 60 000 hours of unlabeled speech segments in 21 languages and dialects spoken in sub-Saharan Africa. On the SSA subset of the FLEURS-102 dataset, our approach based on a HuBERT$_{base}$ (0.09B) architecture shows competitive results, for <b>ASR</b> downstream task, compared to the w2v-bert-51 (0.6B) pre-trained model proposed in the FLEURS <b>benchmark,</b> while being more efficient by using 7x less data and 6x less parameters. Furthermore, in the context of a LID downstream task, our approach outperforms FLEURS baselines accuracy by over 22%.</p></p class="citation"></blockquote><h3 id=4169--41327-towards-better-generalization-in-open-domain-question-answering-by-mitigating-context-memorization-zixuan-zhang-et-al-2024>(41/69 | 41/327) Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization (Zixuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zixuan Zhang, Revanth Gangi Reddy, Kevin Small, Tong Zhang, Heng Ji. (2024)<br><strong>Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization</strong><br><button class=copy-to-clipboard title="Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Open-Domain Question Answering, Question Answering, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01652v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01652v1.pdf filename=2404.01652v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Open-domain</b> <b>Question</b> <b>Answering</b> (OpenQA) aims at answering factual <b>questions</b> <b>with</b> an external large-scale knowledge corpus. However, real-world knowledge is not static; it updates and evolves continually. Such a dynamic characteristic of knowledge poses a vital challenge for these models, as the trained models need to constantly adapt to the latest information to make sure that the answers remain accurate. In addition, it is still unclear how well an OpenQA model can transfer to completely new knowledge domains. In this paper, we investigate the generalization performance of a retrieval-augmented <b>QA</b> model in two specific scenarios: 1) adapting to updated versions of the same knowledge corpus; 2) switching to completely different knowledge domains. We observe that the generalization challenges of OpenQA models stem from the reader&rsquo;s over-reliance on memorizing the knowledge from the external corpus, which hinders the model from generalizing to a new knowledge corpus. We introduce Corpus-Invariant Tuning (CIT), a simple but effective training strategy, to mitigate the knowledge over-memorization by controlling the likelihood of retrieved contexts during training. Extensive experimental results on multiple OpenQA <b>benchmarks</b> show that CIT achieves significantly better generalizability without compromising the model&rsquo;s performance in its original corpus and domain.</p></p class="citation"></blockquote><h3 id=4269--42327-rematch-robust-and-efficient-matching-of-local-knowledge-graphs-to-improve-structural-and-semantic-similarity-zoher-kachwala-et-al-2024>(42/69 | 42/327) Rematch: Robust and Efficient Matching of Local Knowledge Graphs to Improve Structural and Semantic Similarity (Zoher Kachwala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zoher Kachwala, Jisun An, Haewoon Kwak, Filippo Menczer. (2024)<br><strong>Rematch: Robust and Efficient Matching of Local Knowledge Graphs to Improve Structural and Semantic Similarity</strong><br><button class=copy-to-clipboard title="Rematch: Robust and Efficient Matching of Local Knowledge Graphs to Improve Structural and Semantic Similarity" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 31<br>Keywords: Graph, Benchmarking, Knowledge Graph, Fact Verification, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02126v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02126v1.pdf filename=2404.02126v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>graphs</b> play a pivotal role in various applications, such as <b>question-answering</b> <b>and</b> <b>fact-checking.</b> <b>Abstract</b> Meaning Representation (AMR) represents text as <b>knowledge</b> <b>graphs.</b> Evaluating the quality of these <b>graphs</b> involves matching them structurally to each other and semantically to the source text. Existing AMR metrics are inefficient and struggle to capture semantic similarity. We also lack a systematic evaluation <b>benchmark</b> for assessing structural similarity between AMR <b>graphs.</b> To overcome these limitations, we introduce a novel AMR similarity metric, rematch, alongside a new evaluation for structural similarity called RARE. Among state-of-the-art metrics, rematch ranks second in structural similarity; and first in semantic similarity by 1&ndash;5 percentage points on the STS-B and SICK-R <b>benchmarks.</b> Rematch is also five times faster than the next most efficient metric.</p></p class="citation"></blockquote><h3 id=4369--43327-multi-bert-leveraging-adapters-and-prompt-tuning-for-low-resource-multi-domain-adaptation-parham-abed-azad-et-al-2024>(43/69 | 43/327) Multi-BERT: Leveraging Adapters and Prompt Tuning for Low-Resource Multi-Domain Adaptation (Parham Abed Azad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Parham Abed Azad, Hamid Beigy. (2024)<br><strong>Multi-BERT: Leveraging Adapters and Prompt Tuning for Low-Resource Multi-Domain Adaptation</strong><br><button class=copy-to-clipboard title="Multi-BERT: Leveraging Adapters and Prompt Tuning for Low-Resource Multi-Domain Adaptation" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Low-Resource, Named Entity Recognition, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02335v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02335v1.pdf filename=2404.02335v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid expansion of texts&rsquo; volume and diversity presents formidable challenges in multi-domain settings. These challenges are also visible in the Persian name entity recognition <b>(NER)</b> settings. Traditional approaches, either employing a unified model for multiple domains or individual models for each domain, frequently pose significant limitations. Single models often struggle to capture the nuances of diverse domains, while utilizing multiple large models can lead to resource constraints, rendering the training of a model for each domain virtually impractical. Therefore, this paper introduces a novel approach composed of one core model with multiple sets of domain-specific parameters. We utilize techniques such as <b>prompt</b> tuning and adapters, combined with the incorporation of additional layers, to add parameters that we can train for the specific domains. This enables the model to perform comparably to individual models for each domain. Experimental results on different formal and informal datasets show that by employing these added parameters, the proposed model significantly surpasses existing practical models in performance. Remarkably, the proposed model requires only one instance for training and storage, yet achieves outstanding results across all domains, even surpassing the state-of-the-art in some. Moreover, we analyze each adaptation strategy, delineating its strengths, weaknesses, and optimal hyper-parameters for the Persian <b>NER</b> settings. Finally, we introduce a document-based domain detection pipeline tailored for scenarios with unknown text domains, enhancing the adaptability and practicality of this paper in real-world applications.</p></p class="citation"></blockquote><h3 id=4469--44327-textttlmtexttt2-a-simple-society-of-language-models-solves-complex-reasoning-gurusha-juneja-et-al-2024>(44/69 | 44/327) $\texttt{LM}^\texttt{2}$: A Simple Society of Language Models Solves Complex Reasoning (Gurusha Juneja et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gurusha Juneja, Subhabrata Dutta, Tanmoy Chakraborty. (2024)<br><strong>$\texttt{LM}^\texttt{2}$: A Simple Society of Language Models Solves Complex Reasoning</strong><br><button class=copy-to-clipboard title="$\texttt{LM}^\texttt{2}$: A Simple Society of Language Models Solves Complex Reasoning" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02255v1.pdf filename=2404.02255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite demonstrating emergent <b>reasoning</b> abilities, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMS)</b> often lose track of complex, multi-step <b>reasoning.</b> Existing studies show that providing guidance via decomposing the original question into multiple subproblems elicits more robustness in <b>LLM</b> <b>reasoning</b> &ndash; a decomposer generates the subproblems, and a solver solves each of these subproblems. However, these techniques fail to accommodate coordination between the decomposer and the solver modules (either in a single model or different specialized ones) &ndash; the decomposer does not keep track of the ability of the solver to follow the decomposed <b>reasoning.</b> In this paper, we propose LM2 to address these challenges. LM2 modularizes the decomposition, solution, and verification into three different language models. The decomposer module identifies the key concepts necessary to solve the problem and generates step-by-step subquestions according to the <b>reasoning</b> requirement. The solver model generates the solution to the subproblems that are then checked by the verifier module; depending upon the feedback from the verifier, the <b>reasoning</b> context is constructed using the subproblems and the solutions. These models are trained to coordinate using policy learning. Exhaustive experimentation suggests the superiority of LM2 over existing methods on in- and out-domain <b>reasoning</b> problems, outperforming the best baselines by $8.1%$ on MATH, $7.71%$ on JEEBench, and $9.7%$ on MedQA problems (code available at <a href=https://github.com/LCS2-IIITD/Language_Model_Multiplex)>https://github.com/LCS2-IIITD/Language_Model_Multiplex)</a>.</p></p class="citation"></blockquote><h3 id=4569--45327-flawn-t5-an-empirical-examination-of-effective-instruction-tuning-data-mixtures-for-legal-reasoning-joel-niklaus-et-al-2024>(45/69 | 45/327) FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning (Joel Niklaus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joel Niklaus, Lucia Zheng, Arya D. McCarthy, Christopher Hahn, Brian M. Rosen, Peter Henderson, Daniel E. Ho, Garrett Honke, Percy Liang, Christopher Manning. (2024)<br><strong>FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning</strong><br><button class=copy-to-clipboard title="FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T50, I-2, cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02127v1.pdf filename=2404.02127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction</b> <b>tuning</b> is an important step in making language models useful for direct user interaction. However, many legal tasks remain out of reach for most open <b>LLMs</b> and there do not yet exist any large scale <b>instruction</b> <b>datasets</b> for the domain. This critically limits research in this application area. In this work, we curate LawInstruct, a large legal <b>instruction</b> <b>dataset,</b> covering 17 jurisdictions, 24 languages and a total of 12M examples. We present evidence that domain-specific pretraining and <b>instruction</b> <b>tuning</b> improve performance on LegalBench, including improving Flan-T5 XL by 8 points or 16% over the baseline. However, the effect does not generalize across all tasks, training regimes, model sizes, and other factors. LawInstruct is a resource for accelerating the development of models with stronger information processing and decision making capabilities in the legal domain.</p></p class="citation"></blockquote><h3 id=4669--46327-breaking-the-silence-detecting-and-mitigating-gendered-abuse-in-hindi-tamil-and-indian-english-online-spaces-advaitha-vetagiri-et-al-2024>(46/69 | 46/327) Breaking the Silence Detecting and Mitigating Gendered Abuse in Hindi, Tamil, and Indian English Online Spaces (Advaitha Vetagiri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Advaitha Vetagiri, Gyandeep Kalita, Eisha Halder, Chetna Taparia, Partha Pakray, Riyanka Manna. (2024)<br><strong>Breaking the Silence Detecting and Mitigating Gendered Abuse in Hindi, Tamil, and Indian English Online Spaces</strong><br><button class=copy-to-clipboard title="Breaking the Silence Detecting and Mitigating Gendered Abuse in Hindi, Tamil, and Indian English Online Spaces" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02013v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02013v2.pdf filename=2404.02013v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Online gender-based harassment is a widespread issue limiting the free expression and participation of women and marginalized genders in digital spaces. Detecting such abusive content can enable platforms to curb this menace. We participated in the Gendered Abuse Detection in Indic Languages shared task at ICON2023 that provided datasets of annotated Twitter posts in English, Hindi and Tamil for building classifiers to identify gendered abuse. Our team CNLP-NITS-PP developed an ensemble approach combining <b>CNN</b> and BiLSTM networks that can effectively model semantic and sequential patterns in textual data. The <b>CNN</b> captures localized features indicative of abusive language through its <b>convolution</b> filters applied on embedded input text. To determine context-based offensiveness, the BiLSTM analyzes this sequence for dependencies among <b>words</b> <b>and</b> phrases. Multiple variations were trained using FastText and GloVe <b>word</b> <b>embeddings</b> for each language dataset comprising over 7,600 crowdsourced annotations across labels for explicit abuse, targeted minority attacks and general offences. The validation scores showed strong performance across f1-measures, especially for English 0.84. Our experiments reveal how customizing embeddings and model hyperparameters can improve detection capability. The proposed architecture ranked 1st in the competition, proving its ability to handle real-world noisy text with code-switching. This technique has a promising scope as platforms aim to combat cyber harassment facing Indic language internet users. Our Code is at <a href=https://github.com/advaithavetagiri/CNLP-NITS-PP>https://github.com/advaithavetagiri/CNLP-NITS-PP</a></p></p class="citation"></blockquote><h3 id=4769--47327-dissecting-paraphrases-the-impact-of-prompt-syntax-and-supplementary-information-on-knowledge-retrieval-from-pretrained-language-models-stephan-linzbach-et-al-2024>(47/69 | 47/327) Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information on Knowledge Retrieval from Pretrained Language Models (Stephan Linzbach et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephan Linzbach, Dimitar Dimitrov, Laura Kallmeyer, Kilian Evang, Hajira Jabeen, Stefan Dietze. (2024)<br><strong>Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information on Knowledge Retrieval from Pretrained Language Models</strong><br><button class=copy-to-clipboard title="Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information on Knowledge Retrieval from Pretrained Language Models" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01992v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01992v1.pdf filename=2404.01992v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pre-trained</b> <b>Language</b> <b>Models</b> <b>(PLMs)</b> are known to contain various kinds of knowledge. One method to infer relational knowledge is through the use of cloze-style <b>prompts,</b> where a model is tasked to predict missing subjects or objects. Typically, designing these <b>prompts</b> is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. Simultaneously, evaluating the impact of either <b>prompt</b> syntax or information is challenging due to their interdependence. We designed CONPARE-LAMA - a dedicated probe, consisting of 34 million distinct <b>prompts</b> that facilitate comparison across minimal paraphrases. These paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations. CONPARE-LAMA enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval performance of <b>PLMs.</b> Extensive knowledge retrieval experiments using our probe reveal that <b>prompts</b> following clausal syntax have several desirable properties in comparison to appositive syntax: i) they are more useful when querying <b>PLMs</b> with a combination of supplementary information, ii) knowledge is more consistently recalled across different combinations of supplementary information, and iii) they decrease response uncertainty when retrieving known facts. In addition, range information can boost knowledge retrieval performance more than domain information, even though domain information is more reliably helpful across syntactic forms.</p></p class="citation"></blockquote><h3 id=4869--48327-beyond-accuracy-evaluating-the-reasoning-behavior-of-large-language-models----a-survey-philipp-mondorf-et-al-2024>(48/69 | 48/327) Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models &ndash; A Survey (Philipp Mondorf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Mondorf, Barbara Plank. (2024)<br><strong>Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models &ndash; A Survey</strong><br><button class=copy-to-clipboard title="Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01869v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01869v1.pdf filename=2404.01869v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have recently shown impressive performance on tasks involving <b>reasoning,</b> leading to a lively debate on whether these models possess <b>reasoning</b> capabilities similar to humans. However, despite these successes, the depth of <b>LLMs&rsquo;</b> <b>reasoning</b> abilities remains uncertain. This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models&rsquo; <b>reasoning</b> behavior. This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models&rsquo; <b>reasoning</b> processes. Furthermore, we survey prevalent methodologies to evaluate the <b>reasoning</b> behavior of <b>LLMs,</b> emphasizing current trends and efforts towards more nuanced <b>reasoning</b> analyses. Our review suggests that <b>LLMs</b> tend to rely on surface-level patterns and correlations in their training data, rather than on genuine <b>reasoning</b> abilities. Additionally, we identify the need for further research that delineates the key differences between human and <b>LLM-based</b> <b>reasoning.</b> Through this survey, we aim to shed light on the complex <b>reasoning</b> processes within <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=4969--49327-m2sa-multimodal-and-multilingual-model-for-sentiment-analysis-of-tweets-gaurish-thakkar-et-al-2024>(49/69 | 49/327) M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets (Gaurish Thakkar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gaurish Thakkar, Sherzod Hakimov, Marko Tadić. (2024)<br><strong>M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets</strong><br><button class=copy-to-clipboard title="M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Sentiment Analysis, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01753v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01753v1.pdf filename=2404.01753v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>multimodal</b> natural language processing, aimed at learning from diverse data types, has garnered significant attention. However, there needs to be more clarity when it comes to analysing <b>multimodal</b> tasks in multi-lingual contexts. While prior studies on <b>sentiment</b> <b>analysis</b> of tweets have predominantly focused on the English language, this paper addresses this gap by transforming an existing textual Twitter <b>sentiment</b> <b>dataset</b> into a <b>multimodal</b> format through a straightforward curation process. Our work opens up new avenues for <b>sentiment-related</b> <b>research</b> within the research community. Additionally, we conduct baseline experiments utilising this augmented dataset and report the findings. Notably, our evaluations reveal that when comparing unimodal and <b>multimodal</b> configurations, using a <b>sentiment-tuned</b> <b>large</b> <b>language</b> <b>model</b> as a text encoder performs exceptionally well.</p></p class="citation"></blockquote><h3 id=5069--50327-a-computational-analysis-of-lyric-similarity-perception-haven-kim-et-al-2024>(50/69 | 50/327) A Computational Analysis of Lyric Similarity Perception (Haven Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haven Kim, Taketo Akama. (2024)<br><strong>A Computational Analysis of Lyric Similarity Perception</strong><br><button class=copy-to-clipboard title="A Computational Analysis of Lyric Similarity Perception" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 20<br>Keywords: Recommendation, BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02342v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02342v1.pdf filename=2404.02342v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In musical compositions that include vocals, lyrics significantly contribute to artistic expression. Consequently, previous studies have introduced the concept of a <b>recommendation</b> system that suggests lyrics similar to a user&rsquo;s favorites or personalized preferences, aiding in the discovery of lyrics among millions of tracks. However, many of these systems do not fully consider human perceptions of lyric similarity, primarily due to limited research in this area. To bridge this gap, we conducted a comparative analysis of computational methods for modeling lyric similarity with human perception. Results indicated that computational models based on similarities between embeddings from pre-trained <b>BERT-based</b> models, the audio from which the lyrics are derived, and phonetic components are indicative of perceptual lyric similarity. This finding underscores the importance of semantic, stylistic, and phonetic similarities in human perception about lyric similarity. We anticipate that our findings will enhance the development of similarity-based lyric <b>recommendation</b> systems by offering pseudo-labels for neural network development and introducing objective evaluation metrics.</p></p class="citation"></blockquote><h3 id=5169--51327-collapse-of-self-trained-language-models-david-herel-et-al-2024>(51/69 | 51/327) Collapse of Self-trained Language Models (David Herel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Herel, Tomas Mikolov. (2024)<br><strong>Collapse of Self-trained Language Models</strong><br><button class=copy-to-clipboard title="Collapse of Self-trained Language Models" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: GPT, GPT-2<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02305v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02305v1.pdf filename=2404.02305v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In various fields of knowledge creation, including science, new ideas often build on pre-existing information. In this work, we explore this concept within the context of language models. Specifically, we explore the potential of self-training models on their own outputs, akin to how humans learn and build on their previous thoughts and actions. While this approach is intuitively appealing, our research reveals its practical limitations. We find that extended self-training of the <b>GPT-2</b> model leads to a significant degradation in performance, resulting in repetitive and collapsed token output.</p></p class="citation"></blockquote><h3 id=5269--52327-extracting-norms-from-contracts-via-chatgpt-opportunities-and-challenges-amanul-haque-et-al-2024>(52/69 | 52/327) Extracting Norms from Contracts Via ChatGPT: Opportunities and Challenges (Amanul Haque et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amanul Haque, Munindar P. Singh. (2024)<br><strong>Extracting Norms from Contracts Via ChatGPT: Opportunities and Challenges</strong><br><button class=copy-to-clipboard title="Extracting Norms from Contracts Via ChatGPT: Opportunities and Challenges" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02269v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02269v1.pdf filename=2404.02269v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the effectiveness of <b>ChatGPT</b> in extracting norms from contracts. Norms provide a natural way to engineer multiagent systems by capturing how to govern the interactions between two or more autonomous parties. We extract norms of commitment, prohibition, authorization, and power, along with associated norm elements (the parties involved, antecedents, and consequents) from contracts. Our investigation reveals <b>ChatGPT&rsquo;s</b> effectiveness and limitations in norm extraction from contracts. <b>ChatGPT</b> demonstrates promising performance in norm extraction without requiring training or <b>fine-tuning,</b> thus obviating the need for annotated data, which is not generally available in this domain. However, we found some limitations of <b>ChatGPT</b> in extracting these norms that lead to incorrect norm extractions. The limitations include oversight of crucial details, hallucination, incorrect parsing of conjunctions, and empty norm elements. Enhanced norm extraction from contracts can foster the development of more transparent and trustworthy formal agent interaction specifications, thereby contributing to the improvement of multiagent systems.</p></p class="citation"></blockquote><h3 id=5369--53327-kallaama-a-transcribed-speech-dataset-about-agriculture-in-the-three-most-widely-spoken-languages-in-senegal-elodie-gauthier-et-al-2024>(53/69 | 53/327) Kallaama: A Transcribed Speech Dataset about Agriculture in the Three Most Widely Spoken Languages in Senegal (Elodie Gauthier et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elodie Gauthier, Aminata Ndiaye, Abdoulaye Guissé. (2024)<br><strong>Kallaama: A Transcribed Speech Dataset about Agriculture in the Three Most Widely Spoken Languages in Senegal</strong><br><button class=copy-to-clipboard title="Kallaama: A Transcribed Speech Dataset about Agriculture in the Three Most Widely Spoken Languages in Senegal" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01991v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01991v1.pdf filename=2404.01991v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work is part of the Kallaama project, whose objective is to produce and disseminate national languages corpora for <b>speech</b> <b>technologies</b> developments, in the field of agriculture. Except for Wolof, which benefits from some language data for natural language processing, national languages of Senegal are largely ignored by language technology providers. However, such technologies are keys to the protection, promotion and teaching of these languages. Kallaama focuses on the 3 main spoken languages by Senegalese people: Wolof, Pulaar and Sereer. These languages are widely spoken by the population, with around 10 million of native Senegalese speakers, not to mention those outside the country. However, they remain under-resourced in terms of machine-readable data that can be used for <b>automatic</b> <b>processing</b> <b>and</b> language technologies, all the more so in the agricultural sector. We release a transcribed <b>speech</b> <b>dataset</b> containing 125 hours of recordings, about agriculture, in each of the above-mentioned languages. These resources are specifically designed for <b>Automatic</b> <b>Speech</b> <b>Recognition</b> purpose, including traditional approaches. To build such technologies, we provide textual corpora in Wolof and Pulaar, and a pronunciation lexicon containing 49,132 entries from the Wolof dataset.</p></p class="citation"></blockquote><h3 id=5469--54327-polarity-calibration-for-opinion-summarization-yuanyuan-lei-et-al-2024>(54/69 | 54/327) Polarity Calibration for Opinion Summarization (Yuanyuan Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanyuan Lei, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Ruihong Huang, Dong Yu. (2024)<br><strong>Polarity Calibration for Opinion Summarization</strong><br><button class=copy-to-clipboard title="Polarity Calibration for Opinion Summarization" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Opinion Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01706v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01706v1.pdf filename=2404.01706v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Opinion</b> <b>summarization</b> is automatically generating summaries from a variety of subjective information, such as product reviews or political <b>opinions.</b> <b>The</b> challenge of <b>opinions</b> <b>summarization</b> lies in presenting divergent or even conflicting <b>opinions.</b> <b>We</b> conduct an analysis of previous <b>summarization</b> models, which reveals their inclination to amplify the polarity bias, emphasizing the majority <b>opinions</b> <b>while</b> ignoring the minority <b>opinions.</b> <b>To</b> address this issue and make the summarizer express both sides of <b>opinions,</b> <b>we</b> introduce the concept of polarity calibration, which aims to align the polarity of output summary with that of input text. Specifically, we develop a reinforcement training approach for polarity calibration. This approach feeds the polarity distance between output summary and input text as reward into the summarizer, and also balance polarity calibration with content preservation and language naturality. We evaluate our Polarity Calibration model (PoCa) on two types of <b>opinions</b> <b>summarization</b> tasks: summarizing product reviews and political <b>opinions</b> <b>articles.</b> Automatic and human evaluation demonstrate that our approach can mitigate the polarity mismatch between output summary and input text, as well as maintain the content semantic and language quality.</p></p class="citation"></blockquote><h3 id=5569--55327-release-of-pre-trained-models-for-the-japanese-language-kei-sawada-et-al-2024>(55/69 | 55/327) Release of Pre-Trained Models for the Japanese Language (Kei Sawada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kei Sawada, Tianyu Zhao, Makoto Shing, Kentaro Mitsui, Akio Kaga, Yukiya Hono, Toshiaki Wakatsuki, Koh Mitsuda. (2024)<br><strong>Release of Pre-Trained Models for the Japanese Language</strong><br><button class=copy-to-clipboard title="Release of Pre-Trained Models for the Japanese Language" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CL, eess-AS<br>Keyword Score: 20<br>Keywords: GPT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01657v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01657v1.pdf filename=2404.01657v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AI democratization aims to create a world in which the average person can utilize AI techniques. To achieve this goal, numerous research institutes have attempted to make their results accessible to the public. In particular, large pre-trained models trained on large-scale data have shown unprecedented potential, and their release has had a significant impact. However, most of the released models specialize in the English language, and thus, AI democratization in non-English-speaking communities is lagging significantly. To reduce this gap in AI access, we released Generative Pre-trained <b>Transformer</b> <b>(GPT),</b> Contrastive Language and Image Pre-training (CLIP), Stable Diffusion, and Hidden-unit Bidirectional Encoder Representations from <b>Transformers</b> (HuBERT) pre-trained in Japanese. By providing these models, users can freely interface with AI that aligns with Japanese cultural values and ensures the identity of Japanese culture, thus enhancing the democratization of AI. Additionally, experiments showed that pre-trained models specialized for Japanese can efficiently achieve high performance in Japanese tasks.</p></p class="citation"></blockquote><h3 id=5669--56327-nlp-systems-that-cant-tell-use-from-mention-censor-counterspeech-but-teaching-the-distinction-helps-kristina-gligoric-et-al-2024>(56/69 | 56/327) NLP Systems That Can&rsquo;t Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps (Kristina Gligoric et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kristina Gligoric, Myra Cheng, Lucia Zheng, Esin Durmus, Dan Jurafsky. (2024)<br><strong>NLP Systems That Can&rsquo;t Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps</strong><br><button class=copy-to-clipboard title="NLP Systems That Can't Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs-HC, cs-SI, cs.CL<br>Keyword Score: 20<br>Keywords: Hate Speech Detection, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01651v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01651v1.pdf filename=2404.01651v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The use of words to convey speaker&rsquo;s intent is traditionally distinguished from the `mention&rsquo; of words for quoting what someone said, or pointing out properties of a word. Here we show that computationally modeling this use-mention distinction is crucial for dealing with counterspeech online. Counterspeech that refutes problematic content often mentions harmful language but is not harmful itself (e.g., calling a vaccine dangerous is not the same as expressing disapproval of someone for calling vaccines dangerous). We show that even recent language models fail at distinguishing use from mention, and that this failure propagates to two key downstream tasks: misinformation and <b>hate</b> <b>speech</b> <b>detection,</b> resulting in censorship of counterspeech. We introduce <b>prompting</b> mitigations that teach the use-mention distinction, and show they reduce these errors. Our work highlights the importance of the use-mention distinction for NLP and CSS and offers ways to address it.</p></p class="citation"></blockquote><h3 id=5769--57327-laying-anchors-semantically-priming-numerals-in-language-modeling-mandar-sharma-et-al-2024>(57/69 | 57/327) Laying Anchors: Semantically Priming Numerals in Language Modeling (Mandar Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mandar Sharma, Rutuja Murlidhar Taware, Pravesh Koirala, Nikhil Muralidhar, Naren Ramakrishnan. (2024)<br><strong>Laying Anchors: Semantically Priming Numerals in Language Modeling</strong><br><button class=copy-to-clipboard title="Laying Anchors: Semantically Priming Numerals in Language Modeling" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Grounding, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01536v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01536v1.pdf filename=2404.01536v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Off-the-shelf <b>pre-trained</b> <b>language</b> <b>models</b> have become the de facto standard in NLP pipelines for a multitude of downstream tasks. However, the inability of these models to properly encode numerals limits their performance on tasks requiring numeric comprehension. We introduce strategies to semantically prime numerals in any corpus by generating anchors governed by the distribution of numerals in said corpus, thereby enabling mathematically grounded representations of these numeral tokens. We establish the superiority of our proposed techniques through evaluation on a range of numeracy tasks for both in-domain (seen) and out-domain (unseen) numerals. Further, we expand our empirical evaluations to numerals ranging from 1 to 10 billion, a significantly broader range compared to previous studies of the same nature, and we demonstrate significant improvements in the mathematical <b>grounding</b> of our learned embeddings.</p></p class="citation"></blockquote><h3 id=5869--58327-sentence-level-media-bias-analysis-with-event-relation-graph-yuanyuan-lei-et-al-2024>(58/69 | 58/327) Sentence-level Media Bias Analysis with Event Relation Graph (Yuanyuan Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanyuan Lei, Ruihong Huang. (2024)<br><strong>Sentence-level Media Bias Analysis with Event Relation Graph</strong><br><button class=copy-to-clipboard title="Sentence-level Media Bias Analysis with Event Relation Graph" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 16<br>Keywords: Graph, Benchmarking, Sentence Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01722v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01722v1.pdf filename=2404.01722v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Media outlets are becoming more partisan and polarized nowadays. In this paper, we identify media bias at the <b>sentence</b> <b>level,</b> and pinpoint bias <b>sentences</b> <b>that</b> intend to sway readers&rsquo; opinions. As bias <b>sentences</b> <b>are</b> often expressed in a neutral and factual way, considering broader context outside a <b>sentence</b> <b>can</b> help reveal the bias. In particular, we observe that events in a bias <b>sentence</b> <b>need</b> to be understood in associations with other events in the document. Therefore, we propose to construct an event relation <b>graph</b> to explicitly reason about event-event relations for <b>sentence-level</b> <b>bias</b> identification. The designed event relation <b>graph</b> consists of events as nodes and four common types of event relations: coreference, temporal, causal, and subevent relations. Then, we incorporate event relation <b>graph</b> for bias <b>sentences</b> <b>identification</b> in two steps: an event-aware language model is built to inject the events and event relations knowledge into the basic language model via soft labels; further, a relation-aware <b>graph</b> attention network is designed to update <b>sentence</b> <b>embedding</b> with events and event relations information based on hard labels. Experiments on two <b>benchmark</b> datasets demonstrate that our approach with the aid of event relation <b>graph</b> improves both precision and recall of bias <b>sentence</b> <b>identification.</b></p></p class="citation"></blockquote><h3 id=5969--59327-entity-disambiguation-via-fusion-entity-decoding-junxiong-wang-et-al-2024>(59/69 | 59/327) Entity Disambiguation via Fusion Entity Decoding (Junxiong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junxiong Wang, Ali Mousavi, Omar Attia, Saloni Potdar, Alexander M. Rush, Umar Farooq Minhas, Yunyao Li. (2024)<br><strong>Entity Disambiguation via Fusion Entity Decoding</strong><br><button class=copy-to-clipboard title="Entity Disambiguation via Fusion Entity Decoding" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Disambiguation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01626v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01626v1.pdf filename=2404.01626v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Entity <b>disambiguation</b> (ED), which links the mentions of ambiguous entities to their referent entities in a knowledge base, serves as a core component in entity linking (EL). Existing generative approaches demonstrate improved accuracy compared to classification approaches under the standardized ZELDA <b>benchmark.</b> Nevertheless, generative approaches suffer from the need for large-scale pre-training and inefficient generation. Most importantly, entity descriptions, which could contain crucial information to distinguish similar entities from each other, are often overlooked. We propose an encoder-decoder model to disambiguate entities with more detailed entity descriptions. Given text and candidate entities, the encoder learns interactions between the text and each candidate entity, producing representations for each entity candidate. The decoder then fuses the representations of entity candidates together and selects the correct entity. Our experiments, conducted on various entity <b>disambiguation</b> <b>benchmarks,</b> demonstrate the strong and robust performance of this model, particularly +1.5% in the ZELDA <b>benchmark</b> compared with GENRE. Furthermore, we integrate this approach into the retrieval/reader framework and observe +1.5% improvements in end-to-end entity linking in the GERBIL <b>benchmark</b> compared with EntQA.</p></p class="citation"></blockquote><h3 id=6069--60327-using-interpretation-methods-for-model-enhancement-zhuo-chen-et-al-2024>(60/69 | 60/327) Using Interpretation Methods for Model Enhancement (Zhuo Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuo Chen, Chengyue Jiang, Kewei Tu. (2024)<br><strong>Using Interpretation Methods for Model Enhancement</strong><br><button class=copy-to-clipboard title="Using Interpretation Methods for Model Enhancement" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Low-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02068v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02068v1.pdf filename=2404.02068v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the age of neural natural language processing, there are plenty of works trying to derive interpretations of neural models. Intuitively, when gold rationales exist during training, one can additionally train the model to match its interpretation with the rationales. However, this intuitive idea has not been fully explored. In this paper, we propose a framework of utilizing interpretation methods and gold rationales to enhance models. Our framework is very general in the sense that it can incorporate various interpretation methods. Previously proposed gradient-based methods can be shown as an instance of our framework. We also propose two novel instances utilizing two other types of interpretation methods, erasure/replace-based and extractor-based methods, for model enhancement. We conduct comprehensive experiments on a variety of tasks. Experimental results show that our framework is effective especially in <b>low-resource</b> settings in enhancing models with various interpretation methods, and our two newly-proposed methods outperform gradient-based methods in most settings. Code is available at <a href=https://github.com/Chord-Chen-30/UIMER>https://github.com/Chord-Chen-30/UIMER</a>.</p></p class="citation"></blockquote><h3 id=6169--61327-bertopic-driven-stock-market-predictions-unraveling-sentiment-insights-enmin-zhu-et-al-2024>(61/69 | 61/327) BERTopic-Driven Stock Market Predictions: Unraveling Sentiment Insights (Enmin Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enmin Zhu, Jerome Yen. (2024)<br><strong>BERTopic-Driven Stock Market Predictions: Unraveling Sentiment Insights</strong><br><button class=copy-to-clipboard title="BERTopic-Driven Stock Market Predictions: Unraveling Sentiment Insights" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CE, cs-CL, cs.CL, q-fin-ST<br>Keyword Score: 10<br>Keywords: Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02053v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02053v2.pdf filename=2404.02053v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the intersection of Natural Language Processing (NLP) and financial analysis, focusing on the impact of <b>sentiment</b> <b>analysis</b> in stock price prediction. We employ BERTopic, an advanced NLP technique, to analyze the <b>sentiment</b> <b>of</b> topics derived from stock market comments. Our methodology integrates this <b>sentiment</b> <b>analysis</b> with various deep learning models, renowned for their effectiveness in time series and stock prediction tasks. Through comprehensive experiments, we demonstrate that incorporating topic <b>sentiment</b> <b>notably</b> enhances the performance of these models. The results indicate that topics in stock market comments provide implicit, valuable insights into stock market volatility and price trends. This study contributes to the field by showcasing the potential of NLP in enriching financial analysis and opens up avenues for further research into real-time <b>sentiment</b> <b>analysis</b> and the exploration of emotional and contextual aspects of market <b>sentiment.</b> <b>The</b> integration of advanced NLP techniques like BERTopic with traditional financial analysis methods marks a step forward in developing more sophisticated tools for understanding and predicting market behaviors.</p></p class="citation"></blockquote><h3 id=6269--62327-preuve-de-concept-dun-bot-vocal-dialoguant-en-wolof-elodie-gauthier-et-al-2024>(62/69 | 62/327) Preuve de concept d&rsquo;un bot vocal dialoguant en wolof (Elodie Gauthier et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elodie Gauthier, Papa-Séga Wade, Thierry Moudenc, Patrice Collen, Emilie De Neef, Oumar Ba, Ndeye Khoyane Cama, Cheikh Ahmadou Bamba Kebe, Ndeye Aissatou Gningue, Thomas Mendo&rsquo;o Aristide. (2024)<br><strong>Preuve de concept d&rsquo;un bot vocal dialoguant en wolof</strong><br><button class=copy-to-clipboard title="Preuve de concept d'un bot vocal dialoguant en wolof" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs.CL<br>Keyword Score: 10<br>Keywords: Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02009v1.pdf filename=2404.02009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents the proof-of-concept of the first automatic voice assistant ever built in Wolof language, the main vehicular language spoken in Senegal. This voicebot is the result of a collaborative research project between Orange Innovation in France, Orange Senegal (aka Sonatel) and ADNCorp, a small IT company based in Dakar, Senegal. The purpose of the voicebot is to provide information to Orange customers about the Sargal loyalty program of Orange Senegal by using the most natural mean to communicate: speech. The voicebot receives in input the customer&rsquo;s oral request that is then processed by a SLU system to reply to the customer&rsquo;s request using audio recordings. The first results of this proof-of-concept are encouraging as we achieved 22% of WER for the <b>ASR</b> task and 78% of F1-score on the NLU task.</p></p class="citation"></blockquote><h3 id=6369--63327-when-abel-kills-cain-what-machine-translation-cannot-capture-aurélien-bénel-et-al-2024>(63/69 | 63/327) When Abel Kills Cain: What Machine Translation Cannot Capture (Aurélien Bénel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aurélien Bénel, Joris Falip, Philippe Lacour. (2024)<br><strong>When Abel Kills Cain: What Machine Translation Cannot Capture</strong><br><button class=copy-to-clipboard title="When Abel Kills Cain: What Machine Translation Cannot Capture" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04279v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04279v1.pdf filename=2404.04279v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The article aims at identifying what, from a structural point of view, AI based automatic translators cannot fully capture. It focuses on the <b>machine&rsquo;s</b> <b>mistakes,</b> in order to try to explain its causes. The biblical story of Ca"in and Abel has been chosen because of its rich interpretive and critical tradition, but also because of its semantic difficulty. The investigation begins with the observation, for the translation of this text, of the language pairs and interfaces offered by the best known <b>machine</b> <b>translation</b> services (Google Translate, DeepL). A typology of the most frequent translation errors is then established. Finally, contemporary translations are compared, in order to underline the unique contribution of each. In conclusion, the article suggests a revision of translation theory and, corArtificial Intelligence, Translation, Limitations, Interpretation, Comparison, Unicityelatively, a reformulation of its technology concerning cultural texts.</p></p class="citation"></blockquote><h3 id=6469--64327-activation-steering-for-robust-type-prediction-in-codellms-francesca-lucchetti-et-al-2024>(64/69 | 64/327) Activation Steering for Robust Type Prediction in CodeLLMs (Francesca Lucchetti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesca Lucchetti, Arjun Guha. (2024)<br><strong>Activation Steering for Robust Type Prediction in CodeLLMs</strong><br><button class=copy-to-clipboard title="Activation Steering for Robust Type Prediction in CodeLLMs" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs-PL, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01903v1.pdf filename=2404.01903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contemporary <b>LLMs</b> pretrained on code are capable of succeeding at a wide variety of programming tasks. However, their performance is very sensitive to syntactic features, such as the names of variables and types, the structure of code, and presence of type hints. We contribute an inference-time technique to make CodeLLMs more robust to syntactic distractors that are semantically irrelevant. Our methodology relies on activation steering, which involves editing internal model activations to steer the model towards the correct prediction. We contribute a novel way to construct steering vectors by taking inspiration from mutation testing, which constructs minimal semantics-breaking code edits. In contrast, we construct steering vectors from semantics-preserving code edits. We apply our approach to the task of type prediction for the gradually typed languages Python and TypeScript. This approach corrects up to 90% of type mispredictions. Finally, we show that steering vectors calculated from Python activations reliably correct type mispredictions in TypeScript, and vice versa. This result suggests that <b>LLMs</b> may be learning to transfer knowledge of types across programming languages.</p></p class="citation"></blockquote><h3 id=6569--65327-self-strae-at-semeval-2024-task-1-making-self-structuring-autoencoders-learn-more-with-less-mattia-opper-et-al-2024>(65/69 | 65/327) Self-StrAE at SemEval-2024 Task 1: Making Self-Structuring AutoEncoders Learn More With Less (Mattia Opper et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mattia Opper, N. Siddharth. (2024)<br><strong>Self-StrAE at SemEval-2024 Task 1: Making Self-Structuring AutoEncoders Learn More With Less</strong><br><button class=copy-to-clipboard title="Self-StrAE at SemEval-2024 Task 1: Making Self-Structuring AutoEncoders Learn More With Less" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01860v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01860v1.pdf filename=2404.01860v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents two simple improvements to the Self-Structuring <b>AutoEncoder</b> (Self-StrAE). Firstly, we show that including reconstruction to the vocabulary as an auxiliary objective improves representation quality. Secondly, we demonstrate that increasing the number of independent channels leads to significant improvements in embedding quality, while simultaneously reducing the number of parameters. Surprisingly, we demonstrate that this trend can be followed to the extreme, even to point of reducing the total number of non-embedding parameters to seven. Our system can be pre-trained from scratch with as little as 10M tokens of input data, and proves effective across English, Spanish and Afrikaans.</p></p class="citation"></blockquote><h3 id=6669--66327-poro-34b-and-the-blessing-of-multilinguality-risto-luukkonen-et-al-2024>(66/69 | 66/327) Poro 34B and the Blessing of Multilinguality (Risto Luukkonen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Risto Luukkonen, Jonathan Burdge, Elaine Zosa, Aarne Talman, Ville Komulainen, Väinö Hatanpää, Peter Sarlin, Sampo Pyysalo. (2024)<br><strong>Poro 34B and the Blessing of Multilinguality</strong><br><button class=copy-to-clipboard title="Poro 34B and the Blessing of Multilinguality" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01856v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01856v1.pdf filename=2404.01856v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pretraining of state-of-the-art <b>large</b> <b>language</b> <b>models</b> now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual <b>large</b> <b>languages.</b> <b>We</b> believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at <a href=https://huggingface.co/LumiOpen/Poro-34B>https://huggingface.co/LumiOpen/Poro-34B</a>.</p></p class="citation"></blockquote><h3 id=6769--67327-generative-ai-for-immersive-communication-the-next-frontier-in-internet-of-senses-through-6g-nassim-sehad-et-al-2024>(67/69 | 67/327) Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G (Nassim Sehad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nassim Sehad, Lina Bariah, Wassim Hamidouche, Hamed Hellaoui, Riku Jäntti, Mérouane Debbah. (2024)<br><strong>Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G</strong><br><button class=copy-to-clipboard title="Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs-MM, cs-NI, cs.CL<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01713v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01713v1.pdf filename=2404.01713v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the past two decades, the Internet-of-Things (IoT) has been a transformative concept, and as we approach 2030, a new paradigm known as the Internet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR), IoS seeks to provide multi-sensory experiences, acknowledging that in our physical reality, our perception extends far beyond just sight and sound; it encompasses a range of senses. This article explores existing technologies driving immersive multi-sensory media, delving into their capabilities and potential applications. This exploration includes a comparative analysis between conventional immersive media streaming and a proposed use case that leverages semantic communication empowered by <b>generative</b> <b>Artificial</b> Intelligence (AI). The focal point of this analysis is the substantial reduction in bandwidth consumption by 99.93% in the proposed scheme. Through this comparison, we aim to underscore the practical applications of <b>generative</b> <b>AI</b> for immersive media while addressing the challenges and outlining future trajectories.</p></p class="citation"></blockquote><h3 id=6869--68327-event-detection-from-social-media-for-epidemic-prediction-tanmay-parekh-et-al-2024>(68/69 | 68/327) Event Detection from Social Media for Epidemic Prediction (Tanmay Parekh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanmay Parekh, Anh Mac, Jiarui Yu, Yuxuan Dong, Syed Shahriar, Bonnie Liu, Eric Yang, Kuan-Hao Huang, Wei Wang, Nanyun Peng, Kai-Wei Chang. (2024)<br><strong>Event Detection from Social Media for Epidemic Prediction</strong><br><button class=copy-to-clipboard title="Event Detection from Social Media for Epidemic Prediction" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SI, cs.CL, physics-soc-ph<br>Keyword Score: 10<br>Keywords: Event Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01679v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01679v1.pdf filename=2404.01679v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social media is an easy-to-access platform providing timely updates about societal trends and <b>events.</b> <b>Discussions</b> regarding epidemic-related <b>events</b> <b>such</b> as infections, symptoms, and social interactions can be crucial for informing policymaking during epidemic outbreaks. In our work, we pioneer exploiting <b>Event</b> <b>Detection</b> (ED) for better preparedness and early warnings of any upcoming epidemic by developing a framework to extract and analyze epidemic-related <b>events</b> <b>from</b> social media posts. To this end, we curate an epidemic <b>event</b> <b>ontology</b> comprising seven disease-agnostic <b>event</b> <b>types</b> and construct a Twitter dataset SPEED with human-annotated <b>events</b> <b>focused</b> on the COVID-19 pandemic. Experimentation reveals how ED models trained on COVID-based SPEED can effectively detect epidemic <b>events</b> <b>for</b> three unseen epidemics of Monkeypox, Zika, and Dengue; while models trained on existing ED datasets fail miserably. Furthermore, we show that reporting sharp increases in the extracted <b>events</b> <b>by</b> our framework can provide warnings 4-9 weeks earlier than the WHO epidemic declaration for Monkeypox. This utility of our framework lays the foundations for better preparedness against emerging epidemics.</p></p class="citation"></blockquote><h3 id=6969--69327-lastresort-at-semeval-2024-task-3-exploring-multimodal-emotion-cause-pair-extraction-as-sequence-labelling-task-suyash-vardhan-mathur-et-al-2024>(69/69 | 69/327) LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task (Suyash Vardhan Mathur et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suyash Vardhan Mathur, Akshett Rai Jindal, Hardik Mittal, Manish Shrivastava. (2024)<br><strong>LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task</strong><br><button class=copy-to-clipboard title="LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02088v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02088v1.pdf filename=2404.02088v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversation is the most natural form of human communication, where each utterance can range over a variety of possible emotions. While significant work has been done towards the detection of emotions in text, relatively little work has been done towards finding the cause of the said emotions, especially in <b>multimodal</b> settings. SemEval 2024 introduces the task of <b>Multimodal</b> Emotion Cause Analysis in Conversations, which aims to extract emotions reflected in individual utterances in a conversation involving multiple modalities (textual, audio, and visual modalities) along with the corresponding utterances that were the cause for the emotion. In this paper, we propose models that tackle this task as an utterance labeling and a sequence labeling problem and perform a comparative study of these models, involving baselines using different encoders, using BiLSTM for adding contextual information of the conversation, and finally adding a CRF layer to try to model the inter-dependencies between adjacent utterances more effectively. In the official leaderboard for the task, our architecture was ranked 8th, achieving an F1-score of 0.1759 on the leaderboard.</p></p class="citation"></blockquote><h2 id=csai-4>cs.AI (4)</h2><h3 id=14--70327-advancing-llm-reasoning-generalists-with-preference-trees-lifan-yuan-et-al-2024>(1/4 | 70/327) Advancing LLM Reasoning Generalists with Preference Trees (Lifan Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, Maosong Sun. (2024)<br><strong>Advancing LLM Reasoning Generalists with Preference Trees</strong><br><button class=copy-to-clipboard title="Advancing LLM Reasoning Generalists with Preference Trees" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 116<br>Keywords: Benchmarking, Benchmarking, Fine-tuning, Fine-tuning, Supervised Learning, GPT, GPT-3, GPT-3.5, Mistral, Code Generation, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02078v1.pdf filename=2404.02078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Eurus, a suite of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> optimized for <b>reasoning.</b> <b>Finetuned</b> from <b>Mistral-7B</b> and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of <b>benchmarks</b> covering mathematics, <b>code</b> <b>generation,</b> and logical <b>reasoning</b> problems. Notably, Eurus-70B beats <b>GPT-3.5</b> Turbo in <b>reasoning</b> through a comprehensive <b>benchmarking</b> across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging <b>benchmarks,</b> substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of Eurus can be primarily attributed to UltraInteract, our newly-curated <b>large-scale,</b> <b>high-quality</b> <b>alignment</b> dataset specifically designed for complex <b>reasoning</b> tasks. UltraInteract can be used in both <b>supervised</b> <b>fine-tuning</b> and preference learning. For each instruction, it includes a preference tree consisting of (1) <b>reasoning</b> chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. UltraInteract allows us to conduct an in-depth exploration of preference learning for <b>reasoning</b> tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for <b>reasoning</b> tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with UltraInteract, leads to a strong reward model.</p></p class="citation"></blockquote><h3 id=24--71327-a-survey-on-large-language-model-based-game-agents-sihao-hu-et-al-2024>(2/4 | 71/327) A Survey on Large Language Model-Based Game Agents (Sihao Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, Ling Liu. (2024)<br><strong>A Survey on Large Language Model-Based Game Agents</strong><br><button class=copy-to-clipboard title="A Survey on Large Language Model-Based Game Agents" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02039v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02039v1.pdf filename=2404.02039v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of game agents holds a critical role in advancing towards Artificial General Intelligence (AGI). The progress of <b>LLMs</b> and their <b>multimodal</b> counterparts (MLLMs) offers an unprecedented opportunity to evolve and empower game agents with human-like decision-making capabilities in complex computer game environments. This paper provides a comprehensive overview of <b>LLM-based</b> game agents from a holistic viewpoint. First, we introduce the conceptual architecture of <b>LLM-based</b> game agents, centered around six essential functional components: perception, memory, thinking, role-playing, action, and learning. Second, we survey existing representative <b>LLM-based</b> game agents documented in the literature with respect to methodologies and adaptation agility across six genres of games, including adventure, communication, competition, cooperation, <b>simulation,</b> and crafting & exploration games. Finally, we present an outlook of future research and development directions in this burgeoning field. A curated list of relevant papers is maintained and made accessible at: <a href=https://github.com/git-disl/awesome-LLM-game-agent-papers>https://github.com/git-disl/awesome-LLM-game-agent-papers</a>.</p></p class="citation"></blockquote><h3 id=34--72327-towards-generalizable-and-faithful-logic-reasoning-over-natural-language-via-resolution-refutation-zhouhao-sun-et-al-2024>(3/4 | 72/327) Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation (Zhouhao Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhouhao Sun, Xiao Ding, Li Du, Bibo Cai, Jinglong Gao, Ting Liu, Qin Bing. (2024)<br><strong>Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation</strong><br><button class=copy-to-clipboard title="Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01677v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01677v2.pdf filename=2404.01677v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved significant performance in various natural language <b>reasoning</b> tasks. However, they still struggle with performing first-order logic <b>reasoning</b> over formal logical theories expressed in natural language. This is because the previous <b>LLMs-based</b> <b>reasoning</b> systems have the theoretical incompleteness issue. As a result, it can only address a limited set of simple <b>reasoning</b> problems, which significantly decreases their generalization ability. To address this issue, we propose a novel framework, named Generalizable and Faithful Reasoner (GFaiR), which introduces the paradigm of resolution refutation. Resolution refutation has the capability to solve all first-order logic <b>reasoning</b> problems by extending <b>reasoning</b> rules and employing the principle of proof by contradiction, so our system&rsquo;s completeness can be improved by introducing resolution refutation. Experimental results demonstrate that our system outperforms previous works by achieving state-of-the-art performances in complex scenarios while maintaining performances in simple scenarios. Besides, we observe that GFaiR is faithful to its <b>reasoning</b> process.</p></p class="citation"></blockquote><h3 id=44--73327-imitation-game-a-model-based-and-imitation-learning-deep-reinforcement-learning-hybrid-eric-msp-veith-et-al-2024>(4/4 | 73/327) Imitation Game: A Model-based and Imitation Learning Deep Reinforcement Learning Hybrid (Eric MSP Veith et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric MSP Veith, Torben Logemann, Aleksandr Berezin, Arlena Wellßow, Stephan Balduin. (2024)<br><strong>Imitation Game: A Model-based and Imitation Learning Deep Reinforcement Learning Hybrid</strong><br><button class=copy-to-clipboard title="Imitation Game: A Model-based and Imitation Learning Deep Reinforcement Learning Hybrid" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01794v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01794v1.pdf filename=2404.01794v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous and learning systems based on Deep <b>Reinforcement</b> <b>Learning</b> have firmly established themselves as a foundation for approaches to creating resilient and efficient Cyber-Physical Energy Systems. However, most current approaches suffer from two distinct problems: Modern model-free algorithms such as Soft Actor Critic need a high number of samples to learn a meaningful policy, as well as a fallback to ward against concept drifts (e. g., catastrophic forgetting). In this paper, we present the work in progress towards a hybrid agent architecture that combines model-based Deep <b>Reinforcement</b> <b>Learning</b> with imitation learning to overcome both problems.</p></p class="citation"></blockquote><h2 id=cscr-10>cs.CR (10)</h2><h3 id=110--74327-jailbreaking-leading-safety-aligned-llms-with-simple-adaptive-attacks-maksym-andriushchenko-et-al-2024>(1/10 | 74/327) Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks (Maksym Andriushchenko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion. (2024)<br><strong>Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks</strong><br><button class=copy-to-clipboard title="Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR, stat-ML<br>Keyword Score: 90<br>Keywords: Claude, GPT, GPT-3, GPT-4, LLaMA, In-context Learning, In-context Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02151v1.pdf filename=2404.02151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show that even the most recent safety-aligned <b>LLMs</b> are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial <b>prompt</b> template (sometimes adapted to the target <b>LLM),</b> and then we apply random search on a suffix to maximize the target logprob (e.g., of the token &ldquo;Sure&rdquo;), potentially with multiple restarts. In this way, we achieve nearly 100% attack success rate &ndash; according to <b>GPT-4</b> as a judge &ndash; on <b>GPT-3.5/4,</b> <b>Llama-2-Chat-7B/13B/70B,</b> Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all <b>Claude</b> models &ndash; that do not expose logprobs &ndash; via either a transfer or prefilling attack with 100% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models &ndash; a task that shares many similarities with jailbreaking &ndash; which is the algorithm that brought us the first place in the SaTML'24 Trojan Detection Competition. The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different <b>prompting</b> templates (e.g., R2D2 is very sensitive to <b>in-context</b> <b>learning</b> <b>prompts),</b> some models have unique vulnerabilities based on their APIs (e.g., prefilling for <b>Claude),</b> and in some settings it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection). We provide the code, <b>prompts,</b> and logs of the attacks at <a href=https://github.com/tml-epfl/llm-adaptive-attacks>https://github.com/tml-epfl/llm-adaptive-attacks</a>.</p></p class="citation"></blockquote><h3 id=210--75327-great-now-write-an-article-about-that-the-crescendo-multi-turn-llm-jailbreak-attack-mark-russinovich-et-al-2024>(2/10 | 75/327) Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack (Mark Russinovich et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mark Russinovich, Ahmed Salem, Ronen Eldan. (2024)<br><strong>Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack</strong><br><button class=copy-to-clipboard title="Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 60<br>Keywords: ChatGPT, Gemini, LLaMA, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01833v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01833v1.pdf filename=2404.01833v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have risen significantly in popularity and are increasingly being adopted across multiple applications. These <b>LLMs</b> are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as &ldquo;jailbreaks&rdquo;, seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. In this paper, we introduce a novel jailbreak attack called Crescendo. Unlike existing jailbreak methods, Crescendo is a multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general <b>prompt</b> or question about the task at hand and then gradually escalates the dialogue by referencing the model&rsquo;s replies, progressively leading to a successful jailbreak. We evaluate Crescendo on various public systems, including <b>ChatGPT,</b> <b>Gemini</b> Pro, <b>Gemini-Ultra,</b> <b>LlaMA-2</b> 70b Chat, and Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks. Furthermore, we introduce Crescendomation, a tool that automates the Crescendo attack, and our evaluation showcases its effectiveness against state-of-the-art models.</p></p class="citation"></blockquote><h3 id=310--76327-jailbreaking-prompt-attack-a-controllable-adversarial-attack-against-diffusion-models-jiachen-ma-et-al-2024>(3/10 | 76/327) Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models (Jiachen Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiachen Ma, Anda Cao, Zhiqing Xiao, Jie Zhang, Chao Ye, Junbo Zhao. (2024)<br><strong>Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models</strong><br><button class=copy-to-clipboard title="Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 35<br>Keywords: Diffusion Model, Black Box, Prompt, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02928v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02928v1.pdf filename=2404.02928v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The fast advance of the image generation community has attracted attention worldwide. The safety issue needs to be further scrutinized and studied. There have been a few works around this area mostly achieving a post-processing design, model-specific, or yielding suboptimal image quality generation. Despite that, in this article, we discover a <b>black-box</b> <b>attack</b> method that enjoys three merits. It enables (i)-attacks both directed and semantic-driven that theoretically and practically pose a hazard to this vast user community, (ii)-surprisingly surpasses the white-box attack in a <b>black-box</b> <b>manner</b> and (iii)-without requiring any post-processing effort. Core to our approach is inspired by the concept guidance intriguing property of Classifier-Free guidance (CFG) in T2I models, and we discover that conducting frustratingly simple guidance in the CLIP embedding space, coupled with the semantic loss and an additionally sensitive word list works very well. Moreover, our results expose and highlight the vulnerabilities in existing defense mechanisms.</p></p class="citation"></blockquote><h3 id=410--77327-topic-based-watermarks-for-llm-generated-text-alexander-nemecek-et-al-2024>(4/10 | 77/327) Topic-based Watermarks for LLM-Generated Text (Alexander Nemecek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Nemecek, Yuzhou Jiang, Erman Ayday. (2024)<br><strong>Topic-based Watermarks for LLM-Generated Text</strong><br><button class=copy-to-clipboard title="Topic-based Watermarks for LLM-Generated Text" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02138v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02138v1.pdf filename=2404.02138v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within <b>LLM-generated</b> output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an <b>LLM</b> generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a &ldquo;topic-based watermarking algorithm&rdquo; for <b>LLMs.</b> The proposed algorithm determines how to generate tokens for the watermarked <b>LLM</b> output based on extracted topics of an input <b>prompt</b> or the output of a non-watermarked <b>LLM.</b> Inspired from previous work, we propose using a pair of lists (that are generated based on the specified extracted topic(s)) that specify certain tokens to be included or excluded while generating the watermarked output of the <b>LLM.</b> Using the proposed watermarking algorithm, we show the practicality of a watermark detection algorithm. Furthermore, we discuss a wide range of attacks that can emerge against watermarking algorithms for <b>LLMs</b> and the benefit of the proposed watermarking scheme for the feasibility of modeling a potential attacker considering its benefit vs. loss.</p></p class="citation"></blockquote><h3 id=510--78327-digital-forgetting-in-large-language-models-a-survey-of-unlearning-methods-alberto-blanco-justicia-et-al-2024>(5/10 | 78/327) Digital Forgetting in Large Language Models: A Survey of Unlearning Methods (Alberto Blanco-Justicia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alberto Blanco-Justicia, Najeeb Jebreel, Benet Manzanares, David Sánchez, Josep Domingo-Ferrer, Guillem Collell, Kuan Eeik Tan. (2024)<br><strong>Digital Forgetting in Large Language Models: A Survey of Unlearning Methods</strong><br><button class=copy-to-clipboard title="Digital Forgetting in Large Language Models: A Survey of Unlearning Methods" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: 68, K-4-1; I-2-6; I-2-7, cs-AI, cs-CR, cs-LG, cs.CR<br>Keyword Score: 30<br>Keywords: Machine Unlearning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02062v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02062v1.pdf filename=2404.02062v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The objective of digital forgetting is, given a model with undesirable knowledge or behavior, obtain a new model where the detected issues are no longer present. The motivations for forgetting include privacy protection, copyright protection, elimination of biases and discrimination, and prevention of harmful content generation. Effective digital forgetting has to be effective (meaning how well the new model has forgotten the undesired knowledge/behavior), retain the performance of the original model on the desirable tasks, and be scalable (in particular forgetting has to be more efficient than retraining from scratch on just the tasks/data to be retained). This survey focuses on forgetting in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> We first provide background on <b>LLMs,</b> including their components, the types of <b>LLMs,</b> and their usual training pipeline. Second, we describe the motivations, types, and desired properties of digital forgetting. Third, we introduce the approaches to digital forgetting in <b>LLMs,</b> among which unlearning methodologies stand out as the state of the art. Fourth, we provide a detailed taxonomy of <b>machine</b> <b>unlearning</b> methods for <b>LLMs,</b> and we survey and compare current approaches. Fifth, we detail datasets, models and metrics used for the evaluation of forgetting, retaining and runtime. Sixth, we discuss challenges in the area. Finally, we provide some concluding remarks.</p></p class="citation"></blockquote><h3 id=610--79327-aaa-an-adaptive-mechanism-for-locally-differential-private-mean-estimation-fei-wei-et-al-2024>(6/10 | 79/327) AAA: an Adaptive Mechanism for Locally Differential Private Mean Estimation (Fei Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fei Wei, Ergute Bao, Xiaokui Xiao, Yin Yang, Bolin Ding. (2024)<br><strong>AAA: an Adaptive Mechanism for Locally Differential Private Mean Estimation</strong><br><button class=copy-to-clipboard title="AAA: an Adaptive Mechanism for Locally Differential Private Mean Estimation" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Quantization, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01625v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01625v2.pdf filename=2404.01625v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Local <b>differential</b> <b>privacy</b> (LDP) is a strong privacy standard that has been adopted by popular software systems. The main idea is that each individual perturbs their own data locally, and only submits the resulting noisy version to a data aggregator. Although much effort has been devoted to computing various types of aggregates and building machine learning applications under LDP, research on fundamental perturbation mechanisms has not achieved significant improvement in recent years. Towards a more refined result utility, existing works mainly focus on improving the worst-case guarantee. However, this approach does not necessarily promise a better average performance given the fact that the data in practice obey a certain distribution, which is not known beforehand. In this paper, we propose the advanced adaptive additive (AAA) mechanism, which is a distribution-aware approach that addresses the average utility and tackles the classical mean estimation problem. AAA is carried out in a two-step approach: first, as the global data distribution is not available beforehand, the data aggregator selects a random subset of individuals to compute a (noisy) <b>quantized</b> data descriptor; then, the data aggregator collects data from the remaining individuals, which are perturbed in a distribution-aware fashion. The perturbation involved in the latter step is obtained by solving an optimization problem, which is formulated with the data descriptor obtained in the former step and the desired properties of task-determined utilities. We provide rigorous privacy proofs, utility analyses, and extensive experiments comparing AAA with state-of-the-art mechanisms. The evaluation results demonstrate that the AAA mechanism consistently outperforms existing solutions with a clear margin in terms of result utility, on a wide range of privacy constraints and real-world and synthetic datasets.</p></p class="citation"></blockquote><h3 id=710--80327-software-defined-cryptography-a-design-feature-of-cryptographic-agility-jihoon-cho-et-al-2024>(7/10 | 80/327) Software-Defined Cryptography: A Design Feature of Cryptographic Agility (Jihoon Cho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jihoon Cho, Changhoon Lee, Eunkyung Kim, Jieun Lee, Beumjin Cho. (2024)<br><strong>Software-Defined Cryptography: A Design Feature of Cryptographic Agility</strong><br><button class=copy-to-clipboard title="Software-Defined Cryptography: A Design Feature of Cryptographic Agility" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Zero Trust<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01808v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01808v1.pdf filename=2404.01808v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cryptographic agility, or crypto-agility, is a design feature that enables agile updates to new cryptographic algorithms and standards without the need to modify or replace the surrounding infrastructure. This paper examines the prerequisites for crypto-agility and proposes its desired design feature. More specifically, we investigate the design characteristics of widely deployed cybersecurity paradigms, i.e., <b>zero</b> <b>trust,</b> and apply its design feature to crypto-agility, achieving greater visibility and automation in cryptographic management.</p></p class="citation"></blockquote><h3 id=810--81327-haina-storage-a-decentralized-secure-storage-framework-based-on-improved-blockchain-structure-zijian-zhou-et-al-2024>(8/10 | 81/327) Haina Storage: A Decentralized Secure Storage Framework Based on Improved Blockchain Structure (Zijian Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijian Zhou, Caimei Wang, Xiaoheng Deng, Jianhao Lu, Qilue Wen, Chen Zhang, Hong Li. (2024)<br><strong>Haina Storage: A Decentralized Secure Storage Framework Based on Improved Blockchain Structure</strong><br><button class=copy-to-clipboard title="Haina Storage: A Decentralized Secure Storage Framework Based on Improved Blockchain Structure" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-DC, cs.CR<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01606v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01606v1.pdf filename=2404.01606v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although the decentralized storage technology based on the blockchain can effectively realize secure data storage on cloud services. However, there are still some problems in the existing schemes, such as low storage capacity and low efficiency. To address related issues, we propose a novel decentralized storage framework, which mainly includes four aspects: (1) we proposed a Bi-direction Circular Linked Chain Structure (BCLCS), which improves data&rsquo;s storage capacity and applicability in decentralized storage. (2) A Proof of Resources (PoR) decision model is proposed. By introducing the network environment as an essential evaluation parameter of storage right decision, the energy and time consumption of decision-making are reduced, and the <b>fairness</b> of decision-making is improved. (3) A chain structure dynamic locking mechanism (CSDLM) is designed to realize anti-traverse and access control. (4) A Bi-directional data Access Mechanism (BDAM) is proposed, which improves the efficiency of data access and acquisition in decentralized storage mode. The experimental results show that the framework has significantly improved the shortcomings of the current decentralized storage.</p></p class="citation"></blockquote><h3 id=910--82327-what-blocks-my-blockchains-throughput-developing-a-generalizable-approach-for-identifying-bottlenecks-in-permissioned-blockchains-orestis-papageorgiou-et-al-2024>(9/10 | 82/327) What Blocks My Blockchain&rsquo;s Throughput? Developing a Generalizable Approach for Identifying Bottlenecks in Permissioned Blockchains (Orestis Papageorgiou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Orestis Papageorgiou, Lasse Börtzler, Egor Ermolaev, Jyoti Kumari, Johannes Sedlmeir. (2024)<br><strong>What Blocks My Blockchain&rsquo;s Throughput? Developing a Generalizable Approach for Identifying Bottlenecks in Permissioned Blockchains</strong><br><button class=copy-to-clipboard title="What Blocks My Blockchain's Throughput? Developing a Generalizable Approach for Identifying Bottlenecks in Permissioned Blockchains" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-DB, cs.CR<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02930v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02930v1.pdf filename=2404.02930v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Permissioned blockchains have been proposed for a variety of use cases that require decentralization yet address enterprise requirements that permissionless blockchains to date cannot satisfy &ndash; particularly in terms of performance. However, popular permissioned blockchains still exhibit a relatively low maximum throughput in comparison to established centralized systems. Consequently, researchers have conducted several <b>benchmarking</b> studies on different permissioned blockchains to identify their limitations and &ndash; in some cases &ndash; their bottlenecks in an attempt to find avenues for improvement. Yet, these approaches are highly heterogeneous, difficult to compare, and require a high level of expertise in the implementation of the underlying specific blockchain. In this paper, we develop a more unified and graphical approach for identifying bottlenecks in permissioned blockchains based on a systematic review of related work, experiments with the Distributed Ledger Performance Scan (DLPS), and an extension of its graphical evaluation functionalities. We conduct in-depth case studies on Hyperledger Fabric and Quorum, two widely used permissioned blockchains with distinct architectural designs, demonstrating the adaptability of our framework across different blockchains. We provide researchers and practitioners working on evaluating or improving permissioned blockchains with a toolkit, guidelines on what data to document, and insights on how to proceed in the search process for bottlenecks.</p></p class="citation"></blockquote><h3 id=1010--83327-making-privacy-preserving-federated-graph-analytics-with-strong-guarantees-practical-for-certain-queries-kunlong-liu-et-al-2024>(10/10 | 83/327) Making Privacy-preserving Federated Graph Analytics with Strong Guarantees Practical (for Certain Queries) (Kunlong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kunlong Liu, Trinabh Gupta. (2024)<br><strong>Making Privacy-preserving Federated Graph Analytics with Strong Guarantees Practical (for Certain Queries)</strong><br><button class=copy-to-clipboard title="Making Privacy-preserving Federated Graph Analytics with Strong Guarantees Practical (for Certain Queries)" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-SI, cs.CR<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01619v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01619v1.pdf filename=2404.01619v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Privacy-preserving federated <b>graph</b> analytics is an emerging area of research. The goal is to run <b>graph</b> analytics queries over a set of devices that are organized as a <b>graph</b> while keeping the raw data on the devices rather than centralizing it. Further, no entity may learn any new information except for the final query result. For instance, a device may not learn a neighbor&rsquo;s data. The state-of-the-art prior work for this problem provides privacy guarantees for a broad set of queries in a strong threat model where the devices can be malicious. However, it imposes an impractical overhead: each device locally requires over 8.79 hours of cpu time and 5.73 GiBs of network transfers per query. This paper presents Colo, a new, low-cost system for privacy-preserving federated <b>graph</b> analytics that requires minutes of cpu time and a few MiBs in network transfers, for a particular subset of queries. At the heart of Colo is a new secure computation protocol that enables a device to securely and efficiently evaluate a <b>graph</b> query in its local neighborhood while hiding device data, edge data, and topology data. An implementation and evaluation of Colo shows that for running a variety of COVID-19 queries over a population of 1M devices, it requires less than 8.4 minutes of a device&rsquo;s CPU time and 4.93 MiBs in network transfers - improvements of up to three orders of magnitude.</p></p class="citation"></blockquote><h2 id=csro-19>cs.RO (19)</h2><h3 id=119--84327-bridging-language-vision-and-action-multimodal-vaes-in-robotic-manipulation-tasks-gabriela-sejnova-et-al-2024>(1/19 | 84/327) Bridging Language, Vision and Action: Multimodal VAEs in Robotic Manipulation Tasks (Gabriela Sejnova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriela Sejnova, Michal Vavrecka, Karla Stepanova. (2024)<br><strong>Bridging Language, Vision and Action: Multimodal VAEs in Robotic Manipulation Tasks</strong><br><button class=copy-to-clipboard title="Bridging Language, Vision and Action: Multimodal VAEs in Robotic Manipulation Tasks" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 86<br>Keywords: Autoencoder, Fine-tuning, Multi-modal, Multi-modal, Unsupervised Learning, Unsupervised Learning, Variational Autoencoder, Image2text, Vision-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01932v1.pdf filename=2404.01932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we focus on <b>unsupervised</b> <b>vision-language-action</b> mapping in the area of robotic manipulation. Recently, multiple approaches employing pre-trained large language and vision models have been proposed for this task. However, they are computationally demanding and require careful <b>fine-tuning</b> of the produced outputs. A more lightweight alternative would be the implementation of <b>multimodal</b> <b>Variational</b> <b>Autoencoders</b> (VAEs) which can extract the latent features of the data and integrate them into a joint representation, as has been demonstrated mostly on image-image or <b>image-text</b> data for the state-of-the-art models. Here we explore whether and how can <b>multimodal</b> VAEs be employed in <b>unsupervised</b> <b>robotic</b> manipulation tasks in a simulated environment. Based on the obtained results, we propose a model-invariant training alternative that improves the models&rsquo; performance in a simulator by up to 55%. Moreover, we systematically evaluate the challenges raised by the individual tasks such as object or robot position variability, number of distractors or the task length. Our work thus also sheds light on the potential benefits and limitations of using the current <b>multimodal</b> VAEs for <b>unsupervised</b> <b>learning</b> of robotic motion trajectories based on vision and language.</p></p class="citation"></blockquote><h3 id=219--85327-large-language-models-for-orchestrating-bimanual-robots-kun-chu-et-al-2024>(2/19 | 85/327) Large Language Models for Orchestrating Bimanual Robots (Kun Chu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kun Chu, Xufeng Zhao, Cornelius Weber, Mengdi Li, Wenhao Lu, Stefan Wermter. (2024)<br><strong>Large Language Models for Orchestrating Bimanual Robots</strong><br><button class=copy-to-clipboard title="Large Language Models for Orchestrating Bimanual Robots" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: Reasoning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02018v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02018v1.pdf filename=2404.02018v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although there has been rapid progress in endowing robots with the ability to solve complex manipulation tasks, generating control policies for bimanual robots to solve tasks involving two hands is still challenging because of the difficulties in effective temporal and spatial coordination. With emergent abilities in terms of step-by-step <b>reasoning</b> and <b>in-context</b> <b>learning,</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have taken control of a variety of robotic tasks. However, the nature of language communication via a single sequence of discrete symbols makes <b>LLM-based</b> coordination in continuous space a particular challenge for bimanual tasks. To tackle this challenge for the first time by an <b>LLM,</b> we present LAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizing an <b>LLM</b> to analyze task configurations and devise coordination control policies for addressing long-horizon bimanual tasks. In the simulated environment, the LABOR agent is evaluated through several everyday tasks on the NICOL humanoid robot. Reported success rates indicate that overall coordination efficiency is close to optimal performance, while the analysis of failure causes, classified into spatial and temporal coordination and skill selection, shows that these vary over tasks. The project website can be found at <a href=http://labor-agent.github.io>http://labor-agent.github.io</a></p></p class="citation"></blockquote><h3 id=319--86327-active-exploration-in-bayesian-model-based-reinforcement-learning-for-robot-manipulation-carlos-plou-et-al-2024>(3/19 | 86/327) Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation (Carlos Plou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carlos Plou, Ana C. Murillo, Ruben Martinez-Cantin. (2024)<br><strong>Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation</strong><br><button class=copy-to-clipboard title="Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: Active Learning, Reinforcement Learning, Simulation, Simulator, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01867v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01867v1.pdf filename=2404.01867v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficiently tackling multiple tasks within complex environment, such as those found in robot manipulation, remains an ongoing challenge in robotics and an opportunity for data-driven solutions, such as <b>reinforcement</b> <b>learning</b> (RL). Model-based RL, by building a dynamic model of the robot, enables data reuse and <b>transfer</b> <b>learning</b> between tasks with the same robot and similar environment. Furthermore, data gathering in robotics is expensive and we must rely on data efficient approaches such as model-based RL, where policy learning is mostly conducted on cheaper <b>simulations</b> based on the learned model. Therefore, the quality of the model is fundamental for the performance of the posterior tasks. In this work, we focus on improving the quality of the model and maintaining the data efficiency by performing <b>active</b> <b>learning</b> of the dynamic model during a preliminary exploration phase based on maximize information gathering. We employ Bayesian neural network models to represent, in a probabilistic way, both the belief and information encoded in the dynamic model during exploration. With our presented strategies we manage to actively estimate the novelty of each transition, using this as the exploration reward. In this work, we compare several Bayesian inference methods for neural networks, some of which have never been used in a robotics context, and evaluate them in a realistic robot manipulation setup. Our experiments show the advantages of our Bayesian model-based RL approach, with similar quality in the results than relevant alternatives with much lower requirements regarding robot execution steps. Unlike related previous studies that focused the validation solely on toy problems, our research takes a step towards more realistic setups, tackling robotic arm end-tasks.</p></p class="citation"></blockquote><h3 id=419--87327-task-priority-intermediated-hierarchical-distributed-policies-reinforcement-learning-of-adaptive-multi-robot-cooperative-transport-yusei-naito-et-al-2024>(4/19 | 87/327) Task-priority Intermediated Hierarchical Distributed Policies: Reinforcement Learning of Adaptive Multi-robot Cooperative Transport (Yusei Naito et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yusei Naito, Tomohiko Jimbo, Tadashi Odashima, Takamitsu Matsubara. (2024)<br><strong>Task-priority Intermediated Hierarchical Distributed Policies: Reinforcement Learning of Adaptive Multi-robot Cooperative Transport</strong><br><button class=copy-to-clipboard title="Task-priority Intermediated Hierarchical Distributed Policies: Reinforcement Learning of Adaptive Multi-robot Cooperative Transport" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02362v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02362v1.pdf filename=2404.02362v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-robot cooperative transport is crucial in logistics, housekeeping, and disaster response. However, it poses significant challenges in environments where objects of various weights are mixed and the number of robots and objects varies. This paper presents Task-priority Intermediated Hierarchical Distributed Policies (TIHDP), a multi-agent <b>Reinforcement</b> <b>Learning</b> (RL) framework that addresses these challenges through a hierarchical policy structure. TIHDP consists of three layers: task allocation policy (higher layer), dynamic task priority (intermediate layer), and robot control policy (lower layer). Whereas the dynamic task priority layer can manipulate the priority of any object to be transported by receiving global object information and communicating with other robots, the task allocation and robot control policies are restricted by local observations/actions so that they are not affected by changes in the number of objects and robots. Through <b>simulations</b> and real-robot demonstrations, TIHDP shows promising adaptability and performance of the learned multi-robot cooperative transport, even in environments with varying numbers of robots and objects. Video is available at <a href=https://youtu.be/Rmhv5ovj0xM>https://youtu.be/Rmhv5ovj0xM</a></p></p class="citation"></blockquote><h3 id=519--88327-zerocap-zero-shot-multi-robot-context-aware-pattern-formation-via-large-language-models-vishnunandan-l-n-venkatesh-et-al-2024>(5/19 | 88/327) ZeroCAP: Zero-Shot Multi-Robot Context Aware Pattern Formation via Large Language Models (Vishnunandan L. N. Venkatesh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vishnunandan L. N. Venkatesh, Byung-Cheol Min. (2024)<br><strong>ZeroCAP: Zero-Shot Multi-Robot Context Aware Pattern Formation via Large Language Models</strong><br><button class=copy-to-clipboard title="ZeroCAP: Zero-Shot Multi-Robot Context Aware Pattern Formation via Large Language Models" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Zero-shot, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02318v1.pdf filename=2404.02318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Incorporating language comprehension into robotic operations unlocks significant advancements in robotics, but also presents distinct challenges, particularly in executing spatially oriented tasks like pattern formation. This paper introduces ZeroCAP, a novel system that integrates <b>large</b> <b>language</b> <b>models</b> with multi-robot systems for <b>zero-shot</b> context aware pattern formation. Grounded in the principles of language-conditioned robotics, ZeroCAP leverages the interpretative power of language models to translate natural language instructions into actionable robotic configurations. This approach combines the synergy of <b>vision-language</b> models, cutting-edge segmentation techniques and shape descriptors, enabling the realization of complex, context-driven pattern formations in the realm of multi robot coordination. Through extensive experiments, we demonstrate the systems proficiency in executing complex context aware pattern formations across a spectrum of tasks, from surrounding and caging objects to infilling regions. This not only validates the system&rsquo;s capability to interpret and implement intricate context-driven tasks but also underscores its adaptability and effectiveness across varied environments and scenarios. More details about this work are available at: <a href=https://sites.google.com/view/zerocap/home>https://sites.google.com/view/zerocap/home</a></p></p class="citation"></blockquote><h3 id=619--89327-constrained-robotic-navigation-on-preferred-terrains-using-llms-and-speech-instruction-exploiting-the-power-of-adverbs-faraz-lotfi-et-al-2024>(6/19 | 89/327) Constrained Robotic Navigation on Preferred Terrains Using LLMs and Speech Instruction: Exploiting the Power of Adverbs (Faraz Lotfi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Faraz Lotfi, Farnoosh Faraji, Nikhil Kakodkar, Travis Manderson, David Meger, Gregory Dudek. (2024)<br><strong>Constrained Robotic Navigation on Preferred Terrains Using LLMs and Speech Instruction: Exploiting the Power of Adverbs</strong><br><button class=copy-to-clipboard title="Constrained Robotic Navigation on Preferred Terrains Using LLMs and Speech Instruction: Exploiting the Power of Adverbs" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Generative AI, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02294v1.pdf filename=2404.02294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores leveraging <b>large</b> <b>language</b> <b>models</b> for map-free off-road navigation using <b>generative</b> <b>AI,</b> reducing the need for traditional data collection and annotation. We propose a method where a robot receives verbal instructions, converted to text through Whisper, and a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> model extracts landmarks, preferred terrains, and crucial adverbs translated into speed settings for constrained navigation. A language-driven semantic segmentation model generates text-based masks for identifying landmarks and terrain types in images. By translating 2D image points to the vehicle&rsquo;s motion plane using camera parameters, an MPC controller can guides the vehicle towards the desired terrain. This approach enhances adaptation to diverse environments and facilitates the use of high-level instructions for navigating complex and challenging terrains.</p></p class="citation"></blockquote><h3 id=719--90327-continuous-sculpting-persistent-swarm-shape-formation-adaptable-to-local-environmental-changes-andrew-g-curtis-et-al-2024>(7/19 | 90/327) Continuous Sculpting: Persistent Swarm Shape Formation Adaptable to Local Environmental Changes (Andrew G. Curtis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew G. Curtis, Mark Yim, Michael Rubenstein. (2024)<br><strong>Continuous Sculpting: Persistent Swarm Shape Formation Adaptable to Local Environmental Changes</strong><br><button class=copy-to-clipboard title="Continuous Sculpting: Persistent Swarm Shape Formation Adaptable to Local Environmental Changes" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Message-Passing, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02265v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02265v1.pdf filename=2404.02265v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite their growing popularity, swarms of robots remain limited by the operating time of each individual. We present algorithms which allow a human to sculpt a swarm of robots into a shape that persists in space perpetually, independent of onboard energy constraints such as batteries. Robots generate a path through a shape such that robots cycle in and out of the shape. Robots inside the shape react to human initiated changes and adapt the path through the shape accordingly. Robots outside the shape recharge and return to the shape so that the shape can persist indefinitely. The presented algorithms communicate shape changes throughout the swarm using message passing and robot motion. These algorithms enable the swarm to persist through any arbitrary changes to the shape. We describe these algorithms in detail and present their performance in <b>simulation</b> and on a swarm of mobile robots. The result is a swarm behavior more suitable for extended duration, dynamic shape-based tasks in applications such as agriculture and emergency response.</p></p class="citation"></blockquote><h3 id=819--91327-carlos-an-open-modular-and-scalable-simulation-framework-for-the-development-and-testing-of-software-for-c-its-christian-geller-et-al-2024>(8/19 | 91/327) CARLOS: An Open, Modular, and Scalable Simulation Framework for the Development and Testing of Software for C-ITS (Christian Geller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Geller, Benedikt Haas, Amarin Kloeker, Jona Hermens, Bastian Lampe, Lutz Eckstein. (2024)<br><strong>CARLOS: An Open, Modular, and Scalable Simulation Framework for the Development and Testing of Software for C-ITS</strong><br><button class=copy-to-clipboard title="CARLOS: An Open, Modular, and Scalable Simulation Framework for the Development and Testing of Software for C-ITS" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SE, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01836v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01836v2.pdf filename=2404.01836v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Future mobility systems and their components are increasingly defined by their software. The complexity of these cooperative intelligent transport systems (C-ITS) and the everchanging requirements posed at the software require continual software updates. The dynamic nature of the system and the practically innumerable scenarios in which different software components work together necessitate efficient and automated development and testing procedures that use <b>simulations</b> as one core methodology. The availability of such <b>simulation</b> architectures is a common interest among many stakeholders, especially in the field of automated driving. That is why we propose CARLOS - an open, modular, and scalable <b>simulation</b> framework for the development and testing of software in C-ITS that leverages the rich CARLA and ROS ecosystems. We provide core building blocks for this framework and explain how it can be used and extended by the community. Its architecture builds upon modern microservice and DevOps principles such as containerization and continuous integration. In our paper, we motivate the architecture by describing important design principles and showcasing three major use cases - software prototyping, data-driven development, and automated testing. We make CARLOS and example implementations of the three use cases publicly available at github.com/ika-rwth-aachen/carlos</p></p class="citation"></blockquote><h3 id=919--92327-towards-scalable--efficient-interaction-aware-planning-in-autonomous-vehicles-using-knowledge-distillation-piyush-gupta-et-al-2024>(9/19 | 92/327) Towards Scalable & Efficient Interaction-Aware Planning in Autonomous Vehicles using Knowledge Distillation (Piyush Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Piyush Gupta, David Isele, Sangjae Bae. (2024)<br><strong>Towards Scalable & Efficient Interaction-Aware Planning in Autonomous Vehicles using Knowledge Distillation</strong><br><button class=copy-to-clipboard title="Towards Scalable & Efficient Interaction-Aware Planning in Autonomous Vehicles using Knowledge Distillation" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01746v1.pdf filename=2404.01746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-world driving involves intricate interactions among vehicles navigating through dense traffic scenarios. Recent research focuses on enhancing the interaction awareness of autonomous vehicles to leverage these interactions in decision-making. These interaction-aware planners rely on neural-network-based prediction models to capture inter-vehicle interactions, aiming to integrate these predictions with traditional control techniques such as Model Predictive Control. However, this integration of deep learning-based models with traditional control paradigms often results in computationally demanding optimization problems, relying on heuristic methods. This study introduces a principled and efficient method for combining deep learning with constrained optimization, employing <b>knowledge</b> <b>distillation</b> to train smaller and more efficient networks, thereby mitigating complexity. We demonstrate that these refined networks maintain the problem-solving efficacy of larger models while significantly accelerating optimization. Specifically, in the domain of interaction-aware trajectory planning for autonomous vehicles, we illustrate that training a smaller prediction network using <b>knowledge</b> <b>distillation</b> speeds up optimization without sacrificing accuracy.</p></p class="citation"></blockquote><h3 id=1019--93327-interaction-aware-vehicle-motion-planning-with-collision-avoidance-constraints-in-highway-traffic-dongryul-kim-et-al-2024>(10/19 | 93/327) Interaction-Aware Vehicle Motion Planning with Collision Avoidance Constraints in Highway Traffic (Dongryul Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongryul Kim, Hyeonjeong Kim, Kyoungseok Han. (2024)<br><strong>Interaction-Aware Vehicle Motion Planning with Collision Avoidance Constraints in Highway Traffic</strong><br><button class=copy-to-clipboard title="Interaction-Aware Vehicle Motion Planning with Collision Avoidance Constraints in Highway Traffic" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01661v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01661v1.pdf filename=2404.01661v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes collision-free optimal trajectory planning for autonomous vehicles in highway traffic, where vehicles need to deal with the interaction among each other. To address this issue, a novel optimal control framework is suggested, which couples the trajectory of surrounding vehicles with collision avoidance constraints. Additionally, we describe a trajectory optimization technique under state constraints, utilizing a planner based on Pontryagin&rsquo;s Minimum Principle, capable of numerically solving collision avoidance scenarios with surrounding vehicles. <b>Simulation</b> results demonstrate the effectiveness of the proposed approach regarding interaction-based motion planning for different scenarios.</p></p class="citation"></blockquote><h3 id=1119--94327-uncertainty-aware-active-learning-of-nerf-based-object-models-for-robot-manipulators-using-visual-and-re-orientation-actions-saptarshi-dasgupta-et-al-2024>(11/19 | 94/327) Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions (Saptarshi Dasgupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saptarshi Dasgupta, Akshat Gupta, Shreshth Tuli, Rohan Paul. (2024)<br><strong>Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions</strong><br><button class=copy-to-clipboard title="Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Active Learning, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01812v1.pdf filename=2404.01812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Manipulating unseen objects is challenging without a 3D representation, as objects generally have occluded surfaces. This requires physical interaction with objects to build their internal representations. This paper presents an approach that enables a robot to rapidly learn the complete 3D model of a given object for manipulation in unfamiliar orientations. We use an ensemble of partially constructed NeRF models to quantify model uncertainty to determine the next action (a visual or re-orientation action) by optimizing informativeness and feasibility. Further, our approach determines when and how to grasp and re-orient an object given its partial NeRF model and re-estimates the object pose to rectify misalignments introduced during the interaction. Experiments with a simulated Franka Emika Robot Manipulator operating in a tabletop environment with <b>benchmark</b> objects demonstrate an improvement of (i) 14% in visual reconstruction quality (PSNR), (ii) 20% in the geometric/depth reconstruction of the object surface (F-score) and (iii) 71% in the task success rate of manipulating objects a-priori unseen orientations/stable configurations in the scene; over current methods. The project page can be found here: <a href=https://actnerf.github.io>https://actnerf.github.io</a>.</p></p class="citation"></blockquote><h3 id=1219--95327-multi-robot-collaborative-navigation-with-formation-adaptation-zihao-deng-et-al-2024>(12/19 | 95/327) Multi-Robot Collaborative Navigation with Formation Adaptation (Zihao Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihao Deng, Peng Gao, Williard Joshua Jose, Hao Zhang. (2024)<br><strong>Multi-Robot Collaborative Navigation with Formation Adaptation</strong><br><button class=copy-to-clipboard title="Multi-Robot Collaborative Navigation with Formation Adaptation" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Graph, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01618v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01618v1.pdf filename=2404.01618v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-robot collaborative navigation is an essential ability where teamwork and synchronization are keys. In complex and uncertain environments, adaptive formation is vital, as rigid formations prove to be inadequate. The ability of robots to dynamically adjust their formation enables navigation through unpredictable spaces, maintaining cohesion, and effectively responding to environmental challenges. In this paper, we introduce a novel approach that uses bi-level learning framework. Specifically, we use <b>graph</b> learning at a high level for group coordination and <b>reinforcement</b> <b>learning</b> for individual navigation. We innovate by integrating a spring-damper model within the <b>reinforcement</b> <b>learning</b> reward mechanism, addressing the rigidity of traditional formation control methods. During execution, our approach enables a team of robots to successfully navigate challenging environments, maintain a desired formation shape, and dynamically adjust their formation scale based on environmental information. We conduct extensive experiments to evaluate our approach across three distinct formation scenarios in multi-robot navigation: circle, line, and wedge. Experimental results show that our approach achieves promising results and scalability on multi-robot navigation with formation adaptation.</p></p class="citation"></blockquote><h3 id=1319--96327-learning-from-demonstration-framework-for-multi-robot-systems-using-interaction-keypoints-and-soft-actor-critic-methods-vishnunandan-l-n-venkatesh-et-al-2024>(13/19 | 96/327) Learning from Demonstration Framework for Multi-Robot Systems Using Interaction Keypoints and Soft Actor-Critic Methods (Vishnunandan L. N. Venkatesh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vishnunandan L. N. Venkatesh, Byung-Cheol Min. (2024)<br><strong>Learning from Demonstration Framework for Multi-Robot Systems Using Interaction Keypoints and Soft Actor-Critic Methods</strong><br><button class=copy-to-clipboard title="Learning from Demonstration Framework for Multi-Robot Systems Using Interaction Keypoints and Soft Actor-Critic Methods" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02324v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02324v1.pdf filename=2404.02324v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning from Demonstration (LfD) is a promising approach to enable Multi-Robot Systems (MRS) to acquire complex skills and behaviors. However, the intricate interactions and coordination challenges in MRS pose significant hurdles for effective LfD. In this paper, we present a novel LfD framework specifically designed for MRS, which leverages visual demonstrations to capture and learn from robot-robot and robot-object interactions. Our framework introduces the concept of Interaction Keypoints (IKs) to transform the visual demonstrations into a representation that facilitates the inference of various skills necessary for the task. The robots then execute the task using sensorimotor actions and <b>reinforcement</b> <b>learning</b> (RL) policies when required. A key feature of our approach is the ability to handle unseen contact-based skills that emerge during the demonstration. In such cases, RL is employed to learn the skill using a classifier-based reward function, eliminating the need for manual reward engineering and ensuring adaptability to environmental changes. We evaluate our framework across a range of mobile robot tasks, covering both behavior-based and contact-based domains. The results demonstrate the effectiveness of our approach in enabling robots to learn complex multi-robot tasks and behaviors from visual demonstrations.</p></p class="citation"></blockquote><h3 id=1419--97327-federated-multi-agent-mapping-for-planetary-exploration-tiberiu-ioan-szatmari-et-al-2024>(14/19 | 97/327) Federated Multi-Agent Mapping for Planetary Exploration (Tiberiu-Ioan Szatmari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tiberiu-Ioan Szatmari, Abhishek Cauligi. (2024)<br><strong>Federated Multi-Agent Mapping for Planetary Exploration</strong><br><button class=copy-to-clipboard title="Federated Multi-Agent Mapping for Planetary Exploration" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: I-2-11; I-2-9, cs-LG, cs-MA, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02289v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02289v1.pdf filename=2404.02289v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In multi-agent robotic exploration, managing and effectively utilizing the vast, heterogeneous data generated from dynamic environments poses a significant challenge. <b>Federated</b> <b>learning</b> (FL) is a promising approach for distributed mapping, addressing the challenges of decentralized data in collaborative learning. FL enables joint model training across multiple agents without requiring the centralization or sharing of raw data, overcoming bandwidth and storage constraints. Our approach leverages implicit neural mapping, representing maps as continuous functions learned by neural networks, for compact and adaptable representations. We further enhance this approach with meta-initialization on Earth datasets, pre-training the network to quickly learn new map structures. This combination demonstrates strong generalization to diverse domains like Martian terrain and glaciers. We rigorously evaluate this approach, demonstrating its effectiveness for real-world deployment in multi-agent exploration scenarios.</p></p class="citation"></blockquote><h3 id=1519--98327-apex-ambidextrous-dual-arm-robotic-manipulation-using-collision-free-generative-diffusion-models-apan-dastider-et-al-2024>(15/19 | 98/327) APEX: Ambidextrous Dual-Arm Robotic Manipulation Using Collision-Free Generative Diffusion Models (Apan Dastider et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Apan Dastider, Hao Fang, Mingjie Lin. (2024)<br><strong>APEX: Ambidextrous Dual-Arm Robotic Manipulation Using Collision-Free Generative Diffusion Models</strong><br><button class=copy-to-clipboard title="APEX: Ambidextrous Dual-Arm Robotic Manipulation Using Collision-Free Generative Diffusion Models" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02284v1.pdf filename=2404.02284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dexterous manipulation, particularly adept coordinating and grasping, constitutes a fundamental and indispensable capability for robots, facilitating the emulation of human-like behaviors. Integrating this capability into robots empowers them to supplement and even supplant humans in undertaking increasingly intricate tasks in both daily life and industrial settings. Unfortunately, contemporary methodologies encounter serious challenges in devising manipulation trajectories owing to the intricacies of tasks, the expansive robotic manipulation space, and dynamic obstacles. We propose a novel approach, APEX, to address all these difficulties by introducing a collision-free latent <b>diffusion</b> <b>model</b> for both robotic motion planning and manipulation. Firstly, we simplify the complexity of real-life ambidextrous dual-arm robotic manipulation tasks by abstracting them as aligning two vectors. Secondly, we devise latent <b>diffusion</b> <b>models</b> to produce a variety of robotic manipulation trajectories. Furthermore, we integrate obstacle information utilizing a classifier-guidance technique, thereby guaranteeing both the feasibility and safety of the generated manipulation trajectories. Lastly, we validate our proposed algorithm through extensive experiments conducted on the hardware platform of ambidextrous dual-arm robots. Our algorithm consistently generates successful and seamless trajectories across diverse tasks, surpassing conventional robotic motion planning algorithms. These results carry significant implications for the future design of <b>diffusion</b> <b>robots,</b> enhancing their capability to tackle more intricate robotic manipulation tasks with increased efficiency and safety. Complete video demonstrations of our experiments can be found in <a href=https://sites.google.com/view/apex-dual-arm/home>https://sites.google.com/view/apex-dual-arm/home</a>.</p></p class="citation"></blockquote><h3 id=1619--99327-predicting-the-intention-to-interact-with-a-service-robotthe-role-of-gaze-cues-simone-arreghini-et-al-2024>(16/19 | 99/327) Predicting the Intention to Interact with a Service Robot:the Role of Gaze Cues (Simone Arreghini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simone Arreghini, Gabriele Abbate, Alessandro Giusti, Antonio Paolillo. (2024)<br><strong>Predicting the Intention to Interact with a Service Robot:the Role of Gaze Cues</strong><br><button class=copy-to-clipboard title="Predicting the Intention to Interact with a Service Robot:the Role of Gaze Cues" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01986v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01986v1.pdf filename=2404.01986v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For a service robot, it is crucial to perceive as early as possible that an approaching person intends to interact: in this case, it can proactively enact friendly behaviors that lead to an improved user experience. We solve this perception task with a sequence-to-sequence classifier of a potential user intention to interact, which can be trained in a <b>self-supervised</b> way. Our main contribution is a study of the benefit of features representing the person&rsquo;s gaze in this context. Extensive experiments on a novel dataset show that the inclusion of gaze cues significantly improves the classifier performance (AUROC increases from 84.5% to 91.2%); the distance at which an accurate classification can be achieved improves from 2.4 m to 3.2 m. We also quantify the system&rsquo;s ability to adapt to new environments without external supervision. Qualitative experiments show practical applications with a waiter robot.</p></p class="citation"></blockquote><h3 id=1719--100327-a-tutorial-on-gaussian-process-learning-based-model-predictive-control-jie-wang-et-al-2024>(17/19 | 100/327) A Tutorial on Gaussian Process Learning-based Model Predictive Control (Jie Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Wang, Youmin Zhang. (2024)<br><strong>A Tutorial on Gaussian Process Learning-based Model Predictive Control</strong><br><button class=copy-to-clipboard title="A Tutorial on Gaussian Process Learning-based Model Predictive Control" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03689v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03689v1.pdf filename=2404.03689v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This tutorial provides a systematic introduction to <b>Gaussian</b> <b>process</b> learning-based model predictive control (GP-MPC), an advanced approach integrating <b>Gaussian</b> <b>process</b> (GP) with model predictive control (MPC) for enhanced control in complex systems. It begins with GP regression fundamentals, illustrating how it enriches MPC with enhanced predictive accuracy and robust handling of uncertainties. A central contribution of this tutorial is the first detailed, systematic mathematical formulation of GP-MPC in literature, focusing on deriving the approximation of means and variances propagation for GP multi-step predictions. Practical applications in robotics control, such as path-following for mobile robots in challenging terrains and mixed-vehicle platooning, are discussed to demonstrate the real-world effectiveness and adaptability of GP-MPC. This tutorial aims to make GP-MPC accessible to researchers and practitioners, enriching the learning-based control field with in-depth theoretical and practical insights and fostering further innovations in complex system control.</p></p class="citation"></blockquote><h3 id=1819--101327-prism-topomap-online-topological-mapping-with-place-recognition-and-scan-matching-kirill-muravyev-et-al-2024>(18/19 | 101/327) PRISM-TopoMap: Online Topological Mapping with Place Recognition and Scan Matching (Kirill Muravyev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kirill Muravyev, Alexander Melekhin, Dmitriy Yudin, Konstantin Yakovlev. (2024)<br><strong>PRISM-TopoMap: Online Topological Mapping with Place Recognition and Scan Matching</strong><br><button class=copy-to-clipboard title="PRISM-TopoMap: Online Topological Mapping with Place Recognition and Scan Matching" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: I-2-9; I-2-10, cs-CV, cs-RO, cs.RO<br>Keyword Score: 9<br>Keywords: Graph, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01674v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01674v1.pdf filename=2404.01674v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mapping is one of the crucial tasks enabling autonomous navigation of a mobile robot. Conventional mapping methods output dense geometric map representation, e.g. an occupancy grid, which is not trivial to keep consistent for the prolonged runs covering large environments. Meanwhile, capturing the topological structure of the workspace enables fast path planning, is less prone to odometry error accumulation and does not consume much memory. Following this idea, this paper introduces PRISM-TopoMap &ndash; a topological mapping method that maintains a <b>graph</b> of locally aligned locations not relying on global metric coordinates. The proposed method involves learnable <b>multimodal</b> place recognition paired with the scan matching pipeline for localization and loop closure in the <b>graph</b> of locations. The latter is updated online and the robot is localized in a proper node at each time step. We conduct a broad experimental evaluation of the suggested approach in a range of photo-realistic environments and on a real robot (wheeled differential driven Husky robot), and compare it to state of the art. The results of the empirical evaluation confirm that PRISM-Topomap consistently outperforms competitors across several measures of mapping and navigation efficiency and performs well on a real robot. The code of PRISM-Topomap is open-sourced and available at <a href=https://github.com/kirillMouraviev/prism-topomap>https://github.com/kirillMouraviev/prism-topomap</a>.</p></p class="citation"></blockquote><h3 id=1919--102327-generalizing-6-dof-grasp-detection-via-domain-prior-knowledge-haoxiang-ma-et-al-2024>(19/19 | 102/327) Generalizing 6-DoF Grasp Detection via Domain Prior Knowledge (Haoxiang Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoxiang Ma, Modi Shi, Boyang Gao, Di Huang. (2024)<br><strong>Generalizing 6-DoF Grasp Detection via Domain Prior Knowledge</strong><br><button class=copy-to-clipboard title="Generalizing 6-DoF Grasp Detection via Domain Prior Knowledge" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01727v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01727v1.pdf filename=2404.01727v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We focus on the generalization ability of the 6-DoF grasp detection method in this paper. While learning-based grasp detection methods can predict grasp poses for unseen objects using the grasp distribution learned from the training set, they often exhibit a significant performance drop when encountering objects with diverse shapes and structures. To enhance the grasp detection methods&rsquo; generalization ability, we incorporate domain prior knowledge of robotic grasping, enabling better adaptation to objects with significant shape and structure differences. More specifically, we employ the physical constraint regularization during the training phase to guide the model towards predicting grasps that comply with the physical rule on grasping. For the unstable grasp poses predicted on novel objects, we design a contact-score joint optimization using the projection contact map to refine these poses in cluttered scenarios. Extensive experiments conducted on the GraspNet-1billion <b>benchmark</b> demonstrate a substantial performance gain on the novel object set and the real-world grasping experiments also demonstrate the effectiveness of our generalizing 6-DoF grasp detection method.</p></p class="citation"></blockquote><h2 id=cscv-82>cs.CV (82)</h2><h3 id=182--103327-samba-semantic-segmentation-of-remotely-sensed-images-with-state-space-model-qinfeng-zhu-et-al-2024>(1/82 | 103/327) Samba: Semantic Segmentation of Remotely Sensed Images with State Space Model (Qinfeng Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qinfeng Zhu, Yuanzhi Cai, Yuan Fang, Yihan Yang, Cheng Chen, Lei Fan, Anh Nguyen. (2024)<br><strong>Samba: Semantic Segmentation of Remotely Sensed Images with State Space Model</strong><br><button class=copy-to-clipboard title="Samba: Semantic Segmentation of Remotely Sensed Images with State Space Model" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 73<br>Keywords: Vision Transformer, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Information Retrieval, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01705v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01705v1.pdf filename=2404.01705v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High-resolution remotely sensed images poses a challenge for commonly used semantic segmentation methods such as <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> and <b>Vision</b> <b>Transformer</b> (ViT). <b>CNN-based</b> methods struggle with handling such high-resolution images due to their limited receptive field, while ViT faces challenges to handle long sequences. Inspired by Mamba, which adopts a State Space Model (SSM) to efficiently capture global semantic <b>information,</b> <b>we</b> propose a semantic segmentation framework for high-resolution remotely sensed images, named Samba. Samba utilizes an encoder-decoder architecture, with Samba blocks serving as the encoder for efficient multi-level semantic <b>information</b> <b>extraction,</b> and UperNet functioning as the decoder. We evaluate Samba on the LoveDA dataset, comparing its performance against top-performing <b>CNN</b> and ViT methods. The results reveal that Samba achieved unparalleled performance on LoveDA. This represents that the proposed Samba is an effective application of the SSM in semantic segmentation of remotely sensed images, setting a new <b>benchmark</b> in performance for Mamba-based techniques in this specific application. The source code and baseline implementations are available at <a href=https://github.com/zhuqinfeng1999/Samba>https://github.com/zhuqinfeng1999/Samba</a>.</p></p class="citation"></blockquote><h3 id=282--104327-pre-trained-vision-and-language-transformers-are-few-shot-incremental-learners-keon-hee-park-et-al-2024>(2/82 | 104/327) Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners (Keon-Hee Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keon-Hee Park, Kyungwoo Song, Gyeong-Moon Park. (2024)<br><strong>Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners</strong><br><button class=copy-to-clipboard title="Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Few-shot, Knowledge Distillation, Knowledge Distillation, Knowledge Transfer, Transformer, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02117v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02117v1.pdf filename=2404.02117v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-Shot</b> Class Incremental Learning (FSCIL) is a task that requires a model to learn new classes incrementally without forgetting when only a few samples for each class are given. FSCIL encounters two significant challenges: catastrophic forgetting and overfitting, and these challenges have driven prior studies to primarily rely on shallow models, such as ResNet-18. Even though their limited capacity can mitigate both forgetting and overfitting issues, it leads to inadequate <b>knowledge</b> <b>transfer</b> during <b>few-shot</b> incremental sessions. In this paper, we argue that large models such as vision and language <b>transformers</b> pre-trained on large datasets can be excellent <b>few-shot</b> incremental learners. To this end, we propose a novel FSCIL framework called PriViLege, Pre-trained Vision and Language <b>transformers</b> with <b>prompting</b> functions and <b>knowledge</b> <b>distillation.</b> Our framework effectively addresses the challenges of catastrophic forgetting and overfitting in large models through new pre-trained <b>knowledge</b> <b>tuning</b> (PKT) and two losses: entropy-based divergence loss and semantic <b>knowledge</b> <b>distillation</b> loss. Experimental results show that the proposed PriViLege significantly outperforms the existing state-of-the-art methods with a large margin, e.g., +9.38% in CUB200, +20.58% in CIFAR-100, and +13.36% in miniImageNet. Our implementation code is available at <a href=https://github.com/KHU-AGI/PriViLege>https://github.com/KHU-AGI/PriViLege</a>.</p></p class="citation"></blockquote><h3 id=382--105327-minimize-quantization-output-error-with-bias-compensation-cheng-gong-et-al-2024>(3/82 | 105/327) Minimize Quantization Output Error with Bias Compensation (Cheng Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Gong, Haoshuai Zheng, Mengting Hu, Zheng Lin, Deng-Ping Fan, Yuzhi Zhang, Tao Li. (2024)<br><strong>Minimize Quantization Output Error with Bias Compensation</strong><br><button class=copy-to-clipboard title="Minimize Quantization Output Error with Bias Compensation" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Vision Transformer, Fine-tuning, Quantization, Transformer, Large Language Model, Perplexity, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01892v1.pdf filename=2404.01892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Quantization</b> is a promising method that reduces memory usage and computational intensity of Deep Neural Networks (DNNs), but it often leads to significant output error that hinder model deployment. In this paper, we propose Bias Compensation (BC) to minimize the output error, thus realizing ultra-low-precision <b>quantization</b> without model <b>fine-tuning.</b> Instead of optimizing the non-convex <b>quantization</b> process as in most previous methods, the proposed BC bypasses the step to directly minimize the quantizing output error by identifying a bias vector for compensation. We have established that the minimization of output error through BC is a convex problem and provides an efficient strategy to procure optimal solutions associated with minimal output error,without the need for training or <b>fine-tuning.</b> We conduct extensive experiments on <b>Vision</b> <b>Transformer</b> models and <b>Large</b> <b>Language</b> <b>Models,</b> and the results show that our method notably reduces <b>quantization</b> output error, thereby permitting ultra-low-precision post-training <b>quantization</b> and enhancing the task performance of models. Especially, BC improves the accuracy of ViT-B with 4-bit PTQ4ViT by 36.89% on the ImageNet-1k task, and decreases the <b>perplexity</b> of OPT-350M with 3-bit GPTQ by 5.97 on WikiText2.The code is in <a href=https://github.com/GongCheng1919/bias-compensation>https://github.com/GongCheng1919/bias-compensation</a>.</p></p class="citation"></blockquote><h3 id=482--106327-leveraging-digital-perceptual-technologies-for-remote-perception-and-analysis-of-human-biomechanical-processes-a-contactless-approach-for-workload-and-joint-force-assessment-jesudara-omidokun-et-al-2024>(4/82 | 106/327) Leveraging Digital Perceptual Technologies for Remote Perception and Analysis of Human Biomechanical Processes: A Contactless Approach for Workload and Joint Force Assessment (Jesudara Omidokun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jesudara Omidokun, Darlington Egeonu, Bochen Jia, Liang Yang. (2024)<br><strong>Leveraging Digital Perceptual Technologies for Remote Perception and Analysis of Human Biomechanical Processes: A Contactless Approach for Workload and Joint Force Assessment</strong><br><button class=copy-to-clipboard title="Leveraging Digital Perceptual Technologies for Remote Perception and Analysis of Human Biomechanical Processes: A Contactless Approach for Workload and Joint Force Assessment" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-HC, cs.CV<br>Keyword Score: 70<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, LSTM, LSTM, LSTM, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01576v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01576v1.pdf filename=2404.01576v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents an innovative computer vision framework designed to analyze human movements in industrial settings, aiming to enhance biomechanical analysis by integrating seamlessly with existing software. Through a combination of advanced imaging and modeling techniques, the framework allows for comprehensive scrutiny of human motion, providing valuable insights into kinematic patterns and kinetic data. Utilizing <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs),</b> Direct Linear Transform (DLT), and <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> networks, the methodology accurately detects key body points, reconstructs 3D landmarks, and generates detailed 3D body meshes. Extensive evaluations across various movements validate the framework&rsquo;s effectiveness, demonstrating comparable results to traditional marker-based models with minor differences in joint angle estimations and precise estimations of weight and height. Statistical analyses consistently support the framework&rsquo;s reliability, with joint angle estimations showing less than a 5-degree difference for hip flexion, elbow flexion, and knee angle methods. Additionally, weight estimation exhibits an average error of less than 6 % for weight and less than 2 % for height when compared to ground-truth values from 10 subjects. The integration of the Biomech-57 landmark skeleton template further enhances the robustness and reinforces the framework&rsquo;s credibility. This framework shows significant promise for meticulous biomechanical analysis in industrial contexts, eliminating the need for cumbersome markers and extending its utility to diverse research domains, including the study of specific exoskeleton devices&rsquo; impact on facilitating the <b>prompt</b> return of injured workers to their tasks.</p></p class="citation"></blockquote><h3 id=582--107327-vitamin-designing-scalable-vision-models-in-the-vision-language-era-jieneng-chen-et-al-2024>(5/82 | 107/327) ViTamin: Designing Scalable Vision Models in the Vision-Language Era (Jieneng Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, Liang-Chieh Chen. (2024)<br><strong>ViTamin: Designing Scalable Vision Models in the Vision-Language Era</strong><br><button class=copy-to-clipboard title="ViTamin: Designing Scalable Vision Models in the Vision-Language Era" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 66<br>Keywords: Vision Transformer, Benchmarking, Multi-modal, Zero-shot, Transformer, Image2text, Vision Transformer, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02132v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02132v2.pdf filename=2404.02132v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent breakthroughs in <b>vision-language</b> <b>models</b> (VLMs) start a new page in the <b>vision</b> <b>community.</b> The VLMs provide stronger and more generalizable feature embeddings compared to those from ImageNet-pretrained models, thanks to the training on the large-scale Internet <b>image-text</b> pairs. However, despite the amazing achievement from the VLMs, vanilla <b>Vision</b> <b>Transformers</b> (ViTs) remain the default choice for the image encoder. Although pure <b>transformer</b> proves its effectiveness in the text encoding area, it remains questionable whether it is also the case for image encoding, especially considering that various types of networks are proposed on the ImageNet <b>benchmark,</b> which, unfortunately, are rarely studied in VLMs. Due to small data/model scale, the original conclusions of model design on ImageNet can be limited and biased. In this paper, we aim at building an evaluation protocol of <b>vision</b> <b>models</b> in the <b>vision-language</b> <b>era</b> under the contrastive language-image pretraining (CLIP) framework. We provide a comprehensive way to <b>benchmark</b> different <b>vision</b> <b>models,</b> covering their <b>zero-shot</b> performance and scalability in both model and training data sizes. To this end, we introduce ViTamin, a new <b>vision</b> <b>models</b> tailored for VLMs. ViTamin-L significantly outperforms ViT-L by 2.0% ImageNet <b>zero-shot</b> accuracy, when using the same publicly available DataComp-1B dataset and the same OpenCLIP training scheme. ViTamin-L presents promising results on 60 diverse <b>benchmarks,</b> including classification, retrieval, open-vocabulary detection and segmentation, and large <b>multi-modal</b> models. When further scaling up the model size, our ViTamin-XL with only 436M parameters attains 82.9% ImageNet <b>zero-shot</b> accuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters (4.4B).</p></p class="citation"></blockquote><h3 id=682--108327-bi-lora-a-vision-language-approach-for-synthetic-image-detection-mamadou-keita-et-al-2024>(6/82 | 108/327) Bi-LORA: A Vision-Language Approach for Synthetic Image Detection (Mamadou Keita et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdenour Hadid, Abdelmalik Taleb-Ahmed. (2024)<br><strong>Bi-LORA: A Vision-Language Approach for Synthetic Image Detection</strong><br><button class=copy-to-clipboard title="Bi-LORA: A Vision-Language Approach for Synthetic Image Detection" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CR, cs-CV, cs-LG, cs.CV<br>Keyword Score: 60<br>Keywords: Diffusion Model, Generative Adversarial Network, Generative Adversarial Network, Zero-shot, Vision-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01959v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01959v2.pdf filename=2404.01959v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advancements in deep image synthesis techniques, such as <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs)</b> and <b>diffusion</b> <b>models</b> (DMs), have ushered in an era of generating highly realistic images. While this technological progress has captured significant interest, it has also raised concerns about the potential difficulty in distinguishing real images from their synthetic counterparts. This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the <b>zero-shot</b> nature of <b>vision-language</b> models (VLMs). We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images. The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM, notably bootstrapping language image pre-training (BLIP2). Rigorous and comprehensive experiments are conducted to validate the effectiveness of our proposed approach, particularly in detecting unseen <b>diffusion-generated</b> <b>images</b> from unknown <b>diffusion-based</b> <b>generative</b> <b>models</b> <b>during</b> training, showcasing robustness to noise, and demonstrating generalization capabilities to <b>GANs.</b> The obtained results showcase an impressive average accuracy of 93.41% in synthetic image detection on unseen generation models. The code and models associated with this research can be publicly accessed at <a href=https://github.com/Mamadou-Keita/VLM-DETECT>https://github.com/Mamadou-Keita/VLM-DETECT</a>.</p></p class="citation"></blockquote><h3 id=782--109327-wcdt-world-centric-diffusion-transformer-for-traffic-scene-generation-chen-yang-et-al-2024>(7/82 | 109/327) WcDT: World-centric Diffusion Transformer for Traffic Scene Generation (Chen Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Yang, Aaron Xuxiang Tian, Dong Chen, Tianyu Shi, Arsalan Heydarian. (2024)<br><strong>WcDT: World-centric Diffusion Transformer for Traffic Scene Generation</strong><br><button class=copy-to-clipboard title="WcDT: World-centric Diffusion Transformer for Traffic Scene Generation" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Diffusion Model, Multi-modal, Multi-modal, Probabilistic Model, Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02082v1.pdf filename=2404.02082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a novel approach for autonomous driving trajectory generation by harnessing the complementary strengths of <b>diffusion</b> <b>probabilistic</b> <b>models</b> (a.k.a., <b>diffusion</b> <b>models)</b> and <b>transformers.</b> Our proposed framework, termed the &ldquo;World-Centric <b>Diffusion</b> <b>Transformer&rdquo;</b> (WcDT), optimizes the entire trajectory generation process, from feature extraction to model inference. To enhance the scene diversity and stochasticity, the historical trajectory data is first preprocessed and encoded into latent space using Denoising <b>Diffusion</b> <b>Probabilistic</b> <b>Models</b> (DDPM) enhanced with <b>Diffusion</b> <b>with</b> <b>Transformer</b> (DiT) blocks. Then, the latent features, historical trajectories, HD map features, and historical traffic signal information are fused with various <b>transformer-based</b> encoders. The encoded traffic scenes are then decoded by a trajectory decoder to generate <b>multimodal</b> future trajectories. Comprehensive experimental results show that the proposed approach exhibits superior performance in generating both realistic and diverse trajectories, showing its potential for integration into automatic driving <b>simulation</b> systems.</p></p class="citation"></blockquote><h3 id=882--110327-red-teaming-segment-anything-model-krzysztof-jankowski-et-al-2024>(8/82 | 110/327) Red-Teaming Segment Anything Model (Krzysztof Jankowski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Krzysztof Jankowski, Bartlomiej Sobieski, Mateusz Kwiatkowski, Jakub Szulc, Michal Janik, Hubert Baniecki, Przemyslaw Biecek. (2024)<br><strong>Red-Teaming Segment Anything Model</strong><br><button class=copy-to-clipboard title="Red-Teaming Segment Anything Model" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 55<br>Keywords: Black Box, Fine-tuning, Foundation Model, Style Transfer, Prompt, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02067v1.pdf filename=2404.02067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> have emerged as pivotal tools, tackling many complex tasks through pre-training on vast datasets and subsequent <b>fine-tuning</b> for specific applications. The Segment Anything Model is one of the first and most well-known <b>foundation</b> <b>models</b> for computer vision segmentation tasks. This work presents a multi-faceted red-teaming analysis that tests the Segment Anything Model against challenging tasks: (1) We analyze the impact of <b>style</b> <b>transfer</b> on segmentation masks, demonstrating that applying adverse weather conditions and raindrops to dashboard images of city roads significantly distorts generated masks. (2) We focus on assessing whether the model can be used for attacks on privacy, such as recognizing celebrities&rsquo; faces, and show that the model possesses some undesired knowledge in this task. (3) Finally, we check how robust the model is to <b>adversarial</b> <b>attacks</b> on segmentation masks under text <b>prompts.</b> We not only show the effectiveness of popular white-box attacks and resistance to <b>black-box</b> <b>attacks</b> but also introduce a novel approach - Focused Iterative Gradient Attack (FIGA) that combines white-box approaches to construct an efficient attack resulting in a smaller number of modified pixels. All of our testing methods and analyses indicate a need for enhanced safety measures in <b>foundation</b> <b>models</b> for image segmentation.</p></p class="citation"></blockquote><h3 id=982--111327-semantic-augmentation-in-images-using-language-sahiti-yerramilli-et-al-2024>(9/82 | 111/327) Semantic Augmentation in Images using Language (Sahiti Yerramilli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sahiti Yerramilli, Jayant Sravan Tamarapalli, Tanmay Girish Kulkarni, Jonathan Francis, Eric Nyberg. (2024)<br><strong>Semantic Augmentation in Images using Language</strong><br><button class=copy-to-clipboard title="Semantic Augmentation in Images using Language" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Data Augmentation, Out-of-domain, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02353v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02353v1.pdf filename=2404.02353v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Learning models are incredibly <b>data-hungry</b> <b>and</b> require very large labeled datasets for <b>supervised</b> <b>learning.</b> As a consequence, these models often suffer from overfitting, limiting their ability to generalize to real-world examples. Recent advancements in <b>diffusion</b> <b>models</b> have enabled the generation of photorealistic images based on textual inputs. Leveraging the substantial datasets used to train these <b>diffusion</b> <b>models,</b> we propose a technique to utilize generated images to augment existing datasets. This paper explores various strategies for effective <b>data</b> <b>augmentation</b> to improve the <b>out-of-domain</b> generalization capabilities of deep learning models.</p></p class="citation"></blockquote><h3 id=1082--112327-rave-residual-vector-embedding-for-clip-guided-backlit-image-enhancement-tatiana-gaintseva-et-al-2024>(10/82 | 112/327) RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image Enhancement (Tatiana Gaintseva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tatiana Gaintseva, Martin Benning, Gregory Slabaugh. (2024)<br><strong>RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image Enhancement</strong><br><button class=copy-to-clipboard title="RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image Enhancement" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Supervised Learning, Unsupervised Learning, Text2image, Prompt, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01889v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01889v2.pdf filename=2404.01889v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we propose a novel modification of Contrastive Language-Image Pre-Training (CLIP) guidance for the task of <b>unsupervised</b> backlit image enhancement. Our work builds on the state-of-the-art CLIP-LIT approach, which learns a <b>prompt</b> pair by constraining the <b>text-image</b> <b>similarity</b> between a <b>prompt</b> (negative/positive sample) and a corresponding image (backlit image/well-lit image) in the CLIP embedding space. Learned <b>prompts</b> then guide an image enhancement network. Based on the CLIP-LIT framework, we propose two novel methods for CLIP guidance. First, we show that instead of tuning <b>prompts</b> in the space of <b>text</b> <b>embeddings,</b> it is possible to directly tune their embeddings in the latent space without any loss in quality. This accelerates training and potentially enables the use of additional encoders that do not have a <b>text</b> <b>encoder.</b> Second, we propose a novel approach that does not require any <b>prompt</b> tuning. Instead, based on CLIP embeddings of backlit and well-lit images from training data, we compute the residual vector in the embedding space as a simple difference between the mean embeddings of the well-lit and backlit images. This vector then guides the enhancement network during training, pushing a backlit image towards the space of well-lit images. This approach further dramatically reduces training time, stabilizes training and produces high quality enhanced images without artifacts, both in <b>supervised</b> and <b>unsupervised</b> training regimes. Additionally, we show that residual vectors can be interpreted, revealing biases in training data, and thereby enabling potential bias correction.</p></p class="citation"></blockquote><h3 id=1182--113327-leveraging-yolo-world-and-gpt-4v-lmms-for-zero-shot-person-detection-and-action-recognition-in-drone-imagery-christian-limberg-et-al-2024>(11/82 | 113/327) Leveraging YOLO-World and GPT-4V LMMs for Zero-Shot Person Detection and Action Recognition in Drone Imagery (Christian Limberg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Limberg, Artur Gonçalves, Bastien Rigault, Helmut Prendinger. (2024)<br><strong>Leveraging YOLO-World and GPT-4V LMMs for Zero-Shot Person Detection and Action Recognition in Drone Imagery</strong><br><button class=copy-to-clipboard title="Leveraging YOLO-World and GPT-4V LMMs for Zero-Shot Person Detection and Action Recognition in Drone Imagery" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 46<br>Keywords: Yolo, Multi-modal, Multi-modal, Zero-shot, GPT, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01571v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01571v1.pdf filename=2404.01571v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this article, we explore the potential of <b>zero-shot</b> Large <b>Multimodal</b> Models (LMMs) in the domain of drone perception. We focus on person detection and action recognition tasks and evaluate two prominent LMMs, namely <b>YOLO-World</b> and <b>GPT-4V(ision)</b> using a publicly available dataset captured from aerial views. Traditional deep learning approaches rely heavily on large and high-quality training datasets. However, in certain robotic settings, acquiring such datasets can be resource-intensive or impractical within a reasonable timeframe. The flexibility of <b>prompt-based</b> Large <b>Multimodal</b> Models (LMMs) and their exceptional generalization capabilities have the potential to revolutionize robotics applications in these scenarios. Our findings suggest that <b>YOLO-World</b> demonstrates good detection performance. <b>GPT-4V</b> struggles with accurately classifying action classes but delivers promising results in filtering out unwanted region proposals and in providing a general description of the scenery. This research represents an initial step in leveraging LMMs for drone perception and establishes a foundation for future investigations in this area.</p></p class="citation"></blockquote><h3 id=1282--114327-lp-a-surprisingly-strong-linear-probe-for-few-shot-clip-yunshi-huang-et-al-2024>(12/82 | 114/327) LP++: A Surprisingly Strong Linear Probe for Few-Shot CLIP (Yunshi Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunshi Huang, Fereshteh Shakeri, Jose Dolz, Malik Boudiaf, Houda Bahig, Ismail Ben Ayed. (2024)<br><strong>LP++: A Surprisingly Strong Linear Probe for Few-Shot CLIP</strong><br><button class=copy-to-clipboard title="LP++: A Surprisingly Strong Linear Probe for Few-Shot CLIP" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 45<br>Keywords: Black Box, Few-shot, Prompt, Prompt Learning, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02285v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02285v1.pdf filename=2404.02285v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In a recent, strongly emergent literature on <b>few-shot</b> CLIP adaptation, Linear Probe (LP) has been often reported as a weak baseline. This has motivated intensive research building convoluted <b>prompt</b> <b>learning</b> or feature adaptation strategies. In this work, we propose and examine from convex-optimization perspectives a generalization of the standard LP baseline, in which the linear classifier weights are learnable functions of the <b>text</b> <b>embedding,</b> with class-wise multipliers blending image and <b>text</b> <b>knowledge.</b> As our objective function depends on two types of variables, i.e., the class visual prototypes and the learnable blending parameters, we propose a computationally efficient block coordinate Majorize-Minimize (MM) descent algorithm. In our full-batch MM optimizer, which we coin LP++, step sizes are implicit, unlike standard gradient descent practices where learning rates are intensively searched over validation sets. By examining the mathematical properties of our loss (e.g., Lipschitz gradient continuity), we build majorizing functions yielding data-driven learning rates and derive approximations of the loss&rsquo;s minima, which provide data-informed initialization of the variables. Our image-language objective function, along with these non-trivial optimization insights and ingredients, yields, surprisingly, highly competitive <b>few-shot</b> CLIP performances. Furthermore, LP++ operates in <b>black-box,</b> <b>relaxes</b> intensive validation searches for the optimization hyper-parameters, and runs orders-of-magnitudes faster than state-of-the-art <b>few-shot</b> CLIP adaptation methods. Our code is available at: \url{https://github.com/FereshteShakeri/FewShot-CLIP-Strong-Baseline.git}.</p></p class="citation"></blockquote><h3 id=1382--115327-contrastcad-contrastive-learning-based-representation-learning-for-computer-aided-design-models-minseop-jung-et-al-2024>(13/82 | 115/327) ContrastCAD: Contrastive Learning-based Representation Learning for Computer-Aided Design Models (Minseop Jung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minseop Jung, Minseong Kim, Jibum Kim. (2024)<br><strong>ContrastCAD: Contrastive Learning-based Representation Learning for Computer-Aided Design Models</strong><br><button class=copy-to-clipboard title="ContrastCAD: Contrastive Learning-based Representation Learning for Computer-Aided Design Models" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 45<br>Keywords: Autoencoder, Contrastive Learning, Data Augmentation, Representation Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01645v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01645v1.pdf filename=2404.01645v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The success of <b>Transformer-based</b> models has encouraged many researchers to learn CAD models using sequence-based approaches. However, learning CAD models is still a challenge, because they can be represented as complex shapes with long construction sequences. Furthermore, the same CAD model can be expressed using different CAD construction sequences. We propose a novel <b>contrastive</b> <b>learning-based</b> approach, named ContrastCAD, that effectively captures semantic information within the construction sequences of the CAD model. ContrastCAD generates augmented views using dropout techniques without altering the shape of the CAD model. We also propose a new CAD <b>data</b> <b>augmentation</b> method, called a Random Replace and Extrude (RRE) method, to enhance the learning performance of the model when training an imbalanced training CAD dataset. Experimental results show that the proposed RRE augmentation method significantly enhances the learning performance of <b>Transformer-based</b> <b>autoencoders,</b> even for complex CAD models having very long construction sequences. The proposed ContrastCAD model is shown to be robust to permutation changes of construction sequences and performs better <b>representation</b> <b>learning</b> by generating <b>representation</b> <b>spaces</b> where similar CAD models are more closely clustered. Our codes are available at <a href=https://github.com/cm8908/ContrastCAD>https://github.com/cm8908/ContrastCAD</a>.</p></p class="citation"></blockquote><h3 id=1482--116327-egtr-extracting-graph-from-transformer-for-scene-graph-generation-jinbae-im-et-al-2024>(14/82 | 116/327) EGTR: Extracting Graph from Transformer for Scene Graph Generation (Jinbae Im et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinbae Im, JeongYeon Nam, Nokyung Park, Hyungmin Lee, Seunghyun Park. (2024)<br><strong>EGTR: Extracting Graph from Transformer for Scene Graph Generation</strong><br><button class=copy-to-clipboard title="EGTR: Extracting Graph from Transformer for Scene Graph Generation" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 43<br>Keywords: Object Detection, Graph, Transformer, Relation Extraction, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02072v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02072v3.pdf filename=2404.02072v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene <b>Graph</b> Generation (SGG) is a challenging task of detecting <b>objects</b> <b>and</b> predicting relationships between <b>objects.</b> <b>After</b> DETR was developed, one-stage SGG models based on a one-stage <b>object</b> <b>detector</b> have been actively studied. However, complex modeling is used to predict the relationship between <b>objects,</b> <b>and</b> the inherent relationship between <b>object</b> <b>queries</b> learned in the multi-head <b>self-attention</b> of the <b>object</b> <b>detector</b> has been neglected. We propose a lightweight one-stage SGG model that extracts the <b>relation</b> <b>graph</b> from the various relationships learned in the multi-head <b>self-attention</b> layers of the DETR decoder. By fully utilizing the <b>self-attention</b> by-products, the <b>relation</b> <b>graph</b> can be extracted effectively with a shallow <b>relation</b> <b>extraction</b> head. Considering the dependency of the <b>relation</b> <b>extraction</b> task on the <b>object</b> <b>detection</b> task, we propose a novel <b>relation</b> <b>smoothing</b> technique that adjusts the <b>relation</b> <b>label</b> adaptively according to the quality of the detected <b>objects.</b> <b>By</b> the <b>relation</b> <b>smoothing,</b> the model is trained according to the continuous curriculum that focuses on <b>object</b> <b>detection</b> task at the beginning of training and performs multi-task learning as the <b>object</b> <b>detection</b> performance gradually improves. Furthermore, we propose a connectivity prediction task that predicts whether a <b>relation</b> <b>exists</b> between <b>object</b> <b>pairs</b> as an auxiliary task of the <b>relation</b> <b>extraction.</b> We demonstrate the effectiveness and efficiency of our method for the Visual Genome and Open Image V6 datasets. Our code is publicly available at <a href=https://github.com/naver-ai/egtr>https://github.com/naver-ai/egtr</a>.</p></p class="citation"></blockquote><h3 id=1582--117327-3d-scene-generation-from-scene-graphs-and-self-attention-pietro-bonazzi-2024>(15/82 | 117/327) 3D scene generation from scene graphs and self-attention (Pietro Bonazzi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pietro Bonazzi. (2024)<br><strong>3D scene generation from scene graphs and self-attention</strong><br><button class=copy-to-clipboard title="3D scene generation from scene graphs and self-attention" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Graph, Autoencoder, Variational Autoencoder, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01887v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01887v2.pdf filename=2404.01887v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Synthesizing realistic and diverse indoor 3D scene layouts in a controllable fashion opens up applications in simulated navigation and virtual reality. As concise and robust representations of a scene, scene <b>graphs</b> have proven to be well-suited as the semantic control on the generated layout. We present a variant of the conditional <b>variational</b> <b>autoencoder</b> (cVAE) model to synthesize 3D scenes from scene <b>graphs</b> and floor plans. We exploit the properties of <b>self-attention</b> layers to capture high-level relationships between objects in a scene, and use these as the building blocks of our model. Our model, leverages <b>graph</b> <b>transformers</b> to estimate the size, dimension and orientation of the objects in a room while satisfying relationships in the given scene <b>graph.</b> Our experiments shows <b>self-attention</b> layers leads to sparser (7.9x compared to Graphto3D) and more diverse scenes (16%).</p></p class="citation"></blockquote><h3 id=1682--118327-braven-improving-self-supervised-pre-training-for-visual-and-auditory-speech-recognition-alexandros-haliassos-et-al-2024>(16/82 | 118/327) BRAVEn: Improving Self-Supervised Pre-training for Visual and Auditory Speech Recognition (Alexandros Haliassos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandros Haliassos, Andreas Zinonos, Rodrigo Mira, Stavros Petridis, Maja Pantic. (2024)<br><strong>BRAVEn: Improving Self-Supervised Pre-training for Visual and Auditory Speech Recognition</strong><br><button class=copy-to-clipboard title="BRAVEn: Improving Self-Supervised Pre-training for Visual and Auditory Speech Recognition" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Self-supervised Learning, Self-supervised Pre-training, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02098v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02098v1.pdf filename=2404.02098v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Self-supervision has recently shown great promise for learning visual and auditory <b>speech</b> <b>representations</b> from unlabelled data. In this work, we propose BRAVEn, an extension to the recent RAVEn method, which learns <b>speech</b> <b>representations</b> entirely from raw audio-visual data. Our modifications to RAVEn enable BRAVEn to achieve state-of-the-art results among <b>self-supervised</b> <b>methods</b> in various settings. Moreover, we observe favourable scaling behaviour by increasing the amount of unlabelled data well beyond other <b>self-supervised</b> <b>works.</b> In particular, we achieve 20.0% / 1.7% word error rate for VSR / <b>ASR</b> on the LRS3 test set, with only 30 hours of labelled data and no external <b>ASR</b> models. Our results suggest that readily available unlabelled audio-visual data can largely replace costly transcribed data.</p></p class="citation"></blockquote><h3 id=1782--119327-t-vsl-text-guided-visual-sound-source-localization-in-mixtures-tanvir-mahmud-et-al-2024>(17/82 | 119/327) T-VSL: Text-Guided Visual Sound Source Localization in Mixtures (Tanvir Mahmud et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanvir Mahmud, Yapeng Tian, Diana Marculescu. (2024)<br><strong>T-VSL: Text-Guided Visual Sound Source Localization in Mixtures</strong><br><button class=copy-to-clipboard title="T-VSL: Text-Guided Visual Sound Source Localization in Mixtures" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-SD, cs.CV, eess-AS<br>Keyword Score: 40<br>Keywords: Self-supervised Learning, Supervised Learning, Weakly-supervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01751v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01751v1.pdf filename=2404.01751v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual sound source localization poses a significant challenge in identifying the semantic region of each sounding source within a video. Existing <b>self-supervised</b> and weakly <b>supervised</b> source localization methods struggle to accurately distinguish the semantic regions of each sounding object, particularly in multi-source mixtures. These methods often rely on audio-visual correspondence as guidance, which can lead to substantial performance drops in complex multi-source localization scenarios. The lack of access to individual source sounds in multi-source mixtures during training exacerbates the difficulty of learning effective audio-visual correspondence for localization. To address this limitation, in this paper, we propose incorporating the text modality as an intermediate feature guide using tri-modal joint embedding models (e.g., AudioCLIP) to disentangle the semantic audio-visual source correspondence in multi-source mixtures. Our framework, dubbed T-VSL, begins by predicting the class of sounding entities in mixtures. Subsequently, the textual representation of each sounding source is employed as guidance to disentangle fine-grained audio-visual source correspondence from multi-source mixtures, leveraging the tri-modal AudioCLIP embedding. This approach enables our framework to handle a flexible number of sources and exhibits promising <b>zero-shot</b> transferability to unseen classes during test time. Extensive experiments conducted on the MUSIC, VGGSound, and VGGSound-Instruments datasets demonstrate significant performance improvements over state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=1882--120327-exploring-latent-pathways-enhancing-the-interpretability-of-autonomous-driving-with-a-variational-autoencoder-anass-bairouk-et-al-2024>(18/82 | 120/327) Exploring Latent Pathways: Enhancing the Interpretability of Autonomous Driving with a Variational Autoencoder (Anass Bairouk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anass Bairouk, Mirjana Maras, Simon Herlin, Alexander Amini, Marc Blanchon, Ramin Hasani, Patrick Chareyre, Daniela Rus. (2024)<br><strong>Exploring Latent Pathways: Enhancing the Interpretability of Autonomous Driving with a Variational Autoencoder</strong><br><button class=copy-to-clipboard title="Exploring Latent Pathways: Enhancing the Interpretability of Autonomous Driving with a Variational Autoencoder" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Autoencoder, Convolution, Convolutional Neural Network, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01750v1.pdf filename=2404.01750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous driving presents a complex challenge, which is usually addressed with artificial intelligence models that are end-to-end or modular in nature. Within the landscape of modular approaches, a bio-inspired neural circuit policy model has emerged as an innovative control module, offering a compact and inherently interpretable system to infer a steering wheel command from abstract visual features. Here, we take a leap forward by integrating a <b>variational</b> <b>autoencoder</b> with the neural circuit policy controller, forming a solution that directly generates steering commands from input camera images. By substituting the traditional <b>convolutional</b> <b>neural</b> <b>network</b> approach to feature extraction with a <b>variational</b> <b>autoencoder,</b> we enhance the system&rsquo;s interpretability, enabling a more transparent and understandable decision-making process. In addition to the architectural shift toward a <b>variational</b> <b>autoencoder,</b> this study introduces the automatic latent perturbation tool, a novel contribution designed to probe and elucidate the latent features within the <b>variational</b> <b>autoencoder.</b> The automatic latent perturbation tool automates the interpretability process, offering granular insights into how specific latent variables influence the overall model&rsquo;s behavior. Through a series of numerical experiments, we demonstrate the interpretative power of the <b>variational</b> <b>autoencoder-neural</b> circuit policy model and the utility of the automatic latent perturbation tool in making the inner workings of autonomous driving systems more transparent.</p></p class="citation"></blockquote><h3 id=1982--121327-addsr-accelerating-diffusion-based-blind-super-resolution-with-adversarial-diffusion-distillation-rui-xie-et-al-2024>(19/82 | 121/327) AddSR: Accelerating Diffusion-based Blind Super-Resolution with Adversarial Diffusion Distillation (Rui Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Xie, Ying Tai, Kai Zhang, Zhenyu Zhang, Jun Zhou, Jian Yang. (2024)<br><strong>AddSR: Accelerating Diffusion-based Blind Super-Resolution with Adversarial Diffusion Distillation</strong><br><button class=copy-to-clipboard title="AddSR: Accelerating Diffusion-based Blind Super-Resolution with Adversarial Diffusion Distillation" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 40<br>Keywords: ControlNet, Knowledge Distillation, Stemming, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01717v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01717v2.pdf filename=2404.01717v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Blind super-resolution methods based on stable diffusion showcase formidable generative capabilities in reconstructing clear high-resolution images with intricate details from low-resolution inputs. However, their practical applicability is often hampered by poor efficiency, <b>stemming</b> from the requirement of thousands or hundreds of sampling steps. Inspired by the efficient <b>text-to-image</b> approach adversarial diffusion <b>distillation</b> (ADD), we design AddSR to address this issue by incorporating the ideas of both <b>distillation</b> and <b>ControlNet.</b> Specifically, we first propose a prediction-based self-refinement strategy to provide high-frequency information in the student model output with marginal additional time cost. Furthermore, we refine the training process by employing HR images, rather than LR images, to regulate the teacher model, providing a more robust constraint for <b>distillation.</b> Second, we introduce a timestep-adapting loss to address the perception-distortion imbalance problem introduced by ADD. Extensive experiments demonstrate our AddSR generates better restoration results, while achieving faster speed than previous SD-based state-of-the-art models (e.g., 7x faster than SeeSR).</p></p class="citation"></blockquote><h3 id=2082--122327-unleash-the-potential-of-clip-for-video-highlight-detection-donghoon-han-et-al-2024>(20/82 | 122/327) Unleash the Potential of CLIP for Video Highlight Detection (Donghoon Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Donghoon Han, Seunghyeon Seo, Eunhwan Park, Seong-Uk Nam, Nojun Kwak. (2024)<br><strong>Unleash the Potential of CLIP for Video Highlight Detection</strong><br><button class=copy-to-clipboard title="Unleash the Potential of CLIP for Video Highlight Detection" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 39<br>Keywords: Benchmarking, Fine-tuning, Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01745v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01745v1.pdf filename=2404.01745v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> and <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have revolutionized the utilization of open-world knowledge, unlocking novel potentials across various tasks and applications. Among these domains, the video domain has notably benefited from their capabilities. In this paper, we present Highlight-CLIP (HL-CLIP), a method designed to excel in the video highlight detection task by leveraging the pre-trained knowledge embedded in <b>multimodal</b> models. By simply <b>fine-tuning</b> the <b>multimodal</b> encoder in combination with our innovative saliency pooling technique, we have achieved the state-of-the-art performance in the highlight detection task, the QVHighlight <b>Benchmark,</b> to the best of our knowledge.</p></p class="citation"></blockquote><h3 id=2182--123327-mchartqa-a-universal-benchmark-for-multimodal-chart-question-answer-based-on-vision-language-alignment-and-reasoning-jingxuan-wei-et-al-2024>(21/82 | 123/327) mChartQA: A universal benchmark for multimodal Chart Question Answer based on Vision-Language Alignment and Reasoning (Jingxuan Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingxuan Wei, Nan Xu, Guiyong Chang, Yin Luo, BiHui Yu, Ruifeng Guo. (2024)<br><strong>mChartQA: A universal benchmark for multimodal Chart Question Answer based on Vision-Language Alignment and Reasoning</strong><br><button class=copy-to-clipboard title="mChartQA: A universal benchmark for multimodal Chart Question Answer based on Vision-Language Alignment and Reasoning" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Question Answering, Reasoning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01548v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01548v1.pdf filename=2404.01548v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the fields of computer vision and natural language processing, <b>multimodal</b> chart <b>question-answering,</b> <b>especially</b> involving color, structure, and textless charts, poses significant challenges. Traditional methods, which typically involve either direct <b>multimodal</b> processing or a table-to-text conversion followed by language model analysis, have limitations in effectively handling these complex scenarios. This paper introduces a novel <b>multimodal</b> chart <b>question-answering</b> <b>model,</b> specifically designed to address these intricate tasks. Our model integrates visual and linguistic processing, overcoming the constraints of existing methods. We adopt a dual-phase training approach: the initial phase focuses on aligning image and text representations, while the subsequent phase concentrates on optimizing the model&rsquo;s interpretative and analytical abilities in chart-related queries. This approach has demonstrated superior performance on multiple public datasets, particularly in handling color, structure, and textless chart <b>questions,</b> <b>indicating</b> its effectiveness in complex <b>multimodal</b> tasks.</p></p class="citation"></blockquote><h3 id=2282--124327-iterated-learning-improves-compositionality-in-large-vision-language-models-chenhao-zheng-et-al-2024>(22/82 | 124/327) Iterated Learning Improves Compositionality in Large Vision-Language Models (Chenhao Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenhao Zheng, Jieyu Zhang, Aniruddha Kembhavi, Ranjay Krishna. (2024)<br><strong>Iterated Learning Improves Compositionality in Large Vision-Language Models</strong><br><button class=copy-to-clipboard title="Iterated Learning Improves Compositionality in Large Vision-Language Models" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Contrastive Learning, Vision-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02145v1.pdf filename=2404.02145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A fundamental characteristic common to both human vision and natural language is their compositional nature. Yet, despite the performance gains contributed by large vision and language pretraining, recent investigations find that most-if not all-our state-of-the-art <b>vision-language</b> models struggle at compositionality. They are unable to distinguish between images of " a girl in white facing a man in black" and &ldquo;a girl in black facing a man in white&rdquo;. Moreover, prior work suggests that compositionality doesn&rsquo;t arise with scale: larger model sizes or training data don&rsquo;t help. This paper develops a new iterated training algorithm that incentivizes compositionality. We draw on decades of cognitive science research that identifies cultural transmission-the need to teach a new generation-as a necessary inductive prior that incentivizes humans to develop compositional languages. Specifically, we reframe <b>vision-language</b> <b>contrastive</b> <b>learning</b> as the Lewis Signaling Game between a vision agent and a language agent, and operationalize cultural transmission by iteratively resetting one of the agent&rsquo;s weights during training. After every iteration, this training paradigm induces representations that become &ldquo;easier to learn&rdquo;, a property of compositional languages: e.g. our model trained on CC3M and CC12M improves standard CLIP by 4.7%, 4.0% respectfully in the SugarCrepe <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=2382--125327-multi-level-label-correction-by-distilling-proximate-patterns-for-semi-supervised-semantic-segmentation-hui-xiao-et-al-2024>(23/82 | 125/327) Multi-Level Label Correction by Distilling Proximate Patterns for Semi-supervised Semantic Segmentation (Hui Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hui Xiao, Yuting Hong, Li Dong, Diqun Yan, Jiayan Zhuang, Junjie Xiong, Dongtai Liang, Chengbin Peng. (2024)<br><strong>Multi-Level Label Correction by Distilling Proximate Patterns for Semi-supervised Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Multi-Level Label Correction by Distilling Proximate Patterns for Semi-supervised Semantic Segmentation" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Knowledge Distillation, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02065v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02065v1.pdf filename=2404.02065v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semi-supervised semantic segmentation relieves the reliance on large-scale labeled data by leveraging unlabeled data. Recent semi-supervised semantic segmentation approaches mainly resort to pseudo-labeling methods to exploit unlabeled data. However, unreliable pseudo-labeling can undermine the semi-supervision processes. In this paper, we propose an algorithm called Multi-Level Label Correction (MLLC), which aims to use <b>graph</b> <b>neural</b> <b>networks</b> to capture structural relationships in Semantic-Level <b>Graphs</b> <b>(SLGs)</b> <b>and</b> Class-Level <b>Graphs</b> <b>(CLGs)</b> <b>to</b> rectify erroneous pseudo-labels. Specifically, SLGs represent semantic affinities between pairs of pixel features, and CLGs describe classification consistencies between pairs of pixel labels. With the support of proximate pattern information from <b>graphs,</b> <b>MLLC</b> <b>can</b> rectify incorrectly predicted pseudo-labels and can facilitate discriminative feature representations. We design an end-to-end network to train and perform this effective label corrections mechanism. Experiments demonstrate that MLLC can significantly improve <b>supervised</b> baselines and outperforms state-of-the-art approaches in different scenarios on Cityscapes and PASCAL VOC 2012 datasets. Specifically, MLLC improves the <b>supervised</b> baseline by at least 5% and 2% with DeepLabV2 and DeepLabV3+ respectively under different partition protocols.</p></p class="citation"></blockquote><h3 id=2482--126327-delan-dual-level-alignment-for-vision-and-language-navigation-by-cross-modal-contrastive-learning-mengfei-du-et-al-2024>(24/82 | 126/327) DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning (Mengfei Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengfei Du, Binhao Wu, Jiwen Zhang, Zhihao Fan, Zejun Li, Ruipu Luo, Xuanjing Huang, Zhongyu Wei. (2024)<br><strong>DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning</strong><br><button class=copy-to-clipboard title="DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Contrastive Learning, Self-supervised Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01994v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01994v1.pdf filename=2404.01994v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-and-Language</b> navigation (VLN) requires an agent to navigate in unseen environment by following natural language instruction. For task completion, the agent needs to align and integrate various navigation modalities, including instruction, observation and navigation history. Existing works primarily concentrate on cross-modal attention at the fusion stage to achieve this objective. Nevertheless, modality features generated by disparate uni-encoders reside in their own spaces, leading to a decline in the quality of cross-modal fusion and decision. To address this problem, we propose a Dual-levEL AligNment (DELAN) framework by cross-modal <b>contrastive</b> <b>learning.</b> This framework is designed to align various navigation-related modalities before fusion, thereby enhancing cross-modal interaction and action decision-making. Specifically, we divide the pre-fusion alignment into dual levels: instruction-history level and landmark-observation level according to their semantic correlations. We also reconstruct a dual-level instruction for adaptation to the dual-level alignment. As the training signals for pre-fusion alignment are extremely limited, <b>self-supervised</b> <b>contrastive</b> <b>learning</b> strategies are employed to enforce the matching between different modalities. Our approach seamlessly integrates with the majority of existing models, resulting in improved navigation performance on various VLN <b>benchmarks,</b> including R2R, R4R, RxR and CVDN.</p></p class="citation"></blockquote><h3 id=2582--127327-linear-combination-of-saved-checkpoints-makes-consistency-and-diffusion-models-better-enshu-liu-et-al-2024>(25/82 | 127/327) Linear Combination of Saved Checkpoints Makes Consistency and Diffusion Models Better (Enshu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enshu Liu, Junyi Zhu, Zinan Lin, Xuefei Ning, Matthew B. Blaschko, Sergey Yekhanin, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang. (2024)<br><strong>Linear Combination of Saved Checkpoints Makes Consistency and Diffusion Models Better</strong><br><button class=copy-to-clipboard title="Linear Combination of Saved Checkpoints Makes Consistency and Diffusion Models Better" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Knowledge Distillation, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02241v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02241v2.pdf filename=2404.02241v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>Models</b> (DM) and Consistency Models (CM) are two types of popular generative models with good generation quality on various tasks. When training DM and CM, intermediate weight checkpoints are not fully utilized and only the last converged checkpoint is used. In this work, we find that high-quality model weights often lie in a basin which cannot be reached by <b>SGD</b> but can be obtained by proper checkpoint averaging. Based on these observations, we propose LCSC, a simple but effective and efficient method to enhance the performance of DM and CM, by combining checkpoints along the training trajectory with coefficients deduced from evolutionary search. We demonstrate the value of LCSC through two use cases: $\textbf{(a) Reducing training cost.}$ With LCSC, we only need to train DM/CM with fewer number of iterations and/or lower batch sizes to obtain comparable sample quality with the fully trained model. For example, LCSC achieves considerable training speedups for CM (23$\times$ on CIFAR-10 and 15$\times$ on ImageNet-64). $\textbf{(b) Enhancing pre-trained models.}$ Assuming full training is already done, LCSC can further improve the generation quality or speed of the final converged models. For example, LCSC achieves better performance using 1 number of function evaluation (NFE) than the base model with 2 NFE on consistency <b>distillation,</b> and decreases the NFE of DM from 15 to 9 while maintaining the generation quality on CIFAR-10. Our code is available at <a href=https://github.com/imagination-research/LCSC>https://github.com/imagination-research/LCSC</a>.</p></p class="citation"></blockquote><h3 id=2682--128327-diffusion2-dynamic-3d-content-generation-via-score-composition-of-orthogonal-diffusion-models-zeyu-yang-et-al-2024>(26/82 | 128/327) Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of Orthogonal Diffusion Models (Zeyu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyu Yang, Zijie Pan, Chun Gu, Li Zhang. (2024)<br><strong>Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of Orthogonal Diffusion Models</strong><br><button class=copy-to-clipboard title="Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of Orthogonal Diffusion Models" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Fine-tuning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02148v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02148v1.pdf filename=2404.02148v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in 3D generation are predominantly propelled by improvements in 3D-aware image <b>diffusion</b> <b>models</b> which are pretrained on Internet-scale image data and <b>fine-tuned</b> on massive 3D data, offering the capability of producing highly consistent multi-view images. However, due to the scarcity of synchronized multi-view video data, it is impractical to adapt this paradigm to 4D generation directly. Despite that, the available video and 3D data are adequate for training video and multi-view <b>diffusion</b> <b>models</b> that can provide satisfactory dynamic and geometric priors respectively. In this paper, we present Diffusion$^2$, a novel framework for dynamic 3D content creation that leverages the knowledge about geometric consistency and temporal smoothness from these models to directly sample dense multi-view and multi-frame images which can be employed to optimize continuous 4D representation. Specifically, we design a simple yet effective denoising strategy via score composition of video and multi-view <b>diffusion</b> <b>models</b> based on the probability structure of the images to be generated. Owing to the high parallelism of the image generation and the efficiency of the modern 4D reconstruction pipeline, our framework can generate 4D content within few minutes. Furthermore, our method circumvents the reliance on 4D data, thereby having the potential to benefit from the scalability of the foundation video and multi-view <b>diffusion</b> <b>models.</b> Extensive experiments demonstrate the efficacy of our proposed framework and its capability to flexibly adapt to various types of <b>prompts.</b></p></p class="citation"></blockquote><h3 id=2782--129327-enhancing-ship-classification-in-optical-satellite-imagery-integrating-convolutional-block-attention-module-with-resnet-for-improved-performance-ryan-donghan-kwon-et-al-2024>(27/82 | 129/327) Enhancing Ship Classification in Optical Satellite Imagery: Integrating Convolutional Block Attention Module with ResNet for Improved Performance (Ryan Donghan Kwon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryan Donghan Kwon, Gangjoo Robin Nam, Jisoo Tak, Junseob Shin, Hyerin Cha, Yeom Hyeok, Seung Won Lee. (2024)<br><strong>Enhancing Ship Classification in Optical Satellite Imagery: Integrating Convolutional Block Attention Module with ResNet for Improved Performance</strong><br><button class=copy-to-clipboard title="Enhancing Ship Classification in Optical Satellite Imagery: Integrating Convolutional Block Attention Module with ResNet for Improved Performance" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02135v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02135v3.pdf filename=2404.02135v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents an advanced <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> architecture for ship classification from optical satellite imagery, significantly enhancing performance through the integration of the <b>Convolutional</b> <b>Block</b> <b>Attention</b> Module (CBAM) and additional architectural innovations. Building upon the foundational ResNet50 model, we first incorporated a standard CBAM to direct the model&rsquo;s focus towards more informative features, achieving an accuracy of 87% compared to the baseline ResNet50&rsquo;s 85%. Further augmentations involved multi-scale feature integration, depthwise separable <b>convolutions,</b> and dilated <b>convolutions,</b> culminating in the Enhanced ResNet Model with Improved CBAM. This model demonstrated a remarkable accuracy of 95%, with precision, recall, and f1-scores all witnessing substantial improvements across various ship classes. The bulk carrier and oil tanker classes, in particular, showcased nearly perfect precision and recall rates, underscoring the model&rsquo;s enhanced capability in accurately identifying and classifying ships. Attention heatmap analyses further validated the improved model&rsquo;s efficacy, revealing a more focused attention on relevant ship features, regardless of background complexities. These findings underscore the potential of integrating attention mechanisms and architectural innovations in <b>CNNs</b> for high-resolution satellite imagery classification. The study navigates through the challenges of class imbalance and computational costs, proposing future directions towards scalability and adaptability in new or rare ship type recognition. This research lays a groundwork for the application of advanced deep learning techniques in the domain of remote sensing, offering insights into scalable and efficient satellite image classification.</p></p class="citation"></blockquote><h3 id=2882--130327-cooperative-students-navigating-unsupervised-domain-adaptation-in-nighttime-object-detection-jicheng-yuan-et-al-2024>(28/82 | 130/327) Cooperative Students: Navigating Unsupervised Domain Adaptation in Nighttime Object Detection (Jicheng Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jicheng Yuan, Anh Le-Tuan, Manfred Hauswirth, Danh Le-Phuoc. (2024)<br><strong>Cooperative Students: Navigating Unsupervised Domain Adaptation in Nighttime Object Detection</strong><br><button class=copy-to-clipboard title="Cooperative Students: Navigating Unsupervised Domain Adaptation in Nighttime Object Detection" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01988v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01988v2.pdf filename=2404.01988v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> <b>Domain</b> <b>Adaptation</b> (UDA) has shown significant advancements in <b>object</b> <b>detection</b> under well-lit conditions; however, its performance degrades notably in low-visibility scenarios, especially at night, posing challenges not only for its adaptability in low signal-to-noise ratio (SNR) conditions but also for the reliability and efficiency of automated vehicles. To address this problem, we propose a \textbf{Co}operative \textbf{S}tudents (\textbf{CoS}) framework that innovatively employs global-local transformations (GLT) and a proxy-based target consistency (PTC) mechanism to capture the spatial consistency in day- and night-time scenarios effectively, and thus bridge the significant <b>domain</b> <b>shift</b> across contexts. Building upon this, we further devise an adaptive IoU-informed thresholding (AIT) module to gradually avoid overlooking potential true positives and enrich the latent information in the target <b>domain.</b> <b>Comprehensive</b> experiments show that CoS essentially enhanced UDA performance in low-visibility conditions and surpasses current state-of-the-art techniques, achieving an increase in mAP of 3.0%, 1.9%, and 2.5% on BDD100K, SHIFT, and ACDC datasets, respectively. Code is available at <a href=https://github.com/jichengyuan/Cooperitive_Students>https://github.com/jichengyuan/Cooperitive_Students</a>.</p></p class="citation"></blockquote><h3 id=2982--131327-vlrm-vision-language-models-act-as-reward-models-for-image-captioning-maksim-dzabraev-et-al-2024>(29/82 | 131/327) VLRM: Vision-Language Models act as Reward Models for Image Captioning (Maksim Dzabraev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maksim Dzabraev, Alexander Kunitsyn, Andrei Ivaniuta. (2024)<br><strong>VLRM: Vision-Language Models act as Reward Models for Image Captioning</strong><br><button class=copy-to-clipboard title="VLRM: Vision-Language Models act as Reward Models for Image Captioning" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Unsupervised Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01911v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01911v1.pdf filename=2404.01911v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we present an <b>unsupervised</b> method for enhancing an image captioning model (in our case, BLIP2) using <b>reinforcement</b> <b>learning</b> and <b>vision-language</b> models like CLIP and BLIP2-ITM as reward models. The RL-tuned model is able to generate longer and more comprehensive descriptions. Our model reaches impressive 0.90 R@1 CLIP Recall score on MS-COCO Carpathy Test Split. Weights are available at <a href=https://huggingface.co/sashakunitsyn/vlrm-blip2-opt-2.7b>https://huggingface.co/sashakunitsyn/vlrm-blip2-opt-2.7b</a>.</p></p class="citation"></blockquote><h3 id=3082--132327-scene-adaptive-sparse-transformer-for-event-based-object-detection-yansong-peng-et-al-2024>(30/82 | 132/327) Scene Adaptive Sparse Transformer for Event-based Object Detection (Yansong Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yansong Peng, Hebei Li, Yueyi Zhang, Xiaoyan Sun, Feng Wu. (2024)<br><strong>Scene Adaptive Sparse Transformer for Event-based Object Detection</strong><br><button class=copy-to-clipboard title="Scene Adaptive Sparse Transformer for Event-based Object Detection" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01882v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01882v1.pdf filename=2404.01882v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While recent <b>Transformer-based</b> approaches have shown impressive performances on event-based <b>object</b> <b>detection</b> tasks, their high computational costs still diminish the low power consumption advantage of event cameras. Image-based works attempt to reduce these costs by introducing sparse <b>Transformers.</b> However, they display inadequate sparsity and adaptability when applied to event-based <b>object</b> <b>detection,</b> since these approaches cannot balance the fine granularity of token-level sparsification and the efficiency of window-based <b>Transformers,</b> leading to reduced performance and efficiency. Furthermore, they lack scene-specific sparsity optimization, resulting in information loss and a lower recall rate. To overcome these limitations, we propose the Scene Adaptive Sparse <b>Transformer</b> (SAST). SAST enables window-token co-sparsification, significantly enhancing fault tolerance and reducing computational overhead. Leveraging the innovative scoring and selection modules, along with the Masked Sparse Window <b>Self-Attention,</b> SAST showcases remarkable scene-aware adaptability: It focuses only on important <b>objects</b> <b>and</b> dynamically optimizes sparsity level according to scene complexity, maintaining a remarkable balance between performance and computational cost. The evaluation results show that SAST outperforms all other dense and sparse networks in both performance and efficiency on two large-scale event-based <b>object</b> <b>detection</b> datasets (1Mpx and Gen1). Code: <a href=https://github.com/Peterande/SAST>https://github.com/Peterande/SAST</a></p></p class="citation"></blockquote><h3 id=3182--133327-disentangled-pre-training-for-human-object-interaction-detection-zhuolong-li-et-al-2024>(31/82 | 133/327) Disentangled Pre-training for Human-Object Interaction Detection (Zhuolong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuolong Li, Xingao Li, Changxing Ding, Xiangmin Xu. (2024)<br><strong>Disentangled Pre-training for Human-Object Interaction Detection</strong><br><button class=copy-to-clipboard title="Disentangled Pre-training for Human-Object Interaction Detection" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Knowledge Transfer, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01725v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01725v1.pdf filename=2404.01725v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting human-object interaction (HOI) has long been limited by the amount of <b>supervised</b> data available. Recent approaches address this issue by pre-training according to pseudo-labels, which align <b>object</b> <b>regions</b> with HOI triplets parsed from image captions. However, pseudo-labeling is tricky and noisy, making HOI pre-training a complex process. Therefore, we propose an efficient disentangled pre-training method for HOI detection (DP-HOI) to address this problem. First, DP-HOI utilizes <b>object</b> <b>detection</b> and action recognition datasets to pre-train the detection and interaction decoder layers, respectively. Then, we arrange these decoder layers so that the pre-training architecture is consistent with the downstream HOI detection task. This facilitates efficient <b>knowledge</b> <b>transfer.</b> Specifically, the detection decoder identifies reliable human instances in each action recognition dataset image, generates one corresponding query, and feeds it into the interaction decoder for verb classification. Next, we combine the human instance verb predictions in the same image and impose image-level supervision. The DP-HOI structure can be easily adapted to the HOI detection task, enabling effective model parameter initialization. Therefore, it significantly enhances the performance of existing HOI detection models on a broad range of rare categories. The code and pre-trained weight are available at <a href=https://github.com/xingaoli/DP-HOI>https://github.com/xingaoli/DP-HOI</a>.</p></p class="citation"></blockquote><h3 id=3282--134327-task-integration-distillation-for-object-detectors-hai-su-et-al-2024>(32/82 | 134/327) Task Integration Distillation for Object Detectors (Hai Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hai Su, ZhenWen Jian, Songsen Yu. (2024)<br><strong>Task Integration Distillation for Object Detectors</strong><br><button class=copy-to-clipboard title="Task Integration Distillation for Object Detectors" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01699v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01699v1.pdf filename=2404.01699v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>distillation</b> is a widely adopted technique for model lightening. However, the performance of most <b>knowledge</b> <b>distillation</b> methods in the domain of <b>object</b> <b>detection</b> is not satisfactory. Typically, <b>knowledge</b> <b>distillation</b> approaches consider only the classification task among the two sub-tasks of an <b>object</b> <b>detector,</b> largely overlooking the regression task. This oversight leads to a partial understanding of the <b>object</b> <b>detector&rsquo;s</b> comprehensive task, resulting in skewed estimations and potentially adverse effects. Therefore, we propose a <b>knowledge</b> <b>distillation</b> method that addresses both the classification and regression tasks, incorporating a task significance strategy. By evaluating the importance of features based on the output of the detector&rsquo;s two sub-tasks, our approach ensures a balanced consideration of both classification and regression tasks in <b>object</b> <b>detection.</b> Drawing inspiration from real-world teaching processes and the definition of learning condition, we introduce a method that focuses on both key and weak areas. By assessing the value of features for <b>knowledge</b> <b>distillation</b> based on their importance differences, we accurately capture the current model&rsquo;s learning situation. This method effectively prevents the issue of biased predictions about the model&rsquo;s learning reality caused by an incomplete utilization of the detector&rsquo;s outputs.</p></p class="citation"></blockquote><h3 id=3382--135327-a-universal-knowledge-embedded-contrastive-learning-framework-for-hyperspectral-image-classification-quanwei-liu-et-al-2024>(33/82 | 135/327) A Universal Knowledge Embedded Contrastive Learning Framework for Hyperspectral Image Classification (Quanwei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quanwei Liu, Yanni Dong, Tao Huang, Lefei Zhang, Bo Du. (2024)<br><strong>A Universal Knowledge Embedded Contrastive Learning Framework for Hyperspectral Image Classification</strong><br><button class=copy-to-clipboard title="A Universal Knowledge Embedded Contrastive Learning Framework for Hyperspectral Image Classification" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01673v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01673v2.pdf filename=2404.01673v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hyperspectral image (HSI) classification techniques have been intensively studied and a variety of models have been developed. However, these HSI classification models are confined to pocket models and unrealistic ways of datasets partitioning. The former limits the generalization performance of the model and the latter is partitioned leads to inflated model evaluation metrics, which results in plummeting model performance in the real world. Therefore, we propose a universal knowledge embedded <b>contrastive</b> <b>learning</b> framework (KnowCL) for <b>supervised,</b> <b>unsupervised,</b> and semisupervised HSI classification, which largely closes the gap of HSI classification models between pocket models and standard vision backbones. We present a new HSI processing pipeline in conjunction with a range of data transformation and augmentation techniques that provide diverse data representations and realistic data partitioning. The proposed framework based on this pipeline is compatible with all kinds of backbones and can fully exploit labeled and unlabeled samples with expected training time. Furthermore, we design a new loss function, which can adaptively fuse the <b>supervised</b> loss and <b>unsupervised</b> loss, enhancing the learning performance. This proposed new classification paradigm shows great potentials in exploring for HSI classification technology. The code can be accessed at <a href=https://github.com/quanweiliu/KnowCL>https://github.com/quanweiliu/KnowCL</a>.</p></p class="citation"></blockquote><h3 id=3482--136327-supporting-mitosis-detection-ai-training-with-inter-observer-eye-gaze-consistencies-hongyan-gu-et-al-2024>(34/82 | 136/327) Supporting Mitosis Detection AI Training with Inter-Observer Eye-Gaze Consistencies (Hongyan Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyan Gu, Zihan Yan, Ayesha Alvi, Brandon Day, Chunxu Yang, Zida Wu, Shino Magaki, Mohammad Haeri, Xiang &lsquo;Anthony&rsquo; Chen. (2024)<br><strong>Supporting Mitosis Detection AI Training with Inter-Observer Eye-Gaze Consistencies</strong><br><button class=copy-to-clipboard title="Supporting Mitosis Detection AI Training with Inter-Observer Eye-Gaze Consistencies" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01656v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01656v1.pdf filename=2404.01656v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The expansion of artificial intelligence (AI) in pathology tasks has intensified the demand for doctors&rsquo; annotations in AI development. However, collecting high-quality annotations from doctors is costly and time-consuming, creating a bottleneck in AI progress. This study investigates eye-tracking as a cost-effective technology to collect doctors&rsquo; behavioral data for AI training with a focus on the pathology task of mitosis detection. One major challenge in using eye-gaze data is the low signal-to-noise ratio, which hinders the extraction of meaningful information. We tackled this by levering the properties of inter-observer eye-gaze consistencies and creating eye-gaze labels from consistent eye-fixations shared by a group of observers. Our study involved 14 non-medical participants, from whom we collected eye-gaze data and generated eye-gaze labels based on varying group sizes. We assessed the efficacy of such eye-gaze labels by training <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> and comparing their performance to those trained with ground truth annotations and a heuristic-based baseline. Results indicated that <b>CNNs</b> trained with our eye-gaze labels closely followed the performance of ground-truth-based <b>CNNs,</b> and significantly outperformed the baseline. Although primarily focused on mitosis, we envision that insights from this study can be generalized to other medical imaging tasks.</p></p class="citation"></blockquote><h3 id=3582--137327-spin-up-spin-light-for-natural-light-uncalibrated-photometric-stereo-zongrui-li-et-al-2024>(35/82 | 137/327) Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo (Zongrui Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zongrui Li, Zhan Lu, Haojie Yan, Boxin Shi, Gang Pan, Qian Zheng, Xudong Jiang. (2024)<br><strong>Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo</strong><br><button class=copy-to-clipboard title="Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Supervised Learning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01612v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01612v1.pdf filename=2404.01612v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Natural Light Uncalibrated Photometric Stereo (NaUPS) relieves the strict environment and light assumptions in classical Uncalibrated Photometric Stereo (UPS) methods. However, due to the intrinsic ill-posedness and high-dimensional ambiguities, addressing NaUPS is still an open question. Existing works impose strong assumptions on the environment lights and objects&rsquo; material, restricting the effectiveness in more general scenarios. Alternatively, some methods leverage <b>supervised</b> <b>learning</b> with intricate models while lacking interpretability, resulting in a biased estimation. In this work, we proposed Spin Light Uncalibrated Photometric Stereo (Spin-UP), an <b>unsupervised</b> method to tackle NaUPS in various environment lights and objects. The proposed method uses a novel setup that captures the object&rsquo;s images on a rotatable platform, which mitigates NaUPS&rsquo;s ill-posedness by reducing unknowns and provides reliable priors to alleviate NaUPS&rsquo;s ambiguities. Leveraging neural inverse rendering and the proposed training strategies, Spin-UP recovers surface normals, environment light, and isotropic reflectance under complex natural light with low computational cost. Experiments have shown that Spin-UP outperforms other <b>supervised</b> <b>/</b> <b>unsupervised</b> NaUPS methods and achieves state-of-the-art performance on synthetic and real-world datasets. Codes and data are available at <a href=https://github.com/LMozart/CVPR2024-SpinUP>https://github.com/LMozart/CVPR2024-SpinUP</a>.</p></p class="citation"></blockquote><h3 id=3682--138327-motionchain-conversational-motion-controllers-via-multimodal-prompts-biao-jiang-et-al-2024>(36/82 | 138/327) MotionChain: Conversational Motion Controllers via Multimodal Prompts (Biao Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Biao Jiang, Xin Chen, Chi Zhang, Fukun Yin, Zhuoyuan Li, Gang YU, Jiayuan Fan. (2024)<br><strong>MotionChain: Conversational Motion Controllers via Multimodal Prompts</strong><br><button class=copy-to-clipboard title="MotionChain: Conversational Motion Controllers via Multimodal Prompts" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01700v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01700v2.pdf filename=2404.01700v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in language models have demonstrated their adeptness in conducting multi-turn dialogues and retaining conversational context. However, this proficiency remains largely unexplored in other <b>multimodal</b> generative models, particularly in human motion models. By integrating multi-turn conversations in controlling continuous virtual human movements, generative human motion models can achieve an intuitive and step-by-step process of human task execution for humanoid robotics, game agents, or other embodied systems. In this work, we present MotionChain, a conversational human motion controller to generate continuous and long-term human motion through <b>multimodal</b> <b>prompts.</b> Specifically, MotionChain consists of <b>multi-modal</b> tokenizers that transform various data types such as text, image, and motion, into discrete tokens, coupled with a Vision-Motion-aware Language model. By leveraging large-scale language, <b>vision-language,</b> and vision-motion data to assist motion-related generation tasks, MotionChain thus comprehends each instruction in multi-turn conversation and generates human motions followed by these <b>prompts.</b> Extensive experiments validate the efficacy of MotionChain, demonstrating state-of-the-art performance in conversational motion generation, as well as more intuitive manners of controlling and interacting with virtual humans.</p></p class="citation"></blockquote><h3 id=3782--139327-language-model-guided-interpretable-video-action-reasoning-ning-wang-et-al-2024>(37/82 | 139/327) Language Model Guided Interpretable Video Action Reasoning (Ning Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ning Wang, Guangming Zhu, HS Li, Liang Zhang, Syed Afaq Ali Shah, Mohammed Bennamoun. (2024)<br><strong>Language Model Guided Interpretable Video Action Reasoning</strong><br><button class=copy-to-clipboard title="Language Model Guided Interpretable Video Action Reasoning" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Black Box, Reasoning, Video-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01591v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01591v1.pdf filename=2404.01591v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While neural networks have excelled in video action recognition tasks, their <b>black-box</b> <b>nature</b> often obscures the understanding of their decision-making processes. Recent approaches used inherently interpretable models to analyze video actions in a manner akin to human <b>reasoning.</b> These models, however, usually fall short in performance compared to their <b>black-box</b> <b>counterparts.</b> In this work, we present a new framework named Language-guided Interpretable Action Recognition framework (LaIAR). LaIAR leverages knowledge from language models to enhance both the recognition capabilities and the interpretability of video models. In essence, we redefine the problem of understanding video model decisions as a task of aligning video and language models. Using the logical <b>reasoning</b> captured by the language model, we steer the training of the video model. This integrated approach not only improves the video model&rsquo;s adaptability to different domains but also boosts its overall performance. Extensive experiments on two complex video action datasets, Charades & CAD-120, validates the improved performance and interpretability of our LaIAR framework. The code of LaIAR is available at <a href=https://github.com/NingWang2049/LaIAR>https://github.com/NingWang2049/LaIAR</a>.</p></p class="citation"></blockquote><h3 id=3882--140327-ofmpnet-deep-end-to-end-model-for-occupancy-and-flow-prediction-in-urban-environment-youshaa-murhij-et-al-2024>(38/82 | 140/327) OFMPNet: Deep End-to-End Model for Occupancy and Flow Prediction in Urban Environment (Youshaa Murhij et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Youshaa Murhij, Dmitry Yudin. (2024)<br><strong>OFMPNet: Deep End-to-End Model for Occupancy and Flow Prediction in Urban Environment</strong><br><button class=copy-to-clipboard title="OFMPNet: Deep End-to-End Model for Occupancy and Flow Prediction in Urban Environment" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Convolution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02263v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02263v1.pdf filename=2404.02263v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task of motion prediction is pivotal for autonomous driving systems, providing crucial data to choose a vehicle behavior strategy within its surroundings. Existing motion prediction techniques primarily focus on predicting the future trajectory of each agent in the scene individually, utilizing its past trajectory data. In this paper, we introduce an end-to-end neural network methodology designed to predict the future behaviors of all dynamic objects in the environment. This approach leverages the occupancy map and the scene&rsquo;s motion flow. We are investigatin various alternatives for constructing a deep encoder-decoder model called OFMPNet. This model uses a sequence of bird&rsquo;s-eye-view road images, occupancy grid, and prior motion flow as input data. The encoder of the model can incorporate <b>transformer,</b> attention-based, or <b>convolutional</b> units. The decoder considers the use of both <b>convolutional</b> modules and recurrent blocks. Additionally, we propose a novel time-weighted motion flow loss, whose application has shown a substantial decrease in end-point error. Our approach has achieved state-of-the-art results on the Waymo Occupancy and Flow Prediction <b>benchmark,</b> with a Soft IoU of 52.1% and an AUC of 76.75% on Flow-Grounded Occupancy.</p></p class="citation"></blockquote><h3 id=3982--141327-selfpose3d-self-supervised-multi-person-multi-view-3d-pose-estimation-vinkle-srivastav-et-al-2024>(39/82 | 141/327) SelfPose3d: Self-Supervised Multi-Person Multi-View 3d Pose Estimation (Vinkle Srivastav et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vinkle Srivastav, Keqi Chen, Nicolas Padoy. (2024)<br><strong>SelfPose3d: Self-Supervised Multi-Person Multi-View 3d Pose Estimation</strong><br><button class=copy-to-clipboard title="SelfPose3d: Self-Supervised Multi-Person Multi-View 3d Pose Estimation" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02041v1.pdf filename=2404.02041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a new <b>self-supervised</b> <b>approach,</b> SelfPose3d, for estimating 3d poses of multiple persons from multiple camera views. Unlike current state-of-the-art fully-supervised methods, our approach does not require any 2d or 3d ground-truth poses and uses only the multi-view input images from a calibrated camera setup and 2d pseudo poses generated from an off-the-shelf 2d human pose estimator. We propose two <b>self-supervised</b> <b>learning</b> objectives: <b>self-supervised</b> <b>person</b> localization in 3d space and <b>self-supervised</b> <b>3d</b> pose estimation. We achieve <b>self-supervised</b> <b>3d</b> person localization by training the model on synthetically generated 3d points, serving as 3d person root positions, and on the projected root-heatmaps in all the views. We then model the 3d poses of all the localized persons with a bottleneck representation, map them onto all views obtaining 2d joints, and render them using 2d Gaussian heatmaps in an end-to-end differentiable manner. Afterwards, we use the corresponding 2d joints and heatmaps from the pseudo 2d poses for learning. To alleviate the intrinsic inaccuracy of the pseudo labels, we propose an adaptive supervision attention mechanism to guide the self-supervision. Our experiments and analysis on three public <b>benchmark</b> datasets, including Panoptic, Shelf, and Campus, show the effectiveness of our approach, which is comparable to fully-supervised methods. Code is available at \url{https://github.com/CAMMA-public/SelfPose3D}</p></p class="citation"></blockquote><h3 id=4082--142327-semi-supervised-domain-adaptation-for-wildfire-detection-jooyoung-jang-et-al-2024>(40/82 | 142/327) Semi-Supervised Domain Adaptation for Wildfire Detection (JooYoung Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>JooYoung Jang, Youngseo Cha, Jisu Kim, SooHyung Lee, Geonu Lee, Minkook Cho, Young Hwang, Nojun Kwak. (2024)<br><strong>Semi-Supervised Domain Adaptation for Wildfire Detection</strong><br><button class=copy-to-clipboard title="Semi-Supervised Domain Adaptation for Wildfire Detection" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Object Detection, Benchmarking, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01842v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01842v1.pdf filename=2404.01842v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, both the frequency and intensity of wildfires have increased worldwide, primarily due to climate change. In this paper, we propose a novel protocol for wildfire detection, leveraging semi-supervised <b>Domain</b> <b>Adaptation</b> for <b>object</b> <b>detection,</b> accompanied by a corresponding dataset designed for use by both academics and industries. Our dataset encompasses 30 times more diverse labeled scenes for the current largest <b>benchmark</b> wildfire dataset, HPWREN, and introduces a new labeling policy for wildfire detection. Inspired by CoordConv, we propose a robust baseline, Location-Aware <b>Object</b> <b>Detection</b> for Semi-Supervised <b>Domain</b> <b>Adaptation</b> (LADA), utilizing a teacher-student based framework capable of extracting translational variance features characteristic of wildfires. With only using 1% target <b>domain</b> <b>labeled</b> data, our framework significantly outperforms our source-only baseline by a notable margin of 3.8% in mean Average Precision on the HPWREN wildfire dataset. Our dataset is available at <a href=https://github.com/BloomBerry/LADA>https://github.com/BloomBerry/LADA</a>.</p></p class="citation"></blockquote><h3 id=4182--143327-sparse-semi-detr-sparse-learnable-queries-for-semi-supervised-object-detection-tahira-shehzadi-et-al-2024>(41/82 | 143/327) Sparse Semi-DETR: Sparse Learnable Queries for Semi-Supervised Object Detection (Tahira Shehzadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tahira Shehzadi, Khurram Azeem Hashmi, Didier Stricker, Muhammad Zeshan Afzal. (2024)<br><strong>Sparse Semi-DETR: Sparse Learnable Queries for Semi-Supervised Object Detection</strong><br><button class=copy-to-clipboard title="Sparse Semi-DETR: Sparse Learnable Queries for Semi-Supervised Object Detection" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Object Detection, Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01819v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01819v1.pdf filename=2404.01819v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address the limitations of the DETR-based semi-supervised <b>object</b> <b>detection</b> (SSOD) framework, particularly focusing on the challenges posed by the quality of <b>object</b> <b>queries.</b> In DETR-based SSOD, the one-to-one assignment strategy provides inaccurate pseudo-labels, while the one-to-many assignments strategy leads to overlapping predictions. These issues compromise training efficiency and degrade model performance, especially in detecting small or occluded <b>objects.</b> <b>We</b> introduce Sparse Semi-DETR, a novel <b>transformer-based,</b> end-to-end semi-supervised <b>object</b> <b>detection</b> solution to overcome these challenges. Sparse Semi-DETR incorporates a Query Refinement Module to enhance the quality of <b>object</b> <b>queries,</b> significantly improving detection capabilities for small and partially obscured <b>objects.</b> <b>Additionally,</b> we integrate a Reliable Pseudo-Label Filtering Module that selectively filters high-quality pseudo-labels, thereby enhancing detection accuracy and consistency. On the MS-COCO and Pascal VOC <b>object</b> <b>detection</b> <b>benchmarks,</b> Sparse Semi-DETR achieves a significant improvement over current state-of-the-art methods that highlight Sparse Semi-DETR&rsquo;s effectiveness in semi-supervised <b>object</b> <b>detection,</b> particularly in challenging scenarios involving small or partially obscured objects.</p></p class="citation"></blockquote><h3 id=4282--144327-diffusion-deepfake-chaitali-bhattacharyya-et-al-2024>(42/82 | 144/327) Diffusion Deepfake (Chaitali Bhattacharyya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaitali Bhattacharyya, Hanxiao Wang, Feng Zhang, Sungho Kim, Xiatian Zhu. (2024)<br><strong>Diffusion Deepfake</strong><br><button class=copy-to-clipboard title="Diffusion Deepfake" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Diffusion Model, Benchmarking, Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01579v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01579v1.pdf filename=2404.01579v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent progress in <b>generative</b> <b>AI,</b> primarily through <b>diffusion</b> <b>models,</b> presents significant challenges for real-world deepfake detection. The increased realism in image details, diverse content, and widespread accessibility to the general public complicates the identification of these sophisticated deepfakes. Acknowledging the urgency to address the vulnerability of current deepfake detectors to this evolving threat, our paper introduces two extensive deepfake datasets generated by state-of-the-art <b>diffusion</b> <b>models</b> as other datasets are less diverse and low in quality. Our extensive experiments also showed that our dataset is more challenging compared to the other face deepfake datasets. Our strategic dataset creation not only challenge the deepfake detectors but also sets a new <b>benchmark</b> for more evaluation. Our comprehensive evaluation reveals the struggle of existing detection methods, often optimized for specific image domains and manipulations, to effectively adapt to the intricate nature of <b>diffusion</b> <b>deepfakes,</b> limiting their practical utility. To address this critical issue, we investigate the impact of enhancing training data diversity on representative detection methods. This involves expanding the diversity of both manipulation techniques and image domains. Our findings underscore that increasing training data diversity results in improved generalizability. Moreover, we propose a novel momentum difficulty boosting strategy to tackle the additional challenge posed by training data heterogeneity. This strategy dynamically assigns appropriate sample weights based on learning difficulty, enhancing the model&rsquo;s adaptability to both easy and challenging samples. Extensive experiments on both existing and newly proposed <b>benchmarks</b> demonstrate that our model optimization approach surpasses prior alternatives significantly.</p></p class="citation"></blockquote><h3 id=4382--145327-bidirectional-multi-scale-implicit-neural-representations-for-image-deraining-xiang-chen-et-al-2024>(43/82 | 145/327) Bidirectional Multi-Scale Implicit Neural Representations for Image Deraining (Xiang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Chen, Jinshan Pan, Jiangxin Dong. (2024)<br><strong>Bidirectional Multi-Scale Implicit Neural Representations for Image Deraining</strong><br><button class=copy-to-clipboard title="Bidirectional Multi-Scale Implicit Neural Representations for Image Deraining" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Transformer, Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01547v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01547v1.pdf filename=2404.01547v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How to effectively explore multi-scale representations of rain streaks is important for image deraining. In contrast to existing <b>Transformer-based</b> methods that depend mostly on single-scale rain appearance, we develop an end-to-end multi-scale <b>Transformer</b> that leverages the potentially useful features in various scales to facilitate high-quality image reconstruction. To better explore the common degradation representations from spatially-varying rain streaks, we incorporate intra-scale implicit neural representations based on pixel coordinates with the degraded inputs in a closed-loop design, enabling the learned features to facilitate rain removal and improve the robustness of the model in complex scenarios. To ensure richer collaborative representation from different scales, we embed a simple yet effective inter-scale bidirectional feedback operation into our multi-scale <b>Transformer</b> by performing coarse-to-fine and fine-to-coarse information communication. Extensive experiments demonstrate that our approach, named as <b>NeRD-Rain,</b> performs favorably against the state-of-the-art ones on both synthetic and real-world <b>benchmark</b> datasets. The source code and trained models are available at <a href=https://github.com/cschenxiang/NeRD-Rain>https://github.com/cschenxiang/NeRD-Rain</a>.</p></p class="citation"></blockquote><h3 id=4482--146327-snag-scalable-and-accurate-video-grounding-fangzhou-mu-et-al-2024>(44/82 | 146/327) SnAG: Scalable and Accurate Video Grounding (Fangzhou Mu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangzhou Mu, Sicheng Mo, Yin Li. (2024)<br><strong>SnAG: Scalable and Accurate Video Grounding</strong><br><button class=copy-to-clipboard title="SnAG: Scalable and Accurate Video Grounding" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Grounding, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02257v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02257v2.pdf filename=2404.02257v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Temporal <b>grounding</b> of text descriptions in videos is a central problem in <b>vision-language</b> learning and video understanding. Existing methods often prioritize accuracy over scalability &ndash; they have been optimized for <b>grounding</b> only a few text queries within short videos, and fail to scale up to long videos with hundreds of queries. In this paper, we study the effect of cross-modal fusion on the scalability of video <b>grounding</b> models. Our analysis establishes late fusion as a more cost-effective fusion scheme for long-form videos with many text queries. Moreover, it leads us to a novel, video-centric sampling scheme for efficient training. Based on these findings, we present SnAG, a simple baseline for scalable and accurate video <b>grounding.</b> Without bells and whistles, SnAG is 43% more accurate and 1.5x faster than CONE, a state of the art for long-form video <b>grounding</b> on the challenging MAD dataset, while achieving highly competitive results on short videos.</p></p class="citation"></blockquote><h3 id=4582--147327-towards-robust-3d-pose-transfer-with-adversarial-learning-haoyu-chen-et-al-2024>(45/82 | 147/327) Towards Robust 3D Pose Transfer with Adversarial Learning (Haoyu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Chen, Hao Tang, Ehsan Adeli, Guoying Zhao. (2024)<br><strong>Towards Robust 3D Pose Transfer with Adversarial Learning</strong><br><button class=copy-to-clipboard title="Towards Robust 3D Pose Transfer with Adversarial Learning" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02242v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02242v1.pdf filename=2404.02242v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D pose transfer that aims to transfer the desired pose to a target mesh is one of the most challenging 3D generation tasks. Previous attempts rely on well-defined parametric human models or skeletal joints as driving pose sources. However, to obtain those clean pose sources, cumbersome but necessary pre-processing pipelines are inevitable, hindering implementations of the real-time applications. This work is driven by the intuition that the robustness of the model can be enhanced by introducing <b>adversarial</b> <b>samples</b> into the training, leading to a more invulnerable model to the noisy inputs, which even can be further extended to directly handling the real-world data like raw point clouds/scans without intermediate processing. Furthermore, we propose a novel 3D pose Masked <b>Autoencoder</b> (3D-PoseMAE), a customized MAE that effectively learns 3D extrinsic presentations (i.e., pose). 3D-PoseMAE facilitates learning from the aspect of extrinsic attributes by simultaneously generating <b>adversarial</b> <b>samples</b> that perturb the model and learning the arbitrary raw noisy poses via a multi-scale masking strategy. Both qualitative and quantitative studies show that the transferred meshes given by our network result in much better quality. Besides, we demonstrate the strong generalizability of our method on various poses, different domains, and even raw scans. Experimental results also show meaningful insights that the intermediate <b>adversarial</b> <b>samples</b> generated in the training can successfully attack the existing pose transfer models.</p></p class="citation"></blockquote><h3 id=4682--148327-lookahead-exploration-with-neural-radiance-representation-for-continuous-vision-language-navigation-zihan-wang-et-al-2024>(46/82 | 148/327) Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation (Zihan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, Junjie Hu, Ming Jiang, Shuqiang Jiang. (2024)<br><strong>Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation</strong><br><button class=copy-to-clipboard title="Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 20<br>Keywords: Vision-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01943v1.pdf filename=2404.01943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-and-language</b> navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. At each navigation step, the agent selects from possible candidate locations and then makes the move. For better navigation planning, the lookahead exploration strategy aims to effectively evaluate the agent&rsquo;s next action by accurately anticipating the future environment of candidate locations. To this end, some existing works predict RGB images for future environments, while this strategy suffers from image distortion and high computational cost. To address these issues, we propose the pre-trained hierarchical neural radiance representation model (HNR) to produce multi-level semantic features for future environments, which are more robust and efficient than pixel-wise RGB reconstruction. Furthermore, with the predicted future environmental representations, our lookahead VLN model is able to construct the navigable future path tree and select the optimal path via efficient parallel evaluation. Extensive experiments on the VLN-CE datasets confirm the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=4782--149327-co-speech-gesture-video-generation-via-motion-decoupled-diffusion-model-xu-he-et-al-2024>(47/82 | 149/327) Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model (Xu He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu He, Qiaochu Huang, Zhensong Zhang, Zhiwei Lin, Zhiyong Wu, Sicheng Yang, Minglei Li, Zhiyi Chen, Songcen Xu, Xiaofei Wu. (2024)<br><strong>Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model</strong><br><button class=copy-to-clipboard title="Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-HC, cs-MM, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01862v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01862v1.pdf filename=2404.01862v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Co-speech gestures, if presented in the lively form of videos, can achieve superior visual effects in human-machine interaction. While previous works mostly generate structural human skeletons, resulting in the omission of appearance information, we focus on the direct generation of audio-driven co-speech gesture videos in this work. There are two main challenges: 1) A suitable motion feature is needed to describe complex human movements with crucial appearance information. 2) Gestures and speech exhibit inherent dependencies and should be temporally aligned even of arbitrary length. To solve these problems, we present a novel motion-decoupled framework to generate co-speech gesture videos. Specifically, we first introduce a well-designed nonlinear TPS transformation to obtain latent motion features preserving essential appearance information. Then a <b>transformer-based</b> <b>diffusion</b> <b>model</b> is proposed to learn the temporal correlation between gestures and speech, and performs generation in the latent motion space, followed by an optimal motion selection module to produce long-term coherent and consistent gesture videos. For better visual perception, we further design a refinement network focusing on missing details of certain areas. Extensive experimental results show that our proposed framework significantly outperforms existing approaches in both motion and video-related evaluations. Our code, demos, and more resources are available at <a href=https://github.com/thuhcsi/S2G-MDDiffusion>https://github.com/thuhcsi/S2G-MDDiffusion</a>.</p></p class="citation"></blockquote><h3 id=4882--150327-upsample-guidance-scale-up-diffusion-models-without-training-juno-hwang-et-al-2024>(48/82 | 150/327) Upsample Guidance: Scale Up Diffusion Models without Training (Juno Hwang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juno Hwang, Yong-Hyun Park, Junghyo Jo. (2024)<br><strong>Upsample Guidance: Scale Up Diffusion Models without Training</strong><br><button class=copy-to-clipboard title="Upsample Guidance: Scale Up Diffusion Models without Training" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01709v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01709v1.pdf filename=2404.01709v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have demonstrated superior performance across various generative tasks including images, videos, and audio. However, they encounter difficulties in directly generating high-resolution samples. Previously proposed solutions to this issue involve modifying the architecture, further training, or partitioning the sampling process into multiple stages. These methods have the limitation of not being able to directly utilize pre-trained models as-is, requiring additional work. In this paper, we introduce upsample guidance, a technique that adapts pretrained <b>diffusion</b> <b>model</b> (e.g., $512^2$) to generate higher-resolution images (e.g., $1536^2$) by adding only a single term in the sampling process. Remarkably, this technique does not necessitate any additional training or relying on external models. We demonstrate that upsample guidance can be applied to various models, such as pixel-space, latent space, and video <b>diffusion</b> <b>models.</b> We also observed that the proper selection of guidance scale can improve image quality, fidelity, and <b>prompt</b> alignment.</p></p class="citation"></blockquote><h3 id=4982--151327-learning-to-control-camera-exposure-via-reinforcement-learning-kyunghyun-lee-et-al-2024>(49/82 | 151/327) Learning to Control Camera Exposure via Reinforcement Learning (Kyunghyun Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyunghyun Lee, Ukcheol Shin, Byeong-Uk Lee. (2024)<br><strong>Learning to Control Camera Exposure via Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Learning to Control Camera Exposure via Reinforcement Learning" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs-RO, cs-SY, cs.CV, eess-SY<br>Keyword Score: 20<br>Keywords: Object Detection, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01636v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01636v1.pdf filename=2404.01636v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adjusting camera exposure in arbitrary lighting conditions is the first step to ensure the functionality of computer vision applications. Poorly adjusted camera exposure often leads to critical failure and performance degradation. Traditional camera exposure control methods require multiple convergence steps and time-consuming processes, making them unsuitable for dynamic lighting conditions. In this paper, we propose a new camera exposure control framework that rapidly controls camera exposure while performing real-time processing by exploiting deep <b>reinforcement</b> <b>learning.</b> The proposed framework consists of four contributions: 1) a simplified training ground to simulate real-world&rsquo;s diverse and dynamic lighting changes, 2) flickering and image attribute-aware reward design, along with lightweight state design for real-time processing, 3) a static-to-dynamic lighting curriculum to gradually improve the agent&rsquo;s exposure-adjusting capability, and 4) domain randomization techniques to alleviate the limitation of the training ground and achieve seamless generalization in the wild.As a result, our proposed method rapidly reaches a desired exposure level within five steps with real-time processing (1 ms). Also, the acquired images are well-exposed and show superiority in various computer vision tasks, such as feature extraction and <b>object</b> <b>detection.</b></p></p class="citation"></blockquote><h3 id=5082--152327-learning-equi-angular-representations-for-online-continual-learning-minhyuk-seo-et-al-2024>(50/82 | 152/327) Learning Equi-angular Representations for Online Continual Learning (Minhyuk Seo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minhyuk Seo, Hyunseo Koh, Wonje Jeung, Minjae Lee, San Kim, Hankook Lee, Sungjun Cho, Sungik Choi, Hyunwoo Kim, Jonghyun Choi. (2024)<br><strong>Learning Equi-angular Representations for Online Continual Learning</strong><br><button class=copy-to-clipboard title="Learning Equi-angular Representations for Online Continual Learning" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Continual Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01628v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01628v1.pdf filename=2404.01628v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Online <b>continual</b> <b>learning</b> suffers from an underfitted solution due to insufficient training for <b>prompt</b> model update (e.g., single-epoch training). To address the challenge, we propose an efficient online <b>continual</b> <b>learning</b> method using the neural collapse phenomenon. In particular, we induce neural collapse to form a simplex equiangular tight frame (ETF) structure in the representation space so that the continuously learned model with a single epoch can better fit to the streamed data by proposing preparatory data training and residual correction in the representation space. With an extensive set of empirical validations using CIFAR-10/100, TinyImageNet, ImageNet-200, and ImageNet-1K, we show that our proposed method outperforms state-of-the-art methods by a noticeable margin in various online <b>continual</b> <b>learning</b> scenarios such as disjoint and Gaussian scheduled continuous (i.e., boundary-free) data setups.</p></p class="citation"></blockquote><h3 id=5182--153327-lr-fpn-enhancing-remote-sensing-object-detection-with-location-refined-feature-pyramid-network-hanqian-li-et-al-2024>(51/82 | 153/327) LR-FPN: Enhancing Remote Sensing Object Detection with Location Refined Feature Pyramid Network (Hanqian Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanqian Li, Ruinan Zhang, Ye Pan, Junchi Ren, Fei Shen. (2024)<br><strong>LR-FPN: Enhancing Remote Sensing Object Detection with Location Refined Feature Pyramid Network</strong><br><button class=copy-to-clipboard title="LR-FPN: Enhancing Remote Sensing Object Detection with Location Refined Feature Pyramid Network" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01614v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01614v1.pdf filename=2404.01614v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remote sensing target detection aims to identify and locate critical targets within remote sensing images, finding extensive applications in agriculture and urban planning. Feature pyramid networks (FPNs) are commonly used to extract multi-scale features. However, existing FPNs often overlook extracting low-level positional <b>information</b> <b>and</b> fine-grained context interaction. To address this, we propose a novel location refined feature pyramid network (LR-FPN) to enhance the extraction of shallow positional <b>information</b> <b>and</b> facilitate fine-grained context interaction. The LR-FPN consists of two primary modules: the shallow position <b>information</b> <b>extraction</b> module (SPIEM) and the contextual interaction module (CIM). Specifically, SPIEM first maximizes the retention of solid location <b>information</b> <b>of</b> the target by simultaneously extracting positional and saliency <b>information</b> <b>from</b> the low-level feature map. Subsequently, CIM injects this robust location <b>information</b> <b>into</b> different layers of the original FPN through spatial and channel interaction, explicitly enhancing the <b>object</b> <b>area.</b> Moreover, in spatial interaction, we introduce a simple local and non-local interaction strategy to learn and retain the saliency <b>information</b> <b>of</b> the <b>object.</b> <b>Lastly,</b> the LR-FPN can be readily integrated into common <b>object</b> <b>detection</b> frameworks to improve performance significantly. Extensive experiments on two large-scale remote sensing datasets (i.e., DOTAV1.0 and HRSC2016) demonstrate that the proposed LR-FPN is superior to state-of-the-art <b>object</b> <b>detection</b> approaches. Our code and models will be publicly available.</p></p class="citation"></blockquote><h3 id=5282--154327-tscm-a-teacher-student-model-for-vision-place-recognition-using-cross-metric-knowledge-distillation-yehui-shen-et-al-2024>(52/82 | 154/327) TSCM: A Teacher-Student Model for Vision Place Recognition Using Cross-Metric Knowledge Distillation (Yehui Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yehui Shen, Mingmin Liu, Huimin Lu, Xieyuanli Chen. (2024)<br><strong>TSCM: A Teacher-Student Model for Vision Place Recognition Using Cross-Metric Knowledge Distillation</strong><br><button class=copy-to-clipboard title="TSCM: A Teacher-Student Model for Vision Place Recognition Using Cross-Metric Knowledge Distillation" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01587v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01587v1.pdf filename=2404.01587v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual place recognition (VPR) plays a pivotal role in autonomous exploration and navigation of mobile robots within complex outdoor environments. While cost-effective and easily deployed, camera sensors are sensitive to lighting and weather changes, and even slight image alterations can greatly affect VPR efficiency and precision. Existing methods overcome this by exploiting powerful yet large networks, leading to significant consumption of computational resources. In this paper, we propose a high-performance teacher and lightweight student <b>distillation</b> framework called TSCM. It exploits our devised cross-metric <b>knowledge</b> <b>distillation</b> to narrow the performance gap between the teacher and student models, maintaining superior performance while enabling minimal computational load during deployment. We conduct comprehensive evaluations on large-scale datasets, namely Pittsburgh30k and Pittsburgh250k. Experimental results demonstrate the superiority of our method over baseline models in terms of recognition accuracy and model parameter efficiency. Moreover, our ablation studies show that the proposed <b>knowledge</b> <b>distillation</b> technique surpasses other counterparts. The code of our method has been released at <a href=https://github.com/nubot-nudt/TSCM>https://github.com/nubot-nudt/TSCM</a>.</p></p class="citation"></blockquote><h3 id=5382--155327-prego-online-mistake-detection-in-procedural-egocentric-videos-alessandro-flaborea-et-al-2024>(53/82 | 155/327) PREGO: online mistake detection in PRocedural EGOcentric videos (Alessandro Flaborea et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alessandro Flaborea, Guido Maria D&rsquo;Amely di Melendugno, Leonardo Plini, Luca Scofano, Edoardo De Matteis, Antonino Furnari, Giovanni Maria Farinella, Fabio Galasso. (2024)<br><strong>PREGO: online mistake detection in PRocedural EGOcentric videos</strong><br><button class=copy-to-clipboard title="PREGO: online mistake detection in PRocedural EGOcentric videos" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01933v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01933v1.pdf filename=2404.01933v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Promptly identifying procedural errors from egocentric videos in an online setting is highly challenging and valuable for detecting mistakes as soon as they happen. This capability has a wide range of applications across various fields, such as manufacturing and healthcare. The nature of procedural mistakes is open-set since novel types of failures might occur, which calls for one-class classifiers trained on correctly executed procedures. However, no technique can currently detect open-set procedural mistakes online. We propose PREGO, the first online one-class classification model for mistake detection in PRocedural EGOcentric videos. PREGO is based on an online action recognition component to model the current action, and a symbolic <b>reasoning</b> module to predict the next actions. Mistake detection is performed by comparing the recognized current action with the expected future one. We evaluate PREGO on two procedural egocentric video datasets, Assembly101 and Epic-tent, which we adapt for online <b>benchmarking</b> of procedural mistake detection to establish suitable <b>benchmarks,</b> thus defining the Assembly101-O and Epic-tent-O datasets, respectively.</p></p class="citation"></blockquote><h3 id=5482--156327-fashionengine-interactive-generation-and-editing-of-3d-clothed-humans-tao-hu-et-al-2024>(54/82 | 156/327) FashionEngine: Interactive Generation and Editing of 3D Clothed Humans (Tao Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Hu, Fangzhou Hong, Zhaoxi Chen, Ziwei Liu. (2024)<br><strong>FashionEngine: Interactive Generation and Editing of 3D Clothed Humans</strong><br><button class=copy-to-clipboard title="FashionEngine: Interactive Generation and Editing of 3D Clothed Humans" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Diffusion Model, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01655v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01655v2.pdf filename=2404.01655v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present FashionEngine, an interactive 3D human generation and editing system that allows us to design 3D digital humans in a way that aligns with how humans interact with the world, such as natural languages, visual perceptions, and hand-drawing. FashionEngine automates the 3D human production with three key components: 1) A pre-trained 3D human <b>diffusion</b> <b>model</b> that learns to model 3D humans in a semantic UV latent space from 2D image training data, which provides strong priors for diverse generation and editing tasks. 2) Multimodality-UV Space encoding the texture appearance, shape topology, and textual semantics of human clothing in a canonical UV-aligned space, which faithfully aligns the user <b>multimodal</b> inputs with the implicit UV latent space for controllable 3D human editing. The multimodality-UV space is shared across different user inputs, such as texts, images, and sketches, which enables various joint <b>multimodal</b> editing tasks. 3) Multimodality-UV Aligned Sampler learns to sample high-quality and diverse 3D humans from the <b>diffusion</b> <b>prior</b> for <b>multimodal</b> user inputs. Extensive experiments validate FashionEngine&rsquo;s state-of-the-art performance for conditional generation/editing tasks. In addition, we present an interactive user interface for our FashionEngine that enables both conditional and unconditional generation tasks, and editing tasks including pose/view/shape control, text-, image-, and sketch-driven 3D human editing and 3D virtual try-on, in a unified framework. Our project page is at: <a href=https://taohuumd.github.io/projects/FashionEngine>https://taohuumd.github.io/projects/FashionEngine</a>.</p></p class="citation"></blockquote><h3 id=5582--157327-gears-local-geometry-aware-hand-object-interaction-synthesis-keyang-zhou-et-al-2024>(55/82 | 157/327) GEARS: Local Geometry-aware Hand-object Interaction Synthesis (Keyang Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keyang Zhou, Bharat Lal Bhatnagar, Jan Eric Lenssen, Gerard Pons-moll. (2024)<br><strong>GEARS: Local Geometry-aware Hand-object Interaction Synthesis</strong><br><button class=copy-to-clipboard title="GEARS: Local Geometry-aware Hand-object Interaction Synthesis" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Geometry, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01758v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01758v2.pdf filename=2404.01758v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating realistic hand motion sequences in interaction with objects has gained increasing attention with the growing interest in digital humans. Prior work has illustrated the effectiveness of employing occupancy-based or distance-based virtual sensors to extract hand-object interaction features. Nonetheless, these methods show limited generalizability across object categories, shapes and sizes. We hypothesize that this is due to two reasons: 1) the limited expressiveness of employed virtual sensors, and 2) scarcity of available training data. To tackle this challenge, we introduce a novel joint-centered sensor designed to reason about local object <b>geometry</b> near potential interaction regions. The sensor queries for object surface points in the neighbourhood of each hand joint. As an important step towards mitigating the learning complexity, we transform the points from global frame to hand template frame and use a shared module to process sensor features of each individual joint. This is followed by a spatio-temporal <b>transformer</b> network aimed at capturing correlation among the joints in different dimensions. Moreover, we devise simple heuristic rules to augment the limited training sequences with vast static hand grasping samples. This leads to a broader spectrum of grasping types observed during training, in turn enhancing our model&rsquo;s generalization capability. We evaluate on two public datasets, GRAB and InterCap, where our method shows superiority over baselines both quantitatively and perceptually.</p></p class="citation"></blockquote><h3 id=5682--158327-a-linear-time-and-space-local-point-cloud-geometry-encoder-via-vectorized-kernel-mixture-veckm-dehao-yuan-et-al-2024>(56/82 | 158/327) A Linear Time and Space Local Point Cloud Geometry Encoder via Vectorized Kernel Mixture (VecKM) (Dehao Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dehao Yuan, Cornelia Fermüller, Tahseen Rabbani, Furong Huang, Yiannis Aloimonos. (2024)<br><strong>A Linear Time and Space Local Point Cloud Geometry Encoder via Vectorized Kernel Mixture (VecKM)</strong><br><button class=copy-to-clipboard title="A Linear Time and Space Local Point Cloud Geometry Encoder via Vectorized Kernel Mixture (VecKM)" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CG, cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Geometry, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01568v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01568v1.pdf filename=2404.01568v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose VecKM, a novel local point cloud <b>geometry</b> encoder that is descriptive, efficient and robust to noise. VecKM leverages a unique approach by vectorizing a kernel mixture to represent the local point clouds. Such representation is descriptive and robust to noise, which is supported by two theorems that confirm its ability to reconstruct and preserve the similarity of the local shape. Moreover, VecKM is the first successful attempt to reduce the computation and memory costs from $O(n^2+nKd)$ to $O(nd)$ by sacrificing a marginal constant factor, where $n$ is the size of the point cloud and $K$ is neighborhood size. The efficiency is primarily due to VecKM&rsquo;s unique factorizable property that eliminates the need of explicitly grouping points into neighborhoods. In the normal estimation task, VecKM demonstrates not only 100x faster inference speed but also strongest descriptiveness and robustness compared with existing popular encoders. In classification and segmentation tasks, integrating VecKM as a preprocessing module achieves consistently better performance than the PointNet, PointNet++, and point <b>transformer</b> baselines, and runs consistently faster by up to 10x.</p></p class="citation"></blockquote><h3 id=5782--159327-segment-any-3d-object-with-language-seungjun-lee-et-al-2024>(57/82 | 159/327) Segment Any 3D Object with Language (Seungjun Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seungjun Lee, Yuyang Zhao, Gim Hee Lee. (2024)<br><strong>Segment Any 3D Object with Language</strong><br><button class=copy-to-clipboard title="Segment Any 3D Object with Language" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 14<br>Keywords: Benchmarking, Geometry, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02157v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02157v1.pdf filename=2404.02157v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate Open-Vocabulary 3D Instance Segmentation (OV-3DIS) with free-form language instructions. Earlier works that rely on only annotated base categories for training suffer from limited generalization to unseen novel categories. Recent works mitigate poor generalizability to novel categories by generating class-agnostic masks or projecting generalized masks from 2D to 3D, but disregard semantic or <b>geometry</b> information, leading to sub-optimal performance. Instead, generating generalizable but semantic-related masks directly from 3D point clouds would result in superior outcomes. In this paper, we introduce Segment any 3D Object with LanguagE (SOLE), which is a semantic and geometric-aware visual-language learning framework with strong generalizability by generating semantic-related masks directly from 3D point clouds. Specifically, we propose a <b>multimodal</b> fusion network to incorporate <b>multimodal</b> semantics in both backbone and decoder. In addition, to align the 3D segmentation model with various language instructions and enhance the mask quality, we introduce three types of <b>multimodal</b> associations as supervision. Our SOLE outperforms previous methods by a large margin on ScanNetv2, ScanNet200, and Replica <b>benchmarks,</b> and the results are even close to the fully-supervised counterpart despite the absence of class annotations in the training. Furthermore, extensive qualitative results demonstrate the versatility of our SOLE to language instructions.</p></p class="citation"></blockquote><h3 id=5882--160327-gaitstr-gait-recognition-with-sequential-two-stream-refinement-wanrong-zheng-et-al-2024>(58/82 | 160/327) GaitSTR: Gait Recognition with Sequential Two-stream Refinement (Wanrong Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanrong Zheng, Haidong Zhu, Zhaoheng Zheng, Ram Nevatia. (2024)<br><strong>GaitSTR: Gait Recognition with Sequential Two-stream Refinement</strong><br><button class=copy-to-clipboard title="GaitSTR: Gait Recognition with Sequential Two-stream Refinement" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Graph, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02345v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02345v1.pdf filename=2404.02345v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gait recognition aims to identify a person based on their walking sequences, serving as a useful biometric modality as it can be observed from long distances without requiring cooperation from the subject. In representing a person&rsquo;s walking sequence, silhouettes and skeletons are the two primary modalities used. Silhouette sequences lack detailed part information when overlapping occurs between different body segments and are affected by carried objects and clothing. Skeletons, comprising joints and bones connecting the joints, provide more accurate part information for different segments; however, they are sensitive to occlusions and low-quality images, causing inconsistencies in frame-wise results within a sequence. In this paper, we explore the use of a two-stream representation of skeletons for gait recognition, alongside silhouettes. By fusing the combined data of silhouettes and skeletons, we refine the two-stream skeletons, joints, and bones through self-correction in <b>graph</b> <b>convolution,</b> along with cross-modal correction with temporal consistency from silhouettes. We demonstrate that with refined skeletons, the performance of the gait recognition model can achieve further improvement on public gait recognition datasets compared with state-of-the-art methods without extra annotations.</p></p class="citation"></blockquote><h3 id=5982--161327-oostraj-out-of-sight-trajectory-prediction-with-vision-positioning-denoising-haichao-zhang-et-al-2024>(59/82 | 161/327) OOSTraj: Out-of-Sight Trajectory Prediction With Vision-Positioning Denoising (Haichao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haichao Zhang, Yi Xu, Hongsheng Lu, Takayuki Shimizu, Yun Fu. (2024)<br><strong>OOSTraj: Out-of-Sight Trajectory Prediction With Vision-Positioning Denoising</strong><br><button class=copy-to-clipboard title="OOSTraj: Out-of-Sight Trajectory Prediction With Vision-Positioning Denoising" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs-RO, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02227v1.pdf filename=2404.02227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trajectory prediction is fundamental in computer vision and autonomous driving, particularly for understanding pedestrian behavior and enabling proactive decision-making. Existing approaches in this field often assume precise and complete observational data, neglecting the challenges associated with out-of-view objects and the noise inherent in sensor data due to limited camera range, physical obstructions, and the absence of ground truth for denoised sensor data. Such oversights are critical safety concerns, as they can result in missing essential, non-visible objects. To bridge this gap, we present a novel method for out-of-sight trajectory prediction that leverages a vision-positioning technique. Our approach denoises noisy sensor observations in an <b>unsupervised</b> manner and precisely maps sensor-based trajectories of out-of-sight objects into visual trajectories. This method has demonstrated state-of-the-art performance in out-of-sight noisy sensor trajectory denoising and prediction on the Vi-Fi and JRDB datasets. By enhancing trajectory prediction accuracy and addressing the challenges of out-of-sight objects, our work significantly contributes to improving the safety and reliability of autonomous driving in complex environments. Our work represents the first initiative towards Out-Of-Sight Trajectory prediction (OOSTraj), setting a new <b>benchmark</b> for future research. The code is available at \url{https://github.com/Hai-chao-Zhang/OOSTraj}.</p></p class="citation"></blockquote><h3 id=6082--162327-boosting-visual-recognition-for-autonomous-driving-in-real-world-degradations-with-deep-channel-prior-zhanwen-liu-et-al-2024>(60/82 | 162/327) Boosting Visual Recognition for Autonomous Driving in Real-world Degradations with Deep Channel Prior (Zhanwen Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhanwen Liu, Yuhang Li, Yang Wang, Bolin Gao, Yisheng An, Xiangmo Zhao. (2024)<br><strong>Boosting Visual Recognition for Autonomous Driving in Real-world Degradations with Deep Channel Prior</strong><br><button class=copy-to-clipboard title="Boosting Visual Recognition for Autonomous Driving in Real-world Degradations with Deep Channel Prior" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01703v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01703v1.pdf filename=2404.01703v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The environmental perception of autonomous vehicles in normal conditions have achieved considerable success in the past decade. However, various unfavourable conditions such as fog, low-light, and motion blur will degrade image quality and pose tremendous threats to the safety of autonomous driving. That is, when applied to degraded images, state-of-the-art visual models often suffer performance decline due to the feature content loss and artifact interference caused by statistical and structural properties disruption of captured images. To address this problem, this work proposes a novel Deep Channel Prior (DCP) for degraded visual recognition. Specifically, we observe that, in the deep representation space of pre-trained models, the channel correlations of degraded features with the same degradation type have uniform distribution even if they have different content and semantics, which can facilitate the mapping relationship learning between degraded and clear representations in high-sparsity feature space. Based on this, a novel plug-and-play <b>Unsupervised</b> Feature Enhancement Module (UFEM) is proposed to achieve <b>unsupervised</b> feature correction, where the multi-adversarial mechanism is introduced in the first stage of UFEM to achieve the latent content restoration and artifact removal in high-sparsity feature space. Then, the generated features are transferred to the second stage for global correlation modulation under the guidance of DCP to obtain high-quality and recognition-friendly features. Evaluations of three tasks and eight <b>benchmark</b> datasets demonstrate that our proposed method can comprehensively improve the performance of pre-trained models in real degradation conditions. The source code is available at <a href=https://github.com/liyuhang166/Deep_Channel_Prior>https://github.com/liyuhang166/Deep_Channel_Prior</a></p></p class="citation"></blockquote><h3 id=6182--163327-refqsr-reference-based-quantization-for-image-super-resolution-networks-hongjae-lee-et-al-2024>(61/82 | 163/327) RefQSR: Reference-based Quantization for Image Super-Resolution Networks (Hongjae Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongjae Lee, Jun-Sang Yoo, Seung-Won Jung. (2024)<br><strong>RefQSR: Reference-based Quantization for Image Super-Resolution Networks</strong><br><button class=copy-to-clipboard title="RefQSR: Reference-based Quantization for Image Super-Resolution Networks" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Clustering, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01690v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01690v1.pdf filename=2404.01690v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Single image super-resolution (SISR) aims to reconstruct a high-resolution image from its low-resolution observation. Recent deep learning-based SISR models show high performance at the expense of increased computational costs, limiting their use in resource-constrained environments. As a promising solution for computationally efficient network design, network <b>quantization</b> has been extensively studied. However, existing <b>quantization</b> methods developed for SISR have yet to effectively exploit image self-similarity, which is a new direction for exploration in this study. We introduce a novel method called reference-based <b>quantization</b> for image super-resolution (RefQSR) that applies high-bit <b>quantization</b> to several representative patches and uses them as references for low-bit <b>quantization</b> of the rest of the patches in an image. To this end, we design dedicated patch <b>clustering</b> and reference-based <b>quantization</b> modules and integrate them into existing SISR network <b>quantization</b> methods. The experimental results demonstrate the effectiveness of RefQSR on various SISR networks and <b>quantization</b> methods.</p></p class="citation"></blockquote><h3 id=6282--164327-one-noise-to-rule-them-all-multi-view-adversarial-attacks-with-universal-perturbation-mehmet-ergezer-et-al-2024>(62/82 | 164/327) One Noise to Rule Them All: Multi-View Adversarial Attacks with Universal Perturbation (Mehmet Ergezer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mehmet Ergezer, Phat Duong, Christian Green, Tommy Nguyen, Abdurrahman Zeybey. (2024)<br><strong>One Noise to Rule Them All: Multi-View Adversarial Attacks with Universal Perturbation</strong><br><button class=copy-to-clipboard title="One Noise to Rule Them All: Multi-View Adversarial Attacks with Universal Perturbation" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02287v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02287v1.pdf filename=2404.02287v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel universal perturbation method for generating robust multi-view <b>adversarial</b> <b>examples</b> in 3D object recognition. Unlike conventional attacks limited to single views, our approach operates on multiple 2D images, offering a practical and scalable solution for enhancing model scalability and robustness. This generalizable method bridges the gap between 2D perturbations and 3D-like attack capabilities, making it suitable for real-world applications. Existing <b>adversarial</b> <b>attacks</b> may become ineffective when images undergo transformations like changes in lighting, camera position, or natural deformations. We address this challenge by crafting a single universal noise perturbation applicable to various object views. Experiments on diverse rendered 3D objects demonstrate the effectiveness of our approach. The universal perturbation successfully identified a single <b>adversarial</b> <b>noise</b> for each given set of 3D object renders from multiple poses and viewpoints. Compared to single-view attacks, our universal attacks lower classification confidence across multiple viewing angles, especially at low noise levels. A sample implementation is made available at <a href=https://github.com/memoatwit/UniversalPerturbation>https://github.com/memoatwit/UniversalPerturbation</a>.</p></p class="citation"></blockquote><h3 id=6382--165327-smooth-deep-saliency-rudolf-herdt-et-al-2024>(63/82 | 165/327) Smooth Deep Saliency (Rudolf Herdt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rudolf Herdt, Maximilian Schmidt, Daniel Otero Baguer, Peter Maaß. (2024)<br><strong>Smooth Deep Saliency</strong><br><button class=copy-to-clipboard title="Smooth Deep Saliency" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02282v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02282v2.pdf filename=2404.02282v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we investigate methods to reduce the noise in deep saliency maps coming from <b>convolutional</b> downsampling, with the purpose of explaining how a deep learning model detects tumors in scanned histological tissue samples. Those methods make the investigated models more interpretable for gradient-based saliency maps, computed in hidden layers. We test our approach on different models trained for image classification on ImageNet1K, and models trained for tumor detection on Camelyon16 and in-house real-world digital pathology scans of stained tissue samples. Our results show that the checkerboard noise in the gradient gets reduced, resulting in smoother and therefore easier to interpret saliency maps.</p></p class="citation"></blockquote><h3 id=6482--166327-visual-concept-connectome-vcc-open-world-concept-discovery-and-their-interlayer-connections-in-deep-models-matthew-kowal-et-al-2024>(64/82 | 166/327) Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models (Matthew Kowal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthew Kowal, Richard P. Wildes, Konstantinos G. Derpanis. (2024)<br><strong>Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models</strong><br><button class=copy-to-clipboard title="Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02233v1.pdf filename=2404.02233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding what deep network models capture in their learned representations is a fundamental challenge in computer vision. We present a new methodology to understanding such vision models, the Visual Concept Connectome (VCC), which discovers human interpretable concepts and their interlayer connections in a fully <b>unsupervised</b> manner. Our approach simultaneously reveals fine-grained concepts at a layer, connection weightings across all layers and is amendable to global analysis of network structure (e.g., branching pattern of hierarchical concept assemblies). Previous work yielded ways to extract interpretable concepts from single layers and examine their impact on classification, but did not afford multilayer concept analysis across an entire network architecture. Quantitative and qualitative empirical results show the effectiveness of VCCs in the domain of image classification. Also, we leverage VCCs for the application of failure mode debugging to reveal where mistakes arise in deep networks.</p></p class="citation"></blockquote><h3 id=6582--167327-chosen-contrastive-hypothesis-selection-for-multi-view-depth-refinement-di-qiu-et-al-2024>(65/82 | 167/327) CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement (Di Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Di Qiu, Yinda Zhang, Thabo Beeler, Vladimir Tankovich, Christian Häne, Sean Fanello, Christoph Rhemann, Sergio Orts Escolano. (2024)<br><strong>CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement</strong><br><button class=copy-to-clipboard title="CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02225v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02225v1.pdf filename=2404.02225v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose CHOSEN, a simple yet flexible, robust and effective multi-view depth refinement framework. It can be employed in any existing multi-view stereo pipeline, with straightforward generalization capability for different multi-view capture systems such as camera relative positioning and lenses. Given an initial depth estimation, CHOSEN iteratively re-samples and selects the best hypotheses, and automatically adapts to different metric or intrinsic scales determined by the capture system. The key to our approach is the application of <b>contrastive</b> <b>learning</b> in an appropriate solution space and a carefully designed hypothesis feature, based on which positive and negative hypotheses can be effectively distinguished. Integrated in a simple baseline multi-view stereo pipeline, CHOSEN delivers impressive quality in terms of depth and normal accuracy compared to many current deep learning based multi-view stereo pipelines.</p></p class="citation"></blockquote><h3 id=6682--168327-geneavatar-generic-expression-aware-volumetric-head-avatar-editing-from-a-single-image-chong-bao-et-al-2024>(66/82 | 168/327) GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image (Chong Bao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui. (2024)<br><strong>GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image</strong><br><button class=copy-to-clipboard title="GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02152v1.pdf filename=2404.02152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification <b>distillation</b> scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: <a href=https://zju3dv.github.io/geneavatar/>https://zju3dv.github.io/geneavatar/</a></p></p class="citation"></blockquote><h3 id=6782--169327-adaptive-feature-fusion-neural-network-for-glaucoma-segmentation-on-unseen-fundus-images-jiyuan-zhong-et-al-2024>(67/82 | 169/327) Adaptive Feature Fusion Neural Network for Glaucoma Segmentation on Unseen Fundus Images (Jiyuan Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiyuan Zhong, Hu Ke, Ming Yan. (2024)<br><strong>Adaptive Feature Fusion Neural Network for Glaucoma Segmentation on Unseen Fundus Images</strong><br><button class=copy-to-clipboard title="Adaptive Feature Fusion Neural Network for Glaucoma Segmentation on Unseen Fundus Images" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02084v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02084v1.pdf filename=2404.02084v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fundus image segmentation on unseen domains is challenging, especially for the over-parameterized deep models trained on the small medical datasets. To address this challenge, we propose a method named Adaptive Feature-fusion Neural Network (AFNN) for glaucoma segmentation on unseen domains, which mainly consists of three modules: domain adaptor, feature-fusion network, and <b>self-supervised</b> multi-task learning. Specifically, the domain adaptor helps the pretrained-model fast adapt from other image domains to the medical fundus image domain. Feature-fusion network and <b>self-supervised</b> multi-task learning for the encoder and decoder are introduced to improve the domain generalization ability. In addition, we also design the weighted-dice-loss to improve model performance on complex optic-cup segmentation tasks. Our proposed method achieves a competitive performance over existing fundus segmentation methods on four public glaucoma datasets.</p></p class="citation"></blockquote><h3 id=6882--170327-nerfcodec-neural-feature-compression-meets-neural-radiance-fields-for-memory-efficient-scene-representation-sicheng-li-et-al-2024>(68/82 | 170/327) NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation (Sicheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sicheng Li, Hao Li, Yiyi Liao, Lu Yu. (2024)<br><strong>NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation</strong><br><button class=copy-to-clipboard title="NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02185v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02185v1.pdf filename=2404.02185v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene modeling and novel-view synthesis. As a kind of visual media for 3D scene representation, compression with high rate-distortion performance is an eternal target. Motivated by advances in neural compression and neural field representation, we propose NeRFCodec, an end-to-end NeRF compression framework that integrates non-linear transform, <b>quantization,</b> and entropy coding for memory-efficient scene representation. Since training a non-linear transform directly on a large scale of NeRF feature planes is impractical, we discover that pre-trained neural 2D image codec can be utilized for compressing the features when adding content-specific parameters. Specifically, we reuse neural 2D image codec but modify its encoder and decoder heads, while keeping the other parts of the pre-trained decoder frozen. This allows us to train the full pipeline via supervision of rendering loss and entropy loss, yielding the rate-distortion balance by updating the content-specific parameters. At test time, the bitstreams containing latent code, feature decoder head, and other side information are transmitted for communication. Experimental results demonstrate our method outperforms existing NeRF compression methods, enabling high-quality novel view synthesis with a memory budget of 0.5 MB.</p></p class="citation"></blockquote><h3 id=6982--171327-cam-based-methods-can-see-through-walls-magamed-taimeskhanov-et-al-2024>(69/82 | 171/327) CAM-Based Methods Can See through Walls (Magamed Taimeskhanov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Magamed Taimeskhanov, Ronan Sicre, Damien Garreau. (2024)<br><strong>CAM-Based Methods Can See through Walls</strong><br><button class=copy-to-clipboard title="CAM-Based Methods Can See through Walls" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01964v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01964v1.pdf filename=2404.01964v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>CAM-based methods are widely-used post-hoc interpretability method that produce a saliency map to explain the decision of an image classification model. The saliency map highlights the important areas of the image relevant to the prediction. In this paper, we show that most of these methods can incorrectly attribute an important score to parts of the image that the model cannot see. We show that this phenomenon occurs both theoretically and experimentally. On the theory side, we analyze the behavior of GradCAM on a simple masked <b>CNN</b> model at initialization. Experimentally, we train a VGG-like model constrained to not use the lower part of the image and nevertheless observe positive scores in the unseen part of the image. This behavior is evaluated quantitatively on two new datasets. We believe that this is problematic, potentially leading to mis-interpretation of the model&rsquo;s behavior.</p></p class="citation"></blockquote><h3 id=7082--172327-improving-birds-eye-view-semantic-segmentation-by-task-decomposition-tianhao-zhao-et-al-2024>(70/82 | 172/327) Improving Bird&rsquo;s Eye View Semantic Segmentation by Task Decomposition (Tianhao Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianhao Zhao, Yongcan Chen, Yu Wu, Tianyang Liu, Bo Du, Peilun Xiao, Shi Qiu, Hongda Yang, Guozhen Li, Yi Yang, Yutian Lin. (2024)<br><strong>Improving Bird&rsquo;s Eye View Semantic Segmentation by Task Decomposition</strong><br><button class=copy-to-clipboard title="Improving Bird's Eye View Semantic Segmentation by Task Decomposition" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01925v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01925v1.pdf filename=2404.01925v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic segmentation in bird&rsquo;s eye view (BEV) plays a crucial role in autonomous driving. Previous methods usually follow an end-to-end pipeline, directly predicting the BEV segmentation map from monocular RGB inputs. However, the challenge arises when the RGB inputs and BEV targets from distinct perspectives, making the direct point-to-point predicting hard to optimize. In this paper, we decompose the original BEV segmentation task into two stages, namely BEV map reconstruction and RGB-BEV feature alignment. In the first stage, we train a BEV <b>autoencoder</b> to reconstruct the BEV segmentation maps given corrupted noisy latent representation, which urges the decoder to learn fundamental knowledge of typical BEV patterns. The second stage involves mapping RGB input images into the BEV latent space of the first stage, directly optimizing the correlations between the two views at the feature level. Our approach simplifies the complexity of combining perception and generation into distinct steps, equipping the model to handle intricate and challenging scenes effectively. Besides, we propose to transform the BEV segmentation map from the Cartesian to the polar coordinate system to establish the column-wise correspondence between RGB images and BEV maps. Moreover, our method requires neither multi-scale features nor camera intrinsic parameters for depth estimation and saves computational overhead. Extensive experiments on nuScenes and Argoverse show the effectiveness and efficiency of our method. Code is available at <a href=https://github.com/happytianhao/TaDe>https://github.com/happytianhao/TaDe</a>.</p></p class="citation"></blockquote><h3 id=7182--173327-astra-an-action-spotting-transformer-for-soccer-videos-artur-xarles-et-al-2024>(71/82 | 173/327) ASTRA: An Action Spotting TRAnsformer for Soccer Videos (Artur Xarles et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Artur Xarles, Sergio Escalera, Thomas B. Moeslund, Albert Clapés. (2024)<br><strong>ASTRA: An Action Spotting TRAnsformer for Soccer Videos</strong><br><button class=copy-to-clipboard title="ASTRA: An Action Spotting TRAnsformer for Soccer Videos" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01891v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01891v1.pdf filename=2404.01891v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce ASTRA, a <b>Transformer-based</b> model designed for the task of Action Spotting in soccer matches. ASTRA addresses several challenges inherent in the task and dataset, including the requirement for precise action localization, the presence of a long-tail data distribution, non-visibility in certain actions, and inherent label noise. To do so, ASTRA incorporates (a) a <b>Transformer</b> encoder-decoder architecture to achieve the desired output temporal resolution and to produce precise predictions, (b) a balanced mixup strategy to handle the long-tail distribution of the data, (c) an uncertainty-aware displacement head to capture the label variability, and (d) input audio signal to enhance detection of non-visible actions. Results demonstrate the effectiveness of ASTRA, achieving a tight Average-mAP of 66.82 on the test set. Moreover, in the SoccerNet 2023 Action Spotting challenge, we secure the 3rd position with an Average-mAP of 70.21 on the challenge set.</p></p class="citation"></blockquote><h3 id=7282--174327-super-resolution-analysis-for-landfill-waste-classification-matias-molina-et-al-2024>(72/82 | 174/327) Super-Resolution Analysis for Landfill Waste Classification (Matias Molina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matias Molina, Rita P. Ribeiro, Bruno Veloso, João Gama. (2024)<br><strong>Super-Resolution Analysis for Landfill Waste Classification</strong><br><button class=copy-to-clipboard title="Super-Resolution Analysis for Landfill Waste Classification" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01790v1.pdf filename=2404.01790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Illegal landfills are a critical issue due to their environmental, economic, and public health impacts. This study leverages aerial imagery for environmental crime monitoring. While advances in artificial intelligence and computer vision hold promise, the challenge lies in training models with high-resolution literature datasets and adapting them to open-access low-resolution images. Considering the substantial quality differences and limited annotation, this research explores the adaptability of models across these domains. Motivated by the necessity for a comprehensive evaluation of waste detection algorithms, it advocates cross-domain classification and super-resolution enhancement to analyze the impact of different image resolutions on waste classification as an evaluation to combat the proliferation of illegal landfills. We observed performance improvements by enhancing image quality but noted an influence on model sensitivity, necessitating careful threshold <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=7382--175327-a-noisy-elephant-in-the-room-is-your-out-of-distribution-detector-robust-to-label-noise-galadrielle-humblot-renaux-et-al-2024>(73/82 | 175/327) A noisy elephant in the room: Is your out-of-distribution detector robust to label noise? (Galadrielle Humblot-Renaux et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Galadrielle Humblot-Renaux, Sergio Escalera, Thomas B. Moeslund. (2024)<br><strong>A noisy elephant in the room: Is your out-of-distribution detector robust to label noise?</strong><br><button class=copy-to-clipboard title="A noisy elephant in the room: Is your out-of-distribution detector robust to label noise?" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01775v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01775v1.pdf filename=2404.01775v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability to detect unfamiliar or unexpected images is essential for safe deployment of computer vision systems. In the context of classification, the task of detecting images outside of a model&rsquo;s training domain is known as <b>out-of-distribution</b> (OOD) detection. While there has been a growing research interest in developing post-hoc OOD detection methods, there has been comparably little discussion around how these methods perform when the underlying classifier is not trained on a clean, carefully curated dataset. In this work, we take a closer look at 20 state-of-the-art OOD detection methods in the (more realistic) scenario where the labels used to train the underlying classifier are unreliable (e.g. crowd-sourced or web-scraped labels). Extensive experiments across different datasets, noise types & levels, architectures and checkpointing strategies provide insights into the effect of class label noise on OOD detection, and show that poor separation between incorrectly classified ID samples vs. OOD samples is an overlooked yet important limitation of existing methods. Code: <a href=https://github.com/glhr/ood-labelnoise>https://github.com/glhr/ood-labelnoise</a></p></p class="citation"></blockquote><h3 id=7482--176327-beyond-image-super-resolution-for-image-recognition-with-task-driven-perceptual-loss-jaeha-kim-et-al-2024>(74/82 | 176/327) Beyond Image Super-Resolution for Image Recognition with Task-Driven Perceptual Loss (Jaeha Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaeha Kim, Junghun Oh, Kyoung Mu Lee. (2024)<br><strong>Beyond Image Super-Resolution for Image Recognition with Task-Driven Perceptual Loss</strong><br><button class=copy-to-clipboard title="Beyond Image Super-Resolution for Image Recognition with Task-Driven Perceptual Loss" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01692v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01692v2.pdf filename=2404.01692v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In real-world scenarios, image recognition tasks, such as semantic segmentation and <b>object</b> <b>detection,</b> often pose greater challenges due to the lack of information available within low-resolution (LR) content. Image super-resolution (SR) is one of the promising solutions for addressing the challenges. However, due to the ill-posed property of SR, it is challenging for typical SR methods to restore task-relevant high-frequency contents, which may dilute the advantage of utilizing the SR method. Therefore, in this paper, we propose Super-Resolution for Image Recognition (SR4IR) that effectively guides the generation of SR images beneficial to achieving satisfactory image recognition performance when processing LR images. The critical component of our SR4IR is the task-driven perceptual (TDP) loss that enables the SR network to acquire task-specific knowledge from a network tailored for a specific task. Moreover, we propose a cross-quality patch mix and an alternate training framework that significantly enhances the efficacy of the TDP loss by addressing potential problems when employing the TDP loss. Through extensive experiments, we demonstrate that our SR4IR achieves outstanding task performance by generating SR images useful for a specific image recognition task, including semantic segmentation, <b>object</b> <b>detection,</b> and image classification. The implementation code is available at <a href=https://github.com/JaehaKim97/SR4IR>https://github.com/JaehaKim97/SR4IR</a>.</p></p class="citation"></blockquote><h3 id=7582--177327-learning-temporal-cues-by-predicting-objects-move-for-multi-camera-3d-object-detection-seokha-moon-et-al-2024>(75/82 | 177/327) Learning Temporal Cues by Predicting Objects Move for Multi-camera 3D Object Detection (Seokha Moon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seokha Moon, Hongbeen Park, Jungphil Kwon, Jaekoo Lee, Jinkyu Kim. (2024)<br><strong>Learning Temporal Cues by Predicting Objects Move for Multi-camera 3D Object Detection</strong><br><button class=copy-to-clipboard title="Learning Temporal Cues by Predicting Objects Move for Multi-camera 3D Object Detection" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01580v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01580v1.pdf filename=2404.01580v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In autonomous driving and robotics, there is a growing interest in utilizing short-term historical data to enhance multi-camera 3D <b>object</b> <b>detection,</b> leveraging the continuous and correlated nature of input video streams. Recent work has focused on spatially aligning BEV-based features over timesteps. However, this is often limited as its gain does not scale well with long-term past observations. To address this, we advocate for supervising a model to predict <b>objects&rsquo;</b> <b>poses</b> given past observations, thus explicitly guiding to learn <b>objects&rsquo;</b> <b>temporal</b> cues. To this end, we propose a model called DAP (Detection After Prediction), consisting of a two-branch network: (i) a branch responsible for forecasting the current <b>objects&rsquo;</b> <b>poses</b> given past observations and (ii) another branch that detects <b>objects</b> <b>based</b> on the current and past observations. The features predicting the current <b>objects</b> <b>from</b> branch (i) is fused into branch (ii) to transfer predictive knowledge. We conduct extensive experiments with the large-scale nuScenes datasets, and we observe that utilizing such predictive information significantly improves the overall detection performance. Our model can be used plug-and-play, showing consistent performance gain.</p></p class="citation"></blockquote><h3 id=7682--178327-efficient-3d-implicit-head-avatar-with-mesh-anchored-hash-table-blendshapes-ziqian-bai-et-al-2024>(76/82 | 178/327) Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes (Ziqian Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, Yinda Zhang. (2024)<br><strong>Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes</strong><br><button class=copy-to-clipboard title="Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01543v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01543v1.pdf filename=2404.01543v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a <b>CNN,</b> resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions.</p></p class="citation"></blockquote><h3 id=7782--179327-atom-level-optical-chemical-structure-recognition-with-limited-supervision-martijn-oldenhof-et-al-2024>(77/82 | 179/327) Atom-Level Optical Chemical Structure Recognition with Limited Supervision (Martijn Oldenhof et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martijn Oldenhof, Edward De Brouwer, Adam Arany, Yves Moreau. (2024)<br><strong>Atom-Level Optical Chemical Structure Recognition with Limited Supervision</strong><br><button class=copy-to-clipboard title="Atom-Level Optical Chemical Structure Recognition with Limited Supervision" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01743v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01743v1.pdf filename=2404.01743v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying the chemical structure from a graphical representation, or image, of a molecule is a challenging pattern recognition task that would greatly benefit drug development. Yet, existing methods for chemical structure recognition do not typically generalize well, and show diminished effectiveness when confronted with domains where data is sparse, or costly to generate, such as hand-drawn molecule images. To address this limitation, we propose a new chemical structure recognition tool that delivers state-of-the-art performance and can adapt to new domains with a limited number of data samples and supervision. Unlike previous approaches, our method provides atom-level localization, and can therefore segment the image into the different atoms and bonds. Our model is the first model to perform OCSR with atom-level entity detection with only SMILES supervision. Through rigorous and extensive <b>benchmarking,</b> we demonstrate the preeminence of our chemical structure recognition approach in terms of data efficiency, accuracy, and atom-level entity prediction.</p></p class="citation"></blockquote><h3 id=7882--180327-specularity-factorization-for-low-light-enhancement-saurabh-saini-et-al-2024>(78/82 | 180/327) Specularity Factorization for Low-Light Enhancement (Saurabh Saini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saurabh Saini, P J Narayanan. (2024)<br><strong>Specularity Factorization for Low-Light Enhancement</strong><br><button class=copy-to-clipboard title="Specularity Factorization for Low-Light Enhancement" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01998v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01998v1.pdf filename=2404.01998v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a new additive image factorization technique that treats images to be composed of multiple latent specular components which can be simply estimated recursively by modulating the sparsity during decomposition. Our model-driven {\em RSFNet} estimates these factors by unrolling the optimization into network layers requiring only a few scalars to be learned. The resultant factors are interpretable by design and can be fused for different image enhancement tasks via a network or combined directly by the user in a controllable fashion. Based on RSFNet, we detail a zero-reference Low Light Enhancement (LLE) application trained without paired or unpaired supervision. Our system improves the state-of-the-art performance on standard <b>benchmarks</b> and achieves better generalization on multiple other datasets. We also integrate our factors with other task specific fusion networks for applications like deraining, deblurring and dehazing with negligible overhead thereby highlighting the multi-domain and multi-task generalizability of our proposed RSFNet. The code and data is released for reproducibility on the project homepage.</p></p class="citation"></blockquote><h3 id=7982--181327-joint-task-regularization-for-partially-labeled-multi-task-learning-kento-nishi-et-al-2024>(79/82 | 181/327) Joint-Task Regularization for Partially Labeled Multi-Task Learning (Kento Nishi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kento Nishi, Junsik Kim, Wanhua Li, Hanspeter Pfister. (2024)<br><strong>Joint-Task Regularization for Partially Labeled Multi-Task Learning</strong><br><button class=copy-to-clipboard title="Joint-Task Regularization for Partially Labeled Multi-Task Learning" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01976v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01976v1.pdf filename=2404.01976v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-task learning has become increasingly popular in the machine learning field, but its practicality is hindered by the need for large, labeled datasets. Most multi-task learning methods depend on fully labeled datasets wherein each input example is accompanied by ground-truth labels for all target tasks. Unfortunately, curating such datasets can be prohibitively expensive and impractical, especially for dense prediction tasks which require per-pixel labels for each image. With this in mind, we propose Joint-Task Regularization (JTR), an intuitive technique which leverages cross-task relations to simultaneously regularize all tasks in a single joint-task latent space to improve learning when data is not fully labeled for all tasks. JTR stands out from existing approaches in that it regularizes all tasks jointly rather than separately in pairs &ndash; therefore, it achieves linear complexity relative to the number of tasks while previous methods scale quadratically. To demonstrate the validity of our approach, we extensively <b>benchmark</b> our method across a wide variety of partially labeled scenarios based on NYU-v2, Cityscapes, and Taskonomy.</p></p class="citation"></blockquote><h3 id=8082--182327-surface-reconstruction-from-gaussian-splatting-via-novel-stereo-views-yaniv-wolf-et-al-2024>(80/82 | 182/327) Surface Reconstruction from Gaussian Splatting via Novel Stereo Views (Yaniv Wolf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaniv Wolf, Amit Bracha, Ron Kimmel. (2024)<br><strong>Surface Reconstruction from Gaussian Splatting via Novel Stereo Views</strong><br><button class=copy-to-clipboard title="Surface Reconstruction from Gaussian Splatting via Novel Stereo Views" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01810v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01810v1.pdf filename=2404.01810v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Gaussian splatting for radiance field rendering method has recently emerged as an efficient approach for accurate scene representation. It optimizes the location, size, color, and shape of a cloud of 3D Gaussian elements to visually match, after projection, or splatting, a set of given images taken from various viewing directions. And yet, despite the proximity of Gaussian elements to the shape boundaries, direct surface reconstruction of objects in the scene is a challenge. We propose a novel approach for surface reconstruction from Gaussian splatting models. Rather than relying on the Gaussian elements&rsquo; locations as a prior for surface reconstruction, we leverage the superior novel-view synthesis capabilities of 3DGS. To that end, we use the Gaussian splatting model to render pairs of stereo-calibrated novel views from which we extract depth profiles using a stereo matching method. We then combine the extracted RGB-D images into a geometrically consistent surface. The resulting reconstruction is more accurate and shows finer details when compared to other methods for surface reconstruction from Gaussian splatting models, while requiring significantly less compute time compared to other surface reconstruction methods. We performed extensive testing of the proposed method on in-the-wild scenes, taken by a smartphone, showcasing its superior reconstruction abilities. Additionally, we tested the proposed method on the Tanks and Temples <b>benchmark,</b> and it has surpassed the current leading method for surface reconstruction from Gaussian splatting models. Project page: <a href=https://gs2mesh.github.io/>https://gs2mesh.github.io/</a>.</p></p class="citation"></blockquote><h3 id=8182--183327-jrdb-panotrack-an-open-world-panoptic-segmentation-and-tracking-robotic-dataset-in-crowded-human-environments-duy-tho-le-et-al-2024>(81/82 | 183/327) JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments (Duy-Tho Le et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Duy-Tho Le, Chenhui Gou, Stavya Datta, Hengcan Shi, Ian Reid, Jianfei Cai, Hamid Rezatofighi. (2024)<br><strong>JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments</strong><br><button class=copy-to-clipboard title="JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01686v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01686v1.pdf filename=2404.01686v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous robot systems have attracted increasing research attention in recent years, where environment understanding is a crucial step for robot navigation, human-robot interaction, and decision. Real-world robot systems usually collect visual data from multiple sensors and are required to recognize numerous objects and their movements in complex human-crowded settings. Traditional <b>benchmarks,</b> with their reliance on single sensors and limited object classes and scenarios, fail to provide the comprehensive environmental understanding robots need for accurate navigation, interaction, and decision-making. As an extension of JRDB dataset, we unveil JRDB-PanoTrack, a novel open-world panoptic segmentation and tracking <b>benchmark,</b> towards more comprehensive environmental perception. JRDB-PanoTrack includes (1) various data involving indoor and outdoor crowded scenes, as well as comprehensive 2D and 3D synchronized data modalities; (2) high-quality 2D spatial panoptic segmentation and temporal tracking annotations, with additional 3D label projections for further spatial understanding; (3) diverse object classes for closed- and open-world recognition <b>benchmarks,</b> with OSPA-based metrics for evaluation. Extensive evaluation of leading methods shows significant challenges posed by our dataset.</p></p class="citation"></blockquote><h3 id=8282--184327-wavedh-wavelet-sub-bands-guided-convnet-for-efficient-image-dehazing-seongmin-hwang-et-al-2024>(82/82 | 184/327) WaveDH: Wavelet Sub-bands Guided ConvNet for Efficient Image Dehazing (Seongmin Hwang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seongmin Hwang, Daeyoung Han, Cheolkon Jung, Moongu Jeon. (2024)<br><strong>WaveDH: Wavelet Sub-bands Guided ConvNet for Efficient Image Dehazing</strong><br><button class=copy-to-clipboard title="WaveDH: Wavelet Sub-bands Guided ConvNet for Efficient Image Dehazing" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68T07, I-4-4; I-4-9, cs-CV, cs.CV, eess-IV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01604v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01604v1.pdf filename=2404.01604v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The surge in interest regarding image dehazing has led to notable advancements in deep learning-based single image dehazing approaches, exhibiting impressive performance in recent studies. Despite these strides, many existing methods fall short in meeting the efficiency demands of practical applications. In this paper, we introduce WaveDH, a novel and compact ConvNet designed to address this efficiency gap in image dehazing. Our WaveDH leverages wavelet sub-bands for guided up-and-downsampling and frequency-aware feature refinement. The key idea lies in utilizing wavelet decomposition to extract low-and-high frequency components from feature levels, allowing for faster processing while upholding high-quality reconstruction. The downsampling block employs a novel squeeze-and-attention scheme to optimize the feature downsampling process in a structurally compact manner through wavelet domain learning, preserving discriminative features while discarding noise components. In our upsampling block, we introduce a dual-upsample and fusion mechanism to enhance high-frequency component awareness, aiding in the reconstruction of high-frequency details. Departing from conventional dehazing methods that treat low-and-high frequency components equally, our feature refinement block strategically processes features with a frequency-aware approach. By employing a coarse-to-fine methodology, it not only refines the details at frequency levels but also significantly optimizes computational costs. The refinement is performed in a maximum 8x downsampled feature space, striking a favorable efficiency-vs-accuracy trade-off. Extensive experiments demonstrate that our method, WaveDH, outperforms many state-of-the-art methods on several image dehazing <b>benchmarks</b> with significantly reduced computational costs. Our code is available at <a href=https://github.com/AwesomeHwang/WaveDH>https://github.com/AwesomeHwang/WaveDH</a>.</p></p class="citation"></blockquote><h2 id=csir-6>cs.IR (6)</h2><h3 id=16--185327-where-to-move-next-zero-shot-generalization-of-llms-for-next-poi-recommendation-shanshan-feng-et-al-2024>(1/6 | 185/327) Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation (Shanshan Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shanshan Feng, Haoming Lyu, Caishun Chen, Yew-Soon Ong. (2024)<br><strong>Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation</strong><br><button class=copy-to-clipboard title="Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 70<br>Keywords: Recommendation, Zero-shot, ChatGPT, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01855v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01855v1.pdf filename=2404.01855v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Next Point-of-interest (POI) <b>recommendation</b> provides valuable suggestions for users to explore their surrounding environment. Existing studies rely on building <b>recommendation</b> models from <b>large-scale</b> <b>users&rsquo;</b> <b>check-in</b> data, which is task-specific and needs extensive computational resources. Recently, the pretrained <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved significant advancements in various NLP tasks and have also been investigated for <b>recommendation</b> scenarios. However, the generalization abilities of <b>LLMs</b> still are unexplored to address the next POI <b>recommendations,</b> where users&rsquo; geographical movement patterns should be extracted. Although there are studies that leverage <b>LLMs</b> for next-item <b>recommendations,</b> they fail to consider the geographical influence and sequential transitions. Hence, they cannot effectively solve the next POI <b>recommendation</b> task. To this end, we design novel <b>prompting</b> strategies and conduct empirical studies to assess the capability of <b>LLMs,</b> e.g., <b>ChatGPT,</b> for predicting a user&rsquo;s next check-in. Specifically, we consider several essential factors in human movement behaviors, including user geographical preference, spatial distance, and sequential transitions, and formulate the <b>recommendation</b> task as a ranking problem. Through extensive experiments on two widely used real-world datasets, we derive several key findings. Empirical evaluations demonstrate that <b>LLMs</b> have promising <b>zero-shot</b> <b>recommendation</b> abilities and can provide accurate and reasonable predictions. We also reveal that <b>LLMs</b> cannot accurately comprehend geographical context information and are sensitive to the order of presentation of candidate POIs, which shows the limitations of <b>LLMs</b> and necessitates further research on robust human mobility <b>reasoning</b> mechanisms.</p></p class="citation"></blockquote><h3 id=26--186327-iisan-efficiently-adapting-multimodal-representation-for-sequential-recommendation-with-decoupled-peft-junchen-fu-et-al-2024>(2/6 | 186/327) IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT (Junchen Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junchen Fu, Xuri Ge, Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, Jie Wang, Joemon M Jose. (2024)<br><strong>IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT</strong><br><button class=copy-to-clipboard title="IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CV, cs-IR, cs.IR<br>Keyword Score: 51<br>Keywords: Fine-tuning, Foundation Model, Multi-modal, Multi-modal, Recommendation, Recommender System, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02059v1.pdf filename=2404.02059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>foundation</b> <b>models</b> are transformative in sequential <b>recommender</b> <b>systems,</b> leveraging powerful <b>representation</b> <b>learning</b> capabilities. While Parameter-efficient <b>Fine-tuning</b> (PEFT) is commonly used to adapt <b>foundation</b> <b>models</b> for <b>recommendation</b> tasks, most research prioritizes parameter efficiency, often overlooking critical factors like GPU memory efficiency and training speed. Addressing this gap, our paper introduces IISAN (Intra- and Inter-modal Side Adapted Network for <b>Multimodal</b> <b>Representation),</b> <b>a</b> simple plug-and-play architecture using a Decoupled PEFT structure and exploiting both intra- and inter-modal adaptation. IISAN matches the performance of full <b>fine-tuning</b> (FFT) and state-of-the-art PEFT. More importantly, it significantly reduces GPU memory usage - from 47GB to just 3GB for <b>multimodal</b> sequential <b>recommendation</b> tasks. Additionally, it accelerates training time per epoch from 443s to 22s compared to FFT. This is also a notable improvement over the Adapter and LoRA, which require 37-39 GB GPU memory and 350-380 seconds per epoch for training. Furthermore, we propose a new composite efficiency metric, TPME (Training-time, Parameter, and GPU Memory Efficiency) to alleviate the prevalent misconception that &ldquo;parameter efficiency represents overall efficiency&rdquo;. TPME provides more comprehensive insights into practical efficiency comparisons between different methods. Besides, we give an accessible efficiency analysis of all PEFT and FFT approaches, which demonstrate the superiority of IISAN. We release our codes and other materials at <a href=https://github.com/jjGenAILab/IISAN>https://github.com/jjGenAILab/IISAN</a>.</p></p class="citation"></blockquote><h3 id=36--187327-cirp-cross-item-relational-pre-training-for-multimodal-product-bundling-yunshan-ma-et-al-2024>(3/6 | 187/327) CIRP: Cross-Item Relational Pre-training for Multimodal Product Bundling (Yunshan Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunshan Ma, Yingzhi He, Wenjun Zhong, Xiang Wang, Roger Zimmermann, Tat-Seng Chua. (2024)<br><strong>CIRP: Cross-Item Relational Pre-training for Multimodal Product Bundling</strong><br><button class=copy-to-clipboard title="CIRP: Cross-Item Relational Pre-training for Multimodal Product Bundling" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: H-3-0, cs-IR, cs-MM, cs.IR<br>Keyword Score: 34<br>Keywords: Graph, Multi-modal, Multi-modal, Pruning, Representation Learning, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01735v1.pdf filename=2404.01735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Product bundling has been a prevailing marketing strategy that is beneficial in the online shopping scenario. Effective product bundling methods depend on high-quality item <b>representations,</b> <b>which</b> need to capture both the individual items&rsquo; semantics and cross-item relations. However, previous item <b>representation</b> <b>learning</b> methods, either feature fusion or <b>graph</b> learning, suffer from inadequate cross-modal alignment and struggle to capture the cross-item relations for cold-start items. <b>Multimodal</b> pre-train models could be the potential solutions given their promising performance on various <b>multimodal</b> downstream tasks. However, the cross-item relations have been under-explored in the current <b>multimodal</b> pre-train models. To bridge this gap, we propose a novel and simple framework Cross-Item Relational Pre-training (CIRP) for item <b>representation</b> <b>learning</b> in product bundling. Specifically, we employ a <b>multimodal</b> encoder to generate image and text <b>representations.</b> <b>Then</b> we leverage both the cross-item contrastive loss (CIC) and individual item&rsquo;s <b>image-text</b> contrastive loss (ITC) as the pre-train objectives. Our method seeks to integrate cross-item relation modeling capability into the <b>multimodal</b> encoder, while preserving the in-depth aligned <b>multimodal</b> semantics. Therefore, even for cold-start items that have no relations, their <b>representations</b> <b>are</b> still relation-aware. Furthermore, to eliminate the potential noise and reduce the computational cost, we harness a relation <b>pruning</b> module to remove the noisy and redundant relations. We apply the item <b>representations</b> <b>extracted</b> by CIRP to the product bundling model ItemKNN, and experiments on three e-commerce datasets demonstrate that CIRP outperforms various leading <b>representation</b> <b>learning</b> methods.</p></p class="citation"></blockquote><h3 id=46--188327-multi-granular-adversarial-attacks-against-black-box-neural-ranking-models-yu-an-liu-et-al-2024>(4/6 | 188/327) Multi-granular Adversarial Attacks against Black-box Neural Ranking Models (Yu-An Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu-An Liu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng. (2024)<br><strong>Multi-granular Adversarial Attacks against Black-box Neural Ranking Models</strong><br><button class=copy-to-clipboard title="Multi-granular Adversarial Attacks against Black-box Neural Ranking Models" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CR, cs-IR, cs-LG, cs.IR<br>Keyword Score: 25<br>Keywords: Black Box, Reinforcement Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01574v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01574v1.pdf filename=2404.01574v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>ranking</b> attacks have gained increasing attention due to their success in probing vulnerabilities, and, hence, enhancing the robustness, of neural ranking models. Conventional attack methods employ perturbations at a single granularity, e.g., word-level or sentence-level, to a target document. However, limiting perturbations to a single level of granularity may reduce the flexibility of creating <b>adversarial</b> <b>examples,</b> thereby diminishing the potential threat of the attack. Therefore, we focus on generating high-quality <b>adversarial</b> <b>examples</b> by incorporating multi-granular perturbations. Achieving this objective involves tackling a combinatorial explosion problem, which requires identifying an optimal combination of perturbations across all possible levels of granularity, positions, and textual pieces. To address this challenge, we transform the multi-granular <b>adversarial</b> <b>attack</b> into a sequential decision-making process, where perturbations in the next attack step are influenced by the perturbed document in the current attack step. Since the attack process can only access the final state without direct intermediate signals, we use <b>reinforcement</b> <b>learning</b> to perform multi-granular attacks. During the <b>reinforcement</b> <b>learning</b> process, two agents work cooperatively to identify multi-granular vulnerabilities as attack targets and organize perturbation candidates into a final perturbation sequence. Experimental results show that our attack method surpasses prevailing baselines in both attack effectiveness and imperceptibility.</p></p class="citation"></blockquote><h3 id=56--189327-rat-retrieval-augmented-transformer-for-click-through-rate-prediction-yushen-li-et-al-2024>(5/6 | 189/327) RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction (Yushen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yushen Li, Jinpeng Wang, Tao Dai, Jieming Zhu, Jun Yuan, Rui Zhang, Shu-Tao Xia. (2024)<br><strong>RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction</strong><br><button class=copy-to-clipboard title="RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs-LG, cs-SI, cs.IR<br>Keyword Score: 20<br>Keywords: Transformer, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02249v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02249v2.pdf filename=2404.02249v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting click-through rates (CTR) is a fundamental task for Web applications, where a key issue is to devise effective models for feature interactions. Current methodologies predominantly concentrate on modeling feature interactions within an individual sample, while overlooking the potential cross-sample relationships that can serve as a reference context to enhance the prediction. To make up for such deficiency, this paper develops a Retrieval-Augmented <b>Transformer</b> (RAT), aiming to acquire fine-grained feature interactions within and across samples. By retrieving similar samples, we construct augmented input for each target sample. We then build <b>Transformer</b> layers with cascaded attention to capture both intra- and cross-sample feature interactions, facilitating comprehensive <b>reasoning</b> for improved CTR prediction while retaining efficiency. Extensive experiments on real-world datasets substantiate the effectiveness of RAT and suggest its advantage in long-tail scenarios. The code has been open-sourced at \url{https://github.com/YushenLi807/WWW24-RAT}.</p></p class="citation"></blockquote><h3 id=66--190327-a-survey-of-web-content-control-for-generative-ai-michael-dinzinger-et-al-2024>(6/6 | 190/327) A Survey of Web Content Control for Generative AI (Michael Dinzinger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Dinzinger, Florian Heß, Michael Granitzer. (2024)<br><strong>A Survey of Web Content Control for Generative AI</strong><br><button class=copy-to-clipboard title="A Survey of Web Content Control for Generative AI" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02309v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02309v1.pdf filename=2404.02309v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The groundbreaking advancements around <b>generative</b> <b>AI</b> have recently caused a wave of concern culminating in a row of lawsuits, including high-profile actions against Stability AI and OpenAI. This situation of legal uncertainty has sparked a broad discussion on the rights of content creators and publishers to protect their intellectual property on the web. European as well as US law already provides rough guidelines, setting a direction for technical solutions to regulate web data use. In this course, researchers and practitioners have worked on numerous web standards and opt-out formats that empower publishers to keep their data out of the development of <b>generative</b> <b>AI</b> models. The emerging AI/ML opt-out protocols are valuable in regards to data sovereignty, but again, it creates an adverse situation for a site owners who are overwhelmed by the multitude of recent ad hoc standards to consider. In our work, we want to survey the different proposals, ideas and initiatives, and provide a comprehensive legal and technical background in the context of the current discussion on web publishers control.</p></p class="citation"></blockquote><h2 id=cslg-48>cs.LG (48)</h2><h3 id=148--191327-virtual-sensor-for-real-time-bearing-load-prediction-using-heterogeneous-temporal-graph-neural-networks-mengjie-zhao-et-al-2024>(1/48 | 191/327) Virtual Sensor for Real-Time Bearing Load Prediction Using Heterogeneous Temporal Graph Neural Networks (Mengjie Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengjie Zhao, Cees Taal, Stephan Baggerohr, Olga Fink. (2024)<br><strong>Virtual Sensor for Real-Time Bearing Load Prediction Using Heterogeneous Temporal Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Virtual Sensor for Real-Time Bearing Load Prediction Using Heterogeneous Temporal Graph Neural Networks" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-ET, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02304v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02304v1.pdf filename=2404.02304v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate bearing load monitoring is essential for their Prognostics and Health Management (PHM), enabling damage assessment, wear prediction, and proactive maintenance. While bearing sensors are typically placed on the bearing housing, direct load monitoring requires sensors inside the bearing itself. Recently introduced sensor rollers enable direct bearing load monitoring but are constrained by their battery life. Data-driven virtual sensors can learn from sensor roller data collected during a batterys lifetime to map operating conditions to bearing loads. Although spatially distributed bearing sensors offer insights into load distribution (e.g., correlating temperature with load), traditional machine learning algorithms struggle to fully exploit these spatial-temporal dependencies. To address this gap, we introduce a <b>graph-based</b> <b>virtual</b> <b>sensor</b> that leverages <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> to analyze spatial-temporal dependencies among sensor signals, mapping existing measurements (temperature, vibration) to bearing loads. Since temperature and vibration signals exhibit vastly different dynamics, we propose Heterogeneous Temporal <b>Graph</b> <b>Neural</b> <b>Networks</b> (HTGNN), which explicitly models these signal types and their interactions for effective load prediction. Our results demonstrate that HTGNN outperforms <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs),</b> which struggle to capture both spatial and heterogeneous signal characteristics. These findings highlight the importance of capturing the complex spatial interactions between temperature, vibration, and load.</p></p class="citation"></blockquote><h3 id=248--192327-confidence-aware-reward-optimization-for-fine-tuning-text-to-image-models-kyuyoung-kim-et-al-2024>(2/48 | 192/327) Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models (Kyuyoung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyuyoung Kim, Jongheon Jeong, Minyong An, Mohammad Ghavamzadeh, Krishnamurthy Dvijotham, Jinwoo Shin, Kimin Lee. (2024)<br><strong>Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models</strong><br><button class=copy-to-clipboard title="Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Text2image, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01863v1.pdf filename=2404.01863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> <b>text-to-image</b> models with reward functions trained on human feedback data has proven effective for aligning model behavior with human intent. However, excessive optimization with such reward models, which serve as mere proxy objectives, can compromise the performance of <b>fine-tuned</b> models, a phenomenon known as reward overoptimization. To investigate this issue in depth, we introduce the <b>Text-Image</b> Alignment Assessment (TIA2) <b>benchmark,</b> which comprises a diverse collection of text <b>prompts,</b> images, and human annotations. Our evaluation of several state-of-the-art reward models on this <b>benchmark</b> reveals their frequent misalignment with human assessment. We empirically demonstrate that overoptimization occurs notably when a poorly aligned reward model is used as the <b>fine-tuning</b> objective. To address this, we propose TextNorm, a simple method that enhances alignment based on a measure of reward model confidence estimated across a set of semantically contrastive text <b>prompts.</b> We demonstrate that incorporating the confidence-calibrated rewards in <b>fine-tuning</b> effectively reduces overoptimization, resulting in twice as many wins in human evaluation for <b>text-image</b> alignment compared against the baseline reward models.</p></p class="citation"></blockquote><h3 id=348--193327-remote-sensing-framework-for-geological-mapping-via-stacked-autoencoders-and-clustering-sandeep-nagar-et-al-2024>(3/48 | 193/327) Remote sensing framework for geological mapping via stacked autoencoders and clustering (Sandeep Nagar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sandeep Nagar, Ehsan Farahbakhsh, Joseph Awange, Rohitash Chandra. (2024)<br><strong>Remote sensing framework for geological mapping via stacked autoencoders and clustering</strong><br><button class=copy-to-clipboard title="Remote sensing framework for geological mapping via stacked autoencoders and clustering" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Autoencoder, Clustering, Supervised Learning, Supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02180v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02180v1.pdf filename=2404.02180v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Supervised</b> <b>learning</b> methods for geological mapping via remote sensing face limitations due to the scarcity of accurately labelled training data. In contrast, <b>unsupervised</b> <b>learning</b> methods, such as dimensionality reduction and <b>clustering</b> have the ability to uncover patterns and structures in remote sensing data without relying on predefined labels. Dimensionality reduction methods have the potential to play a crucial role in improving the accuracy of geological maps. Although conventional dimensionality reduction methods may struggle with nonlinear data, <b>unsupervised</b> <b>deep</b> learning models such as <b>autoencoders</b> have the ability to model nonlinear relationship in data. Stacked <b>autoencoders</b> feature multiple interconnected layers to capture hierarchical data representations that can be useful for remote sensing data. In this study, we present an <b>unsupervised</b> <b>machine</b> learning framework for processing remote sensing data by utilizing stacked <b>autoencoders</b> for dimensionality reduction and k-means <b>clustering</b> for mapping geological units. We use the Landsat-8, ASTER, and Sentinel-2 datasets of the Mutawintji region in Western New South Wales, Australia to evaluate the framework for geological mapping. We also provide a comparison of stacked <b>autoencoders</b> with principal component analysis and canonical <b>autoencoders.</b> Our results reveal that the framework produces accurate and interpretable geological maps, efficiently discriminating rock units. We find that the stacked <b>autoencoders</b> provide better accuracy when compared to the counterparts. We also find that the generated maps align with prior geological knowledge of the study area while providing novel insights into geological structures.</p></p class="citation"></blockquote><h3 id=448--194327-predicting-the-performance-of-foundation-models-via-agreement-on-the-line-aman-mehra-et-al-2024>(4/48 | 194/327) Predicting the Performance of Foundation Models via Agreement-on-the-Line (Aman Mehra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aman Mehra, Rahul Saxena, Taeyoun Kim, Christina Baek, Zico Kolter, Aditi Raghunathan. (2024)<br><strong>Predicting the Performance of Foundation Models via Agreement-on-the-Line</strong><br><button class=copy-to-clipboard title="Predicting the Performance of Foundation Models via Agreement-on-the-Line" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Foundation Model, Out-of-distribution, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01542v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01542v1.pdf filename=2404.01542v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating the <b>out-of-distribution</b> performance in regimes where labels are scarce is critical to safely deploy <b>foundation</b> <b>models.</b> Recently, it was shown that ensembles of neural networks observe the phenomena ``agreement-on-the-line&rsquo;&rsquo;, which can be leveraged to reliably predict OOD performance without labels. However, in contrast to classical neural networks that are trained on in-distribution data from scratch for numerous epochs, <b>foundation</b> <b>models</b> undergo minimal <b>finetuning</b> from heavily pretrained weights, which may reduce the ensemble diversity needed to observe agreement-on-the-line. In our work, we demonstrate that when lightly <b>finetuning</b> multiple runs from a $\textit{single}$ <b>foundation</b> <b>model,</b> the choice of randomness during training (linear head initialization, data ordering, and data subsetting) can lead to drastically different levels of agreement-on-the-line in the resulting ensemble. Surprisingly, only random head initialization is able to reliably induce agreement-on-the-line in <b>finetuned</b> <b>foundation</b> <b>models</b> across vision and language <b>benchmarks.</b> Second, we demonstrate that ensembles of $\textit{multiple}$ <b>foundation</b> <b>models</b> pretrained on different datasets but <b>finetuned</b> on the same task can also show agreement-on-the-line. In total, by careful construction of a diverse ensemble, we can utilize agreement-on-the-line-based methods to predict the OOD performance of <b>foundation</b> <b>models</b> with high precision.</p></p class="citation"></blockquote><h3 id=548--195327-is-meta-training-really-necessary-for-molecular-few-shot-learning--philippe-formont-et-al-2024>(5/48 | 195/327) Is Meta-training Really Necessary for Molecular Few-Shot Learning ? (Philippe Formont et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philippe Formont, Hugo Jeannin, Pablo Piantanida, Ismail Ben Ayed. (2024)<br><strong>Is Meta-training Really Necessary for Molecular Few-Shot Learning ?</strong><br><button class=copy-to-clipboard title="Is Meta-training Really Necessary for Molecular Few-Shot Learning ?" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 48<br>Keywords: Benchmarking, Black Box, Few-shot, Few-shot Learning, Fine-tuning, Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02314v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02314v1.pdf filename=2404.02314v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> <b>learning</b> has recently attracted significant interest in drug discovery, with a recent, fast-growing literature mostly involving convoluted <b>meta-learning</b> <b>strategies.</b> We revisit the more straightforward <b>fine-tuning</b> approach for molecular data, and propose a regularized quadratic-probe loss based on the the Mahalanobis distance. We design a dedicated block-coordinate descent optimizer, which avoid the degenerate solutions of our loss. Interestingly, our simple <b>fine-tuning</b> approach achieves highly competitive performances in comparison to state-of-the-art methods, while being applicable to <b>black-box</b> <b>settings</b> and removing the need for specific episodic pre-training strategies. Furthermore, we introduce a new <b>benchmark</b> to assess the robustness of the competing methods to domain shifts. In this setting, our <b>fine-tuning</b> baseline obtains consistently better results than <b>meta-learning</b> <b>methods.</b></p></p class="citation"></blockquote><h3 id=648--196327-selective-temporal-knowledge-graph-reasoning-zhongni-hou-et-al-2024>(6/48 | 196/327) Selective Temporal Knowledge Graph Reasoning (Zhongni Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongni Hou, Xiaolong Jin, Zixuan Li, Long Bai, Jiafeng Guo, Xueqi Cheng. (2024)<br><strong>Selective Temporal Knowledge Graph Reasoning</strong><br><button class=copy-to-clipboard title="Selective Temporal Knowledge Graph Reasoning" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 41<br>Keywords: Graph, Benchmarking, Knowledge Graph, Reasoning, Temporal Knowledge Graph, Temporal Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01695v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01695v1.pdf filename=2404.01695v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Temporal</b> <b>Knowledge</b> <b>Graph</b> <b>(TKG),</b> which characterizes temporally evolving facts in the form of (subject, relation, object, timestamp), has attracted much attention recently. <b>TKG</b> <b>reasoning</b> aims to predict future facts based on given historical ones. However, existing <b>TKG</b> <b>reasoning</b> models are unable to abstain from predictions they are uncertain, which will inevitably bring risks in real-world applications. Thus, in this paper, we propose an abstention mechanism for <b>TKG</b> <b>reasoning,</b> which helps the existing models make selective, instead of indiscriminate, predictions. Specifically, we develop a confidence estimator, called Confidence Estimator with History (CEHis), to enable the existing <b>TKG</b> <b>reasoning</b> models to first estimate their confidence in making predictions, and then abstain from those with low confidence. To do so, CEHis takes two kinds of information into consideration, namely, the certainty of the current prediction and the accuracy of historical predictions. Experiments with representative <b>TKG</b> <b>reasoning</b> models on two <b>benchmark</b> datasets demonstrate the effectiveness of the proposed CEHis.</p></p class="citation"></blockquote><h3 id=748--197327-is-exploration-all-you-need-effective-exploration-characteristics-for-transfer-in-reinforcement-learning-jonathan-c-balloch-et-al-2024>(7/48 | 197/327) Is Exploration All You Need? Effective Exploration Characteristics for Transfer in Reinforcement Learning (Jonathan C. Balloch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan C. Balloch, Rishav Bhagat, Geigh Zollicoffer, Ruoran Jia, Julia Kim, Mark O. Riedl. (2024)<br><strong>Is Exploration All You Need? Effective Exploration Characteristics for Transfer in Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Is Exploration All You Need? Effective Exploration Characteristics for Transfer in Reinforcement Learning" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Markov Decision Process, Recommendation, Reinforcement Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02235v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02235v1.pdf filename=2404.02235v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In deep <b>reinforcement</b> <b>learning</b> (RL) research, there has been a concerted effort to design more efficient and productive exploration methods while solving sparse-reward problems. These exploration methods often share common principles (e.g., improving diversity) and implementation details (e.g., intrinsic reward). Prior work found that non-stationary Markov decision processes <b>(MDPs)</b> require exploration to efficiently adapt to changes in the environment with online <b>transfer</b> <b>learning.</b> However, the relationship between specific exploration characteristics and effective <b>transfer</b> <b>learning</b> in deep RL has not been characterized. In this work, we seek to understand the relationships between salient exploration characteristics and improved performance and efficiency in <b>transfer</b> <b>learning.</b> We test eleven popular exploration algorithms on a variety of <b>transfer</b> <b>types</b> &ndash; or ``novelties&rsquo;&rsquo; &ndash; to identify the characteristics that positively affect online <b>transfer</b> <b>learning.</b> Our analysis shows that some characteristics correlate with improved performance and efficiency across a wide range of <b>transfer</b> <b>tasks,</b> while others only improve <b>transfer</b> <b>performance</b> with respect to specific environment changes. From our analysis, make <b>recommendations</b> about which exploration algorithm characteristics are best suited to specific <b>transfer</b> <b>situations.</b></p></p class="citation"></blockquote><h3 id=848--198327-a-generative-deep-learning-approach-for-crash-severity-modeling-with-imbalanced-data-junlan-chen-et-al-2024>(8/48 | 198/327) A Generative Deep Learning Approach for Crash Severity Modeling with Imbalanced Data (Junlan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junlan Chen, Ziyuan Pu, Nan Zheng, Xiao Wen, Hongliang Ding, Xiucheng Guo. (2024)<br><strong>A Generative Deep Learning Approach for Crash Severity Modeling with Imbalanced Data</strong><br><button class=copy-to-clipboard title="A Generative Deep Learning Approach for Crash Severity Modeling with Imbalanced Data" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02187v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02187v1.pdf filename=2404.02187v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Crash data is often greatly imbalanced, with the majority of crashes being non-fatal crashes, and only a small number being fatal crashes due to their rarity. Such data imbalance issue poses a challenge for crash severity modeling since it struggles to fit and interpret fatal crash outcomes with very limited samples. Usually, such data imbalance issues are addressed by data resampling methods, such as under-sampling and over-sampling techniques. However, most traditional and deep learning-based data resampling methods, such as synthetic minority oversampling technique (SMOTE) and <b>generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GAN)</b> are designed dedicated to processing continuous variables. Though some resampling methods have improved to handle both continuous and discrete variables, they may have difficulties in dealing with the collapse issue associated with sparse discrete risk factors. Moreover, there is a lack of comprehensive studies that compare the performance of various resampling methods in crash severity modeling. To address the aforementioned issues, the current study proposes a crash data generation method based on the Conditional Tabular <b>GAN.</b> After data balancing, a crash severity model is employed to estimate the performance of classification and interpretation. A comparative study is conducted to assess classification accuracy and distribution consistency of the proposed generation method using a 4-year imbalanced crash dataset collected in Washington State, U.S. Additionally, Monte Carlo <b>simulation</b> is employed to estimate the performance of parameter and probability estimation in both two- and three-class imbalance scenarios. The results indicate that using synthetic data generated by CTGAN-RU for crash severity modeling outperforms using original data or synthetic data generated by other resampling methods.</p></p class="citation"></blockquote><h3 id=948--199327-noise-masking-attacks-and-defenses-for-pretrained-speech-models-matthew-jagielski-et-al-2024>(9/48 | 199/327) Noise Masking Attacks and Defenses for Pretrained Speech Models (Matthew Jagielski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthew Jagielski, Om Thakkar, Lun Wang. (2024)<br><strong>Noise Masking Attacks and Defenses for Pretrained Speech Models</strong><br><button class=copy-to-clipboard title="Noise Masking Attacks and Defenses for Pretrained Speech Models" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Fine-tuning, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02052v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02052v1.pdf filename=2404.02052v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Speech</b> <b>models</b> are often trained on sensitive data in order to improve model performance, leading to potential privacy leakage. Our work considers noise masking attacks, introduced by Amid et al. 2022, which attack <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> models by requesting a transcript of an utterance which is partially replaced with noise. They show that when a record has been seen at training time, the model will transcribe the noisy record with its memorized sensitive transcript. In our work, we extend these attacks beyond <b>ASR</b> models, to attack pretrained <b>speech</b> <b>encoders.</b> Our method <b>fine-tunes</b> the encoder to produce an <b>ASR</b> model, and then performs noise masking on this model, which we find recovers private information from the pretraining data, despite the model never having seen transcripts at pretraining time! We show how to improve the precision of these attacks and investigate a number of countermeasures to our attacks.</p></p class="citation"></blockquote><h3 id=1048--200327-accelerating-transformer-pre-training-with-24-sparsity-yuezhou-hu-et-al-2024>(10/48 | 200/327) Accelerating Transformer Pre-Training with 2:4 Sparsity (Yuezhou Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuezhou Hu, Kang Zhao, Weiyu Huang, Jianfei Chen, Jun Zhu. (2024)<br><strong>Accelerating Transformer Pre-Training with 2:4 Sparsity</strong><br><button class=copy-to-clipboard title="Accelerating Transformer Pre-Training with 2:4 Sparsity" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Graph Attention Networks, Convolution, Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01847v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01847v1.pdf filename=2404.01847v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training large <b>Transformers</b> is slow, but recent innovations on GPU architecture gives us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of <b>Transformers</b> in pre-training. First, we define a &ldquo;flip rate&rdquo; to monitor the stability of a 2:4 training process. Utilizing this metric, we suggest two techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the mask decay term on gradients, and to enhance the model&rsquo;s quality by a simple yet effective dense <b>fine-tuning</b> procedure near the end of pre-training. Besides, we devise two effective techniques to practically accelerate training: to calculate transposable 2:4 mask by <b>convolution,</b> and to accelerate <b>gated</b> activation functions by reducing GPU L2 cache miss. Experiments show that a combination of our methods reaches the best performance on multiple <b>Transformers</b> among different 2:4 training methods, while actual acceleration can be observed on different shapes of <b>Transformer</b> block.</p></p class="citation"></blockquote><h3 id=1148--201327-test-time-model-adaptation-with-only-forward-passes-shuaicheng-niu-et-al-2024>(11/48 | 201/327) Test-Time Model Adaptation with Only Forward Passes (Shuaicheng Niu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuaicheng Niu, Chunyan Miao, Guohao Chen, Pengcheng Wu, Peilin Zhao. (2024)<br><strong>Test-Time Model Adaptation with Only Forward Passes</strong><br><button class=copy-to-clipboard title="Test-Time Model Adaptation with Only Forward Passes" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Distribution Shift, Distribution Shift, Quantization, Unsupervised Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01650v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01650v1.pdf filename=2404.01650v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Test-time adaptation has proven effective in adapting a given trained model to unseen test samples with potential <b>distribution</b> <b>shifts.</b> However, in real-world scenarios, models are usually deployed on resource-limited devices, e.g., FPGAs, and are often <b>quantized</b> and hard-coded with non-modifiable parameters for acceleration. In light of this, existing methods are often infeasible since they heavily depend on computation-intensive backpropagation for model updating that may be not supported. To address this, we propose a test-time Forward-Only Adaptation (FOA) method. In FOA, we seek to solely learn a newly added <b>prompt</b> (as model&rsquo;s input) via a derivative-free covariance matrix adaptation evolution strategy. To make this strategy work stably under our online <b>unsupervised</b> setting, we devise a novel fitness function by measuring test-training statistic discrepancy and model prediction entropy. Moreover, we design an activation shifting scheme that directly tunes the model activations for shifted test samples, making them align with the source training domain, thereby further enhancing adaptation performance. Without using any backpropagation and altering model weights, FOA runs on <b>quantized</b> 8-bit ViT outperforms gradient-based TENT on full-precision 32-bit ViT, while achieving an up to 24-fold memory reduction on ImageNet-C. The source code will be released.</p></p class="citation"></blockquote><h3 id=1248--202327-dsgnn-a-dual-view-supergrid-aware-graph-neural-network-for-regional-air-quality-estimation-xin-zhang-et-al-2024>(12/48 | 202/327) DSGNN: A Dual-View Supergrid-Aware Graph Neural Network for Regional Air Quality Estimation (Xin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Zhang, Ling Chen, Xing Tang, Hongyu Shi. (2024)<br><strong>DSGNN: A Dual-View Supergrid-Aware Graph Neural Network for Regional Air Quality Estimation</strong><br><button class=copy-to-clipboard title="DSGNN: A Dual-View Supergrid-Aware Graph Neural Network for Regional Air Quality Estimation" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Message-Passing, Graph, Graph Neural Network, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01975v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01975v1.pdf filename=2404.01975v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Air quality estimation can provide air quality for target regions without air quality stations, which is useful for the public. Existing air quality estimation methods divide the study area into disjointed grid regions, and apply 2D <b>convolution</b> to model the spatial dependencies of adjacent grid regions based on the first law of geography, failing to model the spatial dependencies of distant grid regions. To this end, we propose a Dual-view Supergrid-aware <b>Graph</b> <b>Neural</b> <b>Network</b> (DSGNN) for regional air quality estimation, which can model the spatial dependencies of distant grid regions from dual views (i.e., satellite-derived aerosol optical depth (AOD) and meteorology). Specifically, images are utilized to represent the regional data (i.e., AOD data and meteorology data). The dual-view supergrid learning module is introduced to generate supergrids in a parameterized way. Based on the dual-view supergrids, the dual-view implicit correlation encoding module is introduced to learn the correlations between pairwise supergrids. In addition, the dual-view message passing network is introduced to implement the information interaction on the supergrid <b>graphs</b> <b>and</b> <b>images.</b> Extensive experiments on two real-world datasets demonstrate that DSGNN achieves the state-of-the-art performances on the air quality estimation task, outperforming the best baseline by an average of 19.64% in MAE.</p></p class="citation"></blockquote><h3 id=1348--203327-heat-death-of-generative-models-in-closed-loop-learning-matteo-marchi-et-al-2024>(13/48 | 203/327) Heat Death of Generative Models in Closed-Loop Learning (Matteo Marchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matteo Marchi, Stefano Soatto, Pratik Chaudhari, Paulo Tabuada. (2024)<br><strong>Heat Death of Generative Models in Closed-Loop Learning</strong><br><button class=copy-to-clipboard title="Heat Death of Generative Models in Closed-Loop Learning" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY, math-OC<br>Keyword Score: 30<br>Keywords: Diffusion Model, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02325v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02325v1.pdf filename=2404.02325v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Improvement and adoption of generative machine learning models is rapidly accelerating, as exemplified by the popularity of <b>LLMs</b> <b>(Large</b> <b>Language</b> <b>Models)</b> for text, and <b>diffusion</b> <b>models</b> for image generation.As generative models become widespread, data they generate is incorporated into shared content through the public web. This opens the question of what happens when data generated by a model is fed back to the model in subsequent training campaigns. This is a question about the stability of the training process, whether the distribution of publicly accessible content, which we refer to as &ldquo;knowledge&rdquo;, remains stable or collapses. Small scale empirical experiments reported in the literature show that this closed-loop training process is prone to degenerating. Models may start producing gibberish data, or sample from only a small subset of the desired data distribution (a phenomenon referred to as mode collapse). So far there has been only limited theoretical understanding of this process, in part due to the complexity of the deep networks underlying these generative models. The aim of this paper is to provide insights into this process (that we refer to as &ldquo;generative closed-loop learning&rdquo;) by studying the learning dynamics of generative models that are fed back their own produced content in addition to their original training dataset. The sampling of many of these models can be controlled via a &ldquo;temperature&rdquo; parameter. Using dynamical systems tools, we show that, unless a sufficient amount of external data is introduced at each iteration, any non-trivial temperature leads the model to asymptotically degenerate. In fact, either the generative distribution collapses to a small set of outputs, or becomes uniform over a <b>large</b> <b>set</b> <b>of</b> outputs.</p></p class="citation"></blockquote><h3 id=1448--204327-autodiff-autoregressive-diffusion-modeling-for-structure-based-drug-design-xinze-li-et-al-2024>(14/48 | 204/327) AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug Design (Xinze Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinze Li, Penglei Wang, Tianfan Fu, Wenhao Gao, Chengtao Li, Leilei Shi, Junhong Liu. (2024)<br><strong>AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug Design</strong><br><button class=copy-to-clipboard title="AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug Design" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Diffusion Model, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02003v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02003v2.pdf filename=2404.02003v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Structure-based drug design (SBDD), which aims to generate molecules that can bind tightly to the target protein, is an essential problem in drug discovery, and previous approaches have achieved initial success. However, most existing methods still suffer from invalid local structure or unrealistic conformation issues, which are mainly due to the poor leaning of bond angles or torsional angles. To alleviate these problems, we propose AUTODIFF, a <b>diffusion-based</b> <b>fragment-wise</b> autoregressive generation model. Specifically, we design a novel molecule assembly strategy named conformal motif that preserves the conformation of local structures of molecules first, then we encode the interaction of the protein-ligand complex with an SE(3)-equivariant <b>convolutional</b> <b>network</b> and generate molecules motif-by-motif with <b>diffusion</b> <b>modeling.</b> In addition, we also improve the evaluation framework of SBDD by constraining the molecular weights of the generated molecules in the same range, together with some new metrics, which make the evaluation more fair and practical. Extensive experiments on CrossDocked2020 demonstrate that our approach outperforms the existing models in generating realistic molecules with valid structures and conformations while maintaining high binding affinity.</p></p class="citation"></blockquote><h3 id=1548--205327-unifying-qualitative-and-quantitative-safety-verification-of-dnn-controlled-systems-dapeng-zhi-et-al-2024>(15/48 | 205/327) Unifying Qualitative and Quantitative Safety Verification of DNN-Controlled Systems (Dapeng Zhi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dapeng Zhi, Peixin Wang, Si Liu, Luke Ong, Min Zhang. (2024)<br><strong>Unifying Qualitative and Quantitative Safety Verification of DNN-Controlled Systems</strong><br><button class=copy-to-clipboard title="Unifying Qualitative and Quantitative Safety Verification of DNN-Controlled Systems" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01769v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01769v1.pdf filename=2404.01769v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advance of deep <b>reinforcement</b> <b>learning</b> techniques enables the oversight of safety-critical systems through the utilization of Deep Neural Networks (DNNs). This underscores the pressing need to promptly establish certified safety guarantees for such DNN-controlled systems. Most of the existing verification approaches rely on qualitative approaches, predominantly employing reachability analysis. However, qualitative verification proves inadequate for DNN-controlled systems as their behaviors exhibit stochastic tendencies when operating in open and adversarial environments. In this paper, we propose a novel framework for unifying both qualitative and quantitative safety verification problems of DNN-controlled systems. This is achieved by formulating the verification tasks as the synthesis of valid neural barrier certificates (NBCs). Initially, the framework seeks to establish almost-sure safety guarantees through qualitative verification. In cases where qualitative verification fails, our quantitative verification method is invoked, yielding precise lower and upper bounds on probabilistic safety across both infinite and finite time horizons. To facilitate the synthesis of NBCs, we introduce their $k$-inductive variants. We also devise a <b>simulation-guided</b> approach for training NBCs, aiming to achieve tightness in computing precise certified lower and upper bounds. We prototype our approach into a tool called $\textsf{UniQQ}$ and showcase its efficacy on four classic DNN-controlled systems.</p></p class="citation"></blockquote><h3 id=1648--206327-transformer-meets-wcdtw-to-improve-real-time-battery-bids-a-new-approach-to-scenario-selection-sujal-bhavsar-et-al-2024>(16/48 | 206/327) Transformer meets wcDTW to improve real-time battery bids: A new approach to scenario selection (Sujal Bhavsar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sujal Bhavsar, Vera Zaychik Moffitt, Justin Appleby. (2024)<br><strong>Transformer meets wcDTW to improve real-time battery bids: A new approach to scenario selection</strong><br><button class=copy-to-clipboard title="Transformer meets wcDTW to improve real-time battery bids: A new approach to scenario selection" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01646v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01646v1.pdf filename=2404.01646v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stochastic battery bidding in real-time energy markets is a nuanced process, with its efficacy depending on the accuracy of forecasts and the representative scenarios chosen for optimization. In this paper, we introduce a pioneering methodology that amalgamates <b>Transformer-based</b> forecasting with weighted constrained Dynamic Time Warping (wcDTW) to refine scenario selection. Our approach harnesses the predictive capabilities of <b>Transformers</b> to foresee Energy prices, while wcDTW ensures the selection of pertinent historical scenarios by maintaining the coherence between multiple uncertain products. Through extensive <b>simulations</b> in the PJM market for July 2023, our method exhibited a 10% increase in revenue compared to the conventional method, highlighting its potential to revolutionize battery bidding strategies in real-time markets.</p></p class="citation"></blockquote><h3 id=1748--207327-audio-simulation-for-sound-source-localization-in-virtual-evironment-yi-di-yuan-et-al-2024>(17/48 | 207/327) Audio Simulation for Sound Source Localization in Virtual Evironment (Yi Di Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Di Yuan, Swee Liang Wong, Jonathan Pan. (2024)<br><strong>Audio Simulation for Sound Source Localization in Virtual Evironment</strong><br><button class=copy-to-clipboard title="Audio Simulation for Sound Source Localization in Virtual Evironment" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SD, cs.LG, eess-AS<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01611v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01611v1.pdf filename=2404.01611v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Non-line-of-sight localization in signal-deprived environments is a challenging yet pertinent problem. Acoustic methods in such predominantly indoor scenarios encounter difficulty due to the reverberant nature. In this study, we aim to locate sound sources to specific locations within a virtual environment by leveraging physically grounded sound propagation <b>simulations</b> and machine learning methods. This process attempts to overcome the issue of data insufficiency to localize sound sources to their location of occurrence especially in post-event localization. We achieve 0.786+/- 0.0136 F1-score using an audio <b>transformer</b> spectrogram approach.</p></p class="citation"></blockquote><h3 id=1848--208327-mesen-exploit-multimodal-data-to-design-unimodal-human-activity-recognition-with-few-labels-lilin-xu-et-al-2024>(18/48 | 208/327) MESEN: Exploit Multimodal Data to Design Unimodal Human Activity Recognition with Few Labels (Lilin Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lilin Xu, Chaojie Gu, Rui Tan, Shibo He, Jiming Chen. (2024)<br><strong>MESEN: Exploit Multimodal Data to Design Unimodal Human Activity Recognition with Few Labels</strong><br><button class=copy-to-clipboard title="MESEN: Exploit Multimodal Data to Design Unimodal Human Activity Recognition with Few Labels" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Contrastive Learning, Multi-modal, Multi-modal, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01958v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01958v1.pdf filename=2404.01958v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human activity recognition (HAR) will be an essential function of various emerging applications. However, HAR typically encounters challenges related to modality limitations and label scarcity, leading to an application gap between current solutions and real-world requirements. In this work, we propose MESEN, a <b>multimodal-empowered</b> unimodal sensing framework, to utilize unlabeled <b>multimodal</b> data available during the HAR model design phase for unimodal HAR enhancement during the deployment phase. From a study on the impact of <b>supervised</b> <b>multimodal</b> fusion on unimodal feature extraction, MESEN is designed to feature a multi-task mechanism during the <b>multimodal-aided</b> pre-training stage. With the proposed mechanism integrating cross-modal feature <b>contrastive</b> <b>learning</b> and <b>multimodal</b> pseudo-classification aligning, MESEN exploits unlabeled <b>multimodal</b> data to extract effective unimodal features for each modality. Subsequently, MESEN can adapt to downstream unimodal HAR with only a few labeled samples. Extensive experiments on eight public <b>multimodal</b> datasets demonstrate that MESEN achieves significant performance improvements over state-of-the-art baselines in enhancing unimodal HAR by exploiting <b>multimodal</b> data.</p></p class="citation"></blockquote><h3 id=1948--209327-a-more-realistic-evaluation-setup-for-generalisation-of-community-models-on-malicious-content-detection-ivo-verhoeven-et-al-2024>(19/48 | 209/327) A (More) Realistic Evaluation Setup for Generalisation of Community Models on Malicious Content Detection (Ivo Verhoeven et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ivo Verhoeven, Pushkar Mishra, Rahel Beloch, Helen Yannakoudakis, Ekaterina Shutova. (2024)<br><strong>A (More) Realistic Evaluation Setup for Generalisation of Community Models on Malicious Content Detection</strong><br><button class=copy-to-clipboard title="A (More) Realistic Evaluation Setup for Generalisation of Community Models on Malicious Content Detection" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs-SI, cs.LG<br>Keyword Score: 26<br>Keywords: Graph, Benchmarking, Few-shot, Content Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01822v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01822v1.pdf filename=2404.01822v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Community models for malicious <b>content</b> <b>detection,</b> which take into account the context from a social <b>graph</b> alongside the <b>content</b> <b>itself,</b> have shown remarkable performance on <b>benchmark</b> datasets. Yet, misinformation and hate speech continue to propagate on social media networks. This mismatch can be partially attributed to the limitations of current evaluation setups that neglect the rapid evolution of online <b>content</b> <b>and</b> the underlying social <b>graph.</b> In this paper, we propose a novel evaluation setup for model generalisation based on our <b>few-shot</b> subgraph sampling approach. This setup tests for generalisation through few labelled examples in local explorations of a larger <b>graph,</b> emulating more realistic application settings. We show this to be a challenging inductive setup, wherein strong performance on the training <b>graph</b> is not indicative of performance on unseen tasks, domains, or <b>graph</b> structures. Lastly, we show that <b>graph</b> meta-learners trained with our proposed <b>few-shot</b> subgraph sampling outperform standard community models in the inductive setup. We make our code publicly available.</p></p class="citation"></blockquote><h3 id=2048--210327-catgnn-cost-efficient-and-scalable-distributed-training-for-graph-neural-networks-xin-huang-et-al-2024>(20/48 | 210/327) CATGNN: Cost-Efficient and Scalable Distributed Training for Graph Neural Networks (Xin Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Huang, Weipeng Zhuo, Minh Phu Vuong, Shiju Li, Jongryool Kim, Bradley Rees, Chul-Ho Lee. (2024)<br><strong>CATGNN: Cost-Efficient and Scalable Distributed Training for Graph Neural Networks</strong><br><button class=copy-to-clipboard title="CATGNN: Cost-Efficient and Scalable Distributed Training for Graph Neural Networks" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02300v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02300v1.pdf filename=2404.02300v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> have been shown successful in recent years. While different <b>GNN</b> architectures and training systems have been developed, <b>GNN</b> training on large-scale real-world <b>graphs</b> <b>still</b> <b>remains</b> challenging. Existing distributed systems load the entire <b>graph</b> <b>in</b> <b>memory</b> for <b>graph</b> <b>partitioning,</b> <b>requiring</b> a huge memory space to process large <b>graphs</b> <b>and</b> <b>thus</b> hindering <b>GNN</b> training on such large <b>graphs</b> <b>using</b> <b>commodity</b> workstations. In this paper, we propose CATGNN, a cost-efficient and scalable distributed <b>GNN</b> training system which focuses on scaling <b>GNN</b> training to billion-scale or larger <b>graphs</b> <b>under</b> <b>limited</b> computational resources. Among other features, it takes a stream of edges as input, instead of loading the entire <b>graph</b> <b>in</b> <b>memory,</b> for partitioning. We also propose a novel streaming partitioning algorithm named SPRING for distributed <b>GNN</b> training. We verify the correctness and effectiveness of CATGNN with SPRING on 16 open datasets. In particular, we demonstrate that CATGNN can handle the largest publicly available dataset with limited memory, which would have been infeasible without increasing the memory space. SPRING also outperforms state-of-the-art partitioning algorithms significantly, with a 50% reduction in replication factor on average.</p></p class="citation"></blockquote><h3 id=2148--211327-mixture-of-depths-dynamically-allocating-compute-in-transformer-based-language-models-david-raposo-et-al-2024>(21/48 | 211/327) Mixture-of-Depths: Dynamically allocating compute in transformer-based language models (David Raposo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, Adam Santoro. (2024)<br><strong>Mixture-of-Depths: Dynamically allocating compute in transformer-based language models</strong><br><button class=copy-to-clipboard title="Mixture-of-Depths: Dynamically allocating compute in transformer-based language models" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02258v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02258v1.pdf filename=2404.02258v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> language models spread FLOPs uniformly across input sequences. In this work we demonstrate that <b>transformers</b> can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the <b>self-attention</b> and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple procedure uses a static computation <b>graph</b> with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50% faster to step during post-training sampling.</p></p class="citation"></blockquote><h3 id=2248--212327-enhancing-functional-safety-in-automotive-ams-circuits-through-unsupervised-machine-learning-ayush-arunachalam-et-al-2024>(22/48 | 212/327) Enhancing Functional Safety in Automotive AMS Circuits through Unsupervised Machine Learning (Ayush Arunachalam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayush Arunachalam, Ian Kintz, Suvadeep Banerjee, Arnab Raha, Xiankun Jin, Fei Su, Viswanathan Pillai Prasanth, Rubin A. Parekhji, Suriyaprakash Natarajan, Kanad Basu. (2024)<br><strong>Enhancing Functional Safety in Automotive AMS Circuits through Unsupervised Machine Learning</strong><br><button class=copy-to-clipboard title="Enhancing Functional Safety in Automotive AMS Circuits through Unsupervised Machine Learning" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 23<br>Keywords: Anomaly Detection, Clustering, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01632v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01632v1.pdf filename=2404.01632v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the widespread use of safety-critical applications in the automotive field, it is crucial to ensure the Functional Safety (FuSa) of circuits and components within automotive systems. The Analog and Mixed-Signal (AMS) circuits prevalent in these systems are more vulnerable to faults induced by parametric perturbations, noise, environmental stress, and other factors, in comparison to their digital counterparts. However, their continuous signal characteristics present an opportunity for early <b>anomaly</b> <b>detection,</b> enabling the implementation of safety mechanisms to prevent system failure. To address this need, we propose a novel framework based on <b>unsupervised</b> machine learning for early <b>anomaly</b> <b>detection</b> in AMS circuits. The proposed approach involves injecting anomalies at various circuit locations and individual components to create a diverse and comprehensive <b>anomaly</b> <b>dataset,</b> followed by the extraction of features from the observed circuit signals. Subsequently, we employ <b>clustering</b> algorithms to facilitate <b>anomaly</b> <b>detection.</b> Finally, we propose a time series framework to enhance and expedite <b>anomaly</b> <b>detection</b> performance. Our approach encompasses a systematic analysis of <b>anomaly</b> <b>abstraction</b> at multiple levels pertaining to the automotive domain, from hardware- to block-level, where anomalies are injected to create diverse fault scenarios. By monitoring the system behavior under these anomalous conditions, we capture the propagation of anomalies and their effects at different abstraction levels, thereby potentially paving the way for the implementation of reliable safety mechanisms to ensure the FuSa of automotive SoCs. Our experimental findings indicate that our approach achieves 100% <b>anomaly</b> <b>detection</b> accuracy and significantly optimizes the associated latency by 5X, underscoring the effectiveness of our devised solution.</p></p class="citation"></blockquote><h3 id=2348--213327-imagenot-a-contrast-with-imagenet-preserves-model-rankings-olawale-salaudeen-et-al-2024>(23/48 | 213/327) ImageNot: A contrast with ImageNet preserves model rankings (Olawale Salaudeen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olawale Salaudeen, Moritz Hardt. (2024)<br><strong>ImageNot: A contrast with ImageNet preserves model rankings</strong><br><button class=copy-to-clipboard title="ImageNot: A contrast with ImageNet preserves model rankings" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fine-tuning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02112v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02112v1.pdf filename=2404.02112v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce ImageNot, a dataset designed to match the scale of ImageNet while differing drastically in other aspects. We show that key model architectures developed for ImageNet over the years rank identically when trained and evaluated on ImageNot to how they rank on ImageNet. This is true when training models from scratch or <b>fine-tuning</b> them. Moreover, the relative improvements of each model over earlier models strongly correlate in both datasets. We further give evidence that ImageNot has a similar utility as ImageNet for <b>transfer</b> <b>learning</b> purposes. Our work demonstrates a surprising degree of external validity in the relative performance of image classification models. This stands in contrast with absolute accuracy numbers that typically drop sharply even under small changes to a dataset.</p></p class="citation"></blockquote><h3 id=2448--214327-incentives-in-private-collaborative-machine-learning-rachael-hwee-ling-sim-et-al-2024>(24/48 | 214/327) Incentives in Private Collaborative Machine Learning (Rachael Hwee Ling Sim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rachael Hwee Ling Sim, Yehong Zhang, Trong Nghia Hoang, Xinyi Xu, Bryan Kian Hsiang Low, Patrick Jaillet. (2024)<br><strong>Incentives in Private Collaborative Machine Learning</strong><br><button class=copy-to-clipboard title="Incentives in Private Collaborative Machine Learning" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fairness, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01676v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01676v1.pdf filename=2404.01676v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collaborative machine learning involves training models on data from multiple parties but must incentivize their participation. Existing data valuation methods fairly value and reward each party based on shared data or model parameters but neglect the privacy risks involved. To address this, we introduce <b>differential</b> <b>privacy</b> (DP) as an incentive. Each party can select its required DP guarantee and perturb its sufficient statistic (SS) accordingly. The mediator values the perturbed SS by the Bayesian surprise it elicits about the model parameters. As our valuation function enforces a privacy-valuation trade-off, parties are deterred from selecting excessive DP guarantees that reduce the utility of the grand coalition&rsquo;s model. Finally, the mediator rewards each party with different posterior samples of the model parameters. Such rewards still satisfy existing incentives like <b>fairness</b> but additionally preserve DP and a high similarity to the grand coalition&rsquo;s posterior. We empirically demonstrate the effectiveness and practicality of our approach on synthetic and real-world datasets.</p></p class="citation"></blockquote><h3 id=2548--215327-advrepairprovable-repair-of-adversarial-attack-zhiming-chi-et-al-2024>(25/48 | 215/327) ADVREPAIR:Provable Repair of Adversarial Attack (Zhiming Chi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiming Chi, Jianan Ma, Pengfei Yang, Cheng-Chao Huang, Renjue Li, Xiaowei Huang, Lijun Zhang. (2024)<br><strong>ADVREPAIR:Provable Repair of Adversarial Attack</strong><br><button class=copy-to-clipboard title="ADVREPAIR:Provable Repair of Adversarial Attack" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01642v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01642v1.pdf filename=2404.01642v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks (DNNs) are increasingly deployed in safety-critical domains, but their vulnerability to <b>adversarial</b> <b>attacks</b> poses serious safety risks. Existing neuron-level methods using limited data lack efficacy in fixing adversaries due to the inherent complexity of <b>adversarial</b> <b>attack</b> mechanisms, while <b>adversarial</b> <b>training,</b> leveraging a large number of <b>adversarial</b> <b>samples</b> to enhance robustness, lacks provability. In this paper, we propose ADVREPAIR, a novel approach for provable repair of <b>adversarial</b> <b>attacks</b> using limited data. By utilizing formal verification, ADVREPAIR constructs patch modules that, when integrated with the original network, deliver provable and specialized repairs within the robustness neighborhood. Additionally, our approach incorporates a heuristic mechanism for assigning patch modules, allowing this defense against <b>adversarial</b> <b>attacks</b> to generalize to other inputs. ADVREPAIR demonstrates superior efficiency, scalability and repair success rate. Different from existing DNN repair methods, our repair can generalize to general inputs, thereby improving the robustness of the neural network globally, which indicates a significant breakthrough in the generalization capability of ADVREPAIR.</p></p class="citation"></blockquote><h3 id=2648--216327-what-can-transformer-learn-with-varying-depth-case-studies-on-sequence-learning-tasks-xingwu-chen-et-al-2024>(26/48 | 216/327) What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks (Xingwu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingwu Chen, Difan Zou. (2024)<br><strong>What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks</strong><br><button class=copy-to-clipboard title="What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Transformer, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01601v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01601v1.pdf filename=2404.01601v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the capabilities of the <b>transformer</b> architecture with varying depth. Specifically, we designed a novel set of sequence learning tasks to systematically evaluate and comprehend how the depth of <b>transformer</b> affects its ability to perform memorization, <b>reasoning,</b> generalization, and contextual generalization. We show a <b>transformer</b> with only one attention layer can excel in memorization but falls short in other tasks. Then, we show that exhibiting <b>reasoning</b> and generalization ability requires the <b>transformer</b> to have at least two attention layers, while context generalization ability may necessitate three attention layers. Additionally, we identify a class of simple operations that a single attention layer can execute, and show that the complex tasks can be approached as the combinations of these simple operations and thus can be resolved by stacking multiple attention layers. This sheds light on studying more practical and complex tasks beyond our design. Numerical experiments corroborate our theoretical findings.</p></p class="citation"></blockquote><h3 id=2748--217327-pairwise-similarity-distribution-clustering-for-noisy-label-learning-sihan-bai-2024>(27/48 | 217/327) Pairwise Similarity Distribution Clustering for Noisy Label Learning (Sihan Bai, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sihan Bai. (2024)<br><strong>Pairwise Similarity Distribution Clustering for Noisy Label Learning</strong><br><button class=copy-to-clipboard title="Pairwise Similarity Distribution Clustering for Noisy Label Learning" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 16<br>Keywords: Benchmarking, Clustering, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01853v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01853v1.pdf filename=2404.01853v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Noisy label learning aims to train deep neural networks using a large amount of samples with noisy labels, whose main challenge comes from how to deal with the inaccurate supervision caused by wrong labels. Existing works either take the label correction or sample selection paradigm to involve more samples with accurate labels into the training process. In this paper, we propose a simple yet effective sample selection algorithm, termed as Pairwise Similarity Distribution Clustering~(PSDC), to divide the training samples into one clean set and another noisy set, which can power any of the off-the-shelf <b>semi-supervised</b> <b>learning</b> regimes to further train networks for different downstream tasks. Specifically, we take the pairwise similarity between sample pairs to represent the sample structure, and the Gaussian Mixture Model~(GMM) to model the similarity distribution between sample pairs belonging to the same noisy cluster, therefore each sample can be confidently divided into the clean set or noisy set. Even under severe label noise rate, the resulting data partition mechanism has been proved to be more robust in judging the label confidence in both theory and practice. Experimental results on various <b>benchmark</b> datasets, such as CIFAR-10, CIFAR-100 and Clothing1M, demonstrate significant improvements over state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=2848--218327-hemenet-heterogeneous-multichannel-equivariant-network-for-protein-multitask-learning-rong-han-et-al-2024>(28/48 | 218/327) HeMeNet: Heterogeneous Multichannel Equivariant Network for Protein Multitask Learning (Rong Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rong Han, Wenbing Huang, Lingxiao Luo, Xinyan Han, Jiaming Shen, Zhiqiang Zhang, Jun Zhou, Ting Chen. (2024)<br><strong>HeMeNet: Heterogeneous Multichannel Equivariant Network for Protein Multitask Learning</strong><br><button class=copy-to-clipboard title="HeMeNet: Heterogeneous Multichannel Equivariant Network for Protein Multitask Learning" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 16<br>Keywords: Graph, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01693v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01693v1.pdf filename=2404.01693v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding and leveraging the 3D structures of proteins is central to a variety of biological and drug discovery tasks. While deep learning has been applied successfully for structure-based protein function prediction tasks, current methods usually employ distinct training for each task. However, each of the tasks is of small size, and such a single-task strategy hinders the models&rsquo; performance and generalization ability. As some labeled 3D protein datasets are biologically related, combining multi-source datasets for larger-scale multi-task learning is one way to overcome this problem. In this paper, we propose a neural network model to address multiple tasks jointly upon the input of 3D protein structures. In particular, we first construct a standard structure-based multi-task <b>benchmark</b> called Protein-MT, consisting of 6 biologically relevant tasks, including affinity prediction and property prediction, integrated from 4 public datasets. Then, we develop a novel <b>graph</b> <b>neural</b> <b>network</b> for multi-task learning, dubbed Heterogeneous Multichannel Equivariant Network (HeMeNet), which is E(3) equivariant and able to capture heterogeneous relationships between different atoms. Besides, HeMeNet can achieve task-specific learning via the task-aware readout mechanism. Extensive evaluations on our <b>benchmark</b> verify the effectiveness of multi-task learning, and our model generally surpasses state-of-the-art models.</p></p class="citation"></blockquote><h3 id=2948--219327-glemos-benchmark-for-instantaneous-graph-learning-model-selection-namyong-park-et-al-2024>(29/48 | 219/327) GLEMOS: Benchmark for Instantaneous Graph Learning Model Selection (Namyong Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Namyong Park, Ryan Rossi, Xing Wang, Antoine Simoulin, Nesreen Ahmed, Christos Faloutsos. (2024)<br><strong>GLEMOS: Benchmark for Instantaneous Graph Learning Model Selection</strong><br><button class=copy-to-clipboard title="GLEMOS: Benchmark for Instantaneous Graph Learning Model Selection" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 16<br>Keywords: Node Classification, Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01578v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01578v1.pdf filename=2404.01578v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The choice of a <b>graph</b> learning (GL) model (i.e., a GL algorithm and its hyperparameter settings) has a significant impact on the performance of downstream tasks. However, selecting the right GL model becomes increasingly difficult and time consuming as more and more GL models are developed. Accordingly, it is of great significance and practical value to equip users of GL with the ability to perform a near-instantaneous selection of an effective GL model without manual intervention. Despite the recent attempts to tackle this important problem, there has been no comprehensive <b>benchmark</b> environment to evaluate the performance of GL model selection methods. To bridge this gap, we present GLEMOS in this work, a comprehensive <b>benchmark</b> for instantaneous GL model selection that makes the following contributions. (i) GLEMOS provides extensive <b>benchmark</b> data for fundamental GL tasks, i.e., link prediction and <b>node</b> <b>classification,</b> including the performances of 366 models on 457 <b>graphs</b> on these tasks. (ii) GLEMOS designs multiple evaluation settings, and assesses how effectively representative model selection techniques perform in these different settings. (iii) GLEMOS is designed to be easily extended with new models, new <b>graphs,</b> and new performance records. (iv) Based on the experimental results, we discuss the limitations of existing approaches and highlight future research directions. To promote research on this significant problem, we make the <b>benchmark</b> data and code publicly available at <a href=https://github.com/facebookresearch/glemos>https://github.com/facebookresearch/glemos</a>.</p></p class="citation"></blockquote><h3 id=3048--220327-propensity-score-alignment-of-unpaired-multimodal-data-johnny-xi-et-al-2024>(30/48 | 220/327) Propensity Score Alignment of Unpaired Multimodal Data (Johnny Xi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johnny Xi, Jason Hartford. (2024)<br><strong>Propensity Score Alignment of Unpaired Multimodal Data</strong><br><button class=copy-to-clipboard title="Propensity Score Alignment of Unpaired Multimodal Data" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ME, stat-ML<br>Keyword Score: 11<br>Keywords: Multi-modal, Multi-modal, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01595v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01595v1.pdf filename=2404.01595v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>representation</b> <b>learning</b> techniques typically rely on paired samples to learn common <b>representations,</b> <b>but</b> paired samples are challenging to collect in fields such as biology where measurement devices often destroy the samples. This paper presents an approach to address the challenge of aligning unpaired samples across disparate modalities in <b>multimodal</b> <b>representation</b> <b>learning.</b> We draw an analogy between potential outcomes in causal inference and potential views in <b>multimodal</b> observations, which allows us to use Rubin&rsquo;s framework to estimate a common space in which to match samples. Our approach assumes we collect samples that are experimentally perturbed by treatments, and uses this to estimate a propensity score from each modality, which encapsulates all shared information between a latent state and treatment and can be used to define a distance between samples. We experiment with two alignment techniques that leverage this distance &ndash; shared nearest neighbours (SNN) and optimal transport (OT) matching &ndash; and find that OT matching results in significant improvements over state-of-the-art alignment approaches in both a synthetic <b>multi-modal</b> setting and in real-world data from NeurIPS <b>Multimodal</b> Single-Cell Integration Challenge.</p></p class="citation"></blockquote><h3 id=3148--221327-fragnnet-a-deep-probabilistic-model-for-mass-spectrum-prediction-adamo-young-et-al-2024>(31/48 | 221/327) FraGNNet: A Deep Probabilistic Model for Mass Spectrum Prediction (Adamo Young et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adamo Young, Fei Wang, David Wishart, Bo Wang, Hannes Röst, Russ Greiner. (2024)<br><strong>FraGNNet: A Deep Probabilistic Model for Mass Spectrum Prediction</strong><br><button class=copy-to-clipboard title="FraGNNet: A Deep Probabilistic Model for Mass Spectrum Prediction" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-BM<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02360v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02360v1.pdf filename=2404.02360v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The process of identifying a compound from its mass spectrum is a critical step in the analysis of complex mixtures. Typical solutions for the mass spectrum to compound (MS2C) problem involve matching the unknown spectrum against a library of known spectrum-molecule pairs, an approach that is limited by incomplete library coverage. Compound to mass spectrum (C2MS) models can improve retrieval rates by augmenting real libraries with predicted spectra. Unfortunately, many existing C2MS models suffer from problems with prediction resolution, scalability, or interpretability. We develop a new <b>probabilistic</b> <b>method</b> for C2MS prediction, FraGNNet, that can efficiently and accurately predict high-resolution spectra. FraGNNet uses a structured latent space to provide insight into the underlying processes that define the spectrum. Our model achieves state-of-the-art performance in terms of prediction error, and surpasses existing C2MS models as a tool for retrieval-based MS2C.</p></p class="citation"></blockquote><h3 id=3248--222327-deep-neural-networks-with-3d-point-clouds-for-empirical-friction-measurements-in-hydrodynamic-flood-models-francisco-haces-garcia-et-al-2024>(32/48 | 222/327) Deep Neural Networks with 3D Point Clouds for Empirical Friction Measurements in Hydrodynamic Flood Models (Francisco Haces-Garcia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francisco Haces-Garcia, Vasileios Kotzamanis, Craig Glennie, Hanadi Rifai. (2024)<br><strong>Deep Neural Networks with 3D Point Clouds for Empirical Friction Measurements in Hydrodynamic Flood Models</strong><br><button class=copy-to-clipboard title="Deep Neural Networks with 3D Point Clouds for Empirical Friction Measurements in Hydrodynamic Flood Models" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-flu-dyn<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02234v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02234v1.pdf filename=2404.02234v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Friction is one of the cruxes of hydrodynamic modeling; flood conditions are highly sensitive to the Friction Factors (FFs) used to calculate momentum losses. However, empirical FFs are challenging to measure because they require laboratory experiments. Flood models often rely on surrogate observations (such as land use) to estimate FFs, introducing uncertainty. This research presents a laboratory-trained Deep Neural Network (DNN), trained using flume experiments with <b>data</b> <b>augmentation</b> techniques, to measure Manning&rsquo;s n based on Point Cloud <b>data.</b> <b>The</b> DNN was deployed on real-world lidar Point Clouds to directly measure Manning&rsquo;s n under regulatory and extreme storm events, showing improved prediction capabilities in both 1D and 2D hydrodynamic models. For 1D models, the lidar values decreased differences with regulatory models for in-channel water depth when compared to land cover values. For 1D/2D coupled models, the lidar values produced better agreement with flood extents measured from airborne imagery, while better matching flood insurance claim <b>data</b> <b>for</b> Hurricane Harvey. In both 1D and 1D/2D coupled models, lidar resulted in better agreement with validation gauges. For these reasons, the lidar measurements of Manning&rsquo;s n were found to improve both regulatory models and forecasts for extreme storm events, while simultaneously providing a pathway to standardize the measurement of FFs. Changing FFs significantly affected fluvial and pluvial flood models, while surge flooding was generally unaffected. Downstream flow conditions were found to change the importance of FFs to fluvial models, advancing the literature of friction in flood models. This research introduces a reliable, repeatable, and readily-accessible avenue to measure high-resolution FFs based on 3D point clouds, improving flood prediction, and removing uncertainty from hydrodynamic modeling.</p></p class="citation"></blockquote><h3 id=3348--223327-tuning-for-the-unknown-revisiting-evaluation-strategies-for-lifelong-rl-golnaz-mesbahi-et-al-2024>(33/48 | 223/327) Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL (Golnaz Mesbahi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Golnaz Mesbahi, Olya Mastikhina, Parham Mohammad Panahi, Martha White, Adam White. (2024)<br><strong>Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL</strong><br><button class=copy-to-clipboard title="Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02113v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02113v1.pdf filename=2404.02113v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In continual or lifelong <b>reinforcement</b> <b>learning</b> access to the environment should be limited. If we aspire to design algorithms that can run for long-periods of time, continually adapting to new, unexpected situations then we must be willing to deploy our agents without tuning their hyperparameters over the agent&rsquo;s entire lifetime. The standard practice in deep RL &ndash; and even continual RL &ndash; is to assume unfettered access to deployment environment for the full lifetime of the agent. This paper explores the notion that progress in lifelong RL research has been held back by inappropriate empirical methodologies. In this paper we propose a new approach for tuning and evaluating lifelong RL agents where only one percent of the experiment data can be used for hyperparameter tuning. We then conduct an empirical study of DQN and Soft Actor Critic across a variety of continuing and non-stationary domains. We find both methods generally perform poorly when restricted to one-percent tuning, whereas several algorithmic mitigations designed to maintain network plasticity perform surprising well. In addition, we find that properties designed to measure the network&rsquo;s ability to learn continually indeed correlate with performance under one-percent tuning.</p></p class="citation"></blockquote><h3 id=3448--224327-zero-shot-multi-lingual-speaker-verification-in-clinical-trials-ali-akram-et-al-2024>(34/48 | 224/327) Zero-Shot Multi-Lingual Speaker Verification in Clinical Trials (Ali Akram et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Akram, Marija Stanojevic, Malikeh Ehghaghi, Jekaterina Novikova. (2024)<br><strong>Zero-Shot Multi-Lingual Speaker Verification in Clinical Trials</strong><br><button class=copy-to-clipboard title="Zero-Shot Multi-Lingual Speaker Verification in Clinical Trials" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SD, cs.LG, eess-AS<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01981v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01981v2.pdf filename=2404.01981v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the substantial number of clinicians, patients, and data collection environments involved in clinical trials, gathering data of superior quality poses a significant challenge. In clinical trials, patients are assessed based on their speech data to detect and monitor cognitive and mental health disorders. We propose using these speech recordings to verify the identities of enrolled patients and identify and exclude the individuals who try to enroll multiple times in the same trial. Since clinical studies are often conducted across different countries, creating a system that can perform speaker verification in diverse languages without additional development effort is imperative. We evaluate pre-trained TitaNet, ECAPA-TDNN, and SpeakerNet models by enrolling and testing with speech-impaired patients speaking English, German, Danish, Spanish, and Arabic languages. Our results demonstrate that tested models can effectively generalize to clinical speakers, with less than 2.7% EER for European Languages and 8.26% EER for Arabic. This represents a significant step in developing more versatile and efficient speaker verification systems for cognitive and mental health clinical trials that can be used across a wide range of languages and dialects, substantially reducing the effort required to develop speaker verification systems for multiple languages. We also evaluate how speech tasks and number of speakers involved in the trial influence the performance and show that the type of speech tasks impacts the model performance.</p></p class="citation"></blockquote><h3 id=3548--225327-adaptive-combinatorial-maximization-beyond-approximate-greedy-policies-shlomi-weitzman-et-al-2024>(35/48 | 225/327) Adaptive Combinatorial Maximization: Beyond Approximate Greedy Policies (Shlomi Weitzman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shlomi Weitzman, Sivan Sabato. (2024)<br><strong>Adaptive Combinatorial Maximization: Beyond Approximate Greedy Policies</strong><br><button class=copy-to-clipboard title="Adaptive Combinatorial Maximization: Beyond Approximate Greedy Policies" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DM, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01930v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01930v1.pdf filename=2404.01930v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study adaptive combinatorial maximization, which is a core challenge in machine learning, with applications in <b>active</b> <b>learning</b> as well as many other domains. We study the Bayesian setting, and consider the objectives of maximization under a cardinality constraint and minimum cost coverage. We provide new comprehensive approximation guarantees that subsume previous results, as well as considerably strengthen them. Our approximation guarantees simultaneously support the maximal gain ratio as well as near-submodular utility functions, and include both maximization under a cardinality constraint and a minimum cost coverage guarantee. In addition, we provided an approximation guarantee for a modified prior, which is crucial for obtaining <b>active</b> <b>learning</b> guarantees that do not depend on the smallest probability in the prior. Moreover, we discover a new parameter of adaptive selection policies, which we term the &ldquo;maximal gain ratio&rdquo;. We show that this parameter is strictly less restrictive than the greedy approximation parameter that has been used in previous approximation guarantees, and show that it can be used to provide stronger approximation guarantees than previous results. In particular, we show that the maximal gain ratio is never larger than the greedy approximation factor of a policy, and that it can be considerably smaller. This provides a new insight into the properties that make a policy useful for adaptive combinatorial maximization.</p></p class="citation"></blockquote><h3 id=3648--226327-leveraging-machine-learning-for-early-autism-detection-via-indt-asd-indian-database-trapti-shrivastava-et-al-2024>(36/48 | 226/327) Leveraging Machine Learning for Early Autism Detection via INDT-ASD Indian Database (Trapti Shrivastava et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Trapti Shrivastava, Harshal Chaudhari, Vrijendra Singh. (2024)<br><strong>Leveraging Machine Learning for Early Autism Detection via INDT-ASD Indian Database</strong><br><button class=copy-to-clipboard title="Leveraging Machine Learning for Early Autism Detection via INDT-ASD Indian Database" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02181v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02181v1.pdf filename=2404.02181v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning (ML) has advanced quickly, particularly throughout the area of health care. The diagnosis of neurodevelopment problems using ML is a very important area of healthcare. Autism spectrum disorder (ASD) is one of the developmental disorders that is growing the fastest globally. The clinical screening tests used to identify autistic symptoms are expensive and time-consuming. But now that ML has been advanced, it&rsquo;s feasible to identify autism early on. Previously, many different techniques have been used in investigations. Still, none of them have produced the anticipated outcomes when it comes to the capacity to predict autistic features utilizing a clinically validated Indian ASD database. Therefore, this study aimed to develop a simple, quick, and inexpensive technique for identifying ASD by using ML. Various machine learning classifiers, including Adaboost (AB), Gradient Boost (GB), Decision Tree (DT), <b>Logistic</b> <b>Regression</b> (LR), Random Forest (RF), Gaussian Naive Bayes (GNB), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), K-Nearest Neighbors (KNN), and Support Vector Machine (SVM), were used to develop the autism prediction model. The proposed method was tested with records from the AIIMS Modified INDT-ASD (AMI) database, which were collected through an application developed by AIIMS in Delhi, India. Feature engineering has been applied to make the proposed solution easier than already available solutions. Using the proposed model, we succeeded in predicting ASD using a minimized set of 20 questions rather than the 28 questions presented in AMI with promising accuracy. In a comparative evaluation, SVM emerged as the superior model among others, with 100 $\pm$ 0.05% accuracy, higher recall by 5.34%, and improved accuracy by 2.22%-6.67% over RF. We have also introduced a web-based solution supporting both Hindi and English.</p></p class="citation"></blockquote><h3 id=3748--227327-procedural-fairness-in-machine-learning-ziming-wang-et-al-2024>(37/48 | 227/327) Procedural Fairness in Machine Learning (Ziming Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziming Wang, Changwu Huang, Xin Yao. (2024)<br><strong>Procedural Fairness in Machine Learning</strong><br><button class=copy-to-clipboard title="Procedural Fairness in Machine Learning" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01877v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01877v1.pdf filename=2404.01877v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fairness</b> in machine learning (ML) has received much attention. However, existing studies have mainly focused on the distributive <b>fairness</b> of ML models. The other dimension of <b>fairness,</b> i.e., procedural <b>fairness,</b> has been neglected. In this paper, we first define the procedural <b>fairness</b> of ML models, and then give formal definitions of individual and group procedural <b>fairness.</b> We propose a novel metric to evaluate the group procedural <b>fairness</b> of ML models, called $GPF_{FAE}$, which utilizes a widely used explainable artificial intelligence technique, namely feature attribution explanation (FAE), to capture the decision process of the ML models. We validate the effectiveness of $GPF_{FAE}$ on a synthetic dataset and eight real-world datasets. Our experiments reveal the relationship between procedural and distributive <b>fairness</b> of the ML model. Based on our analysis, we propose a method for identifying the features that lead to the procedural unfairness of the model and propose two methods to improve procedural <b>fairness</b> after identifying unfair features. Our experimental results demonstrate that we can accurately identify the features that lead to procedural unfairness in the ML model, and both of our proposed methods can significantly improve procedural <b>fairness</b> with a slight impact on model performance, while also improving distributive <b>fairness.</b></p></p class="citation"></blockquote><h3 id=3848--228327-fast-and-adaptive-questionnaires-for-voting-advice-applications-fynn-bachmann-et-al-2024>(38/48 | 228/327) Fast and Adaptive Questionnaires for Voting Advice Applications (Fynn Bachmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fynn Bachmann, Cristina Sarasua, Abraham Bernstein. (2024)<br><strong>Fast and Adaptive Questionnaires for Voting Advice Applications</strong><br><button class=copy-to-clipboard title="Fast and Adaptive Questionnaires for Voting Advice Applications" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-HC, cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01872v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01872v1.pdf filename=2404.01872v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The effectiveness of Voting Advice Applications (VAA) is often compromised by the length of their questionnaires. To address user fatigue and incomplete responses, some applications (such as the Swiss Smartvote) offer a condensed version of their questionnaire. However, these condensed versions can not ensure the accuracy of recommended parties or candidates, which we show to remain below 40%. To tackle these limitations, this work introduces an adaptive questionnaire approach that selects subsequent questions based on users&rsquo; previous answers, aiming to enhance <b>recommendation</b> accuracy while reducing the number of questions posed to the voters. Our method uses an encoder and decoder module to predict missing values at any completion stage, leveraging a two-dimensional latent space reflective of political science&rsquo;s traditional methods for visualizing political orientations. Additionally, a selector module is proposed to determine the most informative subsequent question based on the voter&rsquo;s current position in the latent space and the remaining unanswered questions. We validated our approach using the Smartvote dataset from the Swiss Federal elections in 2019, testing various spatial models and selection methods to optimize the system&rsquo;s predictive accuracy. Our findings indicate that employing the IDEAL model both as encoder and decoder, combined with a PosteriorRMSE method for question selection, significantly improves the accuracy of <b>recommendations,</b> achieving 74% accuracy after asking the same number of questions as in the condensed version.</p></p class="citation"></blockquote><h3 id=3948--229327-defense-without-forgetting-continual-adversarial-defense-with-anisotropic--isotropic-pseudo-replay-yuhang-zhou-et-al-2024>(39/48 | 229/327) Defense without Forgetting: Continual Adversarial Defense with Anisotropic & Isotropic Pseudo Replay (Yuhang Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhang Zhou, Zhongyun Hua. (2024)<br><strong>Defense without Forgetting: Continual Adversarial Defense with Anisotropic & Isotropic Pseudo Replay</strong><br><button class=copy-to-clipboard title="Defense without Forgetting: Continual Adversarial Defense with Anisotropic & Isotropic Pseudo Replay" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01828v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01828v1.pdf filename=2404.01828v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks have demonstrated susceptibility to <b>adversarial</b> <b>attacks.</b> <b>Adversarial</b> <b>defense</b> techniques often focus on one-shot setting to maintain robustness against attack. However, new attacks can emerge in sequences in real-world deployment scenarios. As a result, it is crucial for a defense model to constantly adapt to new attacks, but the adaptation process can lead to catastrophic forgetting of previously defended against attacks. In this paper, we discuss for the first time the concept of continual <b>adversarial</b> <b>defense</b> under a sequence of attacks, and propose a lifelong defense baseline called Anisotropic & Isotropic Replay (AIR), which offers three advantages: (1) Isotropic replay ensures model consistency in the neighborhood distribution of new data, indirectly aligning the output preference between old and new tasks. (2) Anisotropic replay enables the model to learn a compromise data manifold with fresh mixed semantics for further replay constraints and potential future attacks. (3) A straightforward regularizer mitigates the &lsquo;plasticity-stability&rsquo; trade-off by aligning model output between new and old tasks. Experiment results demonstrate that AIR can approximate or even exceed the empirical performance upper bounds achieved by Joint Training.</p></p class="citation"></blockquote><h3 id=4048--230327-improved-text-emotion-prediction-using-combined-valence-and-arousal-ordinal-classification-michael-mitsios-et-al-2024>(40/48 | 230/327) Improved Text Emotion Prediction Using Combined Valence and Arousal Ordinal Classification (Michael Mitsios et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Mitsios, Georgios Vamvoukakis, Georgia Maniati, Nikolaos Ellinas, Georgios Dimitriou, Konstantinos Markopoulos, Panos Kakoulidis, Alexandra Vioni, Myrsini Christidou, Junkwang Oh, Gunu Jho, Inchul Hwang, Georgios Vardaxoglou, Aimilios Chalamandaris, Pirros Tsiakoulis, Spyros Raptis. (2024)<br><strong>Improved Text Emotion Prediction Using Combined Valence and Arousal Ordinal Classification</strong><br><button class=copy-to-clipboard title="Improved Text Emotion Prediction Using Combined Valence and Arousal Ordinal Classification" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01805v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01805v1.pdf filename=2404.01805v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emotion detection in textual data has received growing interest in recent years, as it is pivotal for developing empathetic human-computer interaction systems. This paper introduces a method for categorizing emotions from text, which acknowledges and differentiates between the diversified similarities and distinctions of various emotions. Initially, we establish a baseline by training a <b>transformer-based</b> model for standard emotion classification, achieving state-of-the-art performance. We argue that not all misclassifications are of the same importance, as there are perceptual similarities among emotional classes. We thus redefine the emotion labeling problem by shifting it from a traditional classification model to an ordinal classification one, where discrete emotions are arranged in a sequential order according to their valence levels. Finally, we propose a method that performs ordinal classification in the two-dimensional emotion space, considering both valence and arousal scales. The results show that our approach not only preserves high accuracy in emotion prediction but also significantly reduces the magnitude of errors in cases of misclassification.</p></p class="citation"></blockquote><h3 id=4148--231327-asymptotics-of-language-model-alignment-joy-qiping-yang-et-al-2024>(41/48 | 231/327) Asymptotics of Language Model Alignment (Joy Qiping Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joy Qiping Yang, Salman Salamatian, Ziteng Sun, Ananda Theertha Suresh, Ahmad Beirami. (2024)<br><strong>Asymptotics of Language Model Alignment</strong><br><button class=copy-to-clipboard title="Asymptotics of Language Model Alignment" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IT, cs-LG, cs.LG, math-IT, stat-ML<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01730v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01730v1.pdf filename=2404.01730v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Let $p$ denote a generative language model. Let $r$ denote a reward model that returns a scalar that captures the degree at which a draw from $p$ is preferred. The goal of language model alignment is to alter $p$ to a new distribution $\phi$ that results in a higher expected reward while keeping $\phi$ close to $p.$ A popular alignment method is the KL-constrained <b>reinforcement</b> <b>learning</b> (RL), which chooses a distribution $\phi_\Delta$ that maximizes $E_{\phi_{\Delta}} r(y)$ subject to a relative entropy constraint $KL(\phi_\Delta || p) \leq \Delta.$ Another simple alignment method is best-of-$N$, where $N$ samples are drawn from $p$ and one with highest reward is selected. In this paper, we offer a closed-form characterization of the optimal KL-constrained RL solution. We demonstrate that any alignment method that achieves a comparable trade-off between KL divergence and reward must approximate the optimal KL-constrained RL solution in terms of relative entropy. To further analyze the properties of alignment methods, we introduce two simplifying assumptions: we let the language model be memoryless, and the reward model be linear. Although these assumptions may not reflect complex real-world scenarios, they enable a precise characterization of the asymptotic behavior of both the best-of-$N$ alignment, and the KL-constrained RL method, in terms of information-theoretic quantities. We prove that the reward of the optimal KL-constrained RL solution satisfies a large deviation principle, and we fully characterize its rate function. We also show that the rate of growth of the scaled cumulants of the reward is characterized by a proper Renyi cross entropy. Finally, we show that best-of-$N$ is asymptotically equivalent to KL-constrained RL solution by proving that their expected rewards are asymptotically equal, and concluding that the two distributions must be close in KL divergence.</p></p class="citation"></blockquote><h3 id=4248--232327-efficient-online-unlearning-via-hessian-free-recollection-of-individual-data-statistics-xinbao-qiao-et-al-2024>(42/48 | 232/327) Efficient Online Unlearning via Hessian-Free Recollection of Individual Data Statistics (Xinbao Qiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinbao Qiao, Meng Zhang, Ming Tang, Ermin Wei. (2024)<br><strong>Efficient Online Unlearning via Hessian-Free Recollection of Individual Data Statistics</strong><br><button class=copy-to-clipboard title="Efficient Online Unlearning via Hessian-Free Recollection of Individual Data Statistics" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Machine Unlearning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01712v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01712v1.pdf filename=2404.01712v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Machine</b> <b>unlearning</b> strives to uphold the data owners&rsquo; right to be forgotten by enabling models to selectively forget specific data. Recent methods suggest that one approach of data forgetting is by precomputing and storing statistics carrying second-order information to improve computational and memory efficiency. However, they rely on restrictive assumptions and the computation/storage suffer from the curse of model parameter dimensionality, making it challenging to apply to most deep neural networks. In this work, we propose a Hessian-free online unlearning method. We propose to maintain a statistical vector for each data point, computed through affine stochastic recursion approximation of the difference between retrained and learned models. Our proposed algorithm achieves near-instantaneous online unlearning as it only requires a vector addition operation. Based on the strategy that recollecting statistics for forgetting data, the proposed method significantly reduces the unlearning runtime. Experimental studies demonstrate that the proposed scheme surpasses existing results by orders of magnitude in terms of time and memory costs, while also enhancing accuracy.</p></p class="citation"></blockquote><h3 id=4348--233327-extremum-seeking-action-selection-for-accelerating-policy-optimization-ya-chien-chang-et-al-2024>(43/48 | 233/327) Extremum-Seeking Action Selection for Accelerating Policy Optimization (Ya-Chien Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ya-Chien Chang, Sicun Gao. (2024)<br><strong>Extremum-Seeking Action Selection for Accelerating Policy Optimization</strong><br><button class=copy-to-clipboard title="Extremum-Seeking Action Selection for Accelerating Policy Optimization" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01598v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01598v1.pdf filename=2404.01598v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> for control over continuous spaces typically uses high-entropy stochastic policies, such as Gaussian distributions, for local exploration and estimating policy gradient to optimize performance. Many robotic control problems deal with complex unstable dynamics, where applying actions that are off the feasible control manifolds can quickly lead to undesirable divergence. In such cases, most samples taken from the ambient action space generate low-value trajectories that hardly contribute to policy improvement, resulting in slow or failed learning. We propose to improve action selection in this model-free RL setting by introducing additional adaptive control steps based on Extremum-Seeking Control (ESC). On each action sampled from stochastic policies, we apply sinusoidal perturbations and query for estimated Q-values as the response signal. Based on ESC, we then dynamically improve the sampled actions to be closer to nearby optima before applying them to the environment. Our methods can be easily added in standard policy optimization to improve learning efficiency, which we demonstrate in various control learning environments.</p></p class="citation"></blockquote><h3 id=4448--234327-universal-representations-for-financial-transactional-data-embracing-local-global-and-external-contexts-alexandra-bazarova-et-al-2024>(44/48 | 234/327) Universal representations for financial transactional data: embracing local, global, and external contexts (Alexandra Bazarova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandra Bazarova, Maria Kovaleva, Ilya Kuleshov, Evgenia Romanenkova, Alexander Stepikin, Alexandr Yugay, Dzhambulat Mollaev, Ivan Kireev, Andrey Savchenko, Alexey Zaytsev. (2024)<br><strong>Universal representations for financial transactional data: embracing local, global, and external contexts</strong><br><button class=copy-to-clipboard title="Universal representations for financial transactional data: embracing local, global, and external contexts" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 8<br>Keywords: Benchmarking, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02047v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02047v1.pdf filename=2404.02047v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective processing of financial transactions is essential for banking data analysis. However, in this domain, most methods focus on specialized solutions to stand-alone problems instead of constructing universal <b>representations</b> <b>suitable</b> for many problems. We present a <b>representation</b> <b>learning</b> framework that addresses diverse business challenges. We also suggest novel generative models that account for data specifics, and a way to integrate external information into a client&rsquo;s <b>representation,</b> <b>leveraging</b> insights from other customers&rsquo; actions. Finally, we offer a <b>benchmark,</b> describing <b>representation</b> <b>quality</b> globally, concerning the entire transaction history; locally, reflecting the client&rsquo;s current state; and dynamically, capturing <b>representation</b> <b>evolution</b> over time. Our generative approach demonstrates superior performance in local tasks, with an increase in ROC-AUC of up to 14% for the next MCC prediction task and up to 46% for downstream tasks from existing contrastive baselines. Incorporating external information improves the scores by an additional 20%.</p></p class="citation"></blockquote><h3 id=4548--235327-attribution-regularization-for-multimodal-paradigms-sahiti-yerramilli-et-al-2024>(45/48 | 235/327) Attribution Regularization for Multimodal Paradigms (Sahiti Yerramilli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sahiti Yerramilli, Jayant Sravan Tamarapalli, Jonathan Francis, Eric Nyberg. (2024)<br><strong>Attribution Regularization for Multimodal Paradigms</strong><br><button class=copy-to-clipboard title="Attribution Regularization for Multimodal Paradigms" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02359v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02359v1.pdf filename=2404.02359v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> machine learning has gained significant attention in recent years due to its potential for integrating information from multiple modalities to enhance learning and decision-making processes. However, it is commonly observed that unimodal models outperform <b>multimodal</b> models, despite the latter having access to richer information. Additionally, the influence of a single modality often dominates the decision-making process, resulting in suboptimal performance. This research project aims to address these challenges by proposing a novel regularization term that encourages <b>multimodal</b> models to effectively utilize information from all modalities when making decisions. The focus of this project lies in the video-audio domain, although the proposed regularization technique holds promise for broader applications in embodied AI research, where multiple modalities are involved. By leveraging this regularization term, the proposed approach aims to mitigate the issue of unimodal dominance and improve the performance of <b>multimodal</b> machine learning systems. Through extensive experimentation and evaluation, the effectiveness and generalizability of the proposed technique will be assessed. The findings of this research project have the potential to significantly contribute to the advancement of <b>multimodal</b> machine learning and facilitate its application in various domains, including multimedia analysis, human-computer interaction, and embodied AI research.</p></p class="citation"></blockquote><h3 id=4648--236327-generalizable-fast-and-accurate-deepqspr-with-fastprop-part-1-framework-and-benchmarks-jackson-burns-et-al-2024>(46/48 | 236/327) Generalizable, Fast, and Accurate DeepQSPR with fastprop Part 1: Framework and Benchmarks (Jackson Burns et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jackson Burns, William Green. (2024)<br><strong>Generalizable, Fast, and Accurate DeepQSPR with fastprop Part 1: Framework and Benchmarks</strong><br><button class=copy-to-clipboard title="Generalizable, Fast, and Accurate DeepQSPR with fastprop Part 1: Framework and Benchmarks" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-chem-ph<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02058v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02058v1.pdf filename=2404.02058v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantitative Structure Property Relationship studies aim to define a mapping between molecular structure and arbitrary quantities of interest. This was historically accomplished via the development of descriptors which requires significant domain expertise and struggles to generalize. Thus the field has morphed into Molecular Property Prediction and been given over to learned representations which are highly generalizable. The paper introduces fastprop, a DeepQSPR framework which uses a cogent set of molecular level descriptors to meet and exceed the performance of learned representations on diverse datasets in dramatically less time. fastprop is freely available on github at github.com/JacksonBurns/fastprop.</p></p class="citation"></blockquote><h3 id=4748--237327-what-is-to-be-gained-by-ensemble-models-in-analysis-of-spectroscopic-data-katarina-domijan-2024>(47/48 | 237/327) What is to be gained by ensemble models in analysis of spectroscopic data? (Katarina Domijan, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Katarina Domijan. (2024)<br><strong>What is to be gained by ensemble models in analysis of spectroscopic data?</strong><br><button class=copy-to-clipboard title="What is to be gained by ensemble models in analysis of spectroscopic data?" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ME<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02184v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02184v1.pdf filename=2404.02184v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An empirical study was carried out to compare different implementations of ensemble models aimed at improving prediction in spectroscopic data. A wide range of candidate models were fitted to <b>benchmark</b> datasets from regression and classification settings. A statistical analysis using linear mixed model was carried out on prediction performance criteria resulting from model fits over random splits of the data. The results showed that the ensemble classifiers were able to consistently outperform candidate models in our application</p></p class="citation"></blockquote><h3 id=4848--238327-settling-time-vs-accuracy-tradeoffs-for-clustering-big-data-andrew-draganov-et-al-2024>(48/48 | 238/327) Settling Time vs. Accuracy Tradeoffs for Clustering Big Data (Andrew Draganov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Draganov, David Saulpic, Chris Schwiegelshohn. (2024)<br><strong>Settling Time vs. Accuracy Tradeoffs for Clustering Big Data</strong><br><button class=copy-to-clipboard title="Settling Time vs. Accuracy Tradeoffs for Clustering Big Data" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DS, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01936v1.pdf filename=2404.01936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the theoretical and practical runtime limits of k-means and k-median <b>clustering</b> on large datasets. Since effectively all <b>clustering</b> methods are slower than the time it takes to read the dataset, the fastest approach is to quickly compress the data and perform the <b>clustering</b> on the compressed representation. Unfortunately, there is no universal best choice for compressing the number of points - while random sampling runs in sublinear time and coresets provide theoretical guarantees, the former does not enforce accuracy while the latter is too slow as the numbers of points and clusters grow. Indeed, it has been conjectured that any sensitivity-based coreset construction requires super-linear time in the dataset size. We examine this relationship by first showing that there does exist an algorithm that obtains coresets via sensitivity sampling in effectively linear time - within log-factors of the time it takes to read the data. Any approach that significantly improves on this must then resort to practical heuristics, leading us to consider the spectrum of sampling strategies across both real and artificial datasets in the static and streaming settings. Through this, we show the conditions in which coresets are necessary for preserving cluster validity as well as the settings in which faster, cruder sampling strategies are sufficient. As a result, we provide a comprehensive theoretical and practical blueprint for effective <b>clustering</b> regardless of data size. Our code is publicly available and has scripts to recreate the experiments.</p></p class="citation"></blockquote><h2 id=cssd-4>cs.SD (4)</h2><h3 id=14--239327-weakly-supervised-audio-separation-via-bi-modal-semantic-similarity-tanvir-mahmud-et-al-2024>(1/4 | 239/327) Weakly-supervised Audio Separation via Bi-modal Semantic Similarity (Tanvir Mahmud et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanvir Mahmud, Saeed Amizadeh, Kazuhito Koishida, Diana Marculescu. (2024)<br><strong>Weakly-supervised Audio Separation via Bi-modal Semantic Similarity</strong><br><button class=copy-to-clipboard title="Weakly-supervised Audio Separation via Bi-modal Semantic Similarity" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 50<br>Keywords: Distribution Shift, Distribution Shift, Supervised Learning, Supervised Learning, Unsupervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01740v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01740v1.pdf filename=2404.01740v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conditional sound separation in multi-source audio mixtures without having access to single source sound data during training is a long standing challenge. Existing mix-and-separate based methods suffer from significant performance drop with multi-source training mixtures due to the lack of supervision signal for single source separation cases during training. However, in the case of language-conditional audio separation, we do have access to corresponding text descriptions for each audio mixture in our training data, which can be seen as (rough) representations of the audio samples in the language modality. To this end, in this paper, we propose a generic bi-modal separation framework which can enhance the existing <b>unsupervised</b> frameworks to separate single-source signals in a target modality (i.e., audio) using the easily separable corresponding signals in the conditioning modality (i.e., language), without having access to single-source samples in the target modality during training. We empirically show that this is well within reach if we have access to a pretrained joint embedding model between the two modalities (i.e., CLAP). Furthermore, we propose to incorporate our framework into two fundamental scenarios to enhance separation performance. First, we show that our proposed methodology significantly improves the performance of purely <b>unsupervised</b> baselines by reducing the <b>distribution</b> <b>shift</b> between training and test samples. In particular, we show that our framework can achieve 71% boost in terms of Signal-to-Distortion Ratio (SDR) over the baseline, reaching 97.5% of the <b>supervised</b> <b>learning</b> performance. Second, we show that we can further improve the performance of the <b>supervised</b> <b>learning</b> itself by 17% if we augment it by our proposed <b>weakly-supervised</b> framework, that enables a powerful semi-supervised framework for audio separation.</p></p class="citation"></blockquote><h3 id=24--240327-smitin-self-monitored-inference-time-intervention-for-generative-music-transformers-junghyun-koo-et-al-2024>(2/4 | 240/327) SMITIN: Self-Monitored Inference-Time INtervention for Generative Music Transformers (Junghyun Koo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junghyun Koo, Gordon Wichern, Francois G. Germain, Sameer Khurana, Jonathan Le Roux. (2024)<br><strong>SMITIN: Self-Monitored Inference-Time INtervention for Generative Music Transformers</strong><br><button class=copy-to-clipboard title="SMITIN: Self-Monitored Inference-Time INtervention for Generative Music Transformers" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 30<br>Keywords: Fine-tuning, Logistic Regression, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02252v1.pdf filename=2404.02252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Self-Monitored Inference-Time INtervention (SMITIN), an approach for controlling an autoregressive generative music <b>transformer</b> using classifier probes. These simple <b>logistic</b> <b>regression</b> probes are trained on the output of each attention head in the <b>transformer</b> using a small dataset of audio examples both exhibiting and missing a specific musical trait (e.g., the presence/absence of drums, or real/synthetic music). We then steer the attention heads in the probe direction, ensuring the generative model output captures the desired musical trait. Additionally, we monitor the probe output to avoid adding an excessive amount of intervention into the autoregressive generation, which could lead to temporally incoherent music. We validate our results objectively and subjectively for both audio continuation and text-to-music applications, demonstrating the ability to add controls to large generative models for which retraining or even <b>fine-tuning</b> is impractical for most musicians. Audio samples of the proposed intervention approach are available on our demo page <a href=http://tinyurl.com/smitin>http://tinyurl.com/smitin</a> .</p></p class="citation"></blockquote><h3 id=34--241327-spmamba-state-space-model-is-all-you-need-in-speech-separation-kai-li-et-al-2024>(3/4 | 241/327) SPMamba: State-space model is all you need in speech separation (Kai Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Li, Guo Chen. (2024)<br><strong>SPMamba: State-space model is all you need in speech separation</strong><br><button class=copy-to-clipboard title="SPMamba: State-space model is all you need in speech separation" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02063v1.pdf filename=2404.02063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In speech separation, both CNN- and <b>Transformer-based</b> models have demonstrated robust separation capabilities, garnering significant attention within the research community. However, <b>CNN-based</b> methods have limited modelling capability for long-sequence audio, leading to suboptimal separation performance. Conversely, <b>Transformer-based</b> methods are limited in practical applications due to their high computational complexity. Notably, within computer vision, Mamba-based methods have been celebrated for their formidable performance and reduced computational requirements. In this paper, we propose a network architecture for speech separation using a state-space model, namely SPMamba. We adopt the TF-GridNet model as the foundational framework and substitute its <b>Transformer</b> component with a bidirectional Mamba module, aiming to capture a broader range of contextual information. Our experimental results reveal an important role in the performance aspects of Mamba-based models. SPMamba demonstrates superior performance with a significant advantage over existing separation models in a dataset built on Librispeech. Notably, SPMamba achieves a substantial improvement in separation quality, with a 2.42 dB enhancement in SI-SNRi compared to the TF-GridNet. The source code for SPMamba is publicly accessible at <a href=https://github.com/JusperLee/SPMamba>https://github.com/JusperLee/SPMamba</a> .</p></p class="citation"></blockquote><h3 id=44--242327-voice-ehr-introducing-multimodal-audio-data-for-health-james-anibal-et-al-2024>(4/4 | 242/327) Voice EHR: Introducing Multimodal Audio Data for Health (James Anibal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>James Anibal, Hannah Huth, Ming Li, Lindsey Hazen, Yen Minh Lam, Nguyen Thi Thu Hang, Michael Kleinman, Shelley Ost, Christopher Jackson, Laura Sprabery, Cheran Elangovan, Balaji Krishnaiah, Lee Akst, Ioan Lina, Iqbal Elyazar, Lenny Ekwati, Stefan Jansen, Richard Nduwayezu, Charisse Garcia, Jeffrey Plum, Jacqueline Brenner, Miranda Song, Emily Ricotta, David Clifton, C. Louise Thwaites, Yael Bensoussan, Bradford Wood. (2024)<br><strong>Voice EHR: Introducing Multimodal Audio Data for Health</strong><br><button class=copy-to-clipboard title="Voice EHR: Introducing Multimodal Audio Data for Health" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-CY, cs-SD, cs.SD, eess-AS<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01620v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01620v1.pdf filename=2404.01620v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large AI models trained on audio data may have the potential to rapidly classify patients, enhancing medical decision-making and potentially improving outcomes through early detection. Existing technologies depend on limited datasets using expensive recording equipment in high-income, English-speaking countries. This challenges deployment in resource-constrained, high-volume settings where audio data may have a profound impact. This report introduces a novel data type and a corresponding collection system that captures health data through guided questions using only a mobile/web application. This application ultimately results in an audio electronic health record (voice EHR) which may contain complex biomarkers of health from conventional voice/respiratory features, speech patterns, and language with semantic meaning - compensating for the typical limitations of unimodal clinical datasets. This report introduces a consortium of partners for global work, presents the application used for data collection, and showcases the potential of informative voice EHR to advance the scalability and diversity of audio AI.</p></p class="citation"></blockquote><h2 id=eessas-2>eess.AS (2)</h2><h3 id=12--243327-transfer-learning-from-whisper-for-microscopic-intelligibility-prediction-paul-best-et-al-2024>(1/2 | 243/327) Transfer Learning from Whisper for Microscopic Intelligibility Prediction (Paul Best et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Best, Santiago Cuervo, Ricard Marxer. (2024)<br><strong>Transfer Learning from Whisper for Microscopic Intelligibility Prediction</strong><br><button class=copy-to-clipboard title="Transfer Learning from Whisper for Microscopic Intelligibility Prediction" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-CL, cs-SD, eess-AS, eess.AS<br>Keyword Score: 50<br>Keywords: Fine-tuning, Transfer Learning, Zero-shot, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01737v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01737v1.pdf filename=2404.01737v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Macroscopic intelligibility models predict the expected human word-error-rate for a given <b>speech-in-noise</b> <b>stimulus.</b> In contrast, microscopic intelligibility models aim to make fine-grained predictions about listeners&rsquo; perception, e.g. predicting phonetic or lexical responses. State-of-the-art macroscopic models use <b>transfer</b> <b>learning</b> from large scale deep learning models for <b>speech</b> <b>processing,</b> whereas such methods have rarely been used for microscopic modeling. In this paper, we study the use of <b>transfer</b> <b>learning</b> from Whisper, a state-of-the-art deep learning model for <b>automatic</b> <b>speech</b> <b>recognition,</b> for microscopic intelligibility prediction at the level of lexical responses. Our method outperforms the considered baselines, even in a <b>zero-shot</b> setup, and yields a relative improvement of up to 66% when <b>fine-tuned</b> to predict listeners&rsquo; responses. Our results showcase the promise of large scale deep learning based methods for microscopic intelligibility prediction.</p></p class="citation"></blockquote><h3 id=22--244327-effective-internal-language-model-training-and-fusion-for-factorized-transducer-model-jinxi-guo-et-al-2024>(2/2 | 244/327) Effective internal language model training and fusion for factorized transducer model (Jinxi Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinxi Guo, Niko Moritz, Yingyi Ma, Frank Seide, Chunyang Wu, Jay Mahadeokar, Ozlem Kalinli, Christian Fuegen, Mike Seltzer. (2024)<br><strong>Effective internal language model training and fusion for factorized transducer model</strong><br><button class=copy-to-clipboard title="Effective internal language model training and fusion for factorized transducer model" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-AI, cs-CL, cs-LG, eess-AS, eess.AS<br>Keyword Score: 10<br>Keywords: Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01716v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01716v1.pdf filename=2404.01716v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The internal language model (ILM) of the neural transducer has been widely studied. In most prior work, it is mainly used for estimating the ILM score and is subsequently subtracted during inference to facilitate improved integration with external language models. Recently, various of factorized transducer models have been proposed, which explicitly embrace a standalone internal language model for non-blank token prediction. However, even with the adoption of factorized transducer models, limited improvement has been observed compared to shallow fusion. In this paper, we propose a novel ILM training and decoding strategy for factorized transducer models, which effectively combines the blank, acoustic and ILM scores. Our experiments show a 17% relative improvement over the standard decoding method when utilizing a well-trained ILM and the proposed decoding strategy on LibriSpeech datasets. Furthermore, when compared to a strong <b>RNN-T</b> baseline enhanced with external LM fusion, the proposed model yields a 5.5% relative improvement on general-sets and an 8.9% WER reduction for rare words. The proposed model can achieve superior performance without relying on external language models, rendering it highly efficient for production use-cases. To further improve the performance, we propose a novel and memory-efficient ILM-fusion-aware minimum word error rate (MWER) training method which improves ILM integration significantly.</p></p class="citation"></blockquote><h2 id=statml-5>stat.ML (5)</h2><h3 id=15--245327-preventing-model-collapse-in-gaussian-process-latent-variable-models-ying-li-et-al-2024>(1/5 | 245/327) Preventing Model Collapse in Gaussian Process Latent Variable Models (Ying Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ying Li, Zhidi Lin, Feng Yin, Michael Minyi Zhang. (2024)<br><strong>Preventing Model Collapse in Gaussian Process Latent Variable Models</strong><br><button class=copy-to-clipboard title="Preventing Model Collapse in Gaussian Process Latent Variable Models" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 50<br>Keywords: Autoencoder, Gaussian Process, Unsupervised Learning, Unsupervised Learning, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01697v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01697v1.pdf filename=2404.01697v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Gaussian</b> <b>process</b> latent variable models (GPLVMs) are a versatile family of <b>unsupervised</b> <b>learning</b> models, commonly used for dimensionality reduction. However, common challenges in modeling data with GPLVMs include inadequate kernel flexibility and improper selection of the projection noise, which leads to a type of model collapse characterized primarily by vague latent representations that do not reflect the underlying structure of the data. This paper addresses these issues by, first, theoretically examining the impact of the projection variance on model collapse through the lens of a linear GPLVM. Second, we address the problem of model collapse due to inadequate kernel flexibility by integrating the spectral mixture (SM) kernel and a differentiable random Fourier feature (RFF) kernel approximation, which ensures computational scalability and efficiency through off-the-shelf automatic differentiation tools for learning the kernel hyperparameters, projection variance, and latent representations within the <b>variational</b> <b>inference</b> framework. The proposed GPLVM, named advisedRFLVM, is evaluated across diverse datasets and consistently outperforms various salient competing models, including state-of-the-art <b>variational</b> <b>autoencoders</b> (VAEs) and GPLVM variants, in terms of informative latent representations and missing data imputation.</p></p class="citation"></blockquote><h3 id=25--246327-on-stronger-computational-separations-between-multimodal-and-unimodal-machine-learning-ari-karchmer-2024>(2/5 | 246/327) On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning (Ari Karchmer, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ari Karchmer. (2024)<br><strong>On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning</strong><br><button class=copy-to-clipboard title="On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-AI, cs-LG, stat-ML, stat.ML<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, GPT, GPT-4, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02254v1.pdf filename=2404.02254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>multimodal</b> machine learning, multiple modalities of data (e.g., <b>text</b> <b>and</b> images) are combined to facilitate the learning of a better machine learning model, which remains applicable to a corresponding unimodal task (e.g., <b>text</b> <b>generation).</b> Recently, <b>multimodal</b> machine learning has enjoyed huge empirical success (e.g. <b>GPT-4).</b> Motivated to develop theoretical justification for this empirical success, Lu (NeurIPS &lsquo;23, ALT &lsquo;24) introduces a theory of <b>multimodal</b> learning, and considers possible separations between theoretical models of <b>multimodal</b> and unimodal learning. In particular, Lu (ALT &lsquo;24) shows a computational separation, which is relevant to worst-case instances of the learning task. In this paper, we give a stronger average-case computational separation, where for &ldquo;typical&rdquo; instances of the learning task, unimodal learning is computationally hard, but <b>multimodal</b> learning is easy. We then question how &ldquo;organic&rdquo; the average-case separation is. Would it be encountered in practice? To this end, we prove that under natural conditions, any given computational separation between average-case unimodal and <b>multimodal</b> learning tasks implies a corresponding cryptographic key agreement protocol. We suggest to interpret this as evidence that very strong computational advantages of <b>multimodal</b> learning may arise infrequently in practice, since they exist only for the &ldquo;pathological&rdquo; case of inherently cryptographic distributions. However, this does not apply to possible (super-polynomial) statistical advantages.</p></p class="citation"></blockquote><h3 id=35--247327-doubly-robust-off-policy-evaluation-with-estimated-logging-policy-kyungbok-lee-et-al-2024>(3/5 | 247/327) Doubly-Robust Off-Policy Evaluation with Estimated Logging Policy (Kyungbok Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyungbok Lee, Myunghee Cho Paik. (2024)<br><strong>Doubly-Robust Off-Policy Evaluation with Estimated Logging Policy</strong><br><button class=copy-to-clipboard title="Doubly-Robust Off-Policy Evaluation with Estimated Logging Policy" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01830v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01830v1.pdf filename=2404.01830v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel doubly-robust (DR) off-policy evaluation (OPE) estimator for Markov decision processes, DRUnknown, designed for situations where both the logging policy and the value function are unknown. The proposed estimator initially estimates the logging policy and then estimates the value function model by minimizing the asymptotic variance of the estimator while considering the estimating effect of the logging policy. When the logging policy model is correctly specified, DRUnknown achieves the smallest asymptotic variance within the class containing existing OPE estimators. When the value function model is also correctly specified, DRUnknown is optimal as its asymptotic variance reaches the semiparametric lower bound. We present experimental results conducted in contextual <b>bandits</b> and <b>reinforcement</b> <b>learning</b> to compare the performance of DRUnknown with that of existing methods.</p></p class="citation"></blockquote><h3 id=45--248327-fairm-learning-invariant-representations-for-algorithmic-fairness-and-domain-generalization-with-minimax-optimality-sai-li-et-al-2024>(4/5 | 248/327) FAIRM: Learning invariant representations for algorithmic fairness and domain generalization with minimax optimality (Sai Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sai Li, Linjun Zhang. (2024)<br><strong>FAIRM: Learning invariant representations for algorithmic fairness and domain generalization with minimax optimality</strong><br><button class=copy-to-clipboard title="FAIRM: Learning invariant representations for algorithmic fairness and domain generalization with minimax optimality" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ME, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: MNIST, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01608v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01608v1.pdf filename=2404.01608v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning methods often assume that the test data have the same distribution as the training data. However, this assumption may not hold due to multiple levels of heterogeneity in applications, raising issues in algorithmic <b>fairness</b> and domain generalization. In this work, we address the problem of fair and generalizable machine learning by invariant principles. We propose a training environment-based oracle, FAIRM, which has desirable <b>fairness</b> and domain generalization properties under a diversity-type condition. We then provide an empirical FAIRM with finite-sample theoretical guarantees under weak distributional assumptions. We then develop efficient algorithms to realize FAIRM in linear models and demonstrate the nonasymptotic performance with minimax optimality. We evaluate our method in numerical experiments with synthetic data and <b>MNIST</b> data and show that it outperforms its counterparts.</p></p class="citation"></blockquote><h3 id=55--249327-adversarial-combinatorial-bandits-with-switching-costs-yanyan-dong-et-al-2024>(5/5 | 249/327) Adversarial Combinatorial Bandits with Switching Costs (Yanyan Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanyan Dong, Vincent Y. F. Tan. (2024)<br><strong>Adversarial Combinatorial Bandits with Switching Costs</strong><br><button class=copy-to-clipboard title="Adversarial Combinatorial Bandits with Switching Costs" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01883v1.pdf filename=2404.01883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of adversarial combinatorial <b>bandit</b> with a switching cost $\lambda$ for a switch of each selected arm in each round, considering both the <b>bandit</b> feedback and semi-bandit feedback settings. In the oblivious adversarial case with $K$ base arms and time horizon $T$, we derive lower bounds for the minimax regret and design algorithms to approach them. To prove these lower bounds, we design stochastic loss sequences for both feedback settings, building on an idea from previous work in Dekel et al. (2014). The lower bound for <b>bandit</b> feedback is $ \tilde{\Omega}\big( (\lambda K)^{\frac{1}{3}} (TI)^{\frac{2}{3}}\big)$ while that for semi-bandit feedback is $ \tilde{\Omega}\big( (\lambda K I)^{\frac{1}{3}} T^{\frac{2}{3}}\big)$ where $I$ is the number of base arms in the combinatorial arm played in each round. To approach these lower bounds, we design algorithms that operate in batches by dividing the time horizon into batches to restrict the number of switches between actions. For the <b>bandit</b> feedback setting, where only the total loss of the combinatorial arm is observed, we introduce the Batched-Exp2 algorithm which achieves a regret upper bound of $\tilde{O}\big((\lambda K)^{\frac{1}{3}}T^{\frac{2}{3}}I^{\frac{4}{3}}\big)$ as $T$ tends to infinity. In the semi-bandit feedback setting, where all losses for the combinatorial arm are observed, we propose the Batched-BROAD algorithm which achieves a regret upper bound of $\tilde{O}\big( (\lambda K)^{\frac{1}{3}} (TI)^{\frac{2}{3}}\big)$.</p></p class="citation"></blockquote><h2 id=cshc-7>cs.HC (7)</h2><h3 id=17--250327-insightlens-discovering-and-exploring-insights-from-conversational-contexts-in-large-language-model-powered-data-analysis-luoxuan-weng-et-al-2024>(1/7 | 250/327) InsightLens: Discovering and Exploring Insights from Conversational Contexts in Large-Language-Model-Powered Data Analysis (Luoxuan Weng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luoxuan Weng, Xingbo Wang, Junyu Lu, Yingchaojie Feng, Yihan Liu, Wei Chen. (2024)<br><strong>InsightLens: Discovering and Exploring Insights from Conversational Contexts in Large-Language-Model-Powered Data Analysis</strong><br><button class=copy-to-clipboard title="InsightLens: Discovering and Exploring Insights from Conversational Contexts in Large-Language-Model-Powered Data Analysis" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 50<br>Keywords: Natural Language Explanation, Natural Language Inference, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01644v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01644v1.pdf filename=2404.01644v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has revolutionized the capabilities of <b>natural</b> <b>language</b> <b>interfaces</b> <b>(NLIs)</b> for data analysis. <b>LLMs</b> can perform multi-step and complex <b>reasoning</b> to generate data insights based on users&rsquo; analytic intents. However, these insights often entangle with an abundance of contexts in analytic conversations such as code, visualizations, and <b>natural</b> <b>language</b> <b>explanations.</b> This hinders efficient identification, verification, and interpretation of insights within the current chat-based interfaces of <b>LLMs.</b> In this paper, we first conduct a formative study with eight experienced data analysts to understand their general workflow and pain points during <b>LLM-powered</b> data analysis. Then, we propose an <b>LLM-based</b> multi-agent framework to automatically extract, associate, and organize insights along with the analysis process. Based on this, we introduce InsightLens, an interactive system that visualizes the intricate conversational contexts from multiple aspects to facilitate insight discovery and exploration. A user study with twelve data analysts demonstrates the effectiveness of InsightLens, showing that it significantly reduces users&rsquo; manual and cognitive effort without disrupting their conversational data analysis workflow, leading to a more efficient analysis experience.</p></p class="citation"></blockquote><h3 id=27--251327-exploring-how-multiple-levels-of-gpt-generated-programming-hints-support-or-disappoint-novices-ruiwei-xiao-et-al-2024>(2/7 | 251/327) Exploring How Multiple Levels of GPT-Generated Programming Hints Support or Disappoint Novices (Ruiwei Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruiwei Xiao, Xinying Hou, John Stamper. (2024)<br><strong>Exploring How Multiple Levels of GPT-Generated Programming Hints Support or Disappoint Novices</strong><br><button class=copy-to-clipboard title="Exploring How Multiple Levels of GPT-Generated Programming Hints Support or Disappoint Novices" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CY, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: GPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02213v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02213v1.pdf filename=2404.02213v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have integrated <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> into diverse educational contexts, including providing adaptive programming hints, a type of feedback focuses on helping students move forward during problem-solving. However, most existing <b>LLM-based</b> hint systems are limited to one single hint type. To investigate whether and how different levels of hints can support students&rsquo; problem-solving and learning, we conducted a think-aloud study with 12 novices using the <b>LLM</b> Hint Factory, a system providing four levels of hints from general natural language guidance to concrete code assistance, varying in format and granularity. We discovered that high-level natural language hints alone can be helpless or even misleading, especially when addressing next-step or syntax-related help requests. Adding lower-level hints, like code examples with in-line comments, can better support students. The findings open up future work on customizing help responses from content, format, and granularity levels to accurately identify and meet students&rsquo; learning needs.</p></p class="citation"></blockquote><h3 id=37--252327-explainability-in-jupyterlab-and-beyond-interactive-xai-systems-for-integrated-and-collaborative-workflows-grace-guo-et-al-2024>(3/7 | 252/327) Explainability in JupyterLab and Beyond: Interactive XAI Systems for Integrated and Collaborative Workflows (Grace Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Grace Guo, Dustin Arendt, Alex Endert. (2024)<br><strong>Explainability in JupyterLab and Beyond: Interactive XAI Systems for Integrated and Collaborative Workflows</strong><br><button class=copy-to-clipboard title="Explainability in JupyterLab and Beyond: Interactive XAI Systems for Integrated and Collaborative Workflows" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Explainable AI, human-in-the-loop, Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02081v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02081v1.pdf filename=2404.02081v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Explainable</b> <b>AI</b> (XAI) tools represent a turn to more human-centered and <b>human-in-the-loop</b> AI approaches that emphasize user needs and perspectives in machine learning model development workflows. However, while the majority of ML resources available today are developed for Python computational environments such as JupyterLab and Jupyter Notebook, the same has not been true of interactive XAI systems, which are often still implemented as standalone interfaces. In this paper, we address this mismatch by identifying three design patterns for embedding front-end XAI interfaces into Jupyter, namely: 1) One-way communication from Python to JavaScript, 2) Two-way data synchronization, and 3) Bi-directional callbacks. We also provide an open-source toolkit, bonXAI, that demonstrates how each design pattern might be used to build interactive XAI tools for a Pytorch <b>text</b> <b>classification</b> workflow. Finally, we conclude with a discussion of best practices and open questions. Our aims for this paper are to discuss how interactive XAI tools might be developed for computational notebooks, and how they can better integrate into existing model development workflows to support more collaborative, human-centered AI.</p></p class="citation"></blockquote><h3 id=47--253327-harder-better-faster-stronger-interactive-visualization-for-human-centered-ai-tools-md-naimul-hoque-et-al-2024>(4/7 | 253/327) Harder, Better, Faster, Stronger: Interactive Visualization for Human-Centered AI Tools (Md Naimul Hoque et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Naimul Hoque, Sungbok Shin, Niklas Elmqvist. (2024)<br><strong>Harder, Better, Faster, Stronger: Interactive Visualization for Human-Centered AI Tools</strong><br><button class=copy-to-clipboard title="Harder, Better, Faster, Stronger: Interactive Visualization for Human-Centered AI Tools" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02147v1.pdf filename=2404.02147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human-centered AI (HCAI), rather than replacing the human, puts the human user in the driver&rsquo;s seat of so-called human-centered AI-infused tools (HCAI tools): interactive software tools that amplify, augment, empower, and enhance human performance using AI models; often novel generative or foundation AI ones. In this paper, we discuss how interactive visualization can be a key enabling technology for creating such human-centered AI tools. Visualization has already been shown to be a fundamental component in <b>explainable</b> <b>AI</b> models, and coupling this with data-driven, semantic, and unified interaction feedback loops will enable a human-centered approach to integrating AI models in the loop with human users. We present several examples of our past and current work on such HCAI tools, including for creative writing, temporal prediction, and user experience analysis. We then draw parallels between these tools to suggest common themes on how interactive visualization can support the design of future HCAI tools.</p></p class="citation"></blockquote><h3 id=57--254327-tell-and-show-combining-multiple-modalities-to-communicate-manipulation-tasks-to-a-robot-petr-vanc-et-al-2024>(5/7 | 254/327) Tell and show: Combining multiple modalities to communicate manipulation tasks to a robot (Petr Vanc et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Petr Vanc, Radoslav Skoviera, Karla Stepanova. (2024)<br><strong>Tell and show: Combining multiple modalities to communicate manipulation tasks to a robot</strong><br><button class=copy-to-clipboard title="Tell and show: Combining multiple modalities to communicate manipulation tasks to a robot" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-RO, cs.HC<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01702v1.pdf filename=2404.01702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As human-robot collaboration is becoming more widespread, there is a need for a more natural way of communicating with the robot. This includes combining data from several modalities together with the context of the situation and background knowledge. Current approaches to communication typically rely only on a single modality or are often very rigid and not robust to missing, misaligned, or noisy data. In this paper, we propose a novel method that takes inspiration from sensor fusion approaches to combine uncertain information from multiple modalities and enhance it with situational awareness (e.g., considering object properties or the scene setup). We first evaluate the proposed solution on simulated bimodal datasets (gestures and language) and show by several ablation experiments the importance of various components of the system and its robustness to noisy, missing, or misaligned observations. Then we implement and evaluate the model on the real setup. In human-robot interaction, we must also consider whether the selected action is probable enough to be executed or if we should better query humans for clarification. For these purposes, we enhance our model with adaptive entropy-based thresholding that detects the appropriate thresholds for different types of interaction showing similar performance as <b>fine-tuned</b> fixed thresholds.</p></p class="citation"></blockquote><h3 id=67--255327-gen4ds-workshop-on-data-storytelling-in-an-era-of-generative-ai-xingyu-lan-et-al-2024>(6/7 | 255/327) Gen4DS: Workshop on Data Storytelling in an Era of Generative AI (Xingyu Lan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyu Lan, Leni Yang, Zezhong Wang, Yun Wang, Danqing Shi, Sheelagh Carpendale. (2024)<br><strong>Gen4DS: Workshop on Data Storytelling in an Era of Generative AI</strong><br><button class=copy-to-clipboard title="Gen4DS: Workshop on Data Storytelling in an Era of Generative AI" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-GR, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01622v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01622v2.pdf filename=2404.01622v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Storytelling is an ancient and precious human ability that has been rejuvenated in the digital age. Over the last decade, there has been a notable surge in the recognition and application of data storytelling, both in academia and industry. Recently, the rapid development of <b>generative</b> <b>AI</b> has brought new opportunities and challenges to this field, sparking numerous new questions. These questions may not necessarily be quickly transformed into papers, but we believe it is necessary to promptly discuss them to help the community better clarify important issues and research agendas for the future. We thus invite you to join our workshop (Gen4DS) to discuss questions such as: How can <b>generative</b> <b>AI</b> facilitate the creation of data stories? How might <b>generative</b> <b>AI</b> alter the workflow of data storytellers? What are the pitfalls and risks of incorporating AI in storytelling? We have designed both paper presentations and interactive activities (including hands-on creation, group discussion pods, and debates on controversial issues) for the workshop. We hope that participants will learn about the latest advances and pioneering work in data storytelling, engage in critical conversations with each other, and have an enjoyable, unforgettable, and meaningful experience at the event.</p></p class="citation"></blockquote><h3 id=77--256327-from-delays-to-densities-exploring-data-uncertainty-through-speech-text-and-visualization-chase-stokes-et-al-2024>(7/7 | 256/327) From Delays to Densities: Exploring Data Uncertainty through Speech, Text, and Visualization (Chase Stokes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chase Stokes, Chelsea Sanker, Bridget Cogley, Vidya Setlur. (2024)<br><strong>From Delays to Densities: Exploring Data Uncertainty through Speech, Text, and Visualization</strong><br><button class=copy-to-clipboard title="From Delays to Densities: Exploring Data Uncertainty through Speech, Text, and Visualization" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02317v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02317v1.pdf filename=2404.02317v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding and communicating data uncertainty is crucial for making informed decisions in sectors like finance and healthcare. Previous work has explored how to express uncertainty in various modes. For example, uncertainty can be expressed visually with quantile dot plots or linguistically with hedge words and prosody. Our research aims to systematically explore how variations within each mode contribute to communicating uncertainty to the user; this allows us to better understand each mode&rsquo;s affordances and limitations. We completed an exploration of the uncertainty design space based on pilot studies and ran two crowdsourced experiments examining how speech, text, and visualization modes and variants within them impact decision-making with uncertain data. Visualization and text were most effective for rational decision-making, though text resulted in lower confidence. Speech garnered the highest trust despite sometimes leading to risky decisions. Results from these studies indicate meaningful trade-offs among modes of information and encourage exploration of <b>multimodal</b> data representations.</p></p class="citation"></blockquote><h2 id=csse-7>cs.SE (7)</h2><h3 id=17--257327-self-organized-agents-a-llm-multi-agent-framework-toward-ultra-large-scale-code-generation-and-optimization-yoichi-ishibashi-et-al-2024>(1/7 | 257/327) Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization (Yoichi Ishibashi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yoichi Ishibashi, Yoshimasa Nishimura. (2024)<br><strong>Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization</strong><br><button class=copy-to-clipboard title="Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-CL, cs-LG, cs-MA, cs-SE, cs.SE<br>Keyword Score: 43<br>Keywords: Benchmarking, Code Generation, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02183v1.pdf filename=2404.02183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in automatic <b>code</b> <b>generation</b> using <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving <b>large-scale,</b> <b>complex</b> <b>codebases</b> due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of <b>large-scale</b> <b>code.</b> <b>In</b> SoA, self-organized agents operate independently to generate and modify <b>code</b> <b>components</b> while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall <b>code</b> <b>volume</b> to be increased indefinitely according to the number of agents, while the amount of <b>code</b> <b>managed</b> by each agent remains constant. We evaluate SoA on the HumanEval <b>benchmark</b> and demonstrate that, compared to a single-agent system, each agent in SoA handles significantly less <b>code,</b> <b>yet</b> the overall generated <b>code</b> <b>is</b> substantially greater. Moreover, SoA surpasses the powerful single-agent baseline by 5% in terms of Pass@1 accuracy.</p></p class="citation"></blockquote><h3 id=27--258327-automated-user-story-generation-with-test-case-specification-using-large-language-model-tajmilur-rahman-et-al-2024>(2/7 | 258/327) Automated User Story Generation with Test Case Specification Using Large Language Model (Tajmilur Rahman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tajmilur Rahman, Yuecai Zhu. (2024)<br><strong>Automated User Story Generation with Test Case Specification Using Large Language Model</strong><br><button class=copy-to-clipboard title="Automated User Story Generation with Test Case Specification Using Large Language Model" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01558v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01558v1.pdf filename=2404.01558v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern Software Engineering era is moving fast with the assistance of artificial intelligence (AI), especially <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM).</b> Researchers have already started automating many parts of the software development workflow. Requirements Engineering (RE) is a crucial phase that begins the software development cycle through multiple discussions on a proposed scope of work documented in different forms. RE phase ends with a list of user-stories for each unit task identified through discussions and usually these are created and tracked on a project management tool such as Jira, AzurDev etc. In this research we developed a tool &ldquo;GeneUS&rdquo; using <b>GPT-4.0</b> to automatically create user stories from requirements document which is the outcome of the RE phase. The output is provided in JSON format leaving the possibilities open for downstream integration to the popular project management tools. Analyzing requirements documents takes significant effort and multiple meetings with stakeholders. We believe, automating this process will certainly reduce additional load off the software engineers, and increase the productivity since they will be able to utilize their time on other prioritized tasks.</p></p class="citation"></blockquote><h3 id=37--259327-peer-aided-repairer-empowering-large-language-models-to-repair-advanced-student-assignments-qianhui-zhao-et-al-2024>(3/7 | 259/327) Peer-aided Repairer: Empowering Large Language Models to Repair Advanced Student Assignments (Qianhui Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianhui Zhao, Fang Liu, Li Zhang, Yang Liu, Zhen Yan, Zhenghao Chen, Yufei Zhou, Jing Jiang, Ge Li. (2024)<br><strong>Peer-aided Repairer: Empowering Large Language Models to Repair Advanced Student Assignments</strong><br><button class=copy-to-clipboard title="Peer-aided Repairer: Empowering Large Language Models to Repair Advanced Student Assignments" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01754v1.pdf filename=2404.01754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated generation of feedback on programming assignments holds significant benefits for programming education, especially when it comes to advanced assignments. Automated Program Repair techniques, especially <b>Large</b> <b>Language</b> <b>Model</b> based approaches, have gained notable recognition for their potential to fix introductory assignments. However, the programs used for evaluation are relatively simple. It remains unclear how existing approaches perform in repairing programs from higher-level programming courses. To address these limitations, we curate a new advanced student assignment dataset named Defects4DS from a higher-level programming course. Subsequently, we identify the challenges related to fixing bugs in advanced assignments. Based on the analysis, we develop a framework called PaR that is powered by the <b>LLM.</b> PaR works in three phases: Peer Solution Selection, Multi-Source <b>Prompt</b> Generation, and Program Repair. Peer Solution Selection identifies the closely related peer programs based on lexical, semantic, and syntactic criteria. Then Multi-Source <b>Prompt</b> Generation adeptly combines multiple sources of information to create a comprehensive and informative <b>prompt</b> for the last Program Repair stage. The evaluation on Defects4DS and another well-investigated ITSP dataset reveals that PaR achieves a new state-of-the-art performance, demonstrating impressive improvements of 19.94% and 15.2% in repair rate compared to prior state-of-the-art LLM- and symbolic-based approaches, respectively</p></p class="citation"></blockquote><h3 id=47--260327-ev2gym-a-flexible-v2g-simulator-for-ev-smart-charging-research-and-benchmarking-stavros-orfanoudakis-et-al-2024>(4/7 | 260/327) EV2Gym: A Flexible V2G Simulator for EV Smart Charging Research and Benchmarking (Stavros Orfanoudakis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stavros Orfanoudakis, Cesar Diaz-Londono, Yunus E. Yılmaz, Peter Palensky, Pedro P. Vergara. (2024)<br><strong>EV2Gym: A Flexible V2G Simulator for EV Smart Charging Research and Benchmarking</strong><br><button class=copy-to-clipboard title="EV2Gym: A Flexible V2G Simulator for EV Smart Charging Research and Benchmarking" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Reinforcement Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01849v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01849v1.pdf filename=2404.01849v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As electric vehicle (EV) numbers rise, concerns about the capacity of current charging and power grid infrastructure grow, necessitating the development of smart charging solutions. While many smart charging simulators have been developed in recent years, only a few support the development of <b>Reinforcement</b> <b>Learning</b> (RL) algorithms in the form of a Gym environment, and those that do usually lack depth in modeling Vehicle-to-Grid (V2G) scenarios. To address the aforementioned issues, this paper introduces the EV2Gym, a realistic simulator platform for the development and assessment of small and large-scale smart charging algorithms within a standardized platform. The proposed simulator is populated with comprehensive EV, charging station, power <b>transformer,</b> and EV behavior models validated using real data. EV2Gym has a highly customizable interface empowering users to choose from pre-designed case studies or craft their own customized scenarios to suit their specific requirements. Moreover, it incorporates a diverse array of RL, mathematical programming, and heuristic algorithms to speed up the development and <b>benchmarking</b> of new solutions. By offering a unified and standardized platform, EV2Gym aims to provide researchers and practitioners with a robust environment for advancing and assessing smart charging algorithms.</p></p class="citation"></blockquote><h3 id=57--261327-multitask-based-evaluation-of-open-source-llm-on-software-vulnerability-xin-yin-et-al-2024>(5/7 | 261/327) Multitask-based Evaluation of Open-Source LLM on Software Vulnerability (Xin Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Yin, Chao Ni. (2024)<br><strong>Multitask-based Evaluation of Open-Source LLM on Software Vulnerability</strong><br><button class=copy-to-clipboard title="Multitask-based Evaluation of Open-Source LLM on Software Vulnerability" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Few-shot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02056v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02056v1.pdf filename=2404.02056v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a pipeline for quantitatively evaluating interactive <b>LLMs</b> using publicly available datasets. We carry out an extensive technical evaluation of <b>LLMs</b> using Big-Vul covering four different common software vulnerability tasks. We evaluate the multitask and multilingual aspects of <b>LLMs</b> based on this dataset. We find that the existing state-of-the-art methods are generally superior to <b>LLMs</b> in software vulnerability detection. Although <b>LLMs</b> improve accuracy when providing context information, they still have limitations in accurately predicting severity ratings for certain CWE types. In addition, <b>LLMs</b> demonstrate some ability to locate vulnerabilities for certain CWE types, but their performance varies among different CWE types. Finally, <b>LLMs</b> show uneven performance in generating CVE descriptions for various CWE types, with limited accuracy in a <b>few-shot</b> setting. Overall, though <b>LLMs</b> perform well in some aspects, they still need improvement in understanding the subtle differences in code vulnerabilities and the ability to describe vulnerabilities to fully realize their potential. Our evaluation pipeline provides valuable insights for further enhancing <b>LLMs&rsquo;</b> software vulnerability handling capabilities.</p></p class="citation"></blockquote><h3 id=67--262327-keeping-behavioral-programs-alive-specifying-and-executing-liveness-requirements-tom-yaacov-et-al-2024>(6/7 | 262/327) Keeping Behavioral Programs Alive: Specifying and Executing Liveness Requirements (Tom Yaacov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tom Yaacov, Achiya Elyasaf, Gera Weiss. (2024)<br><strong>Keeping Behavioral Programs Alive: Specifying and Executing Liveness Requirements</strong><br><button class=copy-to-clipboard title="Keeping Behavioral Programs Alive: Specifying and Executing Liveness Requirements" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01858v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01858v1.pdf filename=2404.01858v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One of the benefits of using executable specifications such as Behavioral Programming (BP) is the ability to align the system implementation with its requirements. This is facilitated in BP by a protocol that allows independent implementation modules that specify what the system may, must, and must not do. By that, each module can enforce a single system requirement, including negative specifications such as &ldquo;don&rsquo;t do X after Y.&rdquo; The existing BP protocol, however, allows only the enforcement of safety requirements and does not support the execution of liveness properties such as &ldquo;do X at least three times.&rdquo; To model liveness requirements in BP directly and independently, we propose idioms for tagging states with &ldquo;must-finish,&rdquo; indicating that tasks are yet to be completed. We show that this idiom allows a direct specification of known requirements patterns from the literature. We also offer semantics and two execution mechanisms, one based on a translation to B"uchi automata and the other based on a <b>Markov</b> <b>decision</b> <b>process</b> (MDP). The latter approach offers the possibility of utilizing deep <b>reinforcement</b> <b>learning</b> (DRL) algorithms, which bear the potential to handle large software systems effectively. This paper presents a qualitative and quantitative assessment of the proposed approach using a proof-of-concept tool. A formal analysis of the MDP-based execution mechanism is given in an appendix.</p></p class="citation"></blockquote><h3 id=77--263327-ft2ra-a-fine-tuning-inspired-approach-to-retrieval-augmented-code-completion-qi-guo-et-al-2024>(7/7 | 263/327) FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion (Qi Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Guo, Xiaohong Li, Xiaofei Xie, Shangqing Liu, Ze Tang, Ruitao Feng, Junjie Wang, Jidong Ge, Lei Bu. (2024)<br><strong>FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion</strong><br><button class=copy-to-clipboard title="FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01554v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01554v1.pdf filename=2404.01554v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of code pre-trained models has significantly enhanced various coding tasks, such as code completion, and tools like GitHub Copilot. However, the substantial size of these models, especially large models, poses a significant challenge when it comes to <b>fine-tuning</b> them for specific downstream tasks. As an alternative approach, retrieval-based methods have emerged as a promising solution, augmenting model predictions without the need for <b>fine-tuning.</b> Despite their potential, a significant challenge is that the designs of these methods often rely on heuristics, leaving critical questions about what information should be stored or retrieved and how to interpolate such information for augmenting predictions. To tackle this challenge, we first perform a theoretical analysis of the <b>fine-tuning</b> process, highlighting the importance of delta logits as a catalyst for improving model predictions. Building on this insight, we develop a novel retrieval-based method, FT2Ra, which aims to mimic genuine <b>fine-tuning.</b> While FT2Ra adopts a retrieval-based mechanism, it uniquely adopts a paradigm with a learning rate and multi-epoch retrievals, which is similar to <b>fine-tuning.In</b> token-level completion, which represents a relatively easier task, FT2Ra achieves a 4.29% improvement in accuracy compared to the best baseline method on UniXcoder. In the more challenging line-level completion task, we observe a substantial more than twice increase in Exact Match (EM) performance, indicating the significant advantages of our theoretical analysis. Notably, even when operating without actual <b>fine-tuning,</b> FT2Ra exhibits competitive performance compared to the models with real <b>fine-tuning.</b></p></p class="citation"></blockquote><h2 id=csce-2>cs.CE (2)</h2><h3 id=12--264327-intelligent-optimization-of-mine-environmental-damage-assessment-and-repair-strategies-based-on-deep-learning-qishuo-cheng-2024>(1/2 | 264/327) Intelligent Optimization of Mine Environmental Damage Assessment and Repair Strategies Based on Deep Learning (Qishuo Cheng, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qishuo Cheng. (2024)<br><strong>Intelligent Optimization of Mine Environmental Damage Assessment and Repair Strategies Based on Deep Learning</strong><br><button class=copy-to-clipboard title="Intelligent Optimization of Mine Environmental Damage Assessment and Repair Strategies Based on Deep Learning" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE, q-fin-CP<br>Keyword Score: 43<br>Keywords: Benchmarking, LSTM, LSTM, LSTM, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01624v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01624v1.pdf filename=2404.01624v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent decades, financial quantification has emerged and matured rapidly. For financial institutions such as funds, investment institutions are increasingly dissatisfied with the situation of passively constructing investment portfolios with average market returns, and are paying more and more attention to active quantitative strategy investment portfolios. This requires the introduction of active stock investment fund management models. Currently, in my country&rsquo;s stock fund investment market, there are many active quantitative investment strategies, and the algorithms used vary widely, such as SVM, random forest, <b>RNN</b> recurrent memory network, etc. This article focuses on this trend, using the emerging <b>LSTM-GRU</b> gate-controlled <b>long</b> <b>short-term</b> <b>memory</b> <b>network</b> model in the field of financial stock investment as a basis to build a set of active investment stock strategies, and combining it with SVM, which has been widely used in the field of quantitative stock investment. Comparing models such as <b>RNN,</b> theoretically speaking, compared to SVM that simply relies on kernel functions for high-order mapping and classification of data, neural network algorithms such as <b>RNN</b> and <b>LSTM-GRU</b> have better principles and are more suitable for processing financial stock data. Then, through multiple By comparison, it was finally found that the LSTM- GRU gate-controlled <b>long</b> <b>short-term</b> <b>memory</b> <b>network</b> has a better accuracy. By selecting the <b>LSTM-GRU</b> algorithm to construct a trading strategy based on the Shanghai and Shenzhen 300 Index constituent stocks, the parameters were adjusted and the neural layer connection was adjusted. Finally, It has significantly outperformed the <b>benchmark</b> index CSI 300 over the <b>long</b> <b>term.</b> <b>The</b> <b>conclusion</b> of this article is that the research results can provide certain quantitative strategy references for financial institutions to construct active stock investment portfolios.</p></p class="citation"></blockquote><h3 id=22--265327-enhancing-portfolio-optimization-with-transformer-gan-integration-a-novel-approach-in-the-black-litterman-framework-enmin-zhu-et-al-2024>(2/2 | 265/327) Enhancing Portfolio Optimization with Transformer-GAN Integration: A Novel Approach in the Black-Litterman Framework (Enmin Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enmin Zhu, Jerome Yen. (2024)<br><strong>Enhancing Portfolio Optimization with Transformer-GAN Integration: A Novel Approach in the Black-Litterman Framework</strong><br><button class=copy-to-clipboard title="Enhancing Portfolio Optimization with Transformer-GAN Integration: A Novel Approach in the Black-Litterman Framework" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 30<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02029v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02029v2.pdf filename=2404.02029v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents an innovative approach to portfolio optimization by integrating <b>Transformer</b> models with <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> within the Black-Litterman (BL) framework. Capitalizing on <b>Transformers&rsquo;</b> ability to discern long-range dependencies and <b>GANs&rsquo;</b> proficiency in generating accurate predictive models, our method enhances the generation of refined predictive views for BL portfolio allocations. This fusion of our model with BL&rsquo;s structured method for merging objective views with market equilibrium offers a potent tool for modern portfolio management, outperforming traditional forecasting methods. Our integrated approach not only demonstrates the potential to improve investment decision-making but also contributes a new approach to capture the complexities of financial markets for robust portfolio optimization.</p></p class="citation"></blockquote><h2 id=csma-3>cs.MA (3)</h2><h3 id=13--266327-distributed-autonomous-swarm-formation-for-dynamic-network-bridging-raffaele-galliera-et-al-2024>(1/3 | 266/327) Distributed Autonomous Swarm Formation for Dynamic Network Bridging (Raffaele Galliera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raffaele Galliera, Thies Möhlenhof, Alessandro Amato, Daniel Duran, Kristen Brent Venable, Niranjan Suri. (2024)<br><strong>Distributed Autonomous Swarm Formation for Dynamic Network Bridging</strong><br><button class=copy-to-clipboard title="Distributed Autonomous Swarm Formation for Dynamic Network Bridging" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-LG, cs-MA, cs-RO, cs.MA<br>Keyword Score: 43<br>Keywords: Graph, Convolution, Markov Decision Process, Reinforcement Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01557v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01557v1.pdf filename=2404.01557v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective operation and seamless cooperation of robotic systems are a fundamental component of next-generation technologies and applications. In contexts such as disaster response, swarm operations require coordinated behavior and mobility control to be handled in a distributed manner, with the quality of the agents&rsquo; actions heavily relying on the communication between them and the underlying network. In this paper, we formulate the problem of dynamic network bridging in a novel Decentralized Partially Observable <b>Markov</b> <b>Decision</b> <b>Process</b> (Dec-POMDP), where a swarm of agents cooperates to form a link between two distant moving targets. Furthermore, we propose a Multi-Agent <b>Reinforcement</b> <b>Learning</b> (MARL) approach for the problem based on <b>Graph</b> <b>Convolutional</b> <b>Reinforcement</b> <b>Learning</b> (DGN) which naturally applies to the networked, distributed nature of the task. The proposed method is evaluated in a simulated environment and compared to a centralized heuristic baseline showing promising results. Moreover, a further step in the direction of sim-to-real transfer is presented, by additionally evaluating the proposed approach in a near Live Virtual Constructive (LVC) UAV framework.</p></p class="citation"></blockquote><h3 id=23--267327-energaize-multi-agent-deep-deterministic-policy-gradient-for-vehicle-to-grid-energy-management-tiago-fonseca-et-al-2024>(2/3 | 267/327) EnergAIze: Multi Agent Deep Deterministic Policy Gradient for Vehicle to Grid Energy Management (Tiago Fonseca et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tiago Fonseca, Luis Ferreira, Bernardo Cabral, Ricardo Severino, Isabel Praca. (2024)<br><strong>EnergAIze: Multi Agent Deep Deterministic Policy Gradient for Vehicle to Grid Energy Management</strong><br><button class=copy-to-clipboard title="EnergAIze: Multi Agent Deep Deterministic Policy Gradient for Vehicle to Grid Energy Management" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-MA, cs.MA<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02361v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02361v1.pdf filename=2404.02361v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the increasing roles of Renewable Energy Sources (RES) and Electric Vehicles (EVs). While indicating a new era of sustainable energy, these also introduce complex challenges, including the need to balance supply and demand and smooth peak consumptions amidst rising EV adoption rates. Addressing these challenges requires innovative solutions such as Demand Response (DR), energy flexibility management, Renewable Energy Communities (RECs), and more specifically for EVs, Vehicle-to-Grid (V2G). However, existing V2G approaches often fall short in real-world adaptability, global REC optimization with other flexible assets, scalability, and user engagement. To bridge this gap, this paper introduces EnergAIze, a Multi-Agent <b>Reinforcement</b> <b>Learning</b> (MARL) energy management framework, leveraging the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm. EnergAIze enables user-centric and multi-objective energy management by allowing each prosumer to select from a range of personal management objectives, thus encouraging engagement. Additionally, it architects&rsquo; data protection and ownership through decentralized computing, where each prosumer can situate an energy management optimization node directly at their own dwelling. The local node not only manages local energy assets but also fosters REC wide optimization. The efficacy of EnergAIze was evaluated through case studies employing the CityLearn <b>simulation</b> framework. These <b>simulations</b> were instrumental in demonstrating EnergAIze&rsquo;s adeptness at implementing V2G technology within a REC and other energy assets. The results show reduction in peak loads, ramping, carbon emissions, and electricity costs at the REC level while optimizing for individual prosumers objectives.</p></p class="citation"></blockquote><h3 id=33--268327-multi-agent-reinforcement-learning-with-control-theoretic-safety-guarantees-for-dynamic-network-bridging-raffaele-galliera-et-al-2024>(3/3 | 268/327) Multi-Agent Reinforcement Learning with Control-Theoretic Safety Guarantees for Dynamic Network Bridging (Raffaele Galliera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raffaele Galliera, Konstantinos Mitsopoulos, Niranjan Suri, Raffaele Romagnoli. (2024)<br><strong>Multi-Agent Reinforcement Learning with Control-Theoretic Safety Guarantees for Dynamic Network Bridging</strong><br><button class=copy-to-clipboard title="Multi-Agent Reinforcement Learning with Control-Theoretic Safety Guarantees for Dynamic Network Bridging" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-LG, cs-MA, cs-NI, cs-SY, cs.MA, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01551v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01551v1.pdf filename=2404.01551v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Addressing complex cooperative tasks in safety-critical environments poses significant challenges for Multi-Agent Systems, especially under conditions of partial observability. This work introduces a hybrid approach that integrates Multi-Agent <b>Reinforcement</b> <b>Learning</b> with control-theoretic methods to ensure safe and efficient distributed strategies. Our contributions include a novel setpoint update algorithm that dynamically adjusts agents&rsquo; positions to preserve safety conditions without compromising the mission&rsquo;s objectives. Through experimental validation, we demonstrate significant advantages over conventional MARL strategies, achieving comparable task performance with zero safety violations. Our findings indicate that integrating safe control with learning approaches not only enhances safety compliance but also achieves good performance in mission objectives.</p></p class="citation"></blockquote><h2 id=eessiv-7>eess.IV (7)</h2><h3 id=17--269327-guidelines-for-cerebrovascular-segmentation-managing-imperfect-annotations-in-the-context-of-semi-supervised-learning-pierre-rougé-et-al-2024>(1/7 | 269/327) Guidelines for Cerebrovascular Segmentation: Managing Imperfect Annotations in the context of Semi-Supervised Learning (Pierre Rougé et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pierre Rougé, Pierre-Henri Conze, Nicolas Passat, Odyssée Merveille. (2024)<br><strong>Guidelines for Cerebrovascular Segmentation: Managing Imperfect Annotations in the context of Semi-Supervised Learning</strong><br><button class=copy-to-clipboard title="Guidelines for Cerebrovascular Segmentation: Managing Imperfect Annotations in the context of Semi-Supervised Learning" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Semi-Supervised Learning, Supervised Learning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01765v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01765v1.pdf filename=2404.01765v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Segmentation in medical imaging is an essential and often preliminary task in the image processing chain, driving numerous efforts towards the design of robust segmentation algorithms. <b>Supervised</b> <b>learning</b> methods achieve excellent performances when fed with a sufficient amount of labeled data. However, such labels are typically highly time-consuming, error-prone and expensive to produce. Alternatively, <b>semi-supervised</b> <b>learning</b> approaches leverage both labeled and unlabeled data, and are very useful when only a small fraction of the dataset is labeled. They are particularly useful for cerebrovascular segmentation, given that labeling a single volume requires several hours for an expert. In addition to the challenge posed by insufficient annotations, there are concerns regarding annotation consistency. The task of annotating the cerebrovascular tree is inherently ambiguous. Due to the discrete nature of images, the borders and extremities of vessels are often unclear. Consequently, annotations heavily rely on the expert subjectivity and on the underlying clinical objective. These discrepancies significantly increase the complexity of the segmentation task for the model and consequently impair the results. Consequently, it becomes imperative to provide clinicians with precise guidelines to improve the annotation process and construct more uniform datasets. In this article, we investigate the data dependency of deep learning methods within the context of imperfect data and <b>semi-supervised</b> <b>learning,</b> for cerebrovascular segmentation. Specifically, this study compares various state-of-the-art <b>semi-supervised</b> <b>methods</b> based on <b>unsupervised</b> regularization and evaluates their performance in diverse quantity and quality data scenarios. Based on these experiments, we provide guidelines for the annotation and training of cerebrovascular segmentation models.</p></p class="citation"></blockquote><h3 id=27--270327-contextual-embedding-learning-to-enhance-2d-networks-for-volumetric-image-segmentation-zhuoyuan-wang-et-al-2024>(2/7 | 270/327) Contextual Embedding Learning to Enhance 2D Networks for Volumetric Image Segmentation (Zhuoyuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuoyuan Wang, Dong Sun, Xiangyun Zeng, Ruodai Wu, Yi Wang. (2024)<br><strong>Contextual Embedding Learning to Enhance 2D Networks for Volumetric Image Segmentation</strong><br><button class=copy-to-clipboard title="Contextual Embedding Learning to Enhance 2D Networks for Volumetric Image Segmentation" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Contextual Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01723v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01723v1.pdf filename=2404.01723v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The segmentation of organs in volumetric medical images plays an important role in computer-aided diagnosis and treatment/surgery planning. Conventional 2D <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> can hardly exploit the spatial correlation of volumetric data. Current 3D <b>CNNs</b> have the advantage to extract more powerful volumetric representations but they usually suffer from occupying excessive memory and computation nevertheless. In this study we aim to enhance the 2D networks with <b>contextual</b> <b>information</b> for better volumetric image segmentation. Accordingly, we propose a <b>contextual</b> <b>embedding</b> learning approach to facilitate 2D <b>CNNs</b> capturing spatial information properly. Our approach leverages the learned embedding and the slice-wisely neighboring matching as a soft cue to guide the network. In such a way, the <b>contextual</b> <b>information</b> can be transferred slice-by-slice thus boosting the volumetric representation of the network. Experiments on challenging prostate MRI dataset (PROMISE12) and abdominal CT dataset (CHAOS) show that our <b>contextual</b> <b>embedding</b> learning can effectively leverage the inter-slice context and improve segmentation performance. The proposed approach is a plug-and-play, and memory-efficient solution to enhance the 2D networks for volumetric segmentation. The code will be publicly available.</p></p class="citation"></blockquote><h3 id=37--271327-rethinking-annotator-simulation-realistic-evaluation-of-whole-body-pet-lesion-interactive-segmentation-methods-zdravko-marinov-et-al-2024>(3/7 | 271/327) Rethinking Annotator Simulation: Realistic Evaluation of Whole-Body PET Lesion Interactive Segmentation Methods (Zdravko Marinov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zdravko Marinov, Moon Kim, Jens Kleesiek, Rainer Stiefelhagen. (2024)<br><strong>Rethinking Annotator Simulation: Realistic Evaluation of Whole-Body PET Lesion Interactive Segmentation Methods</strong><br><button class=copy-to-clipboard title="Rethinking Annotator Simulation: Realistic Evaluation of Whole-Body PET Lesion Interactive Segmentation Methods" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-HC, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01816v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01816v1.pdf filename=2404.01816v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interactive segmentation plays a crucial role in accelerating the annotation, particularly in domains requiring specialized expertise such as nuclear medicine. For example, annotating lesions in whole-body Positron Emission Tomography (PET) images can require over an hour per volume. While previous works evaluate interactive segmentation models through either real user studies or simulated annotators, both approaches present challenges. Real user studies are expensive and often limited in scale, while simulated annotators, also known as robot users, tend to overestimate model performance due to their idealized nature. To address these limitations, we introduce four evaluation metrics that quantify the user shift between real and simulated annotators. In an initial user study involving four annotators, we assess existing robot users using our proposed metrics and find that robot users significantly deviate in performance and annotation behavior compared to real annotators. Based on these findings, we propose a more realistic robot user that reduces the user shift by incorporating human factors such as click variation and inter-annotator disagreement. We validate our robot user in a second user study, involving four other annotators, and show it consistently reduces the simulated-to-real user shift compared to traditional robot users. By employing our robot user, we can conduct more large-scale and cost-efficient evaluations of interactive segmentation models, while preserving the fidelity of real user studies. Our implementation is based on MONAI Label and will be made publicly available.</p></p class="citation"></blockquote><h3 id=47--272327-a-closer-look-at-spatial-slice-features-learning-for-covid-19-detection-chih-chung-hsu-et-al-2024>(4/7 | 272/327) A Closer Look at Spatial-Slice Features Learning for COVID-19 Detection (Chih-Chung Hsu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chih-Chung Hsu, Chia-Ming Lee, Yang Fan Chiang, Yi-Shiuan Chou, Chih-Yu Jiang, Shen-Chieh Tai, Chi-Han Tsai. (2024)<br><strong>A Closer Look at Spatial-Slice Features Learning for COVID-19 Detection</strong><br><button class=copy-to-clipboard title="A Closer Look at Spatial-Slice Features Learning for COVID-19 Detection" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01643v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01643v1.pdf filename=2404.01643v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional Computed Tomography (CT) imaging recognition faces two significant challenges: (1) There is often considerable variability in the resolution and size of each CT scan, necessitating strict requirements for the input size and adaptability of models. (2) CT-scan contains large number of <b>out-of-distribution</b> (OOD) slices. The crucial features may only be present in specific spatial regions and slices of the entire CT scan. How can we effectively figure out where these are located? To deal with this, we introduce an enhanced Spatial-Slice Feature Learning (SSFL++) framework specifically designed for CT scan. It aim to filter out a OOD data within whole CT scan, enabling our to select crucial spatial-slice for analysis by reducing 70% redundancy totally. Meanwhile, we proposed Kernel-Density-based slice Sampling <b>(KDS)</b> method to improve the stability when training and inference stage, therefore speeding up the rate of convergence and boosting performance. As a result, the experiments demonstrate the promising performance of our model using a simple EfficientNet-2D (E2D) model, even with only 1% of the training data. The efficacy of our approach has been validated on the COVID-19-CT-DB datasets provided by the DEF-AI-MIA workshop, in conjunction with CVPR 2024. Our source code will be made available.</p></p class="citation"></blockquote><h3 id=57--273327-covid-19-detection-based-on-blood-test-parameters-using-various-artificial-intelligence-methods-kavian-khanjani-et-al-2024>(5/7 | 273/327) COVID-19 Detection Based on Blood Test Parameters using Various Artificial Intelligence Methods (Kavian Khanjani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kavian Khanjani, Seyed Rasoul Hosseini, Shahrzad Shashaani, Mohammad Teshnehlab. (2024)<br><strong>COVID-19 Detection Based on Blood Test Parameters using Various Artificial Intelligence Methods</strong><br><button class=copy-to-clipboard title="COVID-19 Detection Based on Blood Test Parameters using Various Artificial Intelligence Methods" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02348v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02348v1.pdf filename=2404.02348v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In 2019, the world faced a new challenge: a COVID-19 disease caused by the novel coronavirus, SARS-CoV-2. The virus rapidly spread across the globe, leading to a high rate of mortality, which <b>prompted</b> health organizations to take measures to control its transmission. Early disease detection is crucial in the treatment process, and computer-based automatic detection systems have been developed to aid in this effort. These systems often rely on artificial intelligence (AI) approaches such as machine learning, neural networks, fuzzy systems, and deep learning to classify diseases. This study aimed to differentiate COVID-19 patients from others using self-categorizing classifiers and employing various AI methods. This study used two datasets: the blood test samples and radiography images. The best results for the blood test samples obtained from San Raphael Hospital, which include two classes of individuals, those with COVID-19 and those with non-COVID diseases, were achieved through the use of the Ensemble method (a combination of a neural network and two machines learning methods). The results showed that this approach for COVID-19 diagnosis is cost-effective and provides results in a shorter amount of time than other methods. The proposed model achieved an accuracy of 94.09% on the dataset used. Secondly, the radiographic images were divided into four classes: normal, viral pneumonia, ground glass opacity, and COVID-19 infection. These were used for segmentation and classification. The lung lobes were extracted from the images and then categorized into specific classes. We achieved an accuracy of 91.1% on the image dataset. Generally, this study highlights the potential of AI in detecting and managing COVID-19 and underscores the importance of continued research and development in this field.</p></p class="citation"></blockquote><h3 id=67--274327-synthetic-data-for-robust-stroke-segmentation-liam-chalcroft-et-al-2024>(6/7 | 274/327) Synthetic Data for Robust Stroke Segmentation (Liam Chalcroft et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liam Chalcroft, Ioannis Pappas, Cathy J. Price, John Ashburner. (2024)<br><strong>Synthetic Data for Robust Stroke Segmentation</strong><br><button class=copy-to-clipboard title="Synthetic Data for Robust Stroke Segmentation" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Out-of-domain<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01946v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01946v1.pdf filename=2404.01946v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning-based semantic segmentation in neuroimaging currently requires high-resolution scans and extensive annotated datasets, posing significant barriers to clinical applicability. We present a novel synthetic framework for the task of lesion segmentation, extending the capabilities of the established SynthSeg approach to accommodate large heterogeneous pathologies with lesion-specific augmentation strategies. Our method trains deep learning models, demonstrated here with the UNet architecture, using label maps derived from healthy and stroke datasets, facilitating the segmentation of both healthy tissue and pathological lesions without sequence-specific training data. Evaluated against in-domain and <b>out-of-domain</b> (OOD) datasets, our framework demonstrates robust performance, rivaling current methods within the training domain and significantly outperforming them on OOD data. This contribution holds promise for advancing medical imaging analysis in clinical settings, especially for stroke pathology, by enabling reliable segmentation across varied imaging sequences with reduced dependency on large annotated corpora. Code and weights available at <a href=https://github.com/liamchalcroft/SynthStroke>https://github.com/liamchalcroft/SynthStroke</a>.</p></p class="citation"></blockquote><h3 id=77--275327-towards-enhanced-analysis-of-lung-cancer-lesions-in-ebus-tbna----a-semi-supervised-video-object-detection-method-jyun-an-lin-et-al-2024>(7/7 | 275/327) Towards Enhanced Analysis of Lung Cancer Lesions in EBUS-TBNA &ndash; A Semi-Supervised Video Object Detection Method (Jyun-An Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jyun-An Lin, Yun-Chien Cheng, Ching-Kai Lin. (2024)<br><strong>Towards Enhanced Analysis of Lung Cancer Lesions in EBUS-TBNA &ndash; A Semi-Supervised Video Object Detection Method</strong><br><button class=copy-to-clipboard title="Towards Enhanced Analysis of Lung Cancer Lesions in EBUS-TBNA -- A Semi-Supervised Video Object Detection Method" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01929v1.pdf filename=2404.01929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study aims to establish a computer-aided diagnostic system for lung lesions using bronchoscope endobronchial ultrasound (EBUS) to assist physicians in identifying lesion areas. During EBUS-transbronchial needle aspiration (EBUS-TBNA) procedures, physicians rely on grayscale ultrasound images to determine the location of lesions. However, these images often contain significant noise and can be influenced by surrounding tissues or blood vessels, making interpretation challenging. Previous research has lacked the application of <b>object</b> <b>detection</b> models to EBUS-TBNA, and there has been no well-defined solution for annotating the EBUS-TBNA dataset. In related studies on ultrasound images, although models have been successful in capturing target regions for their respective tasks, their training and predictions have been based on two-dimensional images, limiting their ability to leverage temporal features for improved predictions. This study introduces a three-dimensional image-based <b>object</b> <b>detection</b> model. It utilizes an attention mechanism to capture temporal correlations and we will implements a filtering mechanism to select relevant information from previous frames. Subsequently, a teacher-student model training approach is employed to optimize the model further, leveraging unlabeled data. To mitigate the impact of poor-quality pseudo-labels on the student model, we will add a special Gaussian Mixture Model (GMM) to ensure the quality of pseudo-labels.</p></p class="citation"></blockquote><h2 id=csni-6>cs.NI (6)</h2><h3 id=16--276327-guided-mutation-genetic-algorithm-for-mobile-iot-network-relay-gyupil-kam-et-al-2024>(1/6 | 276/327) Guided-Mutation Genetic Algorithm for Mobile IoT Network Relay (Gyupil Kam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gyupil Kam, Kiseop Chung. (2024)<br><strong>Guided-Mutation Genetic Algorithm for Mobile IoT Network Relay</strong><br><button class=copy-to-clipboard title="Guided-Mutation Genetic Algorithm for Mobile IoT Network Relay" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 40<br>Keywords: Autoencoder, Simulation, Simulator, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01683v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01683v1.pdf filename=2404.01683v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Internet of Things (IoT) is a communication scheme which allows various objects to exchange several types of information, enabling functions such as home automation, production management, healthcare, etc. Moreover, energy-harvesting (EH) technology is considered for IoT environment in order to reduce the need for management and enhance maintainability. However, since environments considering outdoor elements such as pedestrians, vehicles and drones have been on the rise recently, it is important to consider mobility when designing an IoT network management scheme. In order to handle this challenge, prior research has made an attempt to solve this problem via <b>variational</b> <b>autoencoder</b> (VAE) and backward-pass rate evaluation method. In this article, we propose a guided-mutation genetic algorithm (GMGA) to derive a sub-optimal relaying topology for IoT systems considering energy-harvesting. Furthermore, we propose a mobility-aware iterative relaying topology algorithm, which calculates the sub-optimal relaying topology of current time frame using the topology result of the previous one. <b>Simulation</b> results verify that our proposed scheme effectively solves formulated IoT network problems compared to other conventional schemes, and also effectively handles IoT environments in terms of mobility.</p></p class="citation"></blockquote><h3 id=26--277327-collaborative-optimization-of-wireless-communication-and-computing-resource-allocation-based-on-multi-agent-federated-weighting-deep-reinforcement-learning-junjie-wu-et-al-2024>(2/6 | 277/327) Collaborative Optimization of Wireless Communication and Computing Resource Allocation based on Multi-Agent Federated Weighting Deep Reinforcement Learning (Junjie Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junjie Wu, Xuming Fang. (2024)<br><strong>Collaborative Optimization of Wireless Communication and Computing Resource Allocation based on Multi-Agent Federated Weighting Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Collaborative Optimization of Wireless Communication and Computing Resource Allocation based on Multi-Agent Federated Weighting Deep Reinforcement Learning" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 40<br>Keywords: Federated Learning, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01638v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01638v1.pdf filename=2404.01638v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As artificial intelligence (AI)-enabled wireless communication systems continue their evolution, distributed learning has gained widespread attention for its ability to offer enhanced data privacy protection, improved resource utilization, and enhanced fault tolerance within wireless communication applications. <b>Federated</b> <b>learning</b> further enhances the ability of resource coordination and model generalization across nodes based on the above foundation, enabling the realization of an AI-driven communication and computing integrated wireless network. This paper proposes a novel wireless communication system to cater to a personalized service needs of both privacy-sensitive and privacy-insensitive users. We design the system based on based on multi-agent <b>federated</b> <b>weighting</b> deep <b>reinforcement</b> <b>learning</b> (MAFWDRL). The system, while fulfilling service requirements for users, facilitates real-time optimization of local communication resources allocation and concurrent decision-making concerning computing resources. Additionally, exploration noise is incorporated to enhance the exploration process of off-policy deep <b>reinforcement</b> <b>learning</b> (DRL) for wireless channels. <b>Federated</b> <b>weighting</b> (FedWgt) effectively compensates for heterogeneous differences in channel status between communication nodes. Extensive <b>simulation</b> experiments demonstrate that the proposed scheme outperforms baseline methods significantly in terms of throughput, calculation latency, and energy consumption improvement.</p></p class="citation"></blockquote><h3 id=36--278327-llm-abr-designing-adaptive-bitrate-algorithms-via-large-language-models-zhiyuan-he-et-al-2024>(3/6 | 278/327) LLM-ABR: Designing Adaptive Bitrate Algorithms via Large Language Models (Zhiyuan He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyuan He, Aashish Gottipati, Lili Qiu, Francis Y. Yan, Xufang Luo, Kenuo Xu, Yuqing Yang. (2024)<br><strong>LLM-ABR: Designing Adaptive Bitrate Algorithms via Large Language Models</strong><br><button class=copy-to-clipboard title="LLM-ABR: Designing Adaptive Bitrate Algorithms via Large Language Models" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-LG, cs-MM, cs-NI, cs.NI<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01617v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01617v1.pdf filename=2404.01617v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present <b>LLM-ABR,</b> the first system that utilizes the generative capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to autonomously design adaptive bitrate (ABR) algorithms tailored for diverse network characteristics. Operating within a <b>reinforcement</b> <b>learning</b> framework, <b>LLM-ABR</b> empowers <b>LLMs</b> to design key components such as states and neural network architectures. We evaluate <b>LLM-ABR</b> across diverse network settings, including broadband, satellite, 4G, and 5G. <b>LLM-ABR</b> consistently outperforms default ABR algorithms.</p></p class="citation"></blockquote><h3 id=46--279327-defining-problem-from-solutions-inverse-reinforcement-learning-irl-and-its-applications-for-next-generation-networking-yinqiu-liu-et-al-2024>(4/6 | 279/327) Defining Problem from Solutions: Inverse Reinforcement Learning (IRL) and Its Applications for Next-Generation Networking (Yinqiu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinqiu Liu, Ruichen Zhang, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Dong In Kim. (2024)<br><strong>Defining Problem from Solutions: Inverse Reinforcement Learning (IRL) and Its Applications for Next-Generation Networking</strong><br><button class=copy-to-clipboard title="Defining Problem from Solutions: Inverse Reinforcement Learning (IRL) and Its Applications for Next-Generation Networking" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 30<br>Keywords: Generative AI, Reinforcement Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01583v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01583v1.pdf filename=2404.01583v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Performance optimization is a critical concern in networking, on which Deep <b>Reinforcement</b> <b>Learning</b> (DRL) has achieved great success. Nonetheless, DRL training relies on precisely defined reward functions, which formulate the optimization objective and indicate the positive/negative progress towards the optimal. With the ever-increasing environmental complexity and human participation in Next-Generation Networking (NGN), defining appropriate reward functions become challenging. In this article, we explore the applications of Inverse <b>Reinforcement</b> <b>Learning</b> (IRL) in NGN. Particularly, if DRL aims to find optimal solutions to the problem, IRL finds a problem from the optimal solutions, where the optimal solutions are collected from experts, and the problem is defined by reward inference. Specifically, we first formally introduce the IRL technique, including its fundamentals, workflow, and difference from DRL. Afterward, we present the motivations of IRL applications in NGN and survey existing studies. Furthermore, to demonstrate the process of applying IRL in NGN, we perform a case study about human-centric <b>prompt</b> engineering in <b>Generative</b> <b>AI-enabled</b> networks. We demonstrate the effectiveness of using both DRL and IRL techniques and prove the superiority of IRL.</p></p class="citation"></blockquote><h3 id=56--280327-dcp-and-vardis-an-ad-hoc-protocol-stack-for-dynamic-swarms-and-formations-of-drones----extended-version-samuel-pell-et-al-2024>(5/6 | 280/327) DCP and VarDis: An Ad-Hoc Protocol Stack for Dynamic Swarms and Formations of Drones &ndash; Extended Version (Samuel Pell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Pell, Andreas Willig. (2024)<br><strong>DCP and VarDis: An Ad-Hoc Protocol Stack for Dynamic Swarms and Formations of Drones &ndash; Extended Version</strong><br><button class=copy-to-clipboard title="DCP and VarDis: An Ad-Hoc Protocol Stack for Dynamic Swarms and Formations of Drones -- Extended Version" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01570v1.pdf filename=2404.01570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, swarms or formations of drones have received increased interest both in the literature and in applications. To dynamically adapt to their operating environment, swarm members need to communicate wirelessly for control and coordination tasks. One fundamental communication pattern required for basic safety purposes, such as collision avoidance, is beaconing, where drones frequently transmit information about their position, speed, heading, and other operational data to a local neighbourhood, using a local broadcast service. In this paper, we propose and analyse a protocol stack which allows to use the recurring-beaconing primitive for additional purposes. In particular, we propose the VarDis (Variable Dissemination) protocol, which creates the abstraction of variables to which all members of a drone swarm have (read) access, and which can naturally be used for centralized control of a swarm, amongst other applications. We describe the involved protocols and provide a mainly <b>simulation-based</b> performance analysis of VarDis.</p></p class="citation"></blockquote><h3 id=66--281327-smartt-reps-sender-based-marked-rapidly-adapting-trimmed--timed-transport-with-recycled-entropies-tommaso-bonato-et-al-2024>(6/6 | 281/327) SMaRTT-REPS: Sender-based Marked Rapidly-adapting Trimmed & Timed Transport with Recycled Entropies (Tommaso Bonato et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tommaso Bonato, Abdul Kabbani, Daniele De Sensi, Rong Pan, Yanfang Le, Costin Raiciu, Mark Handley, Timo Schneider, Nils Blach, Ahmad Ghalayini, Daniel Alves, Michael Papamichael, Adrian Caulfield, Torsten Hoefler. (2024)<br><strong>SMaRTT-REPS: Sender-based Marked Rapidly-adapting Trimmed & Timed Transport with Recycled Entropies</strong><br><button class=copy-to-clipboard title="SMaRTT-REPS: Sender-based Marked Rapidly-adapting Trimmed & Timed Transport with Recycled Entropies" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01630v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01630v1.pdf filename=2404.01630v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid growth of machine learning (ML) workloads in datacenters, existing congestion control (CC) algorithms fail to deliver the required performance at scale. ML traffic is bursty and bulk-synchronous and thus requires quick reaction and strong <b>fairness.</b> We show that existing CC algorithms that use delay as a main signal react too slowly and are not always fair. We design SMaRTT, a simple sender-based CC algorithm that combines delay, ECN, and optional packet trimming for fast and precise window adjustments. At the core of SMaRTT lies the novel QuickAdapt algorithm that accurately estimates the bandwidth at the receiver. We show how to combine SMaRTT with a new per-packet traffic load-balancing algorithm called REPS to effectively reroute packets around congested hotspots as well as flaky or failing links. Our evaluation shows that SMaRTT alone outperforms EQDS, Swift, BBR, and MPRDMA by up to 50% on modern datacenter networks.</p></p class="citation"></blockquote><h2 id=csne-3>cs.NE (3)</h2><h3 id=13--282327-continuous-spiking-graph-neural-networks-nan-yin-et-al-2024>(1/3 | 282/327) Continuous Spiking Graph Neural Networks (Nan Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nan Yin, Mengzhu Wan, Li Shen, Hitesh Laxmichand Patel, Baopu Li, Bin Gu, Huan Xiong. (2024)<br><strong>Continuous Spiking Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Continuous Spiking Graph Neural Networks" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-LG, cs-NE, cs.NE<br>Keyword Score: 33<br>Keywords: Continuous Graph, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01897v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01897v1.pdf filename=2404.01897v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continuous</b> <b>graph</b> <b>neural</b> <b>networks</b> (CGNNs) have garnered significant attention due to their ability to generalize existing discrete <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> by introducing <b>continuous</b> <b>dynamics.</b> They typically draw inspiration from diffusion-based methods to introduce a novel propagation scheme, which is analyzed using ordinary differential equations (ODE). However, the implementation of CGNNs requires significant computational power, making them challenging to deploy on battery-powered devices. Inspired by recent spiking neural networks (SNNs), which emulate a biological inference process and provide an energy-efficient neural architecture, we incorporate the SNNs with CGNNs in a unified framework, named <b>Continuous</b> <b>Spiking</b> <b>Graph</b> <b>Neural</b> <b>Networks</b> (COS-GNN). We employ SNNs for <b>graph</b> <b>node</b> <b>representation</b> at each time step, which are further integrated into the ODE process along with time. To enhance information preservation and mitigate information loss in SNNs, we introduce the high-order structure of COS-GNN, which utilizes the second-order ODE for spiking representation and <b>continuous</b> <b>propagation.</b> Moreover, we provide the theoretical proof that COS-GNN effectively mitigates the issues of exploding and vanishing gradients, enabling us to capture long-range dependencies between nodes. Experimental results on <b>graph-based</b> <b>learning</b> <b>tasks</b> demonstrate the effectiveness of the proposed COS-GNN over competitive baselines.</p></p class="citation"></blockquote><h3 id=23--283327-already-moderate-population-sizes-provably-yield-strong-robustness-to-noise-denis-antipov-et-al-2024>(2/3 | 283/327) Already Moderate Population Sizes Provably Yield Strong Robustness to Noise (Denis Antipov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Denis Antipov, Benjamin Doerr, Alexandra Ivanova. (2024)<br><strong>Already Moderate Population Sizes Provably Yield Strong Robustness to Noise</strong><br><button class=copy-to-clipboard title="Already Moderate Population Sizes Provably Yield Strong Robustness to Noise" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02090v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02090v2.pdf filename=2404.02090v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Experience shows that typical evolutionary algorithms can cope well with stochastic disturbances such as noisy function evaluations. In this first mathematical runtime analysis of the $(1+\lambda)$ and $(1,\lambda)$ evolutionary algorithms in the presence of prior bit-wise noise, we show that both algorithms can tolerate constant noise probabilities without increasing the asymptotic runtime on the OneMax <b>benchmark.</b> For this, a population size $\lambda$ suffices that is at least logarithmic in the problem size $n$. The only previous result in this direction regarded the less realistic one-bit noise model, required a population size super-linear in the problem size, and proved a runtime guarantee roughly cubic in the noiseless runtime for the OneMax <b>benchmark.</b> Our significantly stronger results are based on the novel proof argument that the noiseless offspring can be seen as a biased uniform crossover between the parent and the noisy offspring. We are optimistic that the technical lemmas resulting from this insight will find applications also in future mathematical runtime analyses of evolutionary algorithms.</p></p class="citation"></blockquote><h3 id=33--284327-tensorized-neuroevolution-of-augmenting-topologies-for-gpu-acceleration-lishuang-wang-et-al-2024>(3/3 | 284/327) Tensorized NeuroEvolution of Augmenting Topologies for GPU Acceleration (Lishuang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lishuang Wang, Mengfei Zhao, Enyu Liu, Kebin Sun, Ran Cheng. (2024)<br><strong>Tensorized NeuroEvolution of Augmenting Topologies for GPU Acceleration</strong><br><button class=copy-to-clipboard title="Tensorized NeuroEvolution of Augmenting Topologies for GPU Acceleration" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01817v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01817v2.pdf filename=2404.01817v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The NeuroEvolution of Augmenting Topologies (NEAT) algorithm has received considerable recognition in the field of neuroevolution. Its effectiveness is derived from initiating with simple networks and incrementally evolving both their topologies and weights. Although its capability across various challenges is evident, the algorithm&rsquo;s computational efficiency remains an impediment, limiting its scalability potential. In response, this paper introduces a tensorization method for the NEAT algorithm, enabling the transformation of its diverse network topologies and associated operations into uniformly shaped tensors for computation. This advancement facilitates the execution of the NEAT algorithm in a parallelized manner across the entire population. Furthermore, we develop TensorNEAT, a library that implements the tensorized NEAT algorithm and its variants, such as CPPN and HyperNEAT. Building upon JAX, TensorNEAT promotes efficient parallel computations via automated function vectorization and hardware acceleration. Moreover, the TensorNEAT library supports various <b>benchmark</b> environments including Gym, Brax, and gymnax. Through evaluations across a spectrum of robotics control environments in Brax, TensorNEAT achieves up to 500x speedups compared to the existing implementations such as NEAT-Python. Source codes are available at: <a href=https://github.com/EMI-Group/tensorneat>https://github.com/EMI-Group/tensorneat</a>.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=11--285327-deterministic-search-on-complete-bipartite-graphs-by-continuous-time-quantum-walk-honghong-lin-et-al-2024>(1/1 | 285/327) Deterministic Search on Complete Bipartite Graphs by Continuous Time Quantum Walk (Honghong Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Honghong Lin, Yun Shang. (2024)<br><strong>Deterministic Search on Complete Bipartite Graphs by Continuous Time Quantum Walk</strong><br><button class=copy-to-clipboard title="Deterministic Search on Complete Bipartite Graphs by Continuous Time Quantum Walk" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-DS, quant-ph, quant-ph<br>Keyword Score: 33<br>Keywords: Graph, Continuous Time, Continuous Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01640v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01640v1.pdf filename=2404.01640v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a deterministic search algorithm on complete bipartite <b>graphs.</b> Our algorithm adopts the simple form of alternating iterations of an oracle and a <b>continuous-time</b> <b>quantum</b> walk operator, which is a generalization of Grover&rsquo;s search algorithm. We address the most general case of multiple marked states, so there is a problem of estimating the number of marked states. To this end, we construct a quantum counting algorithm based on the spectrum structure of the search operator. To implement the <b>continuous-time</b> <b>quantum</b> walk operator, we perform Hamiltonian <b>simulation</b> in the quantum circuit model. We achieve <b>simulation</b> in constant time, that is, the complexity of the quantum circuit does not scale with the evolution time. Besides, deterministic search serves as a simple tool for perfect state transfer (PST). As an application, we explore the problem of PST on complete bipartite <b>graphs.</b></p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--286327-robust-constrained-consensus-and-inequality-constrained-distributed-optimization-with-guaranteed-differential-privacy-and-accurate-convergence-yongqiang-wang-et-al-2024>(1/1 | 286/327) Robust Constrained Consensus and Inequality-constrained Distributed Optimization with Guaranteed Differential Privacy and Accurate Convergence (Yongqiang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongqiang Wang, Angelia Nedic. (2024)<br><strong>Robust Constrained Consensus and Inequality-constrained Distributed Optimization with Guaranteed Differential Privacy and Accurate Convergence</strong><br><button class=copy-to-clipboard title="Robust Constrained Consensus and Inequality-constrained Distributed Optimization with Guaranteed Differential Privacy and Accurate Convergence" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02327v1.pdf filename=2404.02327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address <b>differential</b> <b>privacy</b> for fully distributed optimization subject to a shared inequality constraint. By co-designing the distributed optimization mechanism and the <b>differential-privacy</b> <b>noise</b> injection mechanism, we propose the first distributed constrained optimization algorithm that can ensure both provable convergence to a global optimal solution and rigorous $\epsilon$-differential privacy, even when the number of iterations tends to infinity. Our approach does not require the Lagrangian function to be strictly convex/concave, and allows the global objective function to be non-separable. As a byproduct of the co-design, we also propose a new constrained consensus algorithm that can achieve rigorous $\epsilon$-differential privacy while maintaining accurate convergence, which, to our knowledge, has not been achieved before. Numerical <b>simulation</b> results on a demand response control problem in smart grid confirm the effectiveness of the proposed approach.</p></p class="citation"></blockquote><h2 id=cssi-2>cs.SI (2)</h2><h3 id=12--287327-a-holistic-indicator-of-polarization-to-measure-online-sexism-vahid-ghafouri-et-al-2024>(1/2 | 287/327) A Holistic Indicator of Polarization to Measure Online Sexism (Vahid Ghafouri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vahid Ghafouri, Jose Such, Guillermo Suarez-Tangil. (2024)<br><strong>A Holistic Indicator of Polarization to Measure Online Sexism</strong><br><button class=copy-to-clipboard title="A Holistic Indicator of Polarization to Measure Online Sexism" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-SI, cs.SI<br>Keyword Score: 30<br>Keywords: Supervised Learning, Unsupervised Learning, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02205v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02205v1.pdf filename=2404.02205v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The online trend of the manosphere and feminist discourse on social networks requires a holistic measure of the level of sexism in an online community. This indicator is important for policymakers and moderators of online communities (e.g., subreddits) and computational social scientists, either to revise moderation strategies based on the degree of sexism or to match and compare the temporal sexism across different platforms and communities with real-time events and infer social scientific insights. In this paper, we build a model that can provide a comparable holistic indicator of toxicity targeted toward male and female identity and male and female individuals. Despite previous <b>supervised</b> NLP methods that require annotation of toxic comments at the target level (e.g. annotating comments that are specifically toxic toward women) to detect targeted toxic comments, our indicator uses <b>supervised</b> NLP to detect the presence of toxicity and <b>unsupervised</b> <b>word</b> <b>embedding</b> association test to detect the target automatically. We apply our model to gender discourse communities (e.g., r/TheRedPill, r/MGTOW, r/FemaleDatingStrategy) to detect the level of toxicity toward genders (i.e., sexism). Our results show that our framework accurately and consistently (93% correlation) measures the level of sexism in a community. We finally discuss how our framework can be generalized in the future to measure qualities other than toxicity (e.g. sentiment, humor) toward general-purpose targets and turn into an indicator of different sorts of polarizations.</p></p class="citation"></blockquote><h3 id=22--288327-how-covid-19-has-impacted-the-anti-vaccine-discourse-a-large-scale-twitter-study-spanning-pre-covid-and-post-covid-era-soham-poddar-et-al-2024>(2/2 | 288/327) How COVID-19 has Impacted the Anti-Vaccine Discourse: A Large-Scale Twitter Study Spanning Pre-COVID and Post-COVID Era (Soham Poddar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soham Poddar, Rajdeep Mukherjee, Subhendu Khatuya, Niloy Ganguly, Saptarshi Ghosh. (2024)<br><strong>How COVID-19 has Impacted the Anti-Vaccine Discourse: A Large-Scale Twitter Study Spanning Pre-COVID and Post-COVID Era</strong><br><button class=copy-to-clipboard title="How COVID-19 has Impacted the Anti-Vaccine Discourse: A Large-Scale Twitter Study Spanning Pre-COVID and Post-COVID Era" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-CY, cs-SI, cs.SI, physics-soc-ph<br>Keyword Score: 20<br>Keywords: Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01669v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01669v1.pdf filename=2404.01669v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The debate around vaccines has been going on for decades, but the COVID-19 pandemic showed how crucial it is to understand and mitigate anti-vaccine sentiments. While the pandemic may be over, it is still important to understand how the pandemic affected the anti-vaccine discourse, and whether the arguments against non-COVID vaccines (e.g., Flu, MMR, IPV, HPV vaccines) have also changed due to the pandemic. This study attempts to answer these questions through a large-scale study of anti-vaccine posts on Twitter. Almost all prior works that utilized social media to understand anti-vaccine opinions considered only the three broad stances of Anti-Vax, Pro-Vax, and Neutral. There has not been any effort to identify the specific reasons/concerns behind the anti-vax sentiments (e.g., side-effects, conspiracy theories, political reasons) on social media at scale. In this work, we propose two novel methods for classifying tweets into 11 different anti-vax concerns &ndash; a discriminative approach (entailment-based) and a generative approach (based on <b>instruction</b> <b>tuning</b> of <b>LLMs)</b> &ndash; which outperform several strong baselines. We then apply this classifier on anti-vaccine tweets posted over a 5-year period (Jan 2018 - Jan 2023) to understand how the COVID-19 pandemic has impacted the anti-vaccine concerns among the masses. We find that the pandemic has made the anti-vaccine discourse far more complex than in the pre-COVID times, and increased the variety of concerns being voiced. Alarmingly, we find that concerns about COVID vaccines are now being projected onto the non-COVID vaccines, thus making more people hesitant in taking vaccines in the post-COVID era.</p></p class="citation"></blockquote><h2 id=mathna-5>math.NA (5)</h2><h3 id=15--289327-numerical-simulation-of-the-gross-pitaevskii-equation-via-vortex-tracking-thiago-carvalho-corso-et-al-2024>(1/5 | 289/327) Numerical simulation of the Gross-Pitaevskii equation via vortex tracking (Thiago Carvalho Corso et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thiago Carvalho Corso, Gaspard Kemlin, Christof Melcher, Benjamin Stamm. (2024)<br><strong>Numerical simulation of the Gross-Pitaevskii equation via vortex tracking</strong><br><button class=copy-to-clipboard title="Numerical simulation of the Gross-Pitaevskii equation via vortex tracking" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 30<br>Keywords: Quantization, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02133v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02133v1.pdf filename=2404.02133v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper deals with the numerical <b>simulation</b> of the Gross-Pitaevskii (GP) equation, for which a well-known feature is the appearance of <b>quantized</b> vortices with core size of the order of a small parameter $\varepsilon$. Without a magnetic field and with suitable initial conditions, these vortices interact, in the singular limit $\varepsilon\to0$, through an explicit Hamiltonian dynamics. Using this analytical framework, we develop and analyze a numerical strategy based on the reduced-order Hamiltonian system to efficiently simulate the infinite-dimensional GP equation for small, but finite, $\varepsilon$. This method allows us to avoid numerical stability issues in solving the GP equation, where small values of $\varepsilon$ typically require very fine meshes and time steps. We also provide a mathematical justification of our method in terms of rigorous error estimates of the error in the supercurrent, together with numerical illustrations.</p></p class="citation"></blockquote><h3 id=25--290327-adaptive-gradient-enhanced-gaussian-process-surrogates-for-inverse-problems-phillip-semler-et-al-2024>(2/5 | 290/327) Adaptive Gradient Enhanced Gaussian Process Surrogates for Inverse Problems (Phillip Semler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Phillip Semler, Martin Weiser. (2024)<br><strong>Adaptive Gradient Enhanced Gaussian Process Surrogates for Inverse Problems</strong><br><button class=copy-to-clipboard title="Adaptive Gradient Enhanced Gaussian Process Surrogates for Inverse Problems" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65N21, 65K10, 65N30, 90C31, cs-NA, math-NA, math.NA<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01864v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01864v1.pdf filename=2404.01864v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating simulated training data needed for constructing sufficiently accurate surrogate models to be used for efficient optimization or parameter identification can incur a huge computational effort in the offline phase. We consider a fully adaptive greedy approach to the computational design of experiments problem using gradient-enhanced <b>Gaussian</b> <b>process</b> regression as surrogates. Designs are incrementally defined by solving an optimization problem for accuracy given a certain computational budget. We address not only the choice of evaluation points but also of required <b>simulation</b> accuracy, both of values and gradients of the forward model. Numerical results show a significant reduction of the computational effort compared to just position-adaptive and static designs as well as a clear benefit of including gradient information into the surrogate training.</p></p class="citation"></blockquote><h3 id=35--291327-comparison-of-different-elastic-strain-definitions-for-largely-deformed-sei-of-chemo-mechanically-coupled-silicon-battery-particles-raphael-schoof-et-al-2024>(3/5 | 291/327) Comparison of Different Elastic Strain Definitions for Largely Deformed SEI of Chemo-Mechanically Coupled Silicon Battery Particles (Raphael Schoof et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raphael Schoof, Giuseppe Fabian Castelli, Willy Dörfler. (2024)<br><strong>Comparison of Different Elastic Strain Definitions for Largely Deformed SEI of Chemo-Mechanically Coupled Silicon Battery Particles</strong><br><button class=copy-to-clipboard title="Comparison of Different Elastic Strain Definitions for Largely Deformed SEI of Chemo-Mechanically Coupled Silicon Battery Particles" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 74C15, 74C20, 74S05, 65M22, 90C33, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01884v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01884v1.pdf filename=2404.01884v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Amorphous silicon is a highly promising anode material for next-generation lithium-ion batteries. Large volume changes of the silicon particle have a critical effect on the surrounding solid-electrolyte interphase (SEI) due to repeated fracture and healing during cycling. Based on a thermodynamically consistent chemo-elasto-plastic continuum model we investigate the stress development inside the particle and the SEI. Using the example of a particle with SEI, we apply a higher order finite element method together with a variable-step, variable-order time integration scheme on a nonlinear system of partial differential equations. Starting from a single silicon particle setting, the surrounding SEI is added in a first step with the typically used elastic Green&ndash;St-Venant (GSV) strain definition for a purely elastic deformation. For this type of deformation, the definition of the elastic strain is crucial to get reasonable <b>simulation</b> results. In case of the elastic GSV strain, the <b>simulation</b> aborts. We overcome the <b>simulation</b> failure by using the definition of the logarithmic Hencky strain. However, the particle remains unaffected by the elastic strain definitions in the particle domain. Compared to GSV, plastic deformation with the Hencky strain is straightforward to take into account. For the plastic SEI deformation, a rate-independent and a rate-dependent plastic deformation are newly introduced and numerically compared for three half cycles for the example of a radial symmetric particle.</p></p class="citation"></blockquote><h3 id=45--292327-a-second-order-correction-method-for-loosely-coupled-discretizations-applied-to-parabolic-parabolic-interface-problems-erik-burman-et-al-2024>(4/5 | 292/327) A second-order correction method for loosely coupled discretizations applied to parabolic-parabolic interface problems (Erik Burman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erik Burman, Rebecca Durst, Miguel A. Fernández, Johnny Guzmán, Sijing Liu. (2024)<br><strong>A second-order correction method for loosely coupled discretizations applied to parabolic-parabolic interface problems</strong><br><button class=copy-to-clipboard title="A second-order correction method for loosely coupled discretizations applied to parabolic-parabolic interface problems" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01599v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01599v1.pdf filename=2404.01599v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a parabolic-parabolic interface problem and construct a loosely coupled prediction-correction scheme based on the Robin-Robin splitting method analyzed in [J. Numer. Math., 31(1):59&ndash;77, 2023]. We show that the errors of the correction step converge at $\mathcal O((\Delta t)^2)$, under suitable convergence rate assumptions on the <b>discrete</b> <b>time</b> derivative of the prediction step, where $\Delta t$ stands for the time-step length. Numerical results are shown to support our analysis and the assumptions.</p></p class="citation"></blockquote><h3 id=55--293327-estimates-of-discrete-time-derivatives-for-the-parabolic-parabolic-robin-robin-coupling-method-erik-burman-et-al-2024>(5/5 | 293/327) Estimates of discrete time derivatives for the parabolic-parabolic Robin-Robin coupling method (Erik Burman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erik Burman, Rebecca Durst, Miguel A. Fernández, Johnny Guzmán, Sijing Liu. (2024)<br><strong>Estimates of discrete time derivatives for the parabolic-parabolic Robin-Robin coupling method</strong><br><button class=copy-to-clipboard title="Estimates of discrete time derivatives for the parabolic-parabolic Robin-Robin coupling method" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01594v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01594v1.pdf filename=2404.01594v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a loosely coupled, non-iterative Robin-Robin coupling method proposed and analyzed in [J. Numer. Math., 31(1):59&ndash;77, 2023] for a parabolic-parabolic interface problem and prove estimates for the <b>discrete</b> <b>time</b> derivatives of the scalar field in different norms. When the interface is flat and perpendicular to two of the edges of the domain we prove error estimates in the $H^2$-norm. Such estimates are key ingredients to analyze a defect correction method for the parabolic-parabolic interface problem. Numerical results are shown to support our findings.</p></p class="citation"></blockquote><h2 id=physicsbio-ph-1>physics.bio-ph (1)</h2><h3 id=11--294327-emergence-of-chemotactic-strategies-with-multi-agent-reinforcement-learning-samuel-tovey-et-al-2024>(1/1 | 294/327) Emergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning (Samuel Tovey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Tovey, Christoph Lohrmann, Christian Holm. (2024)<br><strong>Emergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Emergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.bio-ph<br>Categories: cs-LG, cs-MA, physics-bio-ph, physics.bio-ph<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01999v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01999v1.pdf filename=2404.01999v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL) is a flexible and efficient method for programming micro-robots in complex environments. Here we investigate whether <b>reinforcement</b> <b>learning</b> can provide insights into biological systems when trained to perform chemotaxis. Namely, whether we can learn about how intelligent agents process given information in order to swim towards a target. We run <b>simulations</b> covering a range of agent shapes, sizes, and swim speeds to determine if the physical constraints on biological swimmers, namely Brownian motion, lead to regions where <b>reinforcement</b> <b>learners&rsquo;</b> training fails. We find that the RL agents can perform chemotaxis as soon as it is physically possible and, in some cases, even before the active swimming overpowers the stochastic environment. We study the efficiency of the emergent policy and identify convergence in agent size and swim speeds. Finally, we study the strategy adopted by the <b>reinforcement</b> <b>learning</b> algorithm to explain how the agents perform their tasks. To this end, we identify three emerging dominant strategies and several rare approaches taken. These strategies, whilst producing almost identical trajectories in <b>simulation,</b> are distinct and give insight into the possible mechanisms behind which biological agents explore their environment and respond to changing conditions.</p></p class="citation"></blockquote><h2 id=csdl-2>cs.DL (2)</h2><h3 id=12--295327-sentiment-analysis-of-citations-in-scientific-articles-using-chatgpt-identifying-potential-biases-and-conflicts-of-interest-walid-hariri-2024>(1/2 | 295/327) Sentiment Analysis of Citations in Scientific Articles Using ChatGPT: Identifying Potential Biases and Conflicts of Interest (Walid Hariri, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Walid Hariri. (2024)<br><strong>Sentiment Analysis of Citations in Scientific Articles Using ChatGPT: Identifying Potential Biases and Conflicts of Interest</strong><br><button class=copy-to-clipboard title="Sentiment Analysis of Citations in Scientific Articles Using ChatGPT: Identifying Potential Biases and Conflicts of Interest" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-CL, cs-DL, cs.DL<br>Keyword Score: 30<br>Keywords: ChatGPT, Sentiment Analysis, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01800v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01800v2.pdf filename=2404.01800v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scientific articles play a crucial role in advancing knowledge and informing research directions. One key aspect of evaluating scientific articles is the analysis of citations, which provides insights into the impact and reception of the cited works. This article introduces the innovative use of <b>large</b> <b>language</b> <b>models,</b> particularly <b>ChatGPT,</b> for comprehensive <b>sentiment</b> <b>analysis</b> of citations within scientific articles. By leveraging advanced natural language processing (NLP) techniques, <b>ChatGPT</b> can discern the nuanced positivity or negativity of citations, offering insights into the reception and impact of cited works. Furthermore, <b>ChatGPT&rsquo;s</b> capabilities extend to detecting potential biases and conflicts of interest in citations, enhancing the objectivity and reliability of scientific literature evaluation. This study showcases the transformative potential of artificial intelligence (AI)-powered tools in enhancing citation analysis and promoting integrity in scholarly research.</p></p class="citation"></blockquote><h3 id=22--296327-the-open-access-coverage-of-openalex-scopus-and-web-of-science-marc-andre-simard-et-al-2024>(2/2 | 296/327) The open access coverage of OpenAlex, Scopus and Web of Science (Marc-Andre Simard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marc-Andre Simard, Isabel Basson, Madelaine Hare, Vincent Lariviere, Philippe Mongeon. (2024)<br><strong>The open access coverage of OpenAlex, Scopus and Web of Science</strong><br><button class=copy-to-clipboard title="The open access coverage of OpenAlex, Scopus and Web of Science" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-DL, cs.DL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01985v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01985v1.pdf filename=2404.01985v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diamond open access (OA) journals offer a publishing model that is free for both authors and readers, but their lack of indexing in major bibliographic databases presents challenges in assessing the uptake of these journals. Furthermore, OA characteristics such as publication language and country of publication have often been used to support the argument that OA journals are more diverse and aim to serve a local community, but there is a current lack of empirical evidence related to the geographical and linguistic characteristics of OA journals. Using OpenAlex and the Directory of Open Access Journals as a <b>benchmark,</b> this paper investigates the coverage of diamond and gold through authorship and journal coverage in the Web of Science and Scopus by field, country, and language. Results show their lower coverage in WoS and Scopus, and the local scope of diamond OA. The share of English-only journals is considerably higher among gold journals. High-income countries have the highest share of authorship in every domain and type of journal, except for diamond journals in the social sciences and humanities. Understanding the current landscape of diamond OA indexing can aid the scholarly communications network with advancing policy and practices towards more inclusive OA models.</p></p class="citation"></blockquote><h2 id=astro-phim-1>astro-ph.IM (1)</h2><h3 id=11--297327-csst-strong-lensing-preparation-a-framework-for-detecting-strong-lenses-in-the-multi-color-imaging-survey-by-the-china-survey-space-telescope-csst-xu-li-et-al-2024>(1/1 | 297/327) CSST Strong Lensing Preparation: a Framework for Detecting Strong Lenses in the Multi-color Imaging Survey by the China Survey Space Telescope (CSST) (Xu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Li, Ruiqi Sun, Jiameng Lv, Peng Jia, Nan Li, Chengliang Wei, Zou Hu, Xinzhong Er, Yun Chen, Zhang Ban, Yuedong Fang, Qi Guo, Dezi Liu, Guoliang Li, Lin Lin, Ming Li, Ran Li, Xiaobo Li, Yu Luo, Xianmin Meng, Jundan Nie, Zhaoxiang Qi, Yisheng Qiu, Li Shao, Hao Tian, Lei Wang, Wei Wang, Jingtian Xian, Youhua Xu, Tianmeng Zhang, Xin Zhang, Zhimin Zhou. (2024)<br><strong>CSST Strong Lensing Preparation: a Framework for Detecting Strong Lenses in the Multi-color Imaging Survey by the China Survey Space Telescope (CSST)</strong><br><button class=copy-to-clipboard title="CSST Strong Lensing Preparation: a Framework for Detecting Strong Lenses in the Multi-color Imaging Survey by the China Survey Space Telescope (CSST)" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.IM<br>Categories: astro-ph-GA, astro-ph-IM, astro-ph.IM, cs-CV<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01780v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01780v1.pdf filename=2404.01780v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Strong gravitational lensing is a powerful tool for investigating dark matter and dark energy properties. With the advent of large-scale sky surveys, we can discover strong lensing systems on an unprecedented scale, which requires efficient tools to extract them from billions of astronomical objects. The existing mainstream lens-finding tools are based on machine learning algorithms and applied to cut-out-centered galaxies. However, according to the design and survey strategy of optical surveys by CSST, preparing cutouts with multiple bands requires considerable efforts. To overcome these challenges, we have developed a framework based on a hierarchical visual <b>Transformer</b> with a sliding window technique to search for strong lensing systems within entire images. Moreover, given that multi-color images of strong lensing systems can provide insights into their physical characteristics, our framework is specifically crafted to identify strong lensing systems in images with any number of channels. As evaluated using CSST mock data based on an Semi-Analytic Model named CosmoDC2, our framework achieves precision and recall rates of 0.98 and 0.90, respectively. To evaluate the effectiveness of our method in real observations, we have applied it to a subset of images from the DESI Legacy Imaging Surveys and media images from Euclid Early Release Observations. 61 new strong lensing system candidates are discovered by our method. However, we also identified false positives arising primarily from the simplified galaxy morphology assumptions within the <b>simulation.</b> This underscores the practical limitations of our approach while simultaneously highlighting potential avenues for future improvements.</p></p class="citation"></blockquote><h2 id=statme-1>stat.ME (1)</h2><h3 id=11--298327-robustly-estimating-heterogeneity-in-factorial-data-using-rashomon-partitions-aparajithan-venkateswaran-et-al-2024>(1/1 | 298/327) Robustly estimating heterogeneity in factorial data using Rashomon Partitions (Aparajithan Venkateswaran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aparajithan Venkateswaran, Anirudh Sankar, Arun G. Chandrasekhar, Tyler H. McCormick. (2024)<br><strong>Robustly estimating heterogeneity in factorial data using Rashomon Partitions</strong><br><button class=copy-to-clipboard title="Robustly estimating heterogeneity in factorial data using Rashomon Partitions" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-LG, econ-EM, stat-CO, stat-ME, stat-ML, stat.ME<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02141v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02141v1.pdf filename=2404.02141v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many statistical analyses, in both observational data and randomized control trials, ask: how does the outcome of interest vary with combinations of observable covariates? How do various drug combinations affect health outcomes, or how does technology adoption depend on incentives and demographics? Our goal is to partition this factorial space into <code>pools'' of covariate combinations where the outcome differs across the pools (but not within a pool). Existing approaches (i) search for a single </code>optimal&rsquo;&rsquo; partition under assumptions about the association between covariates or (ii) sample from the entire set of possible partitions. Both these approaches ignore the reality that, especially with correlation structure in covariates, many ways to partition the covariate space may be statistically indistinguishable, despite very different implications for policy or science. We develop an alternative perspective, called Rashomon Partition Sets (RPSs). Each item in the RPS partitions the space of covariates using a tree-like <b>geometry.</b> RPSs incorporate all partitions that have posterior values near the maximum a posteriori partition, even if they offer substantively different explanations, and do so using a prior that makes no assumptions about associations between covariates. This prior is the $\ell_0$ prior, which we show is minimax optimal. Given the RPS we calculate the posterior of any measurable function of the feature effects vector on outcomes, conditional on being in the RPS. We also characterize approximation error relative to the entire posterior and provide bounds on the size of the RPS. <b>Simulations</b> demonstrate this framework allows for robust conclusions relative to conventional regularization techniques. We apply our method to three empirical settings: price effects on charitable giving, chromosomal structure (telomere length), and the introduction of microfinance.</p></p class="citation"></blockquote><h2 id=csit-3>cs.IT (3)</h2><h3 id=13--299327-the-meta-distribution-of-the-sir-in-joint-communication-and-sensing-networks-kun-ma-et-al-2024>(1/3 | 299/327) The Meta Distribution of the SIR in Joint Communication and Sensing Networks (Kun Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kun Ma, Chenyuan Feng, Giovanni Geraci, Howard H. Yang. (2024)<br><strong>The Meta Distribution of the SIR in Joint Communication and Sensing Networks</strong><br><button class=copy-to-clipboard title="The Meta Distribution of the SIR in Joint Communication and Sensing Networks" index=299>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-299 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01672v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01672v1.pdf filename=2404.01672v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a novel mathematical framework for assessing the performance of joint communication and sensing (JCAS) in wireless networks, employing stochastic <b>geometry</b> as an analytical tool. We focus on deriving the meta distribution of the signal-to-interference ratio (SIR) for JCAS networks. This approach enables a fine-grained quantification of individual user or radar performance intrinsic to these networks. Our work involves the modeling of JCAS networks and the derivation of mathematical expressions for the JCAS SIR meta distribution. Through <b>simulations,</b> we validate both our theoretical analysis and illustrate how the JCAS SIR meta distribution varies with the network deployment density.</p></p class="citation"></blockquote><h3 id=23--300327-learning-based-joint-beamforming-and-antenna-movement-design-for-movable-antenna-systems-caihao-weng-et-al-2024>(2/3 | 300/327) Learning-Based Joint Beamforming and Antenna Movement Design for Movable Antenna Systems (Caihao Weng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Caihao Weng, Yuanbin Chen, Lipeng Zhu, Ying Wang. (2024)<br><strong>Learning-Based Joint Beamforming and Antenna Movement Design for Movable Antenna Systems</strong><br><button class=copy-to-clipboard title="Learning-Based Joint Beamforming and Antenna Movement Design for Movable Antenna Systems" index=300>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-300 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01784v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01784v1.pdf filename=2404.01784v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate a multi-receiver communication system enabled by movable antennas (MAs). Specifically, the transmit beamforming and the double-side antenna movement at the transceiver are jointly designed to maximize the sum-rate of all receivers under imperfect channel state information (CSI). Since the formulated problem is non-convex with highly coupled variables, conventional optimization methods cannot solve it efficiently. To address these challenges, an effective learning-based algorithm is proposed, namely heterogeneous multi-agent deep deterministic policy gradient (MADDPG), which incorporates two agents to learn policies for beamforming and movement of MAs, respectively. Based on the offline learning under numerous imperfect CSI, the proposed heterogeneous MADDPG can output the solutions for transmit beamforming and antenna movement in real time. <b>Simulation</b> results validate the effectiveness of the proposed algorithm, and the MA can significantly improve the sum-rate performance of multiple receivers compared to other <b>benchmark</b> schemes.</p></p class="citation"></blockquote><h3 id=33--301327-generating-gaussian-pseudorandom-noise-with-binary-sequences-francisco-javier-soto-et-al-2024>(3/3 | 301/327) Generating gaussian pseudorandom noise with binary sequences (Francisco-Javier Soto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francisco-Javier Soto, Ana I. Gómez, Domingo Gómez-Pérez. (2024)<br><strong>Generating gaussian pseudorandom noise with binary sequences</strong><br><button class=copy-to-clipboard title="Generating gaussian pseudorandom noise with binary sequences" index=301>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-301 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02251v1.pdf filename=2404.02251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gaussian random number generators attract a widespread interest due to their applications in several fields. Important requirements include easy implementation, tail accuracy, and, finally, a flat spectrum. In this work, we study the applicability of uniform pseudorandom binary generators in combination with the Central Limit Theorem to propose an easy to implement, efficient and flexible algorithm that leverages the properties of the pseudorandom binary generator used as an input, specially with respect to the correlation measure of higher order, to guarantee the quality of the generated samples. Our main result provides a relationship between the pseudorandomness of the input and the statistical moments of the output. We propose a design based on the combination of pseudonoise sequences commonly used on wireless communications with known hardware implementation, which can generate sequences with guaranteed statistical distribution properties sufficient for many real life applications and simple machinery. Initial computer <b>simulations</b> on this construction show promising results in the quality of the output and the computational resources in terms of required memory and complexity.</p></p class="citation"></blockquote><h2 id=csar-3>cs.AR (3)</h2><h3 id=13--302327-netsmith-an-optimization-framework-for-machine-discovered-network-topologies-conor-green-et-al-2024>(1/3 | 302/327) NetSmith: An Optimization Framework for Machine-Discovered Network Topologies (Conor Green et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Conor Green, Mithuna Thottethodi. (2024)<br><strong>NetSmith: An Optimization Framework for Machine-Discovered Network Topologies</strong><br><button class=copy-to-clipboard title="NetSmith: An Optimization Framework for Machine-Discovered Network Topologies" index=302>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-302 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-DC, cs.AR<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02357v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02357v1.pdf filename=2404.02357v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the past few decades, network topology design for general purpose, shared memory multicores has been primarily driven by human experts who use their insights to arrive at network designs that balance the competing goals of performance requirements (e.g., latency, bandwidth) and cost constraints (e.g., router radix, router counts). On the other hand, there have been automatic NoC synthesis methods for SoCs to optimize for application-specific communication and objectives such as resource usage or power. Unfortunately, these techniques do not lend themselves to the general-purpose context, where directly applying these previous NoC synthesis techniques in the general-purpose context yields poor results, even worse than expert-designed networks. We design and develop an automatic network design methodology - NetSmith - to design networks for general-purpose, shared memory multicores that comprehensively outperform expert-designed networks. We employ NetSmith in the context of interposer networks for chiplet-based systems where there has been significant recent work on network topology design (e.g., Kite, Butter Donut, Double Butterfly). NetSmith generated topologies are capable of achieving significantly higher throughput (50% to 75% higher) while also reducing average hop count by 8% to 13.5%) than previous expert-designed and synthesized networks. Full system <b>simulations</b> using PARSEC <b>benchmarks</b> demonstrate that the improved network performance translates to improved application performance with up to 11% mean speedup over previous NoI topologies.</p></p class="citation"></blockquote><h3 id=23--303327-analyzing-the-single-event-upset-vulnerability-of-binarized-neural-networks-on-sram-fpgas-ioanna-souvatzoglou-et-al-2024>(2/3 | 303/327) Analyzing the Single Event Upset Vulnerability of Binarized Neural Networks on SRAM FPGAs (Ioanna Souvatzoglou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ioanna Souvatzoglou, Athanasios Papadimitriou, Aitzan Sari, Vasileios Vlagkoulis, Mihalis Psarakis. (2024)<br><strong>Analyzing the Single Event Upset Vulnerability of Binarized Neural Networks on SRAM FPGAs</strong><br><button class=copy-to-clipboard title="Analyzing the Single Event Upset Vulnerability of Binarized Neural Networks on SRAM FPGAs" index=303>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-303 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 23<br>Keywords: Object Detection, Benchmarking, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01757v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01757v1.pdf filename=2404.01757v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Networks (NNs) are increasingly used in the last decade in several demanding applications, such as <b>object</b> <b>detection</b> and classification, autonomous driving, etc. Among different computing platforms for implementing NNs, FPGAs have multiple advantages due to design flexibility and high performance-to-watt ratio. Moreover, approximation techniques, such as <b>quantization,</b> have been introduced, which reduce the computational and storage requirements, thus enabling the integration of larger NNs into FPGA devices. On the other hand, FPGAs are sensitive to radiation-induced Single Event Upsets (SEUs). In this work, we perform an in-depth reliability analysis in an FPGA-based Binarized Fully Connected Neural Network (BNN) accelerator running a statistical fault injection campaign. The BNN <b>benchmark</b> has been produced by FINN, an open-source framework that provides an end-to-end flow from abstract level to design, making it easy to design customized FPGA NN accelerators, while it also supports various approximation techniques. The campaign includes the injection of faults in the configuration memory of a state-of-the-art Xilinx Ultrascale+ FPGA running the BNN, as well an exhaustive fault injection in the user flip flops. We have analyzed the fault injection results characterizing the SEU vulnerability of the circuit per network layer, per clock cycle, and register. In general, the results show that the BNNs are inherently resilient to soft errors, since a low portion of SEUs in the configuration memory and the flip flops, cause system crashes or misclassification errors.</p></p class="citation"></blockquote><h3 id=33--304327-a-fully-configurable-open-source-software-defined-digital-quantized-spiking-neural-core-architecture-shadi-matinizadeh-et-al-2024>(3/3 | 304/327) A Fully-Configurable Open-Source Software-Defined Digital Quantized Spiking Neural Core Architecture (Shadi Matinizadeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shadi Matinizadeh, Noah Pacik-Nelson, Ioannis Polykretis, Krupa Tishbi, Suman Kumar, M. L. Varshika, Arghavan Mohammadhassani, Abhishek Mishra, Nagarajan Kandasamy, James Shackleford, Eric Gallo, Anup Das. (2024)<br><strong>A Fully-Configurable Open-Source Software-Defined Digital Quantized Spiking Neural Core Architecture</strong><br><button class=copy-to-clipboard title="A Fully-Configurable Open-Source Software-Defined Digital Quantized Spiking Neural Core Architecture" index=304>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-304 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 20<br>Keywords: Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02248v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02248v1.pdf filename=2404.02248v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce QUANTISENC, a fully configurable open-source software-defined digital <b>quantized</b> spiking neural core architecture to advance research in neuromorphic computing. QUANTISENC is designed hierarchically using a bottom-up methodology with multiple neurons in each layer and multiple layers in each core. The number of layers and neurons per layer can be configured via software in a top-down methodology to generate the hardware for a target spiking neural network (SNN) model. QUANTISENC uses leaky integrate and fire neurons (LIF) and current-based excitatory and inhibitory synapses (CUBA). The nonlinear dynamics of a neuron can be configured at run-time via programming its internal control registers. Each neuron performs signed fixed-point arithmetic with user-defined <b>quantization</b> and decimal precision. QUANTISENC supports all-to-all, one-to-one, and Gaussian connections between layers. Its hardware-software interface is integrated with a PyTorch-based SNN simulator. This integration allows to define and train an SNN model in PyTorch and evaluate the hardware performance (e.g., area, power, latency, and throughput) through FPGA prototyping and ASIC design. The hardware-software interface also takes advantage of the layer-based architecture and distributed memory organization of QUANTISENC to enable pipelining by overlapping computations on streaming data. Overall, the proposed software-defined hardware design methodology offers flexibility similar to that of high-level synthesis (HLS), but provides better hardware performance with zero hardware development effort. We evaluate QUANTISENC using three spiking datasets and show its superior performance against state-of the-art designs.</p></p class="citation"></blockquote><h2 id=eesssy-9>eess.SY (9)</h2><h3 id=19--305327-a-neural-network-based-approach-to-hybrid-systems-identification-for-control-filippo-fabiani-et-al-2024>(1/9 | 305/327) A neural network-based approach to hybrid systems identification for control (Filippo Fabiani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Filippo Fabiani, Bartolomeo Stellato, Daniele Masti, Paul J. Goulart. (2024)<br><strong>A neural network-based approach to hybrid systems identification for control</strong><br><button class=copy-to-clipboard title="A neural network-based approach to hybrid systems identification for control" index=305>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-305 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01814v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01814v1.pdf filename=2404.01814v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of designing a machine learning-based model of an unknown dynamical system from a finite number of (state-input)-successor state data points, such that the model obtained is also suitable for optimal control design. We propose a specific neural network (NN) architecture that yields a hybrid system with piecewise-affine dynamics that is differentiable with respect to the network&rsquo;s parameters, thereby enabling the use of derivative-based training procedures. We show that a careful choice of our NN&rsquo;s weights produces a hybrid system model with structural properties that are highly favourable when used as part of a finite horizon optimal control problem (OCP). Specifically, we show that optimal solutions with strong local optimality guarantees can be computed via nonlinear programming, in contrast to classical OCPs for general hybrid systems which typically require mixed-integer optimization. In addition to being well-suited for optimal control design, numerical <b>simulations</b> illustrate that our NN-based technique enjoys very similar performance to state-of-the-art system identification methodologies for hybrid systems and it is competitive on nonlinear <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=29--306327-on-the-regret-of-recursive-methods-for-discrete-time-adaptive-control-with-matched-uncertainty-aren-karapetyan-et-al-2024>(2/9 | 306/327) On the Regret of Recursive Methods for Discrete-Time Adaptive Control with Matched Uncertainty (Aren Karapetyan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aren Karapetyan, Efe C. Balta, Anastasios Tsiamis, Andrea Iannelli, John Lygeros. (2024)<br><strong>On the Regret of Recursive Methods for Discrete-Time Adaptive Control with Matched Uncertainty</strong><br><button class=copy-to-clipboard title="On the Regret of Recursive Methods for Discrete-Time Adaptive Control with Matched Uncertainty" index=306>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-306 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Continuous Time, Continuous Time, Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02023v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02023v1.pdf filename=2404.02023v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continuous-time</b> <b>adaptive</b> controllers for systems with a matched uncertainty often comprise an online parameter estimator and a corresponding parameterized controller to cancel the uncertainty. However, such methods are often unimplementable, as they depend on an unobserved estimation error. We consider the equivalent <b>discrete-time</b> <b>setting</b> with a causal information structure. We propose a novel, online proximal point method-based adaptive controller, that under a weak persistence of excitation (PE) condition is asymptotically stable and achieves finite regret, scaling only with the time required to fulfill the PE condition. We show the same also for the widely-used recursive least squares with exponential forgetting controller under a stronger PE condition.</p></p class="citation"></blockquote><h3 id=39--307327-on-the-effect-of-quantization-on-dynamic-mode-decomposition-dipankar-maity-et-al-2024>(3/9 | 307/327) On the Effect of Quantization on Dynamic Mode Decomposition (Dipankar Maity et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dipankar Maity, Debdipta Goswami, Sriram Narayanan. (2024)<br><strong>On the Effect of Quantization on Dynamic Mode Decomposition</strong><br><button class=copy-to-clipboard title="On the Effect of Quantization on Dynamic Mode Decomposition" index=307>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-307 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02014v1.pdf filename=2404.02014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic Mode Decomposition (DMD) is a widely used data-driven algorithm for estimating the Koopman Operator.This paper investigates how the estimation process is affected when the data is <b>quantized.</b> Specifically, we examine the fundamental connection between estimates of the operator obtained from unquantized data and those from <b>quantized</b> data. Furthermore, using the law of large numbers, we demonstrate that, under a large data regime, the <b>quantized</b> estimate can be considered a regularized version of the unquantized estimate. This key theoretical finding paves the way to accurately recover the unquantized estimate from <b>quantized</b> data. We also explore the relationship between the two estimates in the finite data regime. The theory is validated through repeated numerical experiments conducted on three different dynamical systems.</p></p class="citation"></blockquote><h3 id=49--308327-learning-based-model-augmentation-with-lfrs-jan-h-hoekstra-et-al-2024>(4/9 | 308/327) Learning-based model augmentation with LFRs (Jan H. Hoekstra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan H. Hoekstra, Chris Verhoek, Roland Tóth, Maarten Schoukens. (2024)<br><strong>Learning-based model augmentation with LFRs</strong><br><button class=copy-to-clipboard title="Learning-based model augmentation with LFRs" index=308>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-308 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01901v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01901v1.pdf filename=2404.01901v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial neural networks (ANN) have proven to be effective in dealing with the identification nonlinear models for highly complex systems. To still make use of the prior information available from baseline models derived from, e.g., first-principles (FP), methods have been developed that integrate the prior knowledge into the identification algorithm for the ANN in a variety of methods. These methods have shown better estimation speeds and/or accuracy on unseen data. Among these methods are model augmentation structures. A variety of these structures have been considered in literature, there is however no unifying theory to these. In this paper, we propose a flexible linear-fractional-representation (LFR) based model augmentation structure. This model structure is able to represent many common model augmentation structures, thus unifying them under the proposed model structure. Furthermore, we introduce an identification algorithm capable of estimating the proposed model augmentation structure. The performance and generalization capabilities of the identification algorithm and the augmentation structure is demonstrated on a hardening mass-spring-damper <b>simulation</b> example.</p></p class="citation"></blockquote><h3 id=59--309327-integrating-systemc-ams-power-modeling-with-a-risc-v-iss-for-virtual-prototyping-of-battery-operated-embedded-devices-mohamed-amine-hamdi-et-al-2024>(5/9 | 309/327) Integrating SystemC-AMS Power Modeling with a RISC-V ISS for Virtual Prototyping of Battery-operated Embedded Devices (Mohamed Amine Hamdi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed Amine Hamdi, Giovanni Pollo, Matteo Risso, Germain Haugou, Alessio Burrello, Enrico Macii, Massimo Poncino, Sara Vinco, Daniele Jahier Pagliari. (2024)<br><strong>Integrating SystemC-AMS Power Modeling with a RISC-V ISS for Virtual Prototyping of Battery-operated Embedded Devices</strong><br><button class=copy-to-clipboard title="Integrating SystemC-AMS Power Modeling with a RISC-V ISS for Virtual Prototyping of Battery-operated Embedded Devices" index=309>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-309 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: B-8-2; B-6-3, cs-AR, cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01861v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01861v1.pdf filename=2404.01861v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>RISC-V cores have gained a lot of popularity over the last few years. However, being quite a recent and novel technology, there is still a gap in the availability of comprehensive <b>simulation</b> frameworks for RISC-V that cover both the functional and extra-functional aspects. This gap hinders progress in the field, as fast yet accurate system-level <b>simulation</b> is crucial for Design Space Exploration (DSE). This work presents an open-source framework designed to tackle this challenge, integrating functional RISC-V <b>simulation</b> (achieved with GVSoC) with SystemC-AMS (used to model extra-functional aspects, in detail power storage and distribution). The combination of GVSoC and SystemC-AMS in a single <b>simulation</b> framework allows to perform a DSE that is dependent on the mutual impact between functional and extra-functional aspects. In our experiments, we validate the framework&rsquo;s effectiveness by creating a virtual prototype of a compact, battery-powered embedded system.</p></p class="citation"></blockquote><h3 id=69--310327-identifying-the-largest-rocof-and-its-implications-licheng-wang-et-al-2024>(6/9 | 310/327) Identifying the Largest RoCoF and Its Implications (Licheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Licheng Wang, Luochen Xie, Gang Huang, Changsen Feng. (2024)<br><strong>Identifying the Largest RoCoF and Its Implications</strong><br><button class=copy-to-clipboard title="Identifying the Largest RoCoF and Its Implications" index=310>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-310 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01609v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01609v1.pdf filename=2404.01609v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rate of change of frequency (RoCoF) is a critical factor in ensuring frequency security, particularly in power systems with low inertia. Currently, most RoCoF security constrained optimal inertia dispatch methods and inertia market mechanisms predominantly rely on the center of inertia (COI) model. This model, however, does not account for the disparities in post-contingency frequency dynamics across different regions of a power system. Specifically, regional buses can exhibit significantly larger RoCoFs than that predicted by the system&rsquo;s COI, particularly in systems characterized by unevenly distributed inertia. In this letter, a post-contingency nodal RoCoF model is established, and the maximal initial RoCoF is further proven to occur at generator buses equipped with inertia, rather than at inertia-less load buses. This finding facilitates the development of the optimal nodal inertia dispatch method and the nodal inertia market mechanism in a convex and concise form. Our argument is further verified by the <b>simulation</b> results of the South East Australia power system under various scenarios.</p></p class="citation"></blockquote><h3 id=79--311327-real-time-hybrid-simulation-for-infrastructure-degradation-assessment-conceptual-framework-and-illustrative-application-manuel-salmeron-et-al-2024>(7/9 | 311/327) Real-Time Hybrid Simulation for Infrastructure Degradation Assessment: Conceptual Framework and Illustrative Application (Manuel Salmeron et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manuel Salmeron, Herta Montoya, Edwin Patino, Ingrid E. Madera Sierra, Shirley J. Dyke. (2024)<br><strong>Real-Time Hybrid Simulation for Infrastructure Degradation Assessment: Conceptual Framework and Illustrative Application</strong><br><button class=copy-to-clipboard title="Real-Time Hybrid Simulation for Infrastructure Degradation Assessment: Conceptual Framework and Illustrative Application" index=311>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-311 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01575v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01575v1.pdf filename=2404.01575v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To date, the prospect of using real-time hybrid <b>simulation</b> (RTHS) to study the effects of long-term or &lsquo;wear-and-tear&rsquo; loads, such as exposure to harmful environmental conditions or fatigue, has remained underexplored. This study presents a conceptual framework to assess the impact of long-term degradation on infrastructure systems. The framework integrates the capabilities of RTHS with accelerated degradation techniques to evaluate the behavior of a degrading system over time. Experimental results obtained in this way not only capture the complex interactions but also provide a reliability-based method to determine the expected time-to-failure of the evaluated system. The developed framework is demonstrated using a virtual RTHS platform designed to test fiber-reinforced elastomeric isolators.</p></p class="citation"></blockquote><h3 id=89--312327-on-the-reduction-of-linear-parameter-varying-state-space-models-e-javier-olucha-et-al-2024>(8/9 | 312/327) On the reduction of Linear Parameter-Varying State-Space models (E. Javier Olucha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>E. Javier Olucha, Bogoljub Terzin, Amritam Das, Roland Tóth. (2024)<br><strong>On the reduction of Linear Parameter-Varying State-Space models</strong><br><button class=copy-to-clipboard title="On the reduction of Linear Parameter-Varying State-Space models" index=312>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-312 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 16<br>Keywords: Autoencoder, Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01871v1.pdf filename=2404.01871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an overview and comparative study of the state of the art in State-Order Reduction (SOR) and Scheduling Dimension Reduction (SDR) for Linear Parameter-Varying (LPV) State-Space (SS) models, comparing and <b>benchmarking</b> their capabilities, limitations and performance. The use case chosen for these studies is an interconnected network of nonlinear coupled mass spring damper systems with three different configurations, where some spring coefficients are described by arbitrary user-defined static nonlinear functions. For SOR, the following methods are compared: Linear Time-Invariant (LTI), LPV and LFR-based balanced reductions, moment matching and parameter-varying oblique projection. For SDR, the following methods are compared: Principal Component Analysis (PCA), trajectory PCA, Kernel PCA and LTI balanced truncation, <b>autoencoders</b> and deep neural network. The comparison reveals the most suitable reduction methods for the different <b>benchmark</b> configurations, from which we provide use case SOR and SDR guidelines that can be used to choose the best reduction method for a given LPV-SS model.</p></p class="citation"></blockquote><h3 id=99--313327-a-stability-based-abstraction-framework-for-reach-avoid-control-of-stochastic-dynamical-systems-with-unknown-noise-distributions-thom-badings-et-al-2024>(9/9 | 313/327) A Stability-Based Abstraction Framework for Reach-Avoid Control of Stochastic Dynamical Systems with Unknown Noise Distributions (Thom Badings et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thom Badings, Licio Romao, Alessandro Abate, Nils Jansen. (2024)<br><strong>A Stability-Based Abstraction Framework for Reach-Avoid Control of Stochastic Dynamical Systems with Unknown Noise Distributions</strong><br><button class=copy-to-clipboard title="A Stability-Based Abstraction Framework for Reach-Avoid Control of Stochastic Dynamical Systems with Unknown Noise Distributions" index=313>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-313 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 13<br>Keywords: Graph, Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01726v1.pdf filename=2404.01726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Finite-state abstractions are widely studied for the automated synthesis of correct-by-construction controllers for stochastic dynamical systems. However, existing abstraction methods often lead to prohibitively large finite-state models. To address this issue, we propose a novel abstraction scheme for stochastic linear systems that exploits the system&rsquo;s stability to obtain significantly smaller abstract models. As a unique feature, we first stabilize the open-loop dynamics using a linear feedback gain. We then use a model-based approach to abstract a known part of the stabilized dynamics while using a data-driven method to account for the stochastic uncertainty. We formalize abstractions as Markov decision processes <b>(MDPs)</b> with intervals of transition probabilities. By stabilizing the dynamics, we can further constrain the control input modeled in the abstraction, which leads to smaller abstract models while retaining the correctness of controllers. Moreover, when the stabilizing feedback controller is aligned with the property of interest, then a good trade-off is achieved between the reduction in the abstraction size and the performance loss. The experiments show that our approach can reduce the size of the <b>graph</b> of abstractions by up to 90% with negligible performance loss.</p></p class="citation"></blockquote><h2 id=csds-1>cs.DS (1)</h2><h3 id=11--314327-learning-intersections-of-halfspaces-with-distribution-shift-improved-algorithms-and-sq-lower-bounds-adam-r-klivans-et-al-2024>(1/1 | 314/327) Learning Intersections of Halfspaces with Distribution Shift: Improved Algorithms and SQ Lower Bounds (Adam R. Klivans et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adam R. Klivans, Konstantinos Stavropoulos, Arsen Vasilyan. (2024)<br><strong>Learning Intersections of Halfspaces with Distribution Shift: Improved Algorithms and SQ Lower Bounds</strong><br><button class=copy-to-clipboard title="Learning Intersections of Halfspaces with Distribution Shift: Improved Algorithms and SQ Lower Bounds" index=314>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-314 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs-LG, cs.DS<br>Keyword Score: 20<br>Keywords: Distribution Shift, Distribution Shift, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02364v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02364v1.pdf filename=2404.02364v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work of Klivans, Stavropoulos, and Vasilyan initiated the study of testable learning with <b>distribution</b> <b>shift</b> (TDS learning), where a learner is given labeled samples from training <b>distribution</b> <b>$\mathcal{D}$,</b> unlabeled samples from test <b>distribution</b> <b>$\mathcal{D}&rsquo;$,</b> and the goal is to output a classifier with low error on $\mathcal{D}&rsquo;$ whenever the training samples pass a corresponding test. Their model deviates from all prior work in that no assumptions are made on $\mathcal{D}&rsquo;$. Instead, the test must accept (with high probability) when the marginals of the training and test <b>distributions</b> <b>are</b> equal. Here we focus on the fundamental case of intersections of halfspaces with respect to Gaussian training <b>distributions</b> <b>and</b> prove a variety of new upper bounds including a $2^{(k/\epsilon)^{O(1)}} \mathsf{poly}(d)$-time algorithm for TDS learning intersections of $k$ homogeneous halfspaces to accuracy $\epsilon$ (prior work achieved $d^{(k/\epsilon)^{O(1)}}$). We work under the mild assumption that the Gaussian training <b>distribution</b> <b>contains</b> at least an $\epsilon$ fraction of both positive and negative examples ($\epsilon$-balanced). We also prove the first set of SQ lower-bounds for any TDS learning problem and show (1) the $\epsilon$-balanced assumption is necessary for $\mathsf{poly}(d,1/\epsilon)$-time TDS learning for a single halfspace and (2) a $d^{\tilde{\Omega}(\log 1/\epsilon)}$ lower bound for the intersection of two general halfspaces, even with the $\epsilon$-balanced assumption. Our techniques significantly expand the toolkit for TDS learning. We use dimension reduction and coverings to give efficient algorithms for computing a localized version of discrepancy distance, a key metric from the <b>domain</b> <b>adaptation</b> literature.</p></p class="citation"></blockquote><h2 id=csdc-2>cs.DC (2)</h2><h3 id=12--315327-muxserve-flexible-multiplexing-for-efficient-multiple-llm-serving-jiangfei-duan-et-al-2024>(1/2 | 315/327) MuxServe: Flexible Multiplexing for Efficient Multiple LLM Serving (Jiangfei Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiangfei Duan, Runyu Lu, Haojie Duanmu, Xiuhong Li, Xingcheng Zhang, Dahua Lin, Ion Stoica, Hao Zhang. (2024)<br><strong>MuxServe: Flexible Multiplexing for Efficient Multiple LLM Serving</strong><br><button class=copy-to-clipboard title="MuxServe: Flexible Multiplexing for Efficient Multiple LLM Serving" index=315>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-315 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02015v1.pdf filename=2404.02015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated remarkable performance, and organizations are racing to serve <b>LLMs</b> of varying sizes as endpoints for use-cases like chat, programming and search. However, efficiently serving multiple <b>LLMs</b> poses significant challenges for existing approaches due to varying popularity of <b>LLMs.</b> In the paper, we present MuxServe, a flexible spatial-temporal multiplexing system for efficient multiple <b>LLM</b> serving. The key insight behind is to colocate <b>LLMs</b> considering their popularity to multiplex memory resources, and leverage the characteristics of prefill and decoding phases to separate and flexibly colocate them to multiplex computation resources. MuxServe formally formulates the multiplexing problem, and proposes a novel placement algorithm and adaptive batch scheduling strategy to identify optimal colocations and maximize utilization. MuxServe designs a unified resource manager to enable flexible and efficient multiplexing. Evaluation results show that MuxServe can achieves up to $1.8\times$ higher throughput or processes $2.9\times$ more requests within $99%$ SLO attainment.</p></p class="citation"></blockquote><h3 id=22--316327-a-shared-compilation-stack-for-distributed-memory-parallelism-in-stencil-dsls-george-bisbas-et-al-2024>(2/2 | 316/327) A shared compilation stack for distributed-memory parallelism in stencil DSLs (George Bisbas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>George Bisbas, Anton Lydike, Emilien Bauer, Nick Brown, Mathieu Fehr, Lawrence Mitchell, Gabriel Rodriguez-Canal, Maurice Jamieson, Paul H. J. Kelly, Michel Steuwer, Tobias Grosser. (2024)<br><strong>A shared compilation stack for distributed-memory parallelism in stencil DSLs</strong><br><button class=copy-to-clipboard title="A shared compilation stack for distributed-memory parallelism in stencil DSLs" index=316>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-316 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-MS, cs.DC<br>Keyword Score: 10<br>Keywords: Message-Passing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02218v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02218v1.pdf filename=2404.02218v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Domain Specific Languages (DSLs) increase programmer productivity and provide high performance. Their targeted abstractions allow scientists to express problems at a high level, providing rich details that optimizing compilers can exploit to target current- and next-generation supercomputers. The convenience and performance of DSLs come with significant development and maintenance costs. The siloed design of DSL compilers and the resulting inability to benefit from shared infrastructure cause uncertainties around longevity and the adoption of DSLs at scale. By tailoring the broadly-adopted MLIR compiler framework to HPC, we bring the same synergies that the machine learning community already exploits across their DSLs (e.g. Tensorflow, PyTorch) to the finite-difference stencil HPC community. We introduce new HPC-specific abstractions for message passing targeting distributed stencil computations. We demonstrate the sharing of common components across three distinct HPC stencil-DSL compilers: Devito, PSyclone, and the Open Earth Compiler, showing that our framework generates high-performance executables based upon a shared compiler ecosystem.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--317327-fluid-implicit-particle-simulation-for-cpu-and-gpu-pedro-centeno-et-al-2024>(1/1 | 317/327) Fluid Implicit Particle Simulation for CPU and GPU (Pedro Centeno et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pedro Centeno, João Madeiras Pereira. (2024)<br><strong>Fluid Implicit Particle Simulation for CPU and GPU</strong><br><button class=copy-to-clipboard title="Fluid Implicit Particle Simulation for CPU and GPU" index=317>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-317 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR, physics-comp-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01931v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01931v1.pdf filename=2404.01931v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One of the current challenges in physically-based <b>simulations,</b> and, more specifically, fluid <b>simulations,</b> is to produce visually appealing results at interactive rates, capable of being used in multiple forms of media. In recent times, a lot of effort has been made with regards to this with the use of multi-core architectures, as many of the computations involved in the algorithms for these <b>simulations</b> are very well suited for these architectures. Although there is a considerable amount of works regarding acceleration techniques in this field, there is yet room to further explore and analyze some of them. To investigate this problem, we surveyed the topic of fluid <b>simulations</b> and some of the recent contributions towards this field. Additionally, we implemented two versions of a fluid <b>simulation</b> algorithm, one on the CPU and the other on the GPU using NVIDIA&rsquo;s CUDA framework, with the intent of gaining a better understanding of the effort needed to move these <b>simulations</b> to a multi-core architecture and the performance gains that we get with it.</p></p class="citation"></blockquote><h2 id=physicsflu-dyn-1>physics.flu-dyn (1)</h2><h3 id=11--318327-hyperviscosity-stabilisation-of-the-rbf-fd-solution-to-natural-convection-žiga-vaupotič-et-al-2024>(1/1 | 318/327) Hyperviscosity stabilisation of the RBF-FD solution to natural convection (Žiga Vaupotič et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Žiga Vaupotič, Miha Rot, Gregor Kosec. (2024)<br><strong>Hyperviscosity stabilisation of the RBF-FD solution to natural convection</strong><br><button class=copy-to-clipboard title="Hyperviscosity stabilisation of the RBF-FD solution to natural convection" index=318>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-318 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-NA, math-NA, physics-comp-ph, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01919v1.pdf filename=2404.01919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The numerical stability of fluid flow is an important topic in computational fluid dynamics as fluid flow <b>simulations</b> usually become numerically unstable in the turbulent regime. Many mesh-based methods have already established numerical dissipation procedures that dampen the effects of the unstable advection term. When it comes to meshless methods, the prominent stabilisation scheme is hyperviscosity. It introduces numerical dissipation in the form of a higher-order Laplacian operator. Many papers have already discussed the general effects of hyperviscosity and its parameters. However, hyperviscosity in flow problems has not yet been analyzed in depth. In this paper, we discuss the effects of hyperviscosity on natural convection flow problems as we approach the turbulent regime.</p></p class="citation"></blockquote><h2 id=eesssp-2>eess.SP (2)</h2><h3 id=12--319327-satellite-federated-edge-learning-architecture-design-and-convergence-analysis-yuanming-shi-et-al-2024>(1/2 | 319/327) Satellite Federated Edge Learning: Architecture Design and Convergence Analysis (Yuanming Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanming Shi, Li Zeng, Jingyang Zhu, Yong Zhou, Chunxiao Jiang, Khaled B. Letaief. (2024)<br><strong>Satellite Federated Edge Learning: Architecture Design and Convergence Analysis</strong><br><button class=copy-to-clipboard title="Satellite Federated Edge Learning: Architecture Design and Convergence Analysis" index=319>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-319 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-DC, cs-IT, cs-LG, eess-SP, eess.SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01875v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01875v1.pdf filename=2404.01875v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of low-earth-orbit (LEO) satellite networks leads to the generation of vast volumes of remote sensing data which is traditionally transferred to the ground server for centralized processing, raising privacy and bandwidth concerns. Federated edge learning (FEEL), as a distributed machine learning approach, has the potential to address these challenges by sharing only model parameters instead of raw data. Although promising, the dynamics of LEO networks, characterized by the high mobility of satellites and short ground-to-satellite link (GSL) duration, pose unique challenges for FEEL. Notably, frequent model transmission between the satellites and ground incurs prolonged waiting time and large transmission latency. This paper introduces a novel FEEL algorithm, named FEDMEGA, tailored to LEO mega-constellation networks. By integrating inter-satellite links (ISL) for intra-orbit model aggregation, the proposed algorithm significantly reduces the usage of low data rate and intermittent GSL. Our proposed method includes a ring all-reduce based intra-orbit aggregation mechanism, coupled with a network flow-based transmission scheme for global model aggregation, which enhances transmission efficiency. Theoretical convergence analysis is provided to characterize the algorithm performance. Extensive <b>simulations</b> show that our FEDMEGA algorithm outperforms existing satellite FEEL algorithms, exhibiting an approximate 30% improvement in convergence rate.</p></p class="citation"></blockquote><h3 id=22--320327-intelligent-reflecting-surfaces-assisted-laser-based-optical-wireless-communication-networks-ahrar-n-hamad-et-al-2024>(2/2 | 320/327) Intelligent Reflecting Surfaces assisted Laser-based Optical Wireless Communication Networks (Ahrar N. Hamad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahrar N. Hamad, Walter Zibusiso Ncube, Ahmad Adnan Qidan, Taisir E. H. El-Gorashi, Jaafar M. H. Elmirghani. (2024)<br><strong>Intelligent Reflecting Surfaces assisted Laser-based Optical Wireless Communication Networks</strong><br><button class=copy-to-clipboard title="Intelligent Reflecting Surfaces assisted Laser-based Optical Wireless Communication Networks" index=320>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-320 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-SY, eess-SP, eess-SY, eess.SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01850v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01850v1.pdf filename=2404.01850v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing demand for wireless networks of higher capacity requires key-enabling technologies. Optical wireless communication (OWC) arises as a complementary technology to radio frequency (RF) systems that can support high aggregate data rates. However, OWC systems face some challenges including beam-blockage. Intelligent reflecting surfaces (IRSs) can offer alternative pathways for the optical signal, ensuring continuous connectivity. In this work, we investigate the potential of using IRS in an indoor OWC network. In particular, we define a system model of indoor OWC that employs IRS in conjunction with angle diversity transmitters (ADT) using vertical-cavity surface-emitting laser (VCSEL) arrays. The VCSEL beam is narrow, directed, and easy to block, however, it can deliver high data rates under eye safety regulations. <b>Simulation</b> results show that the deployment of IRS can significantly improve the achievable data rates of Laser-based OWC systems.</p></p class="citation"></blockquote><h2 id=physicsmed-ph-1>physics.med-ph (1)</h2><h3 id=11--321327-a-multicore-parallel-algorithm-for-multiscale-modelling-of-an-entire-human-blood-circulation-network-jiawei-liu-et-al-2024>(1/1 | 321/327) A multicore parallel algorithm for multiscale modelling of an entire human blood circulation network (Jiawei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Liu, Hiroshi Suito. (2024)<br><strong>A multicore parallel algorithm for multiscale modelling of an entire human blood circulation network</strong><br><button class=copy-to-clipboard title="A multicore parallel algorithm for multiscale modelling of an entire human blood circulation network" index=321>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-321 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.med-ph<br>Categories: cs-NA, math-NA, physics-med-ph, physics.med-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01680v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01680v1.pdf filename=2404.01680v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The presented multi-scale, closed-loop blood circulation model includes arterial, venous, and portal venous systems, heart-pulmonary circulation, and micro-circulation in capillaries. One-dimensional models simulate large blood vessel flow, whereas zerodimensional models are used for simulating blood flow in vascular subsystems corresponding to peripheral arteries and organs. Transmission conditions at bifurcation and confluence are solved using Riemann invariants. Blood circulation <b>simulation</b> in the portal venous system and related organs (liver, stomach, spleen, pancreas, intestine) is particularly targeted. Those organs play important roles in metabolic system dynamics. The proposed efficient parallel algorithms for multicore environments solve these equations much faster than serial computations.</p></p class="citation"></blockquote><h2 id=csfl-1>cs.FL (1)</h2><h3 id=11--322327-transformers-as-transducers-lena-strobl-et-al-2024>(1/1 | 322/327) Transformers as Transducers (Lena Strobl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lena Strobl, Dana Angluin, David Chiang, Jonathan Rawski, Ashish Sabharwal. (2024)<br><strong>Transformers as Transducers</strong><br><button class=copy-to-clipboard title="Transformers as Transducers" index=322>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-322 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.FL<br>Categories: cs-FL, cs-LG, cs.FL<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02040v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02040v1.pdf filename=2404.02040v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the sequence-to-sequence mapping capacity of <b>transformers</b> by relating them to finite transducers, and find that they can express surprisingly large classes of transductions. We do so using variants of RASP, a programming language designed to help people &ldquo;think like <b>transformers,"</b> as an intermediate representation. We extend the existing Boolean variant B-RASP to sequence-to-sequence functions and show that it computes exactly the first-order rational functions (such as string rotation). Then, we introduce two new extensions. B-RASP[pos] enables calculations on positions (such as copying the first half of a string) and contains all first-order regular functions. S-RASP adds prefix sum, which enables additional arithmetic operations (such as squaring a string) and contains all first-order polyregular functions. Finally, we show that masked average-hard attention <b>transformers</b> can simulate S-RASP. A corollary of our results is a new proof that <b>transformer</b> decoders are Turing-complete.</p></p class="citation"></blockquote><h2 id=csdb-2>cs.DB (2)</h2><h3 id=12--323327-flexis-flexible-frequent-subgraph-mining-using-maximal-independent-sets-akshit-sharma-et-al-2024>(1/2 | 323/327) FLEXIS: FLEXible Frequent Subgraph Mining using Maximal Independent Sets (Akshit Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akshit Sharma, Sam Reinher, Dinesh Mehta, Bo Wu. (2024)<br><strong>FLEXIS: FLEXible Frequent Subgraph Mining using Maximal Independent Sets</strong><br><button class=copy-to-clipboard title="FLEXIS: FLEXible Frequent Subgraph Mining using Maximal Independent Sets" index=323>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-323 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs-PF, cs.DB<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01585v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01585v1.pdf filename=2404.01585v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Frequent Subgraph Mining (FSM) is the process of identifying common subgraph patterns that surpass a predefined frequency threshold. While FSM is widely applicable in fields like bioinformatics, chemical analysis, and social network <b>anomaly</b> <b>detection,</b> its execution remains time-consuming and complex. This complexity stems from the need to recognize high-frequency subgraphs and ascertain if they exceed the set threshold. Current approaches to identifying these patterns often rely on edge or vertex extension methods. However, these strategies can introduce redundancies and cause increased latency. To address these challenges, this paper introduces a novel approach for identifying potential k-vertex patterns by combining two frequently observed (k - 1)-vertex patterns. This method optimizes the breadth-]first search, which allows for quicker search termination based on vertices count and support value. Another challenge in FSM is the validation of the presumed pattern against a specific threshold. Existing metrics, such as Maximum Independent Set (MIS) and Minimum Node Image (MNI), either demand significant computational time or risk overestimating pattern counts. Our innovative approach aligns with the MIS and identifies independent subgraphs. Through the &ldquo;Maximal Independent Set&rdquo; metric, this paper offers an efficient solution that minimizes latency and provides users with control over pattern overlap. Through extensive experimentation, our proposed method achieves an average of 10.58x speedup when compared to GraMi and an average 3x speedup when compared to T-FSM</p></p class="citation"></blockquote><h3 id=22--324327-heterogeneous-data-access-model-for-concurrency-control-and-methods-to-deal-with-high-data-contention-alexander-thomasian-2024>(2/2 | 324/327) Heterogeneous Data Access Model for Concurrency Control and Methods to Deal with High Data Contention (Alexander Thomasian, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Thomasian. (2024)<br><strong>Heterogeneous Data Access Model for Concurrency Control and Methods to Deal with High Data Contention</strong><br><button class=copy-to-clipboard title="Heterogeneous Data Access Model for Concurrency Control and Methods to Deal with High Data Contention" index=324>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-324 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs-PF, cs.DB<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02276v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02276v1.pdf filename=2404.02276v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>OLTP has stringent performance requirements defined by Service Level Agreements. Transaction response time is used to determine the maximum throughout in <b>benchmarks.</b> Capacity planning tools for OLTP performance are based on queueing network models for hardware resources and database lock contention has a secondary effect on performance. With ever increasing levels of e-commerce and surges in OLTP traffic we discuss the need for studies of database workloads to develop more realistic lock/latch contention models. Predictive formulas to model increased load leading to thrashing for txns with identical and nonidentical steps are presented. We review concurrency control methods to reduce the level of lock/data conflicts in high contention environments.</p></p class="citation"></blockquote><h2 id=mathac-1>math.AC (1)</h2><h3 id=11--325327-the-edge-code-of-hypergraphs-delio-jaramillo-velez-2024>(1/1 | 325/327) The edge code of hypergraphs (Delio Jaramillo-Velez, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Delio Jaramillo-Velez. (2024)<br><strong>The edge code of hypergraphs</strong><br><button class=copy-to-clipboard title="The edge code of hypergraphs" index=325>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-325 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.AC<br>Categories: Primary 13P25, Secondary 05C65, 14G50, 94B27, cs-IT, math-AC, math-CO, math-IT, math.AC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02301v1.pdf filename=2404.02301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a hypergraph $\mathcal{H}$, we introduce a new class of evaluation toric codes called edge codes derived from $\mathcal{H}$. We analyze these codes, focusing on determining their basic parameters. We provide estimations for the minimum distance, particularly in scenarios involving $d$-uniform clutters. Additionally, we demonstrate that these codes exhibit self-orthogonality. Furthermore, we compute the minimum distances of edge codes for all <b>graphs</b> with five vertices.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--326327-generalized-saturation-game-balázs-patkós-et-al-2024>(1/1 | 326/327) Generalized saturation game (Balázs Patkós et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Balázs Patkós, Miloš Stojaković, Jelena Stratijev, Máté Vizer. (2024)<br><strong>Generalized saturation game</strong><br><button class=copy-to-clipboard title="Generalized saturation game" index=326>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-326 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02288v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02288v1.pdf filename=2404.02288v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the following game version of the generalized <b>graph</b> Tur'an problem. For two fixed <b>graphs</b> F and H, two players, Max and Mini, alternately claim unclaimed edges of the complete <b>graph</b> Kn such that the <b>graph</b> G of the claimed edges must remain F-free throughout the game. The game ends when no further edges can be claimed, i.e. when G becomes F-saturated. The H-score of the game is the number of copies of H in G. Max aims to maximize the H-score, while Mini wants to minimize it. The H-score of the game when both players play optimally is denoted by s1(n, #H, F) when Max starts, and by s2(n, #H, F) when Mini starts. We study these values for several natural choices of F and H.</p></p class="citation"></blockquote><h2 id=csdm-1>cs.DM (1)</h2><h3 id=11--327327-a-temporal-graph-model-to-study-the-dynamics-of-collective-behavior-and-performance-in-team-sports-an-application-to-basketball-quentin-bourgeais-et-al-2024>(1/1 | 327/327) A Temporal Graph Model to Study the Dynamics of Collective Behavior and Performance in Team Sports: An Application to Basketball (Quentin Bourgeais et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quentin Bourgeais, Eric Sanlaville, Rodolphe Charrier, Ludovic Seifert. (2024)<br><strong>A Temporal Graph Model to Study the Dynamics of Collective Behavior and Performance in Team Sports: An Application to Basketball</strong><br><button class=copy-to-clipboard title="A Temporal Graph Model to Study the Dynamics of Collective Behavior and Performance in Team Sports: An Application to Basketball" index=327>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-327 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs.DM<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01909v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01909v1.pdf filename=2404.01909v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, a temporal <b>graph</b> model is designed to model the behavior of collective sports teams based on the networks of player interactions. The main motivation for the model is to integrate the temporal dimension into the analysis of players&rsquo; passing networks in order to gain deeper insights into the dynamics of system behavior, particularly how a system exploits the degeneracy property to self-regulate. First, the temporal <b>graph</b> model and the entropy measures used to assess the complexity of the dynamics of the network structure are introduced and illustrated. Second, an experiment using basketball data is conducted to investigate the relationship between the complexity level and team performance. This is accomplished by examining the correlations between the entropy measures in a team&rsquo;s behavior and the team&rsquo;s final performance, as well as the link between the relative score compared to that of the opponent and the entropy in the team&rsquo;s behavior. Results indicate positive correlations between entropy measures and final team performance, and threshold values of relative score associated with changes in team behavior &ndash; thereby revealing common and unique team signatures. From a complexity science perspective, the model proves useful for identifying key performance factors in team sports and for studying the effects of given constraints on the exploitation of degeneracy to organize team behavior through various network structures. Future research can easily extend the model and apply it to other types of social networks.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.04.03</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.04.05</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-69>cs.CL (69)</a><ul><li><a href=#169--1327-deconstructing-in-context-learning-understanding-prompts-via-corruption-namrata-shivagunde-et-al-2024>(1/69 | 1/327) Deconstructing In-Context Learning: Understanding Prompts via Corruption (Namrata Shivagunde et al., 2024)</a></li><li><a href=#269--2327-toward-informal-language-processing-knowledge-of-slang-in-large-language-models-zhewei-sun-et-al-2024>(2/69 | 2/327) Toward Informal Language Processing: Knowledge of Slang in Large Language Models (Zhewei Sun et al., 2024)</a></li><li><a href=#369--3327-comparative-study-of-domain-driven-terms-extraction-using-large-language-models-sandeep-chataut-et-al-2024>(3/69 | 3/327) Comparative Study of Domain Driven Terms Extraction Using Large Language Models (Sandeep Chataut et al., 2024)</a></li><li><a href=#469--4327-sgsh-stimulate-large-language-models-with-skeleton-heuristics-for-knowledge-base-question-generation-shasha-guo-et-al-2024>(4/69 | 4/327) SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation (Shasha Guo et al., 2024)</a></li><li><a href=#569--5327-class-incremental-few-shot-event-detection-kailin-zhao-et-al-2024>(5/69 | 5/327) Class-Incremental Few-Shot Event Detection (Kailin Zhao et al., 2024)</a></li><li><a href=#669--6327-self-improvement-programming-for-temporal-knowledge-graph-question-answering-zhuo-chen-et-al-2024>(6/69 | 6/327) Self-Improvement Programming for Temporal Knowledge Graph Question Answering (Zhuo Chen et al., 2024)</a></li><li><a href=#769--7327-metal-towards-multilingual-meta-evaluation-rishav-hada-et-al-2024>(7/69 | 7/327) METAL: Towards Multilingual Meta-Evaluation (Rishav Hada et al., 2024)</a></li><li><a href=#869--8327-improving-retrieval-augmented-open-domain-question-answering-with-vectorized-contexts-zhuo-chen-et-al-2024>(8/69 | 8/327) Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts (Zhuo Chen et al., 2024)</a></li><li><a href=#969--9327-a-rationale-centric-counterfactual-data-augmentation-method-for-cross-document-event-coreference-resolution-bowen-ding-et-al-2024>(9/69 | 9/327) A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document Event Coreference Resolution (Bowen Ding et al., 2024)</a></li><li><a href=#1069--10327-long-context-llms-struggle-with-long-in-context-learning-tianle-li-et-al-2024>(10/69 | 10/327) Long-context LLMs Struggle with Long In-context Learning (Tianle Li et al., 2024)</a></li><li><a href=#1169--11327-llms-in-the-loop-leveraging-large-language-model-annotations-for-active-learning-in-low-resource-languages-nataliia-kholodna-et-al-2024>(11/69 | 11/327) LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages (Nataliia Kholodna et al., 2024)</a></li><li><a href=#1269--12327-emergent-abilities-in-reduced-scale-generative-language-models-sherin-muckatira-et-al-2024>(12/69 | 12/327) Emergent Abilities in Reduced-Scale Generative Language Models (Sherin Muckatira et al., 2024)</a></li><li><a href=#1369--13327-hyperclova-x-technical-report-kang-min-yoo-et-al-2024>(13/69 | 13/327) HyperCLOVA X Technical Report (Kang Min Yoo et al., 2024)</a></li><li><a href=#1469--14327-octopus-on-device-language-model-for-function-calling-of-software-apis-wei-chen-et-al-2024>(14/69 | 14/327) Octopus: On-device language model for function calling of software APIs (Wei Chen et al., 2024)</a></li><li><a href=#1569--15327-patch----psychometrics-assisted-benchmarking-of-large-language-models-a-case-study-of-mathematics-proficiency-qixiang-fang-et-al-2024>(15/69 | 15/327) PATCH &ndash; Psychometrics-AssisTed benCHmarking of Large Language Models: A Case Study of Mathematics Proficiency (Qixiang Fang et al., 2024)</a></li><li><a href=#1669--16327-auditing-large-language-models-for-enhanced-text-based-stereotype-detection-and-probing-based-bias-evaluation-zekun-wu-et-al-2024>(16/69 | 16/327) Auditing Large Language Models for Enhanced Text-Based Stereotype Detection and Probing-Based Bias Evaluation (Zekun Wu et al., 2024)</a></li><li><a href=#1769--17327-hallucination-diversity-aware-active-learning-for-text-summarization-yu-xia-et-al-2024>(17/69 | 17/327) Hallucination Diversity-Aware Active Learning for Text Summarization (Yu Xia et al., 2024)</a></li><li><a href=#1869--18327-clapnq-cohesive-long-form-answers-from-passages-in-natural-questions-for-rag-systems-sara-rosenthal-et-al-2024>(18/69 | 18/327) CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems (Sara Rosenthal et al., 2024)</a></li><li><a href=#1969--19327-using-large-language-models-to-understand-telecom-standards-athanasios-karapantelakis-et-al-2024>(19/69 | 19/327) Using Large Language Models to Understand Telecom Standards (Athanasios Karapantelakis et al., 2024)</a></li><li><a href=#2069--20327-prompts-as-programs-a-structure-aware-approach-to-efficient-compile-time-prompt-optimization-tobias-schnabel-et-al-2024>(20/69 | 20/327) Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization (Tobias Schnabel et al., 2024)</a></li><li><a href=#2169--21327-exploring-automated-distractor-generation-for-math-multiple-choice-questions-via-large-language-models-wanyong-feng-et-al-2024>(21/69 | 21/327) Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models (Wanyong Feng et al., 2024)</a></li><li><a href=#2269--22327-multiparadetox-extending-text-detoxification-with-parallel-data-to-new-languages-daryna-dementieva-et-al-2024>(22/69 | 22/327) MultiParaDetox: Extending Text Detoxification with Parallel Data to New Languages (Daryna Dementieva et al., 2024)</a></li><li><a href=#2369--23327-team-utsa-nlp-at-semeval-2024-task-5-prompt-ensembling-for-argument-reasoning-in-civil-procedures-with-gpt4-dan-schumacher-et-al-2024>(23/69 | 23/327) Team UTSA-NLP at SemEval 2024 Task 5: Prompt Ensembling for Argument Reasoning in Civil Procedures with GPT4 (Dan Schumacher et al., 2024)</a></li><li><a href=#2469--24327-on-the-role-of-summary-content-units-in-text-summarization-evaluation-marcel-nawrath-et-al-2024>(24/69 | 24/327) On the Role of Summary Content Units in Text Summarization Evaluation (Marcel Nawrath et al., 2024)</a></li><li><a href=#2569--25327-cmat-a-multi-agent-collaboration-tuning-framework-for-enhancing-small-language-models-xuechen-liang-et-al-2024>(25/69 | 25/327) CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models (Xuechen Liang et al., 2024)</a></li><li><a href=#2669--26327-helmsman-of-the-masses-evaluate-the-opinion-leadership-of-large-language-models-in-the-werewolf-game-silin-du-et-al-2024>(26/69 | 26/327) Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game (Silin Du et al., 2024)</a></li><li><a href=#2769--27327-classifying-cancer-stage-with-open-source-clinical-large-language-models-chia-hsuan-chang-et-al-2024>(27/69 | 27/327) Classifying Cancer Stage with Open-Source Clinical Large Language Models (Chia-Hsuan Chang et al., 2024)</a></li><li><a href=#2869--28327-scanner-knowledge-enhanced-approach-for-robust-multi-modal-named-entity-recognition-of-unseen-entities-hyunjong-ok-et-al-2024>(28/69 | 28/327) SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities (Hyunjong Ok et al., 2024)</a></li><li><a href=#2969--29327-humanizing-machine-generated-content-evading-ai-text-detection-through-adversarial-attack-ying-zhou-et-al-2024>(29/69 | 29/327) Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack (Ying Zhou et al., 2024)</a></li><li><a href=#3069--30327-read-improving-relation-extraction-from-an-adversarial-perspective-dawei-li-et-al-2024>(30/69 | 30/327) READ: Improving Relation Extraction from an ADversarial Perspective (Dawei Li et al., 2024)</a></li><li><a href=#3169--31327-transforming-llms-into-cross-modal-and-cross-lingual-retrieval-systems-frank-palma-gomez-et-al-2024>(31/69 | 31/327) Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems (Frank Palma Gomez et al., 2024)</a></li><li><a href=#3269--32327-two-heads-are-better-than-one-nested-poe-for-robust-defense-against-multi-backdoors-victoria-graf-et-al-2024>(32/69 | 32/327) Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors (Victoria Graf et al., 2024)</a></li><li><a href=#3369--33327-ukrainian-texts-classification-exploration-of-cross-lingual-knowledge-transfer-approaches-daryna-dementieva-et-al-2024>(33/69 | 33/327) Ukrainian Texts Classification: Exploration of Cross-lingual Knowledge Transfer Approaches (Daryna Dementieva et al., 2024)</a></li><li><a href=#3469--34327-towards-better-understanding-of-cybercrime-the-role-of-fine-tuned-llms-in-translation-veronica-valeros-et-al-2024>(34/69 | 34/327) Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation (Veronica Valeros et al., 2024)</a></li><li><a href=#3569--35327-indoculture-exploring-geographically-influenced-cultural-commonsense-reasoning-across-eleven-indonesian-provinces-fajri-koto-et-al-2024>(35/69 | 35/327) IndoCulture: Exploring Geographically-Influenced Cultural Commonsense Reasoning Across Eleven Indonesian Provinces (Fajri Koto et al., 2024)</a></li><li><a href=#3669--36327-generative-ai-based-text-generation-methods-using-pre-trained-gpt-2-model-rohit-pandey-et-al-2024>(36/69 | 36/327) Generative AI-Based Text Generation Methods Using Pre-Trained GPT-2 Model (Rohit Pandey et al., 2024)</a></li><li><a href=#3769--37327-octopus-v2-on-device-language-model-for-super-agent-wei-chen-et-al-2024>(37/69 | 37/327) Octopus v2: On-device language model for super agent (Wei Chen et al., 2024)</a></li><li><a href=#3869--38327-evaluating-large-language-models-using-contrast-sets-an-experimental-approach-manish-sanwal-2024>(38/69 | 38/327) Evaluating Large Language Models Using Contrast Sets: An Experimental Approach (Manish Sanwal, 2024)</a></li><li><a href=#3969--39327-ginopic-topic-modeling-with-graph-isomorphism-network-suman-adhya-et-al-2024>(39/69 | 39/327) GINopic: Topic Modeling with Graph Isomorphism Network (Suman Adhya et al., 2024)</a></li><li><a href=#4069--40327-africa-centric-self-supervised-pre-training-for-multilingual-speech-representation-in-a-sub-saharan-context-antoine-caubrière-et-al-2024>(40/69 | 40/327) Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context (Antoine Caubrière et al., 2024)</a></li><li><a href=#4169--41327-towards-better-generalization-in-open-domain-question-answering-by-mitigating-context-memorization-zixuan-zhang-et-al-2024>(41/69 | 41/327) Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization (Zixuan Zhang et al., 2024)</a></li><li><a href=#4269--42327-rematch-robust-and-efficient-matching-of-local-knowledge-graphs-to-improve-structural-and-semantic-similarity-zoher-kachwala-et-al-2024>(42/69 | 42/327) Rematch: Robust and Efficient Matching of Local Knowledge Graphs to Improve Structural and Semantic Similarity (Zoher Kachwala et al., 2024)</a></li><li><a href=#4369--43327-multi-bert-leveraging-adapters-and-prompt-tuning-for-low-resource-multi-domain-adaptation-parham-abed-azad-et-al-2024>(43/69 | 43/327) Multi-BERT: Leveraging Adapters and Prompt Tuning for Low-Resource Multi-Domain Adaptation (Parham Abed Azad et al., 2024)</a></li><li><a href=#4469--44327-textttlmtexttt2-a-simple-society-of-language-models-solves-complex-reasoning-gurusha-juneja-et-al-2024>(44/69 | 44/327) $\texttt{LM}^\texttt{2}$: A Simple Society of Language Models Solves Complex Reasoning (Gurusha Juneja et al., 2024)</a></li><li><a href=#4569--45327-flawn-t5-an-empirical-examination-of-effective-instruction-tuning-data-mixtures-for-legal-reasoning-joel-niklaus-et-al-2024>(45/69 | 45/327) FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning (Joel Niklaus et al., 2024)</a></li><li><a href=#4669--46327-breaking-the-silence-detecting-and-mitigating-gendered-abuse-in-hindi-tamil-and-indian-english-online-spaces-advaitha-vetagiri-et-al-2024>(46/69 | 46/327) Breaking the Silence Detecting and Mitigating Gendered Abuse in Hindi, Tamil, and Indian English Online Spaces (Advaitha Vetagiri et al., 2024)</a></li><li><a href=#4769--47327-dissecting-paraphrases-the-impact-of-prompt-syntax-and-supplementary-information-on-knowledge-retrieval-from-pretrained-language-models-stephan-linzbach-et-al-2024>(47/69 | 47/327) Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information on Knowledge Retrieval from Pretrained Language Models (Stephan Linzbach et al., 2024)</a></li><li><a href=#4869--48327-beyond-accuracy-evaluating-the-reasoning-behavior-of-large-language-models----a-survey-philipp-mondorf-et-al-2024>(48/69 | 48/327) Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models &ndash; A Survey (Philipp Mondorf et al., 2024)</a></li><li><a href=#4969--49327-m2sa-multimodal-and-multilingual-model-for-sentiment-analysis-of-tweets-gaurish-thakkar-et-al-2024>(49/69 | 49/327) M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets (Gaurish Thakkar et al., 2024)</a></li><li><a href=#5069--50327-a-computational-analysis-of-lyric-similarity-perception-haven-kim-et-al-2024>(50/69 | 50/327) A Computational Analysis of Lyric Similarity Perception (Haven Kim et al., 2024)</a></li><li><a href=#5169--51327-collapse-of-self-trained-language-models-david-herel-et-al-2024>(51/69 | 51/327) Collapse of Self-trained Language Models (David Herel et al., 2024)</a></li><li><a href=#5269--52327-extracting-norms-from-contracts-via-chatgpt-opportunities-and-challenges-amanul-haque-et-al-2024>(52/69 | 52/327) Extracting Norms from Contracts Via ChatGPT: Opportunities and Challenges (Amanul Haque et al., 2024)</a></li><li><a href=#5369--53327-kallaama-a-transcribed-speech-dataset-about-agriculture-in-the-three-most-widely-spoken-languages-in-senegal-elodie-gauthier-et-al-2024>(53/69 | 53/327) Kallaama: A Transcribed Speech Dataset about Agriculture in the Three Most Widely Spoken Languages in Senegal (Elodie Gauthier et al., 2024)</a></li><li><a href=#5469--54327-polarity-calibration-for-opinion-summarization-yuanyuan-lei-et-al-2024>(54/69 | 54/327) Polarity Calibration for Opinion Summarization (Yuanyuan Lei et al., 2024)</a></li><li><a href=#5569--55327-release-of-pre-trained-models-for-the-japanese-language-kei-sawada-et-al-2024>(55/69 | 55/327) Release of Pre-Trained Models for the Japanese Language (Kei Sawada et al., 2024)</a></li><li><a href=#5669--56327-nlp-systems-that-cant-tell-use-from-mention-censor-counterspeech-but-teaching-the-distinction-helps-kristina-gligoric-et-al-2024>(56/69 | 56/327) NLP Systems That Can&rsquo;t Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps (Kristina Gligoric et al., 2024)</a></li><li><a href=#5769--57327-laying-anchors-semantically-priming-numerals-in-language-modeling-mandar-sharma-et-al-2024>(57/69 | 57/327) Laying Anchors: Semantically Priming Numerals in Language Modeling (Mandar Sharma et al., 2024)</a></li><li><a href=#5869--58327-sentence-level-media-bias-analysis-with-event-relation-graph-yuanyuan-lei-et-al-2024>(58/69 | 58/327) Sentence-level Media Bias Analysis with Event Relation Graph (Yuanyuan Lei et al., 2024)</a></li><li><a href=#5969--59327-entity-disambiguation-via-fusion-entity-decoding-junxiong-wang-et-al-2024>(59/69 | 59/327) Entity Disambiguation via Fusion Entity Decoding (Junxiong Wang et al., 2024)</a></li><li><a href=#6069--60327-using-interpretation-methods-for-model-enhancement-zhuo-chen-et-al-2024>(60/69 | 60/327) Using Interpretation Methods for Model Enhancement (Zhuo Chen et al., 2024)</a></li><li><a href=#6169--61327-bertopic-driven-stock-market-predictions-unraveling-sentiment-insights-enmin-zhu-et-al-2024>(61/69 | 61/327) BERTopic-Driven Stock Market Predictions: Unraveling Sentiment Insights (Enmin Zhu et al., 2024)</a></li><li><a href=#6269--62327-preuve-de-concept-dun-bot-vocal-dialoguant-en-wolof-elodie-gauthier-et-al-2024>(62/69 | 62/327) Preuve de concept d&rsquo;un bot vocal dialoguant en wolof (Elodie Gauthier et al., 2024)</a></li><li><a href=#6369--63327-when-abel-kills-cain-what-machine-translation-cannot-capture-aurélien-bénel-et-al-2024>(63/69 | 63/327) When Abel Kills Cain: What Machine Translation Cannot Capture (Aurélien Bénel et al., 2024)</a></li><li><a href=#6469--64327-activation-steering-for-robust-type-prediction-in-codellms-francesca-lucchetti-et-al-2024>(64/69 | 64/327) Activation Steering for Robust Type Prediction in CodeLLMs (Francesca Lucchetti et al., 2024)</a></li><li><a href=#6569--65327-self-strae-at-semeval-2024-task-1-making-self-structuring-autoencoders-learn-more-with-less-mattia-opper-et-al-2024>(65/69 | 65/327) Self-StrAE at SemEval-2024 Task 1: Making Self-Structuring AutoEncoders Learn More With Less (Mattia Opper et al., 2024)</a></li><li><a href=#6669--66327-poro-34b-and-the-blessing-of-multilinguality-risto-luukkonen-et-al-2024>(66/69 | 66/327) Poro 34B and the Blessing of Multilinguality (Risto Luukkonen et al., 2024)</a></li><li><a href=#6769--67327-generative-ai-for-immersive-communication-the-next-frontier-in-internet-of-senses-through-6g-nassim-sehad-et-al-2024>(67/69 | 67/327) Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G (Nassim Sehad et al., 2024)</a></li><li><a href=#6869--68327-event-detection-from-social-media-for-epidemic-prediction-tanmay-parekh-et-al-2024>(68/69 | 68/327) Event Detection from Social Media for Epidemic Prediction (Tanmay Parekh et al., 2024)</a></li><li><a href=#6969--69327-lastresort-at-semeval-2024-task-3-exploring-multimodal-emotion-cause-pair-extraction-as-sequence-labelling-task-suyash-vardhan-mathur-et-al-2024>(69/69 | 69/327) LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task (Suyash Vardhan Mathur et al., 2024)</a></li></ul></li><li><a href=#csai-4>cs.AI (4)</a><ul><li><a href=#14--70327-advancing-llm-reasoning-generalists-with-preference-trees-lifan-yuan-et-al-2024>(1/4 | 70/327) Advancing LLM Reasoning Generalists with Preference Trees (Lifan Yuan et al., 2024)</a></li><li><a href=#24--71327-a-survey-on-large-language-model-based-game-agents-sihao-hu-et-al-2024>(2/4 | 71/327) A Survey on Large Language Model-Based Game Agents (Sihao Hu et al., 2024)</a></li><li><a href=#34--72327-towards-generalizable-and-faithful-logic-reasoning-over-natural-language-via-resolution-refutation-zhouhao-sun-et-al-2024>(3/4 | 72/327) Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation (Zhouhao Sun et al., 2024)</a></li><li><a href=#44--73327-imitation-game-a-model-based-and-imitation-learning-deep-reinforcement-learning-hybrid-eric-msp-veith-et-al-2024>(4/4 | 73/327) Imitation Game: A Model-based and Imitation Learning Deep Reinforcement Learning Hybrid (Eric MSP Veith et al., 2024)</a></li></ul></li><li><a href=#cscr-10>cs.CR (10)</a><ul><li><a href=#110--74327-jailbreaking-leading-safety-aligned-llms-with-simple-adaptive-attacks-maksym-andriushchenko-et-al-2024>(1/10 | 74/327) Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks (Maksym Andriushchenko et al., 2024)</a></li><li><a href=#210--75327-great-now-write-an-article-about-that-the-crescendo-multi-turn-llm-jailbreak-attack-mark-russinovich-et-al-2024>(2/10 | 75/327) Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack (Mark Russinovich et al., 2024)</a></li><li><a href=#310--76327-jailbreaking-prompt-attack-a-controllable-adversarial-attack-against-diffusion-models-jiachen-ma-et-al-2024>(3/10 | 76/327) Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models (Jiachen Ma et al., 2024)</a></li><li><a href=#410--77327-topic-based-watermarks-for-llm-generated-text-alexander-nemecek-et-al-2024>(4/10 | 77/327) Topic-based Watermarks for LLM-Generated Text (Alexander Nemecek et al., 2024)</a></li><li><a href=#510--78327-digital-forgetting-in-large-language-models-a-survey-of-unlearning-methods-alberto-blanco-justicia-et-al-2024>(5/10 | 78/327) Digital Forgetting in Large Language Models: A Survey of Unlearning Methods (Alberto Blanco-Justicia et al., 2024)</a></li><li><a href=#610--79327-aaa-an-adaptive-mechanism-for-locally-differential-private-mean-estimation-fei-wei-et-al-2024>(6/10 | 79/327) AAA: an Adaptive Mechanism for Locally Differential Private Mean Estimation (Fei Wei et al., 2024)</a></li><li><a href=#710--80327-software-defined-cryptography-a-design-feature-of-cryptographic-agility-jihoon-cho-et-al-2024>(7/10 | 80/327) Software-Defined Cryptography: A Design Feature of Cryptographic Agility (Jihoon Cho et al., 2024)</a></li><li><a href=#810--81327-haina-storage-a-decentralized-secure-storage-framework-based-on-improved-blockchain-structure-zijian-zhou-et-al-2024>(8/10 | 81/327) Haina Storage: A Decentralized Secure Storage Framework Based on Improved Blockchain Structure (Zijian Zhou et al., 2024)</a></li><li><a href=#910--82327-what-blocks-my-blockchains-throughput-developing-a-generalizable-approach-for-identifying-bottlenecks-in-permissioned-blockchains-orestis-papageorgiou-et-al-2024>(9/10 | 82/327) What Blocks My Blockchain&rsquo;s Throughput? Developing a Generalizable Approach for Identifying Bottlenecks in Permissioned Blockchains (Orestis Papageorgiou et al., 2024)</a></li><li><a href=#1010--83327-making-privacy-preserving-federated-graph-analytics-with-strong-guarantees-practical-for-certain-queries-kunlong-liu-et-al-2024>(10/10 | 83/327) Making Privacy-preserving Federated Graph Analytics with Strong Guarantees Practical (for Certain Queries) (Kunlong Liu et al., 2024)</a></li></ul></li><li><a href=#csro-19>cs.RO (19)</a><ul><li><a href=#119--84327-bridging-language-vision-and-action-multimodal-vaes-in-robotic-manipulation-tasks-gabriela-sejnova-et-al-2024>(1/19 | 84/327) Bridging Language, Vision and Action: Multimodal VAEs in Robotic Manipulation Tasks (Gabriela Sejnova et al., 2024)</a></li><li><a href=#219--85327-large-language-models-for-orchestrating-bimanual-robots-kun-chu-et-al-2024>(2/19 | 85/327) Large Language Models for Orchestrating Bimanual Robots (Kun Chu et al., 2024)</a></li><li><a href=#319--86327-active-exploration-in-bayesian-model-based-reinforcement-learning-for-robot-manipulation-carlos-plou-et-al-2024>(3/19 | 86/327) Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation (Carlos Plou et al., 2024)</a></li><li><a href=#419--87327-task-priority-intermediated-hierarchical-distributed-policies-reinforcement-learning-of-adaptive-multi-robot-cooperative-transport-yusei-naito-et-al-2024>(4/19 | 87/327) Task-priority Intermediated Hierarchical Distributed Policies: Reinforcement Learning of Adaptive Multi-robot Cooperative Transport (Yusei Naito et al., 2024)</a></li><li><a href=#519--88327-zerocap-zero-shot-multi-robot-context-aware-pattern-formation-via-large-language-models-vishnunandan-l-n-venkatesh-et-al-2024>(5/19 | 88/327) ZeroCAP: Zero-Shot Multi-Robot Context Aware Pattern Formation via Large Language Models (Vishnunandan L. N. Venkatesh et al., 2024)</a></li><li><a href=#619--89327-constrained-robotic-navigation-on-preferred-terrains-using-llms-and-speech-instruction-exploiting-the-power-of-adverbs-faraz-lotfi-et-al-2024>(6/19 | 89/327) Constrained Robotic Navigation on Preferred Terrains Using LLMs and Speech Instruction: Exploiting the Power of Adverbs (Faraz Lotfi et al., 2024)</a></li><li><a href=#719--90327-continuous-sculpting-persistent-swarm-shape-formation-adaptable-to-local-environmental-changes-andrew-g-curtis-et-al-2024>(7/19 | 90/327) Continuous Sculpting: Persistent Swarm Shape Formation Adaptable to Local Environmental Changes (Andrew G. Curtis et al., 2024)</a></li><li><a href=#819--91327-carlos-an-open-modular-and-scalable-simulation-framework-for-the-development-and-testing-of-software-for-c-its-christian-geller-et-al-2024>(8/19 | 91/327) CARLOS: An Open, Modular, and Scalable Simulation Framework for the Development and Testing of Software for C-ITS (Christian Geller et al., 2024)</a></li><li><a href=#919--92327-towards-scalable--efficient-interaction-aware-planning-in-autonomous-vehicles-using-knowledge-distillation-piyush-gupta-et-al-2024>(9/19 | 92/327) Towards Scalable & Efficient Interaction-Aware Planning in Autonomous Vehicles using Knowledge Distillation (Piyush Gupta et al., 2024)</a></li><li><a href=#1019--93327-interaction-aware-vehicle-motion-planning-with-collision-avoidance-constraints-in-highway-traffic-dongryul-kim-et-al-2024>(10/19 | 93/327) Interaction-Aware Vehicle Motion Planning with Collision Avoidance Constraints in Highway Traffic (Dongryul Kim et al., 2024)</a></li><li><a href=#1119--94327-uncertainty-aware-active-learning-of-nerf-based-object-models-for-robot-manipulators-using-visual-and-re-orientation-actions-saptarshi-dasgupta-et-al-2024>(11/19 | 94/327) Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions (Saptarshi Dasgupta et al., 2024)</a></li><li><a href=#1219--95327-multi-robot-collaborative-navigation-with-formation-adaptation-zihao-deng-et-al-2024>(12/19 | 95/327) Multi-Robot Collaborative Navigation with Formation Adaptation (Zihao Deng et al., 2024)</a></li><li><a href=#1319--96327-learning-from-demonstration-framework-for-multi-robot-systems-using-interaction-keypoints-and-soft-actor-critic-methods-vishnunandan-l-n-venkatesh-et-al-2024>(13/19 | 96/327) Learning from Demonstration Framework for Multi-Robot Systems Using Interaction Keypoints and Soft Actor-Critic Methods (Vishnunandan L. N. Venkatesh et al., 2024)</a></li><li><a href=#1419--97327-federated-multi-agent-mapping-for-planetary-exploration-tiberiu-ioan-szatmari-et-al-2024>(14/19 | 97/327) Federated Multi-Agent Mapping for Planetary Exploration (Tiberiu-Ioan Szatmari et al., 2024)</a></li><li><a href=#1519--98327-apex-ambidextrous-dual-arm-robotic-manipulation-using-collision-free-generative-diffusion-models-apan-dastider-et-al-2024>(15/19 | 98/327) APEX: Ambidextrous Dual-Arm Robotic Manipulation Using Collision-Free Generative Diffusion Models (Apan Dastider et al., 2024)</a></li><li><a href=#1619--99327-predicting-the-intention-to-interact-with-a-service-robotthe-role-of-gaze-cues-simone-arreghini-et-al-2024>(16/19 | 99/327) Predicting the Intention to Interact with a Service Robot:the Role of Gaze Cues (Simone Arreghini et al., 2024)</a></li><li><a href=#1719--100327-a-tutorial-on-gaussian-process-learning-based-model-predictive-control-jie-wang-et-al-2024>(17/19 | 100/327) A Tutorial on Gaussian Process Learning-based Model Predictive Control (Jie Wang et al., 2024)</a></li><li><a href=#1819--101327-prism-topomap-online-topological-mapping-with-place-recognition-and-scan-matching-kirill-muravyev-et-al-2024>(18/19 | 101/327) PRISM-TopoMap: Online Topological Mapping with Place Recognition and Scan Matching (Kirill Muravyev et al., 2024)</a></li><li><a href=#1919--102327-generalizing-6-dof-grasp-detection-via-domain-prior-knowledge-haoxiang-ma-et-al-2024>(19/19 | 102/327) Generalizing 6-DoF Grasp Detection via Domain Prior Knowledge (Haoxiang Ma et al., 2024)</a></li></ul></li><li><a href=#cscv-82>cs.CV (82)</a><ul><li><a href=#182--103327-samba-semantic-segmentation-of-remotely-sensed-images-with-state-space-model-qinfeng-zhu-et-al-2024>(1/82 | 103/327) Samba: Semantic Segmentation of Remotely Sensed Images with State Space Model (Qinfeng Zhu et al., 2024)</a></li><li><a href=#282--104327-pre-trained-vision-and-language-transformers-are-few-shot-incremental-learners-keon-hee-park-et-al-2024>(2/82 | 104/327) Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners (Keon-Hee Park et al., 2024)</a></li><li><a href=#382--105327-minimize-quantization-output-error-with-bias-compensation-cheng-gong-et-al-2024>(3/82 | 105/327) Minimize Quantization Output Error with Bias Compensation (Cheng Gong et al., 2024)</a></li><li><a href=#482--106327-leveraging-digital-perceptual-technologies-for-remote-perception-and-analysis-of-human-biomechanical-processes-a-contactless-approach-for-workload-and-joint-force-assessment-jesudara-omidokun-et-al-2024>(4/82 | 106/327) Leveraging Digital Perceptual Technologies for Remote Perception and Analysis of Human Biomechanical Processes: A Contactless Approach for Workload and Joint Force Assessment (Jesudara Omidokun et al., 2024)</a></li><li><a href=#582--107327-vitamin-designing-scalable-vision-models-in-the-vision-language-era-jieneng-chen-et-al-2024>(5/82 | 107/327) ViTamin: Designing Scalable Vision Models in the Vision-Language Era (Jieneng Chen et al., 2024)</a></li><li><a href=#682--108327-bi-lora-a-vision-language-approach-for-synthetic-image-detection-mamadou-keita-et-al-2024>(6/82 | 108/327) Bi-LORA: A Vision-Language Approach for Synthetic Image Detection (Mamadou Keita et al., 2024)</a></li><li><a href=#782--109327-wcdt-world-centric-diffusion-transformer-for-traffic-scene-generation-chen-yang-et-al-2024>(7/82 | 109/327) WcDT: World-centric Diffusion Transformer for Traffic Scene Generation (Chen Yang et al., 2024)</a></li><li><a href=#882--110327-red-teaming-segment-anything-model-krzysztof-jankowski-et-al-2024>(8/82 | 110/327) Red-Teaming Segment Anything Model (Krzysztof Jankowski et al., 2024)</a></li><li><a href=#982--111327-semantic-augmentation-in-images-using-language-sahiti-yerramilli-et-al-2024>(9/82 | 111/327) Semantic Augmentation in Images using Language (Sahiti Yerramilli et al., 2024)</a></li><li><a href=#1082--112327-rave-residual-vector-embedding-for-clip-guided-backlit-image-enhancement-tatiana-gaintseva-et-al-2024>(10/82 | 112/327) RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image Enhancement (Tatiana Gaintseva et al., 2024)</a></li><li><a href=#1182--113327-leveraging-yolo-world-and-gpt-4v-lmms-for-zero-shot-person-detection-and-action-recognition-in-drone-imagery-christian-limberg-et-al-2024>(11/82 | 113/327) Leveraging YOLO-World and GPT-4V LMMs for Zero-Shot Person Detection and Action Recognition in Drone Imagery (Christian Limberg et al., 2024)</a></li><li><a href=#1282--114327-lp-a-surprisingly-strong-linear-probe-for-few-shot-clip-yunshi-huang-et-al-2024>(12/82 | 114/327) LP++: A Surprisingly Strong Linear Probe for Few-Shot CLIP (Yunshi Huang et al., 2024)</a></li><li><a href=#1382--115327-contrastcad-contrastive-learning-based-representation-learning-for-computer-aided-design-models-minseop-jung-et-al-2024>(13/82 | 115/327) ContrastCAD: Contrastive Learning-based Representation Learning for Computer-Aided Design Models (Minseop Jung et al., 2024)</a></li><li><a href=#1482--116327-egtr-extracting-graph-from-transformer-for-scene-graph-generation-jinbae-im-et-al-2024>(14/82 | 116/327) EGTR: Extracting Graph from Transformer for Scene Graph Generation (Jinbae Im et al., 2024)</a></li><li><a href=#1582--117327-3d-scene-generation-from-scene-graphs-and-self-attention-pietro-bonazzi-2024>(15/82 | 117/327) 3D scene generation from scene graphs and self-attention (Pietro Bonazzi, 2024)</a></li><li><a href=#1682--118327-braven-improving-self-supervised-pre-training-for-visual-and-auditory-speech-recognition-alexandros-haliassos-et-al-2024>(16/82 | 118/327) BRAVEn: Improving Self-Supervised Pre-training for Visual and Auditory Speech Recognition (Alexandros Haliassos et al., 2024)</a></li><li><a href=#1782--119327-t-vsl-text-guided-visual-sound-source-localization-in-mixtures-tanvir-mahmud-et-al-2024>(17/82 | 119/327) T-VSL: Text-Guided Visual Sound Source Localization in Mixtures (Tanvir Mahmud et al., 2024)</a></li><li><a href=#1882--120327-exploring-latent-pathways-enhancing-the-interpretability-of-autonomous-driving-with-a-variational-autoencoder-anass-bairouk-et-al-2024>(18/82 | 120/327) Exploring Latent Pathways: Enhancing the Interpretability of Autonomous Driving with a Variational Autoencoder (Anass Bairouk et al., 2024)</a></li><li><a href=#1982--121327-addsr-accelerating-diffusion-based-blind-super-resolution-with-adversarial-diffusion-distillation-rui-xie-et-al-2024>(19/82 | 121/327) AddSR: Accelerating Diffusion-based Blind Super-Resolution with Adversarial Diffusion Distillation (Rui Xie et al., 2024)</a></li><li><a href=#2082--122327-unleash-the-potential-of-clip-for-video-highlight-detection-donghoon-han-et-al-2024>(20/82 | 122/327) Unleash the Potential of CLIP for Video Highlight Detection (Donghoon Han et al., 2024)</a></li><li><a href=#2182--123327-mchartqa-a-universal-benchmark-for-multimodal-chart-question-answer-based-on-vision-language-alignment-and-reasoning-jingxuan-wei-et-al-2024>(21/82 | 123/327) mChartQA: A universal benchmark for multimodal Chart Question Answer based on Vision-Language Alignment and Reasoning (Jingxuan Wei et al., 2024)</a></li><li><a href=#2282--124327-iterated-learning-improves-compositionality-in-large-vision-language-models-chenhao-zheng-et-al-2024>(22/82 | 124/327) Iterated Learning Improves Compositionality in Large Vision-Language Models (Chenhao Zheng et al., 2024)</a></li><li><a href=#2382--125327-multi-level-label-correction-by-distilling-proximate-patterns-for-semi-supervised-semantic-segmentation-hui-xiao-et-al-2024>(23/82 | 125/327) Multi-Level Label Correction by Distilling Proximate Patterns for Semi-supervised Semantic Segmentation (Hui Xiao et al., 2024)</a></li><li><a href=#2482--126327-delan-dual-level-alignment-for-vision-and-language-navigation-by-cross-modal-contrastive-learning-mengfei-du-et-al-2024>(24/82 | 126/327) DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning (Mengfei Du et al., 2024)</a></li><li><a href=#2582--127327-linear-combination-of-saved-checkpoints-makes-consistency-and-diffusion-models-better-enshu-liu-et-al-2024>(25/82 | 127/327) Linear Combination of Saved Checkpoints Makes Consistency and Diffusion Models Better (Enshu Liu et al., 2024)</a></li><li><a href=#2682--128327-diffusion2-dynamic-3d-content-generation-via-score-composition-of-orthogonal-diffusion-models-zeyu-yang-et-al-2024>(26/82 | 128/327) Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of Orthogonal Diffusion Models (Zeyu Yang et al., 2024)</a></li><li><a href=#2782--129327-enhancing-ship-classification-in-optical-satellite-imagery-integrating-convolutional-block-attention-module-with-resnet-for-improved-performance-ryan-donghan-kwon-et-al-2024>(27/82 | 129/327) Enhancing Ship Classification in Optical Satellite Imagery: Integrating Convolutional Block Attention Module with ResNet for Improved Performance (Ryan Donghan Kwon et al., 2024)</a></li><li><a href=#2882--130327-cooperative-students-navigating-unsupervised-domain-adaptation-in-nighttime-object-detection-jicheng-yuan-et-al-2024>(28/82 | 130/327) Cooperative Students: Navigating Unsupervised Domain Adaptation in Nighttime Object Detection (Jicheng Yuan et al., 2024)</a></li><li><a href=#2982--131327-vlrm-vision-language-models-act-as-reward-models-for-image-captioning-maksim-dzabraev-et-al-2024>(29/82 | 131/327) VLRM: Vision-Language Models act as Reward Models for Image Captioning (Maksim Dzabraev et al., 2024)</a></li><li><a href=#3082--132327-scene-adaptive-sparse-transformer-for-event-based-object-detection-yansong-peng-et-al-2024>(30/82 | 132/327) Scene Adaptive Sparse Transformer for Event-based Object Detection (Yansong Peng et al., 2024)</a></li><li><a href=#3182--133327-disentangled-pre-training-for-human-object-interaction-detection-zhuolong-li-et-al-2024>(31/82 | 133/327) Disentangled Pre-training for Human-Object Interaction Detection (Zhuolong Li et al., 2024)</a></li><li><a href=#3282--134327-task-integration-distillation-for-object-detectors-hai-su-et-al-2024>(32/82 | 134/327) Task Integration Distillation for Object Detectors (Hai Su et al., 2024)</a></li><li><a href=#3382--135327-a-universal-knowledge-embedded-contrastive-learning-framework-for-hyperspectral-image-classification-quanwei-liu-et-al-2024>(33/82 | 135/327) A Universal Knowledge Embedded Contrastive Learning Framework for Hyperspectral Image Classification (Quanwei Liu et al., 2024)</a></li><li><a href=#3482--136327-supporting-mitosis-detection-ai-training-with-inter-observer-eye-gaze-consistencies-hongyan-gu-et-al-2024>(34/82 | 136/327) Supporting Mitosis Detection AI Training with Inter-Observer Eye-Gaze Consistencies (Hongyan Gu et al., 2024)</a></li><li><a href=#3582--137327-spin-up-spin-light-for-natural-light-uncalibrated-photometric-stereo-zongrui-li-et-al-2024>(35/82 | 137/327) Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo (Zongrui Li et al., 2024)</a></li><li><a href=#3682--138327-motionchain-conversational-motion-controllers-via-multimodal-prompts-biao-jiang-et-al-2024>(36/82 | 138/327) MotionChain: Conversational Motion Controllers via Multimodal Prompts (Biao Jiang et al., 2024)</a></li><li><a href=#3782--139327-language-model-guided-interpretable-video-action-reasoning-ning-wang-et-al-2024>(37/82 | 139/327) Language Model Guided Interpretable Video Action Reasoning (Ning Wang et al., 2024)</a></li><li><a href=#3882--140327-ofmpnet-deep-end-to-end-model-for-occupancy-and-flow-prediction-in-urban-environment-youshaa-murhij-et-al-2024>(38/82 | 140/327) OFMPNet: Deep End-to-End Model for Occupancy and Flow Prediction in Urban Environment (Youshaa Murhij et al., 2024)</a></li><li><a href=#3982--141327-selfpose3d-self-supervised-multi-person-multi-view-3d-pose-estimation-vinkle-srivastav-et-al-2024>(39/82 | 141/327) SelfPose3d: Self-Supervised Multi-Person Multi-View 3d Pose Estimation (Vinkle Srivastav et al., 2024)</a></li><li><a href=#4082--142327-semi-supervised-domain-adaptation-for-wildfire-detection-jooyoung-jang-et-al-2024>(40/82 | 142/327) Semi-Supervised Domain Adaptation for Wildfire Detection (JooYoung Jang et al., 2024)</a></li><li><a href=#4182--143327-sparse-semi-detr-sparse-learnable-queries-for-semi-supervised-object-detection-tahira-shehzadi-et-al-2024>(41/82 | 143/327) Sparse Semi-DETR: Sparse Learnable Queries for Semi-Supervised Object Detection (Tahira Shehzadi et al., 2024)</a></li><li><a href=#4282--144327-diffusion-deepfake-chaitali-bhattacharyya-et-al-2024>(42/82 | 144/327) Diffusion Deepfake (Chaitali Bhattacharyya et al., 2024)</a></li><li><a href=#4382--145327-bidirectional-multi-scale-implicit-neural-representations-for-image-deraining-xiang-chen-et-al-2024>(43/82 | 145/327) Bidirectional Multi-Scale Implicit Neural Representations for Image Deraining (Xiang Chen et al., 2024)</a></li><li><a href=#4482--146327-snag-scalable-and-accurate-video-grounding-fangzhou-mu-et-al-2024>(44/82 | 146/327) SnAG: Scalable and Accurate Video Grounding (Fangzhou Mu et al., 2024)</a></li><li><a href=#4582--147327-towards-robust-3d-pose-transfer-with-adversarial-learning-haoyu-chen-et-al-2024>(45/82 | 147/327) Towards Robust 3D Pose Transfer with Adversarial Learning (Haoyu Chen et al., 2024)</a></li><li><a href=#4682--148327-lookahead-exploration-with-neural-radiance-representation-for-continuous-vision-language-navigation-zihan-wang-et-al-2024>(46/82 | 148/327) Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation (Zihan Wang et al., 2024)</a></li><li><a href=#4782--149327-co-speech-gesture-video-generation-via-motion-decoupled-diffusion-model-xu-he-et-al-2024>(47/82 | 149/327) Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model (Xu He et al., 2024)</a></li><li><a href=#4882--150327-upsample-guidance-scale-up-diffusion-models-without-training-juno-hwang-et-al-2024>(48/82 | 150/327) Upsample Guidance: Scale Up Diffusion Models without Training (Juno Hwang et al., 2024)</a></li><li><a href=#4982--151327-learning-to-control-camera-exposure-via-reinforcement-learning-kyunghyun-lee-et-al-2024>(49/82 | 151/327) Learning to Control Camera Exposure via Reinforcement Learning (Kyunghyun Lee et al., 2024)</a></li><li><a href=#5082--152327-learning-equi-angular-representations-for-online-continual-learning-minhyuk-seo-et-al-2024>(50/82 | 152/327) Learning Equi-angular Representations for Online Continual Learning (Minhyuk Seo et al., 2024)</a></li><li><a href=#5182--153327-lr-fpn-enhancing-remote-sensing-object-detection-with-location-refined-feature-pyramid-network-hanqian-li-et-al-2024>(51/82 | 153/327) LR-FPN: Enhancing Remote Sensing Object Detection with Location Refined Feature Pyramid Network (Hanqian Li et al., 2024)</a></li><li><a href=#5282--154327-tscm-a-teacher-student-model-for-vision-place-recognition-using-cross-metric-knowledge-distillation-yehui-shen-et-al-2024>(52/82 | 154/327) TSCM: A Teacher-Student Model for Vision Place Recognition Using Cross-Metric Knowledge Distillation (Yehui Shen et al., 2024)</a></li><li><a href=#5382--155327-prego-online-mistake-detection-in-procedural-egocentric-videos-alessandro-flaborea-et-al-2024>(53/82 | 155/327) PREGO: online mistake detection in PRocedural EGOcentric videos (Alessandro Flaborea et al., 2024)</a></li><li><a href=#5482--156327-fashionengine-interactive-generation-and-editing-of-3d-clothed-humans-tao-hu-et-al-2024>(54/82 | 156/327) FashionEngine: Interactive Generation and Editing of 3D Clothed Humans (Tao Hu et al., 2024)</a></li><li><a href=#5582--157327-gears-local-geometry-aware-hand-object-interaction-synthesis-keyang-zhou-et-al-2024>(55/82 | 157/327) GEARS: Local Geometry-aware Hand-object Interaction Synthesis (Keyang Zhou et al., 2024)</a></li><li><a href=#5682--158327-a-linear-time-and-space-local-point-cloud-geometry-encoder-via-vectorized-kernel-mixture-veckm-dehao-yuan-et-al-2024>(56/82 | 158/327) A Linear Time and Space Local Point Cloud Geometry Encoder via Vectorized Kernel Mixture (VecKM) (Dehao Yuan et al., 2024)</a></li><li><a href=#5782--159327-segment-any-3d-object-with-language-seungjun-lee-et-al-2024>(57/82 | 159/327) Segment Any 3D Object with Language (Seungjun Lee et al., 2024)</a></li><li><a href=#5882--160327-gaitstr-gait-recognition-with-sequential-two-stream-refinement-wanrong-zheng-et-al-2024>(58/82 | 160/327) GaitSTR: Gait Recognition with Sequential Two-stream Refinement (Wanrong Zheng et al., 2024)</a></li><li><a href=#5982--161327-oostraj-out-of-sight-trajectory-prediction-with-vision-positioning-denoising-haichao-zhang-et-al-2024>(59/82 | 161/327) OOSTraj: Out-of-Sight Trajectory Prediction With Vision-Positioning Denoising (Haichao Zhang et al., 2024)</a></li><li><a href=#6082--162327-boosting-visual-recognition-for-autonomous-driving-in-real-world-degradations-with-deep-channel-prior-zhanwen-liu-et-al-2024>(60/82 | 162/327) Boosting Visual Recognition for Autonomous Driving in Real-world Degradations with Deep Channel Prior (Zhanwen Liu et al., 2024)</a></li><li><a href=#6182--163327-refqsr-reference-based-quantization-for-image-super-resolution-networks-hongjae-lee-et-al-2024>(61/82 | 163/327) RefQSR: Reference-based Quantization for Image Super-Resolution Networks (Hongjae Lee et al., 2024)</a></li><li><a href=#6282--164327-one-noise-to-rule-them-all-multi-view-adversarial-attacks-with-universal-perturbation-mehmet-ergezer-et-al-2024>(62/82 | 164/327) One Noise to Rule Them All: Multi-View Adversarial Attacks with Universal Perturbation (Mehmet Ergezer et al., 2024)</a></li><li><a href=#6382--165327-smooth-deep-saliency-rudolf-herdt-et-al-2024>(63/82 | 165/327) Smooth Deep Saliency (Rudolf Herdt et al., 2024)</a></li><li><a href=#6482--166327-visual-concept-connectome-vcc-open-world-concept-discovery-and-their-interlayer-connections-in-deep-models-matthew-kowal-et-al-2024>(64/82 | 166/327) Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models (Matthew Kowal et al., 2024)</a></li><li><a href=#6582--167327-chosen-contrastive-hypothesis-selection-for-multi-view-depth-refinement-di-qiu-et-al-2024>(65/82 | 167/327) CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement (Di Qiu et al., 2024)</a></li><li><a href=#6682--168327-geneavatar-generic-expression-aware-volumetric-head-avatar-editing-from-a-single-image-chong-bao-et-al-2024>(66/82 | 168/327) GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image (Chong Bao et al., 2024)</a></li><li><a href=#6782--169327-adaptive-feature-fusion-neural-network-for-glaucoma-segmentation-on-unseen-fundus-images-jiyuan-zhong-et-al-2024>(67/82 | 169/327) Adaptive Feature Fusion Neural Network for Glaucoma Segmentation on Unseen Fundus Images (Jiyuan Zhong et al., 2024)</a></li><li><a href=#6882--170327-nerfcodec-neural-feature-compression-meets-neural-radiance-fields-for-memory-efficient-scene-representation-sicheng-li-et-al-2024>(68/82 | 170/327) NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation (Sicheng Li et al., 2024)</a></li><li><a href=#6982--171327-cam-based-methods-can-see-through-walls-magamed-taimeskhanov-et-al-2024>(69/82 | 171/327) CAM-Based Methods Can See through Walls (Magamed Taimeskhanov et al., 2024)</a></li><li><a href=#7082--172327-improving-birds-eye-view-semantic-segmentation-by-task-decomposition-tianhao-zhao-et-al-2024>(70/82 | 172/327) Improving Bird&rsquo;s Eye View Semantic Segmentation by Task Decomposition (Tianhao Zhao et al., 2024)</a></li><li><a href=#7182--173327-astra-an-action-spotting-transformer-for-soccer-videos-artur-xarles-et-al-2024>(71/82 | 173/327) ASTRA: An Action Spotting TRAnsformer for Soccer Videos (Artur Xarles et al., 2024)</a></li><li><a href=#7282--174327-super-resolution-analysis-for-landfill-waste-classification-matias-molina-et-al-2024>(72/82 | 174/327) Super-Resolution Analysis for Landfill Waste Classification (Matias Molina et al., 2024)</a></li><li><a href=#7382--175327-a-noisy-elephant-in-the-room-is-your-out-of-distribution-detector-robust-to-label-noise-galadrielle-humblot-renaux-et-al-2024>(73/82 | 175/327) A noisy elephant in the room: Is your out-of-distribution detector robust to label noise? (Galadrielle Humblot-Renaux et al., 2024)</a></li><li><a href=#7482--176327-beyond-image-super-resolution-for-image-recognition-with-task-driven-perceptual-loss-jaeha-kim-et-al-2024>(74/82 | 176/327) Beyond Image Super-Resolution for Image Recognition with Task-Driven Perceptual Loss (Jaeha Kim et al., 2024)</a></li><li><a href=#7582--177327-learning-temporal-cues-by-predicting-objects-move-for-multi-camera-3d-object-detection-seokha-moon-et-al-2024>(75/82 | 177/327) Learning Temporal Cues by Predicting Objects Move for Multi-camera 3D Object Detection (Seokha Moon et al., 2024)</a></li><li><a href=#7682--178327-efficient-3d-implicit-head-avatar-with-mesh-anchored-hash-table-blendshapes-ziqian-bai-et-al-2024>(76/82 | 178/327) Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes (Ziqian Bai et al., 2024)</a></li><li><a href=#7782--179327-atom-level-optical-chemical-structure-recognition-with-limited-supervision-martijn-oldenhof-et-al-2024>(77/82 | 179/327) Atom-Level Optical Chemical Structure Recognition with Limited Supervision (Martijn Oldenhof et al., 2024)</a></li><li><a href=#7882--180327-specularity-factorization-for-low-light-enhancement-saurabh-saini-et-al-2024>(78/82 | 180/327) Specularity Factorization for Low-Light Enhancement (Saurabh Saini et al., 2024)</a></li><li><a href=#7982--181327-joint-task-regularization-for-partially-labeled-multi-task-learning-kento-nishi-et-al-2024>(79/82 | 181/327) Joint-Task Regularization for Partially Labeled Multi-Task Learning (Kento Nishi et al., 2024)</a></li><li><a href=#8082--182327-surface-reconstruction-from-gaussian-splatting-via-novel-stereo-views-yaniv-wolf-et-al-2024>(80/82 | 182/327) Surface Reconstruction from Gaussian Splatting via Novel Stereo Views (Yaniv Wolf et al., 2024)</a></li><li><a href=#8182--183327-jrdb-panotrack-an-open-world-panoptic-segmentation-and-tracking-robotic-dataset-in-crowded-human-environments-duy-tho-le-et-al-2024>(81/82 | 183/327) JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments (Duy-Tho Le et al., 2024)</a></li><li><a href=#8282--184327-wavedh-wavelet-sub-bands-guided-convnet-for-efficient-image-dehazing-seongmin-hwang-et-al-2024>(82/82 | 184/327) WaveDH: Wavelet Sub-bands Guided ConvNet for Efficient Image Dehazing (Seongmin Hwang et al., 2024)</a></li></ul></li><li><a href=#csir-6>cs.IR (6)</a><ul><li><a href=#16--185327-where-to-move-next-zero-shot-generalization-of-llms-for-next-poi-recommendation-shanshan-feng-et-al-2024>(1/6 | 185/327) Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation (Shanshan Feng et al., 2024)</a></li><li><a href=#26--186327-iisan-efficiently-adapting-multimodal-representation-for-sequential-recommendation-with-decoupled-peft-junchen-fu-et-al-2024>(2/6 | 186/327) IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT (Junchen Fu et al., 2024)</a></li><li><a href=#36--187327-cirp-cross-item-relational-pre-training-for-multimodal-product-bundling-yunshan-ma-et-al-2024>(3/6 | 187/327) CIRP: Cross-Item Relational Pre-training for Multimodal Product Bundling (Yunshan Ma et al., 2024)</a></li><li><a href=#46--188327-multi-granular-adversarial-attacks-against-black-box-neural-ranking-models-yu-an-liu-et-al-2024>(4/6 | 188/327) Multi-granular Adversarial Attacks against Black-box Neural Ranking Models (Yu-An Liu et al., 2024)</a></li><li><a href=#56--189327-rat-retrieval-augmented-transformer-for-click-through-rate-prediction-yushen-li-et-al-2024>(5/6 | 189/327) RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction (Yushen Li et al., 2024)</a></li><li><a href=#66--190327-a-survey-of-web-content-control-for-generative-ai-michael-dinzinger-et-al-2024>(6/6 | 190/327) A Survey of Web Content Control for Generative AI (Michael Dinzinger et al., 2024)</a></li></ul></li><li><a href=#cslg-48>cs.LG (48)</a><ul><li><a href=#148--191327-virtual-sensor-for-real-time-bearing-load-prediction-using-heterogeneous-temporal-graph-neural-networks-mengjie-zhao-et-al-2024>(1/48 | 191/327) Virtual Sensor for Real-Time Bearing Load Prediction Using Heterogeneous Temporal Graph Neural Networks (Mengjie Zhao et al., 2024)</a></li><li><a href=#248--192327-confidence-aware-reward-optimization-for-fine-tuning-text-to-image-models-kyuyoung-kim-et-al-2024>(2/48 | 192/327) Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models (Kyuyoung Kim et al., 2024)</a></li><li><a href=#348--193327-remote-sensing-framework-for-geological-mapping-via-stacked-autoencoders-and-clustering-sandeep-nagar-et-al-2024>(3/48 | 193/327) Remote sensing framework for geological mapping via stacked autoencoders and clustering (Sandeep Nagar et al., 2024)</a></li><li><a href=#448--194327-predicting-the-performance-of-foundation-models-via-agreement-on-the-line-aman-mehra-et-al-2024>(4/48 | 194/327) Predicting the Performance of Foundation Models via Agreement-on-the-Line (Aman Mehra et al., 2024)</a></li><li><a href=#548--195327-is-meta-training-really-necessary-for-molecular-few-shot-learning--philippe-formont-et-al-2024>(5/48 | 195/327) Is Meta-training Really Necessary for Molecular Few-Shot Learning ? (Philippe Formont et al., 2024)</a></li><li><a href=#648--196327-selective-temporal-knowledge-graph-reasoning-zhongni-hou-et-al-2024>(6/48 | 196/327) Selective Temporal Knowledge Graph Reasoning (Zhongni Hou et al., 2024)</a></li><li><a href=#748--197327-is-exploration-all-you-need-effective-exploration-characteristics-for-transfer-in-reinforcement-learning-jonathan-c-balloch-et-al-2024>(7/48 | 197/327) Is Exploration All You Need? Effective Exploration Characteristics for Transfer in Reinforcement Learning (Jonathan C. Balloch et al., 2024)</a></li><li><a href=#848--198327-a-generative-deep-learning-approach-for-crash-severity-modeling-with-imbalanced-data-junlan-chen-et-al-2024>(8/48 | 198/327) A Generative Deep Learning Approach for Crash Severity Modeling with Imbalanced Data (Junlan Chen et al., 2024)</a></li><li><a href=#948--199327-noise-masking-attacks-and-defenses-for-pretrained-speech-models-matthew-jagielski-et-al-2024>(9/48 | 199/327) Noise Masking Attacks and Defenses for Pretrained Speech Models (Matthew Jagielski et al., 2024)</a></li><li><a href=#1048--200327-accelerating-transformer-pre-training-with-24-sparsity-yuezhou-hu-et-al-2024>(10/48 | 200/327) Accelerating Transformer Pre-Training with 2:4 Sparsity (Yuezhou Hu et al., 2024)</a></li><li><a href=#1148--201327-test-time-model-adaptation-with-only-forward-passes-shuaicheng-niu-et-al-2024>(11/48 | 201/327) Test-Time Model Adaptation with Only Forward Passes (Shuaicheng Niu et al., 2024)</a></li><li><a href=#1248--202327-dsgnn-a-dual-view-supergrid-aware-graph-neural-network-for-regional-air-quality-estimation-xin-zhang-et-al-2024>(12/48 | 202/327) DSGNN: A Dual-View Supergrid-Aware Graph Neural Network for Regional Air Quality Estimation (Xin Zhang et al., 2024)</a></li><li><a href=#1348--203327-heat-death-of-generative-models-in-closed-loop-learning-matteo-marchi-et-al-2024>(13/48 | 203/327) Heat Death of Generative Models in Closed-Loop Learning (Matteo Marchi et al., 2024)</a></li><li><a href=#1448--204327-autodiff-autoregressive-diffusion-modeling-for-structure-based-drug-design-xinze-li-et-al-2024>(14/48 | 204/327) AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug Design (Xinze Li et al., 2024)</a></li><li><a href=#1548--205327-unifying-qualitative-and-quantitative-safety-verification-of-dnn-controlled-systems-dapeng-zhi-et-al-2024>(15/48 | 205/327) Unifying Qualitative and Quantitative Safety Verification of DNN-Controlled Systems (Dapeng Zhi et al., 2024)</a></li><li><a href=#1648--206327-transformer-meets-wcdtw-to-improve-real-time-battery-bids-a-new-approach-to-scenario-selection-sujal-bhavsar-et-al-2024>(16/48 | 206/327) Transformer meets wcDTW to improve real-time battery bids: A new approach to scenario selection (Sujal Bhavsar et al., 2024)</a></li><li><a href=#1748--207327-audio-simulation-for-sound-source-localization-in-virtual-evironment-yi-di-yuan-et-al-2024>(17/48 | 207/327) Audio Simulation for Sound Source Localization in Virtual Evironment (Yi Di Yuan et al., 2024)</a></li><li><a href=#1848--208327-mesen-exploit-multimodal-data-to-design-unimodal-human-activity-recognition-with-few-labels-lilin-xu-et-al-2024>(18/48 | 208/327) MESEN: Exploit Multimodal Data to Design Unimodal Human Activity Recognition with Few Labels (Lilin Xu et al., 2024)</a></li><li><a href=#1948--209327-a-more-realistic-evaluation-setup-for-generalisation-of-community-models-on-malicious-content-detection-ivo-verhoeven-et-al-2024>(19/48 | 209/327) A (More) Realistic Evaluation Setup for Generalisation of Community Models on Malicious Content Detection (Ivo Verhoeven et al., 2024)</a></li><li><a href=#2048--210327-catgnn-cost-efficient-and-scalable-distributed-training-for-graph-neural-networks-xin-huang-et-al-2024>(20/48 | 210/327) CATGNN: Cost-Efficient and Scalable Distributed Training for Graph Neural Networks (Xin Huang et al., 2024)</a></li><li><a href=#2148--211327-mixture-of-depths-dynamically-allocating-compute-in-transformer-based-language-models-david-raposo-et-al-2024>(21/48 | 211/327) Mixture-of-Depths: Dynamically allocating compute in transformer-based language models (David Raposo et al., 2024)</a></li><li><a href=#2248--212327-enhancing-functional-safety-in-automotive-ams-circuits-through-unsupervised-machine-learning-ayush-arunachalam-et-al-2024>(22/48 | 212/327) Enhancing Functional Safety in Automotive AMS Circuits through Unsupervised Machine Learning (Ayush Arunachalam et al., 2024)</a></li><li><a href=#2348--213327-imagenot-a-contrast-with-imagenet-preserves-model-rankings-olawale-salaudeen-et-al-2024>(23/48 | 213/327) ImageNot: A contrast with ImageNet preserves model rankings (Olawale Salaudeen et al., 2024)</a></li><li><a href=#2448--214327-incentives-in-private-collaborative-machine-learning-rachael-hwee-ling-sim-et-al-2024>(24/48 | 214/327) Incentives in Private Collaborative Machine Learning (Rachael Hwee Ling Sim et al., 2024)</a></li><li><a href=#2548--215327-advrepairprovable-repair-of-adversarial-attack-zhiming-chi-et-al-2024>(25/48 | 215/327) ADVREPAIR:Provable Repair of Adversarial Attack (Zhiming Chi et al., 2024)</a></li><li><a href=#2648--216327-what-can-transformer-learn-with-varying-depth-case-studies-on-sequence-learning-tasks-xingwu-chen-et-al-2024>(26/48 | 216/327) What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks (Xingwu Chen et al., 2024)</a></li><li><a href=#2748--217327-pairwise-similarity-distribution-clustering-for-noisy-label-learning-sihan-bai-2024>(27/48 | 217/327) Pairwise Similarity Distribution Clustering for Noisy Label Learning (Sihan Bai, 2024)</a></li><li><a href=#2848--218327-hemenet-heterogeneous-multichannel-equivariant-network-for-protein-multitask-learning-rong-han-et-al-2024>(28/48 | 218/327) HeMeNet: Heterogeneous Multichannel Equivariant Network for Protein Multitask Learning (Rong Han et al., 2024)</a></li><li><a href=#2948--219327-glemos-benchmark-for-instantaneous-graph-learning-model-selection-namyong-park-et-al-2024>(29/48 | 219/327) GLEMOS: Benchmark for Instantaneous Graph Learning Model Selection (Namyong Park et al., 2024)</a></li><li><a href=#3048--220327-propensity-score-alignment-of-unpaired-multimodal-data-johnny-xi-et-al-2024>(30/48 | 220/327) Propensity Score Alignment of Unpaired Multimodal Data (Johnny Xi et al., 2024)</a></li><li><a href=#3148--221327-fragnnet-a-deep-probabilistic-model-for-mass-spectrum-prediction-adamo-young-et-al-2024>(31/48 | 221/327) FraGNNet: A Deep Probabilistic Model for Mass Spectrum Prediction (Adamo Young et al., 2024)</a></li><li><a href=#3248--222327-deep-neural-networks-with-3d-point-clouds-for-empirical-friction-measurements-in-hydrodynamic-flood-models-francisco-haces-garcia-et-al-2024>(32/48 | 222/327) Deep Neural Networks with 3D Point Clouds for Empirical Friction Measurements in Hydrodynamic Flood Models (Francisco Haces-Garcia et al., 2024)</a></li><li><a href=#3348--223327-tuning-for-the-unknown-revisiting-evaluation-strategies-for-lifelong-rl-golnaz-mesbahi-et-al-2024>(33/48 | 223/327) Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL (Golnaz Mesbahi et al., 2024)</a></li><li><a href=#3448--224327-zero-shot-multi-lingual-speaker-verification-in-clinical-trials-ali-akram-et-al-2024>(34/48 | 224/327) Zero-Shot Multi-Lingual Speaker Verification in Clinical Trials (Ali Akram et al., 2024)</a></li><li><a href=#3548--225327-adaptive-combinatorial-maximization-beyond-approximate-greedy-policies-shlomi-weitzman-et-al-2024>(35/48 | 225/327) Adaptive Combinatorial Maximization: Beyond Approximate Greedy Policies (Shlomi Weitzman et al., 2024)</a></li><li><a href=#3648--226327-leveraging-machine-learning-for-early-autism-detection-via-indt-asd-indian-database-trapti-shrivastava-et-al-2024>(36/48 | 226/327) Leveraging Machine Learning for Early Autism Detection via INDT-ASD Indian Database (Trapti Shrivastava et al., 2024)</a></li><li><a href=#3748--227327-procedural-fairness-in-machine-learning-ziming-wang-et-al-2024>(37/48 | 227/327) Procedural Fairness in Machine Learning (Ziming Wang et al., 2024)</a></li><li><a href=#3848--228327-fast-and-adaptive-questionnaires-for-voting-advice-applications-fynn-bachmann-et-al-2024>(38/48 | 228/327) Fast and Adaptive Questionnaires for Voting Advice Applications (Fynn Bachmann et al., 2024)</a></li><li><a href=#3948--229327-defense-without-forgetting-continual-adversarial-defense-with-anisotropic--isotropic-pseudo-replay-yuhang-zhou-et-al-2024>(39/48 | 229/327) Defense without Forgetting: Continual Adversarial Defense with Anisotropic & Isotropic Pseudo Replay (Yuhang Zhou et al., 2024)</a></li><li><a href=#4048--230327-improved-text-emotion-prediction-using-combined-valence-and-arousal-ordinal-classification-michael-mitsios-et-al-2024>(40/48 | 230/327) Improved Text Emotion Prediction Using Combined Valence and Arousal Ordinal Classification (Michael Mitsios et al., 2024)</a></li><li><a href=#4148--231327-asymptotics-of-language-model-alignment-joy-qiping-yang-et-al-2024>(41/48 | 231/327) Asymptotics of Language Model Alignment (Joy Qiping Yang et al., 2024)</a></li><li><a href=#4248--232327-efficient-online-unlearning-via-hessian-free-recollection-of-individual-data-statistics-xinbao-qiao-et-al-2024>(42/48 | 232/327) Efficient Online Unlearning via Hessian-Free Recollection of Individual Data Statistics (Xinbao Qiao et al., 2024)</a></li><li><a href=#4348--233327-extremum-seeking-action-selection-for-accelerating-policy-optimization-ya-chien-chang-et-al-2024>(43/48 | 233/327) Extremum-Seeking Action Selection for Accelerating Policy Optimization (Ya-Chien Chang et al., 2024)</a></li><li><a href=#4448--234327-universal-representations-for-financial-transactional-data-embracing-local-global-and-external-contexts-alexandra-bazarova-et-al-2024>(44/48 | 234/327) Universal representations for financial transactional data: embracing local, global, and external contexts (Alexandra Bazarova et al., 2024)</a></li><li><a href=#4548--235327-attribution-regularization-for-multimodal-paradigms-sahiti-yerramilli-et-al-2024>(45/48 | 235/327) Attribution Regularization for Multimodal Paradigms (Sahiti Yerramilli et al., 2024)</a></li><li><a href=#4648--236327-generalizable-fast-and-accurate-deepqspr-with-fastprop-part-1-framework-and-benchmarks-jackson-burns-et-al-2024>(46/48 | 236/327) Generalizable, Fast, and Accurate DeepQSPR with fastprop Part 1: Framework and Benchmarks (Jackson Burns et al., 2024)</a></li><li><a href=#4748--237327-what-is-to-be-gained-by-ensemble-models-in-analysis-of-spectroscopic-data-katarina-domijan-2024>(47/48 | 237/327) What is to be gained by ensemble models in analysis of spectroscopic data? (Katarina Domijan, 2024)</a></li><li><a href=#4848--238327-settling-time-vs-accuracy-tradeoffs-for-clustering-big-data-andrew-draganov-et-al-2024>(48/48 | 238/327) Settling Time vs. Accuracy Tradeoffs for Clustering Big Data (Andrew Draganov et al., 2024)</a></li></ul></li><li><a href=#cssd-4>cs.SD (4)</a><ul><li><a href=#14--239327-weakly-supervised-audio-separation-via-bi-modal-semantic-similarity-tanvir-mahmud-et-al-2024>(1/4 | 239/327) Weakly-supervised Audio Separation via Bi-modal Semantic Similarity (Tanvir Mahmud et al., 2024)</a></li><li><a href=#24--240327-smitin-self-monitored-inference-time-intervention-for-generative-music-transformers-junghyun-koo-et-al-2024>(2/4 | 240/327) SMITIN: Self-Monitored Inference-Time INtervention for Generative Music Transformers (Junghyun Koo et al., 2024)</a></li><li><a href=#34--241327-spmamba-state-space-model-is-all-you-need-in-speech-separation-kai-li-et-al-2024>(3/4 | 241/327) SPMamba: State-space model is all you need in speech separation (Kai Li et al., 2024)</a></li><li><a href=#44--242327-voice-ehr-introducing-multimodal-audio-data-for-health-james-anibal-et-al-2024>(4/4 | 242/327) Voice EHR: Introducing Multimodal Audio Data for Health (James Anibal et al., 2024)</a></li></ul></li><li><a href=#eessas-2>eess.AS (2)</a><ul><li><a href=#12--243327-transfer-learning-from-whisper-for-microscopic-intelligibility-prediction-paul-best-et-al-2024>(1/2 | 243/327) Transfer Learning from Whisper for Microscopic Intelligibility Prediction (Paul Best et al., 2024)</a></li><li><a href=#22--244327-effective-internal-language-model-training-and-fusion-for-factorized-transducer-model-jinxi-guo-et-al-2024>(2/2 | 244/327) Effective internal language model training and fusion for factorized transducer model (Jinxi Guo et al., 2024)</a></li></ul></li><li><a href=#statml-5>stat.ML (5)</a><ul><li><a href=#15--245327-preventing-model-collapse-in-gaussian-process-latent-variable-models-ying-li-et-al-2024>(1/5 | 245/327) Preventing Model Collapse in Gaussian Process Latent Variable Models (Ying Li et al., 2024)</a></li><li><a href=#25--246327-on-stronger-computational-separations-between-multimodal-and-unimodal-machine-learning-ari-karchmer-2024>(2/5 | 246/327) On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning (Ari Karchmer, 2024)</a></li><li><a href=#35--247327-doubly-robust-off-policy-evaluation-with-estimated-logging-policy-kyungbok-lee-et-al-2024>(3/5 | 247/327) Doubly-Robust Off-Policy Evaluation with Estimated Logging Policy (Kyungbok Lee et al., 2024)</a></li><li><a href=#45--248327-fairm-learning-invariant-representations-for-algorithmic-fairness-and-domain-generalization-with-minimax-optimality-sai-li-et-al-2024>(4/5 | 248/327) FAIRM: Learning invariant representations for algorithmic fairness and domain generalization with minimax optimality (Sai Li et al., 2024)</a></li><li><a href=#55--249327-adversarial-combinatorial-bandits-with-switching-costs-yanyan-dong-et-al-2024>(5/5 | 249/327) Adversarial Combinatorial Bandits with Switching Costs (Yanyan Dong et al., 2024)</a></li></ul></li><li><a href=#cshc-7>cs.HC (7)</a><ul><li><a href=#17--250327-insightlens-discovering-and-exploring-insights-from-conversational-contexts-in-large-language-model-powered-data-analysis-luoxuan-weng-et-al-2024>(1/7 | 250/327) InsightLens: Discovering and Exploring Insights from Conversational Contexts in Large-Language-Model-Powered Data Analysis (Luoxuan Weng et al., 2024)</a></li><li><a href=#27--251327-exploring-how-multiple-levels-of-gpt-generated-programming-hints-support-or-disappoint-novices-ruiwei-xiao-et-al-2024>(2/7 | 251/327) Exploring How Multiple Levels of GPT-Generated Programming Hints Support or Disappoint Novices (Ruiwei Xiao et al., 2024)</a></li><li><a href=#37--252327-explainability-in-jupyterlab-and-beyond-interactive-xai-systems-for-integrated-and-collaborative-workflows-grace-guo-et-al-2024>(3/7 | 252/327) Explainability in JupyterLab and Beyond: Interactive XAI Systems for Integrated and Collaborative Workflows (Grace Guo et al., 2024)</a></li><li><a href=#47--253327-harder-better-faster-stronger-interactive-visualization-for-human-centered-ai-tools-md-naimul-hoque-et-al-2024>(4/7 | 253/327) Harder, Better, Faster, Stronger: Interactive Visualization for Human-Centered AI Tools (Md Naimul Hoque et al., 2024)</a></li><li><a href=#57--254327-tell-and-show-combining-multiple-modalities-to-communicate-manipulation-tasks-to-a-robot-petr-vanc-et-al-2024>(5/7 | 254/327) Tell and show: Combining multiple modalities to communicate manipulation tasks to a robot (Petr Vanc et al., 2024)</a></li><li><a href=#67--255327-gen4ds-workshop-on-data-storytelling-in-an-era-of-generative-ai-xingyu-lan-et-al-2024>(6/7 | 255/327) Gen4DS: Workshop on Data Storytelling in an Era of Generative AI (Xingyu Lan et al., 2024)</a></li><li><a href=#77--256327-from-delays-to-densities-exploring-data-uncertainty-through-speech-text-and-visualization-chase-stokes-et-al-2024>(7/7 | 256/327) From Delays to Densities: Exploring Data Uncertainty through Speech, Text, and Visualization (Chase Stokes et al., 2024)</a></li></ul></li><li><a href=#csse-7>cs.SE (7)</a><ul><li><a href=#17--257327-self-organized-agents-a-llm-multi-agent-framework-toward-ultra-large-scale-code-generation-and-optimization-yoichi-ishibashi-et-al-2024>(1/7 | 257/327) Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization (Yoichi Ishibashi et al., 2024)</a></li><li><a href=#27--258327-automated-user-story-generation-with-test-case-specification-using-large-language-model-tajmilur-rahman-et-al-2024>(2/7 | 258/327) Automated User Story Generation with Test Case Specification Using Large Language Model (Tajmilur Rahman et al., 2024)</a></li><li><a href=#37--259327-peer-aided-repairer-empowering-large-language-models-to-repair-advanced-student-assignments-qianhui-zhao-et-al-2024>(3/7 | 259/327) Peer-aided Repairer: Empowering Large Language Models to Repair Advanced Student Assignments (Qianhui Zhao et al., 2024)</a></li><li><a href=#47--260327-ev2gym-a-flexible-v2g-simulator-for-ev-smart-charging-research-and-benchmarking-stavros-orfanoudakis-et-al-2024>(4/7 | 260/327) EV2Gym: A Flexible V2G Simulator for EV Smart Charging Research and Benchmarking (Stavros Orfanoudakis et al., 2024)</a></li><li><a href=#57--261327-multitask-based-evaluation-of-open-source-llm-on-software-vulnerability-xin-yin-et-al-2024>(5/7 | 261/327) Multitask-based Evaluation of Open-Source LLM on Software Vulnerability (Xin Yin et al., 2024)</a></li><li><a href=#67--262327-keeping-behavioral-programs-alive-specifying-and-executing-liveness-requirements-tom-yaacov-et-al-2024>(6/7 | 262/327) Keeping Behavioral Programs Alive: Specifying and Executing Liveness Requirements (Tom Yaacov et al., 2024)</a></li><li><a href=#77--263327-ft2ra-a-fine-tuning-inspired-approach-to-retrieval-augmented-code-completion-qi-guo-et-al-2024>(7/7 | 263/327) FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion (Qi Guo et al., 2024)</a></li></ul></li><li><a href=#csce-2>cs.CE (2)</a><ul><li><a href=#12--264327-intelligent-optimization-of-mine-environmental-damage-assessment-and-repair-strategies-based-on-deep-learning-qishuo-cheng-2024>(1/2 | 264/327) Intelligent Optimization of Mine Environmental Damage Assessment and Repair Strategies Based on Deep Learning (Qishuo Cheng, 2024)</a></li><li><a href=#22--265327-enhancing-portfolio-optimization-with-transformer-gan-integration-a-novel-approach-in-the-black-litterman-framework-enmin-zhu-et-al-2024>(2/2 | 265/327) Enhancing Portfolio Optimization with Transformer-GAN Integration: A Novel Approach in the Black-Litterman Framework (Enmin Zhu et al., 2024)</a></li></ul></li><li><a href=#csma-3>cs.MA (3)</a><ul><li><a href=#13--266327-distributed-autonomous-swarm-formation-for-dynamic-network-bridging-raffaele-galliera-et-al-2024>(1/3 | 266/327) Distributed Autonomous Swarm Formation for Dynamic Network Bridging (Raffaele Galliera et al., 2024)</a></li><li><a href=#23--267327-energaize-multi-agent-deep-deterministic-policy-gradient-for-vehicle-to-grid-energy-management-tiago-fonseca-et-al-2024>(2/3 | 267/327) EnergAIze: Multi Agent Deep Deterministic Policy Gradient for Vehicle to Grid Energy Management (Tiago Fonseca et al., 2024)</a></li><li><a href=#33--268327-multi-agent-reinforcement-learning-with-control-theoretic-safety-guarantees-for-dynamic-network-bridging-raffaele-galliera-et-al-2024>(3/3 | 268/327) Multi-Agent Reinforcement Learning with Control-Theoretic Safety Guarantees for Dynamic Network Bridging (Raffaele Galliera et al., 2024)</a></li></ul></li><li><a href=#eessiv-7>eess.IV (7)</a><ul><li><a href=#17--269327-guidelines-for-cerebrovascular-segmentation-managing-imperfect-annotations-in-the-context-of-semi-supervised-learning-pierre-rougé-et-al-2024>(1/7 | 269/327) Guidelines for Cerebrovascular Segmentation: Managing Imperfect Annotations in the context of Semi-Supervised Learning (Pierre Rougé et al., 2024)</a></li><li><a href=#27--270327-contextual-embedding-learning-to-enhance-2d-networks-for-volumetric-image-segmentation-zhuoyuan-wang-et-al-2024>(2/7 | 270/327) Contextual Embedding Learning to Enhance 2D Networks for Volumetric Image Segmentation (Zhuoyuan Wang et al., 2024)</a></li><li><a href=#37--271327-rethinking-annotator-simulation-realistic-evaluation-of-whole-body-pet-lesion-interactive-segmentation-methods-zdravko-marinov-et-al-2024>(3/7 | 271/327) Rethinking Annotator Simulation: Realistic Evaluation of Whole-Body PET Lesion Interactive Segmentation Methods (Zdravko Marinov et al., 2024)</a></li><li><a href=#47--272327-a-closer-look-at-spatial-slice-features-learning-for-covid-19-detection-chih-chung-hsu-et-al-2024>(4/7 | 272/327) A Closer Look at Spatial-Slice Features Learning for COVID-19 Detection (Chih-Chung Hsu et al., 2024)</a></li><li><a href=#57--273327-covid-19-detection-based-on-blood-test-parameters-using-various-artificial-intelligence-methods-kavian-khanjani-et-al-2024>(5/7 | 273/327) COVID-19 Detection Based on Blood Test Parameters using Various Artificial Intelligence Methods (Kavian Khanjani et al., 2024)</a></li><li><a href=#67--274327-synthetic-data-for-robust-stroke-segmentation-liam-chalcroft-et-al-2024>(6/7 | 274/327) Synthetic Data for Robust Stroke Segmentation (Liam Chalcroft et al., 2024)</a></li><li><a href=#77--275327-towards-enhanced-analysis-of-lung-cancer-lesions-in-ebus-tbna----a-semi-supervised-video-object-detection-method-jyun-an-lin-et-al-2024>(7/7 | 275/327) Towards Enhanced Analysis of Lung Cancer Lesions in EBUS-TBNA &ndash; A Semi-Supervised Video Object Detection Method (Jyun-An Lin et al., 2024)</a></li></ul></li><li><a href=#csni-6>cs.NI (6)</a><ul><li><a href=#16--276327-guided-mutation-genetic-algorithm-for-mobile-iot-network-relay-gyupil-kam-et-al-2024>(1/6 | 276/327) Guided-Mutation Genetic Algorithm for Mobile IoT Network Relay (Gyupil Kam et al., 2024)</a></li><li><a href=#26--277327-collaborative-optimization-of-wireless-communication-and-computing-resource-allocation-based-on-multi-agent-federated-weighting-deep-reinforcement-learning-junjie-wu-et-al-2024>(2/6 | 277/327) Collaborative Optimization of Wireless Communication and Computing Resource Allocation based on Multi-Agent Federated Weighting Deep Reinforcement Learning (Junjie Wu et al., 2024)</a></li><li><a href=#36--278327-llm-abr-designing-adaptive-bitrate-algorithms-via-large-language-models-zhiyuan-he-et-al-2024>(3/6 | 278/327) LLM-ABR: Designing Adaptive Bitrate Algorithms via Large Language Models (Zhiyuan He et al., 2024)</a></li><li><a href=#46--279327-defining-problem-from-solutions-inverse-reinforcement-learning-irl-and-its-applications-for-next-generation-networking-yinqiu-liu-et-al-2024>(4/6 | 279/327) Defining Problem from Solutions: Inverse Reinforcement Learning (IRL) and Its Applications for Next-Generation Networking (Yinqiu Liu et al., 2024)</a></li><li><a href=#56--280327-dcp-and-vardis-an-ad-hoc-protocol-stack-for-dynamic-swarms-and-formations-of-drones----extended-version-samuel-pell-et-al-2024>(5/6 | 280/327) DCP and VarDis: An Ad-Hoc Protocol Stack for Dynamic Swarms and Formations of Drones &ndash; Extended Version (Samuel Pell et al., 2024)</a></li><li><a href=#66--281327-smartt-reps-sender-based-marked-rapidly-adapting-trimmed--timed-transport-with-recycled-entropies-tommaso-bonato-et-al-2024>(6/6 | 281/327) SMaRTT-REPS: Sender-based Marked Rapidly-adapting Trimmed & Timed Transport with Recycled Entropies (Tommaso Bonato et al., 2024)</a></li></ul></li><li><a href=#csne-3>cs.NE (3)</a><ul><li><a href=#13--282327-continuous-spiking-graph-neural-networks-nan-yin-et-al-2024>(1/3 | 282/327) Continuous Spiking Graph Neural Networks (Nan Yin et al., 2024)</a></li><li><a href=#23--283327-already-moderate-population-sizes-provably-yield-strong-robustness-to-noise-denis-antipov-et-al-2024>(2/3 | 283/327) Already Moderate Population Sizes Provably Yield Strong Robustness to Noise (Denis Antipov et al., 2024)</a></li><li><a href=#33--284327-tensorized-neuroevolution-of-augmenting-topologies-for-gpu-acceleration-lishuang-wang-et-al-2024>(3/3 | 284/327) Tensorized NeuroEvolution of Augmenting Topologies for GPU Acceleration (Lishuang Wang et al., 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#11--285327-deterministic-search-on-complete-bipartite-graphs-by-continuous-time-quantum-walk-honghong-lin-et-al-2024>(1/1 | 285/327) Deterministic Search on Complete Bipartite Graphs by Continuous Time Quantum Walk (Honghong Lin et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--286327-robust-constrained-consensus-and-inequality-constrained-distributed-optimization-with-guaranteed-differential-privacy-and-accurate-convergence-yongqiang-wang-et-al-2024>(1/1 | 286/327) Robust Constrained Consensus and Inequality-constrained Distributed Optimization with Guaranteed Differential Privacy and Accurate Convergence (Yongqiang Wang et al., 2024)</a></li></ul></li><li><a href=#cssi-2>cs.SI (2)</a><ul><li><a href=#12--287327-a-holistic-indicator-of-polarization-to-measure-online-sexism-vahid-ghafouri-et-al-2024>(1/2 | 287/327) A Holistic Indicator of Polarization to Measure Online Sexism (Vahid Ghafouri et al., 2024)</a></li><li><a href=#22--288327-how-covid-19-has-impacted-the-anti-vaccine-discourse-a-large-scale-twitter-study-spanning-pre-covid-and-post-covid-era-soham-poddar-et-al-2024>(2/2 | 288/327) How COVID-19 has Impacted the Anti-Vaccine Discourse: A Large-Scale Twitter Study Spanning Pre-COVID and Post-COVID Era (Soham Poddar et al., 2024)</a></li></ul></li><li><a href=#mathna-5>math.NA (5)</a><ul><li><a href=#15--289327-numerical-simulation-of-the-gross-pitaevskii-equation-via-vortex-tracking-thiago-carvalho-corso-et-al-2024>(1/5 | 289/327) Numerical simulation of the Gross-Pitaevskii equation via vortex tracking (Thiago Carvalho Corso et al., 2024)</a></li><li><a href=#25--290327-adaptive-gradient-enhanced-gaussian-process-surrogates-for-inverse-problems-phillip-semler-et-al-2024>(2/5 | 290/327) Adaptive Gradient Enhanced Gaussian Process Surrogates for Inverse Problems (Phillip Semler et al., 2024)</a></li><li><a href=#35--291327-comparison-of-different-elastic-strain-definitions-for-largely-deformed-sei-of-chemo-mechanically-coupled-silicon-battery-particles-raphael-schoof-et-al-2024>(3/5 | 291/327) Comparison of Different Elastic Strain Definitions for Largely Deformed SEI of Chemo-Mechanically Coupled Silicon Battery Particles (Raphael Schoof et al., 2024)</a></li><li><a href=#45--292327-a-second-order-correction-method-for-loosely-coupled-discretizations-applied-to-parabolic-parabolic-interface-problems-erik-burman-et-al-2024>(4/5 | 292/327) A second-order correction method for loosely coupled discretizations applied to parabolic-parabolic interface problems (Erik Burman et al., 2024)</a></li><li><a href=#55--293327-estimates-of-discrete-time-derivatives-for-the-parabolic-parabolic-robin-robin-coupling-method-erik-burman-et-al-2024>(5/5 | 293/327) Estimates of discrete time derivatives for the parabolic-parabolic Robin-Robin coupling method (Erik Burman et al., 2024)</a></li></ul></li><li><a href=#physicsbio-ph-1>physics.bio-ph (1)</a><ul><li><a href=#11--294327-emergence-of-chemotactic-strategies-with-multi-agent-reinforcement-learning-samuel-tovey-et-al-2024>(1/1 | 294/327) Emergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning (Samuel Tovey et al., 2024)</a></li></ul></li><li><a href=#csdl-2>cs.DL (2)</a><ul><li><a href=#12--295327-sentiment-analysis-of-citations-in-scientific-articles-using-chatgpt-identifying-potential-biases-and-conflicts-of-interest-walid-hariri-2024>(1/2 | 295/327) Sentiment Analysis of Citations in Scientific Articles Using ChatGPT: Identifying Potential Biases and Conflicts of Interest (Walid Hariri, 2024)</a></li><li><a href=#22--296327-the-open-access-coverage-of-openalex-scopus-and-web-of-science-marc-andre-simard-et-al-2024>(2/2 | 296/327) The open access coverage of OpenAlex, Scopus and Web of Science (Marc-Andre Simard et al., 2024)</a></li></ul></li><li><a href=#astro-phim-1>astro-ph.IM (1)</a><ul><li><a href=#11--297327-csst-strong-lensing-preparation-a-framework-for-detecting-strong-lenses-in-the-multi-color-imaging-survey-by-the-china-survey-space-telescope-csst-xu-li-et-al-2024>(1/1 | 297/327) CSST Strong Lensing Preparation: a Framework for Detecting Strong Lenses in the Multi-color Imaging Survey by the China Survey Space Telescope (CSST) (Xu Li et al., 2024)</a></li></ul></li><li><a href=#statme-1>stat.ME (1)</a><ul><li><a href=#11--298327-robustly-estimating-heterogeneity-in-factorial-data-using-rashomon-partitions-aparajithan-venkateswaran-et-al-2024>(1/1 | 298/327) Robustly estimating heterogeneity in factorial data using Rashomon Partitions (Aparajithan Venkateswaran et al., 2024)</a></li></ul></li><li><a href=#csit-3>cs.IT (3)</a><ul><li><a href=#13--299327-the-meta-distribution-of-the-sir-in-joint-communication-and-sensing-networks-kun-ma-et-al-2024>(1/3 | 299/327) The Meta Distribution of the SIR in Joint Communication and Sensing Networks (Kun Ma et al., 2024)</a></li><li><a href=#23--300327-learning-based-joint-beamforming-and-antenna-movement-design-for-movable-antenna-systems-caihao-weng-et-al-2024>(2/3 | 300/327) Learning-Based Joint Beamforming and Antenna Movement Design for Movable Antenna Systems (Caihao Weng et al., 2024)</a></li><li><a href=#33--301327-generating-gaussian-pseudorandom-noise-with-binary-sequences-francisco-javier-soto-et-al-2024>(3/3 | 301/327) Generating gaussian pseudorandom noise with binary sequences (Francisco-Javier Soto et al., 2024)</a></li></ul></li><li><a href=#csar-3>cs.AR (3)</a><ul><li><a href=#13--302327-netsmith-an-optimization-framework-for-machine-discovered-network-topologies-conor-green-et-al-2024>(1/3 | 302/327) NetSmith: An Optimization Framework for Machine-Discovered Network Topologies (Conor Green et al., 2024)</a></li><li><a href=#23--303327-analyzing-the-single-event-upset-vulnerability-of-binarized-neural-networks-on-sram-fpgas-ioanna-souvatzoglou-et-al-2024>(2/3 | 303/327) Analyzing the Single Event Upset Vulnerability of Binarized Neural Networks on SRAM FPGAs (Ioanna Souvatzoglou et al., 2024)</a></li><li><a href=#33--304327-a-fully-configurable-open-source-software-defined-digital-quantized-spiking-neural-core-architecture-shadi-matinizadeh-et-al-2024>(3/3 | 304/327) A Fully-Configurable Open-Source Software-Defined Digital Quantized Spiking Neural Core Architecture (Shadi Matinizadeh et al., 2024)</a></li></ul></li><li><a href=#eesssy-9>eess.SY (9)</a><ul><li><a href=#19--305327-a-neural-network-based-approach-to-hybrid-systems-identification-for-control-filippo-fabiani-et-al-2024>(1/9 | 305/327) A neural network-based approach to hybrid systems identification for control (Filippo Fabiani et al., 2024)</a></li><li><a href=#29--306327-on-the-regret-of-recursive-methods-for-discrete-time-adaptive-control-with-matched-uncertainty-aren-karapetyan-et-al-2024>(2/9 | 306/327) On the Regret of Recursive Methods for Discrete-Time Adaptive Control with Matched Uncertainty (Aren Karapetyan et al., 2024)</a></li><li><a href=#39--307327-on-the-effect-of-quantization-on-dynamic-mode-decomposition-dipankar-maity-et-al-2024>(3/9 | 307/327) On the Effect of Quantization on Dynamic Mode Decomposition (Dipankar Maity et al., 2024)</a></li><li><a href=#49--308327-learning-based-model-augmentation-with-lfrs-jan-h-hoekstra-et-al-2024>(4/9 | 308/327) Learning-based model augmentation with LFRs (Jan H. Hoekstra et al., 2024)</a></li><li><a href=#59--309327-integrating-systemc-ams-power-modeling-with-a-risc-v-iss-for-virtual-prototyping-of-battery-operated-embedded-devices-mohamed-amine-hamdi-et-al-2024>(5/9 | 309/327) Integrating SystemC-AMS Power Modeling with a RISC-V ISS for Virtual Prototyping of Battery-operated Embedded Devices (Mohamed Amine Hamdi et al., 2024)</a></li><li><a href=#69--310327-identifying-the-largest-rocof-and-its-implications-licheng-wang-et-al-2024>(6/9 | 310/327) Identifying the Largest RoCoF and Its Implications (Licheng Wang et al., 2024)</a></li><li><a href=#79--311327-real-time-hybrid-simulation-for-infrastructure-degradation-assessment-conceptual-framework-and-illustrative-application-manuel-salmeron-et-al-2024>(7/9 | 311/327) Real-Time Hybrid Simulation for Infrastructure Degradation Assessment: Conceptual Framework and Illustrative Application (Manuel Salmeron et al., 2024)</a></li><li><a href=#89--312327-on-the-reduction-of-linear-parameter-varying-state-space-models-e-javier-olucha-et-al-2024>(8/9 | 312/327) On the reduction of Linear Parameter-Varying State-Space models (E. Javier Olucha et al., 2024)</a></li><li><a href=#99--313327-a-stability-based-abstraction-framework-for-reach-avoid-control-of-stochastic-dynamical-systems-with-unknown-noise-distributions-thom-badings-et-al-2024>(9/9 | 313/327) A Stability-Based Abstraction Framework for Reach-Avoid Control of Stochastic Dynamical Systems with Unknown Noise Distributions (Thom Badings et al., 2024)</a></li></ul></li><li><a href=#csds-1>cs.DS (1)</a><ul><li><a href=#11--314327-learning-intersections-of-halfspaces-with-distribution-shift-improved-algorithms-and-sq-lower-bounds-adam-r-klivans-et-al-2024>(1/1 | 314/327) Learning Intersections of Halfspaces with Distribution Shift: Improved Algorithms and SQ Lower Bounds (Adam R. Klivans et al., 2024)</a></li></ul></li><li><a href=#csdc-2>cs.DC (2)</a><ul><li><a href=#12--315327-muxserve-flexible-multiplexing-for-efficient-multiple-llm-serving-jiangfei-duan-et-al-2024>(1/2 | 315/327) MuxServe: Flexible Multiplexing for Efficient Multiple LLM Serving (Jiangfei Duan et al., 2024)</a></li><li><a href=#22--316327-a-shared-compilation-stack-for-distributed-memory-parallelism-in-stencil-dsls-george-bisbas-et-al-2024>(2/2 | 316/327) A shared compilation stack for distributed-memory parallelism in stencil DSLs (George Bisbas et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--317327-fluid-implicit-particle-simulation-for-cpu-and-gpu-pedro-centeno-et-al-2024>(1/1 | 317/327) Fluid Implicit Particle Simulation for CPU and GPU (Pedro Centeno et al., 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-1>physics.flu-dyn (1)</a><ul><li><a href=#11--318327-hyperviscosity-stabilisation-of-the-rbf-fd-solution-to-natural-convection-žiga-vaupotič-et-al-2024>(1/1 | 318/327) Hyperviscosity stabilisation of the RBF-FD solution to natural convection (Žiga Vaupotič et al., 2024)</a></li></ul></li><li><a href=#eesssp-2>eess.SP (2)</a><ul><li><a href=#12--319327-satellite-federated-edge-learning-architecture-design-and-convergence-analysis-yuanming-shi-et-al-2024>(1/2 | 319/327) Satellite Federated Edge Learning: Architecture Design and Convergence Analysis (Yuanming Shi et al., 2024)</a></li><li><a href=#22--320327-intelligent-reflecting-surfaces-assisted-laser-based-optical-wireless-communication-networks-ahrar-n-hamad-et-al-2024>(2/2 | 320/327) Intelligent Reflecting Surfaces assisted Laser-based Optical Wireless Communication Networks (Ahrar N. Hamad et al., 2024)</a></li></ul></li><li><a href=#physicsmed-ph-1>physics.med-ph (1)</a><ul><li><a href=#11--321327-a-multicore-parallel-algorithm-for-multiscale-modelling-of-an-entire-human-blood-circulation-network-jiawei-liu-et-al-2024>(1/1 | 321/327) A multicore parallel algorithm for multiscale modelling of an entire human blood circulation network (Jiawei Liu et al., 2024)</a></li></ul></li><li><a href=#csfl-1>cs.FL (1)</a><ul><li><a href=#11--322327-transformers-as-transducers-lena-strobl-et-al-2024>(1/1 | 322/327) Transformers as Transducers (Lena Strobl et al., 2024)</a></li></ul></li><li><a href=#csdb-2>cs.DB (2)</a><ul><li><a href=#12--323327-flexis-flexible-frequent-subgraph-mining-using-maximal-independent-sets-akshit-sharma-et-al-2024>(1/2 | 323/327) FLEXIS: FLEXible Frequent Subgraph Mining using Maximal Independent Sets (Akshit Sharma et al., 2024)</a></li><li><a href=#22--324327-heterogeneous-data-access-model-for-concurrency-control-and-methods-to-deal-with-high-data-contention-alexander-thomasian-2024>(2/2 | 324/327) Heterogeneous Data Access Model for Concurrency Control and Methods to Deal with High Data Contention (Alexander Thomasian, 2024)</a></li></ul></li><li><a href=#mathac-1>math.AC (1)</a><ul><li><a href=#11--325327-the-edge-code-of-hypergraphs-delio-jaramillo-velez-2024>(1/1 | 325/327) The edge code of hypergraphs (Delio Jaramillo-Velez, 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--326327-generalized-saturation-game-balázs-patkós-et-al-2024>(1/1 | 326/327) Generalized saturation game (Balázs Patkós et al., 2024)</a></li></ul></li><li><a href=#csdm-1>cs.DM (1)</a><ul><li><a href=#11--327327-a-temporal-graph-model-to-study-the-dynamics-of-collective-behavior-and-performance-in-team-sports-an-application-to-basketball-quentin-bourgeais-et-al-2024>(1/1 | 327/327) A Temporal Graph Model to Study the Dynamics of Collective Behavior and Performance in Team Sports: An Application to Basketball (Quentin Bourgeais et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>