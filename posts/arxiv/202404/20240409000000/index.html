<!doctype html><html><head><title>arXiv @ 2024.04.09</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.04.09"><meta property="og:description" content="Primary Categories cs.AI (4) cs.AR (2) cs.CE (1) cs.CL (18) cs.CR (8) cs.CV (47) cs.CY (1) cs.DS (1) cs.HC (3) cs.IT (9) cs.LG (26) cs.LO (3) cs.NE (1) cs.PL (2) cs.RO (6) cs.SD (1) cs.SE (6) econ.EM (1) econ.TH (1) eess.AS (1) eess.IV (2) eess.SY (3) math.DS (1) math.NA (1) physics.flu-dyn (1) q-bio.TO (1) q-fin.CP (1) Keywords keyword cs.CL cs.CV cs.LG Active Learning 1 Adversarial Learning 1 Anomaly Detection 2 1 Autoencoder 2 1 Automatic Evaluation 1 BLEU 2 Bag-of-Words 1 Benchmarking 6 10 4 ChatGPT 2 1 Claude 1 Clustering 1 2 Continual Learning 1 1 Contrastive Learning 2 1 Convolution 3 Convolutional Neural Network 9 Data Augmentation 2 1 Deep Neural Network 2 1 Diffusion Model 4 1 Distribution Shift 4 Domain Adaptation 1 2 Essay Scoring 1 Event Argument Extraction 1 Fairness 1 3 Federated Learning 1 Few-shot 2 2 Few-shot Learning 1 Fine-tuning 4 7 2 Foundation Model 2 GPT 2 1 GPT-4 2 2 Generative Adversarial Network 2 Geometry 3 1 Graph 3 3 Graph Neural Network 4 Grounding 1 Hallucination Detection 1 Image2text 2 In-context Learning 2 Information Retrieval 1 Instruction Following 1 Instruction Tuning 1 Knowledge Distillation 1 6 1 LLaMA 1 Large Language Model 18 8 11 Low-Resource 1 Mixed Reality (MR) 3 Multi-modal 2 5 1 Multiple Instance Learning 1 1 Mutual Information 1 Named Entity Recognition 2 Natural Language Inference 2 Neural Machine Translation 7 Node Classification 1 Object Detection 4 Offline Reinforcement Learning 1 Out-of-distribution 2 Prompt 2 5 2 Pruning 1 Quantization 1 Question Answering 1 1 Reasoning 4 2 Recurrent Neural Network 2 Reinforcement Learning 1 2 2 Reinforcement Learning from Human Feedback 2 Relation Extraction 1 Representation Learning 2 3 Sample Size 1 Security 1 Self-supervised Learning 2 5 Simulation 1 Simulator 1 Summarization 2 1 1 Supervised Learning 6 2 TF-IDF 1 Table Filling 1 Text Augmentation 1 Text Generation 2 Text2image 1 Transfer Learning 1 Transformer 6 2 Unsupervised Learning 5 4 Variational Autoencoder 1 Vision Transformer 4 Vision-and-Language 5 1 Weakly-supervised Learning 2 Word Embedding 1 Zero-shot 2 6 1 Zero-shot Learning 2 cs."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202404/20240409000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-09T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-09T00:00:00+00:00"><meta name=description content="arXiv @ 2024.04.09"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240408000000/ title="arXiv @ 2024.04.08">arXiv @ 2024.04.08</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/ title="arXiv @ 2024.04.09">arXiv @ 2024.04.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240410000000/ title="arXiv @ 2024.04.10">arXiv @ 2024.04.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/ title="arXiv @ 2024.04.11">arXiv @ 2024.04.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/ title="arXiv @ 2024.04.12">arXiv @ 2024.04.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/ title="arXiv @ 2024.04.13">arXiv @ 2024.04.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240414000000/ title="arXiv @ 2024.04.14">arXiv @ 2024.04.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/ title="arXiv @ 2024.04.15">arXiv @ 2024.04.15</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202404/20240409000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Tuesday, Apr 9, 2024</p></div><div class=title><h1>arXiv @ 2024.04.09</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#csai-4>cs.AI (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#csar-2>cs.AR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#cscl-18>cs.CL (18)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#cscr-8>cs.CR (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#cscv-47>cs.CV (47)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#csds-1>cs.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#cshc-3>cs.HC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#csit-9>cs.IT (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#cslg-26>cs.LG (26)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#cslo-3>cs.LO (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#csne-1>cs.NE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#cspl-2>cs.PL (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#csro-6>cs.RO (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#cssd-1>cs.SD (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#csse-6>cs.SE (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#econem-1>econ.EM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#econth-1>econ.TH (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#eessas-1>eess.AS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#eessiv-2>eess.IV (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#eesssy-3>eess.SY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#mathds-1>math.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#mathna-1>math.NA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#physicsflu-dyn-1>physics.flu-dyn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#q-bioto-1>q-bio.TO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/#q-fincp-1>q-fin.CP (1)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td>1</td></tr><tr><td>Adversarial Learning</td><td></td><td>1</td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td>2</td><td>1</td></tr><tr><td>Autoencoder</td><td></td><td>2</td><td>1</td></tr><tr><td>Automatic Evaluation</td><td>1</td><td></td><td></td></tr><tr><td>BLEU</td><td>2</td><td></td><td></td></tr><tr><td>Bag-of-Words</td><td></td><td></td><td>1</td></tr><tr><td>Benchmarking</td><td>6</td><td>10</td><td>4</td></tr><tr><td>ChatGPT</td><td>2</td><td></td><td>1</td></tr><tr><td>Claude</td><td>1</td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td>1</td><td>2</td></tr><tr><td>Continual Learning</td><td>1</td><td></td><td>1</td></tr><tr><td>Contrastive Learning</td><td></td><td>2</td><td>1</td></tr><tr><td>Convolution</td><td></td><td>3</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>9</td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>2</td><td>1</td></tr><tr><td>Deep Neural Network</td><td></td><td>2</td><td>1</td></tr><tr><td>Diffusion Model</td><td></td><td>4</td><td>1</td></tr><tr><td>Distribution Shift</td><td></td><td></td><td>4</td></tr><tr><td>Domain Adaptation</td><td></td><td>1</td><td>2</td></tr><tr><td>Essay Scoring</td><td>1</td><td></td><td></td></tr><tr><td>Event Argument Extraction</td><td>1</td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td>1</td><td>3</td></tr><tr><td>Federated Learning</td><td></td><td>1</td><td></td></tr><tr><td>Few-shot</td><td>2</td><td>2</td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td>1</td><td></td></tr><tr><td>Fine-tuning</td><td>4</td><td>7</td><td>2</td></tr><tr><td>Foundation Model</td><td></td><td>2</td><td></td></tr><tr><td>GPT</td><td>2</td><td></td><td>1</td></tr><tr><td>GPT-4</td><td>2</td><td></td><td>2</td></tr><tr><td>Generative Adversarial Network</td><td></td><td>2</td><td></td></tr><tr><td>Geometry</td><td></td><td>3</td><td>1</td></tr><tr><td>Graph</td><td></td><td>3</td><td>3</td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>4</td></tr><tr><td>Grounding</td><td></td><td>1</td><td></td></tr><tr><td>Hallucination Detection</td><td>1</td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td>2</td><td></td></tr><tr><td>In-context Learning</td><td>2</td><td></td><td></td></tr><tr><td>Information Retrieval</td><td>1</td><td></td><td></td></tr><tr><td>Instruction Following</td><td></td><td>1</td><td></td></tr><tr><td>Instruction Tuning</td><td></td><td>1</td><td></td></tr><tr><td>Knowledge Distillation</td><td>1</td><td>6</td><td>1</td></tr><tr><td>LLaMA</td><td>1</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>18</td><td>8</td><td>11</td></tr><tr><td>Low-Resource</td><td>1</td><td></td><td></td></tr><tr><td>Mixed Reality (MR)</td><td></td><td>3</td><td></td></tr><tr><td>Multi-modal</td><td>2</td><td>5</td><td>1</td></tr><tr><td>Multiple Instance Learning</td><td>1</td><td>1</td><td></td></tr><tr><td>Mutual Information</td><td></td><td></td><td>1</td></tr><tr><td>Named Entity Recognition</td><td>2</td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td>2</td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>7</td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td>1</td></tr><tr><td>Object Detection</td><td></td><td>4</td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td>1</td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>2</td></tr><tr><td>Prompt</td><td>2</td><td>5</td><td>2</td></tr><tr><td>Pruning</td><td>1</td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td>1</td><td></td></tr><tr><td>Question Answering</td><td>1</td><td>1</td><td></td></tr><tr><td>Reasoning</td><td>4</td><td>2</td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td>2</td></tr><tr><td>Reinforcement Learning</td><td>1</td><td>2</td><td>2</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>2</td><td></td><td></td></tr><tr><td>Relation Extraction</td><td>1</td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td>2</td><td>3</td></tr><tr><td>Sample Size</td><td></td><td></td><td>1</td></tr><tr><td>Security</td><td></td><td></td><td>1</td></tr><tr><td>Self-supervised Learning</td><td></td><td>2</td><td>5</td></tr><tr><td>Simulation</td><td></td><td></td><td>1</td></tr><tr><td>Simulator</td><td></td><td></td><td>1</td></tr><tr><td>Summarization</td><td>2</td><td>1</td><td>1</td></tr><tr><td>Supervised Learning</td><td></td><td>6</td><td>2</td></tr><tr><td>TF-IDF</td><td>1</td><td></td><td></td></tr><tr><td>Table Filling</td><td>1</td><td></td><td></td></tr><tr><td>Text Augmentation</td><td></td><td>1</td><td></td></tr><tr><td>Text Generation</td><td></td><td></td><td>2</td></tr><tr><td>Text2image</td><td></td><td>1</td><td></td></tr><tr><td>Transfer Learning</td><td></td><td>1</td><td></td></tr><tr><td>Transformer</td><td></td><td>6</td><td>2</td></tr><tr><td>Unsupervised Learning</td><td></td><td>5</td><td>4</td></tr><tr><td>Variational Autoencoder</td><td></td><td>1</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>4</td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>5</td><td>1</td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>2</td><td></td></tr><tr><td>Word Embedding</td><td></td><td>1</td><td></td></tr><tr><td>Zero-shot</td><td>2</td><td>6</td><td>1</td></tr><tr><td>Zero-shot Learning</td><td></td><td>2</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-18>cs.CL (18)</h2><h3 id=118--1152-low-resource-machine-translation-through-retrieval-augmented-llm-prompting-a-study-on-the-mambai-language-raphaël-merx-et-al-2024>(1/18 | 1/152) Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language (Raphaël Merx et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raphaël Merx, Aso Mahmudi, Katrina Langford, Leo Alberto de Araujo, Ekaterina Vylomova. (2024)<br><strong>Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language</strong><br><button class=copy-to-clipboard title="Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 120<br>Keywords: Few-shot, Low-Resource, GPT, GPT-4, LLaMA, Neural Machine Translation, Neural Machine Translation, BLEU, Large Language Model, Large Language Model, Prompt, TF-IDF<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04809v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04809v1.pdf filename=2404.04809v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study explores the use of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for translating English into Mambai, a <b>low-resource</b> Austronesian language spoken in Timor-Leste, with approximately 200,000 native speakers. Leveraging a novel corpus derived from a Mambai language manual and additional sentences translated by a native speaker, we examine the efficacy of <b>few-shot</b> <b>LLM</b> <b>prompting</b> for <b>machine</b> <b>translation</b> <b>(MT)</b> in this <b>low-resource</b> context. Our methodology involves the strategic selection of parallel sentences and dictionary entries for <b>prompting,</b> aiming to enhance translation accuracy, using open-source and proprietary <b>LLMs</b> <b>(LlaMa</b> 2 70b, Mixtral 8x7B, <b>GPT-4).</b> We find that including dictionary entries in <b>prompts</b> and a mix of sentences retrieved through <b>TF-IDF</b> and semantic embeddings significantly improves translation quality. However, our findings reveal stark disparities in translation performance across test sets, with <b>BLEU</b> scores reaching as high as 21.2 on materials from the language manual, in contrast to a maximum of 4.4 on a test set provided by a native speaker. These results underscore the importance of diverse and representative corpora in assessing <b>MT</b> for <b>low-resource</b> languages. Our research provides insights into <b>few-shot</b> <b>LLM</b> <b>prompting</b> for <b>low-resource</b> <b>MT,</b> and makes available an initial corpus for the Mambai language.</p></p class="citation"></blockquote><h3 id=218--2152-how-much-reliable-is-chatgpts-prediction-on-information-extraction-under-input-perturbations-ishani-mondal-et-al-2024>(2/18 | 2/152) How much reliable is ChatGPT&rsquo;s prediction on Information Extraction under Input Perturbations? (Ishani Mondal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ishani Mondal, Abhilasha Sancheti. (2024)<br><strong>How much reliable is ChatGPT&rsquo;s prediction on Information Extraction under Input Perturbations?</strong><br><button class=copy-to-clipboard title="How much reliable is ChatGPT's prediction on Information Extraction under Input Perturbations?" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Automatic Evaluation, Few-shot, Zero-shot, ChatGPT, Information Retrieval, Named Entity Recognition, Named Entity Recognition, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05088v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05088v1.pdf filename=2404.05088v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we assess the robustness (reliability) of <b>ChatGPT</b> under input perturbations for one of the most fundamental tasks of <b>Information</b> <b>Extraction</b> (IE) i.e. <b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER).</b> Despite the hype, the majority of the researchers have vouched for its language understanding and generation capabilities; a little attention has been paid to understand its robustness: How the input-perturbations affect 1) the predictions, 2) the confidence of predictions and 3) the quality of rationale behind its prediction. We perform a systematic analysis of <b>ChatGPT&rsquo;s</b> robustness (under both <b>zero-shot</b> and <b>few-shot</b> setup) on two <b>NER</b> datasets using both <b>automatic</b> <b>and</b> human evaluation. Based on <b>automatic</b> <b>evaluation</b> metrics, we find that 1) <b>ChatGPT</b> is more brittle on Drug or Disease replacements (rare entities) compared to the perturbations on widely known Person or Location entities, 2) the quality of explanations for the same entity considerably differ under different types of &ldquo;Entity-Specific&rdquo; and &ldquo;Context-Specific&rdquo; perturbations and the quality can be significantly improved using <b>in-context</b> <b>learning,</b> and 3) it is overconfident for majority of the incorrect predictions, and hence it could lead to misguidance of the end-users.</p></p class="citation"></blockquote><h3 id=318--3152-prompting-large-language-models-for-zero-shot-essay-scoring-via-multi-trait-specialization-sanwoo-lee-et-al-2024>(3/18 | 3/152) Prompting Large Language Models for Zero-shot Essay Scoring via Multi-trait Specialization (Sanwoo Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanwoo Lee, Yida Cai, Desong Meng, Ziyang Wang, Yunfang Wu. (2024)<br><strong>Prompting Large Language Models for Zero-shot Essay Scoring via Multi-trait Specialization</strong><br><button class=copy-to-clipboard title="Prompting Large Language Models for Zero-shot Essay Scoring via Multi-trait Specialization" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Zero-shot, ChatGPT, Essay Scoring, Neural Machine Translation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04941v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04941v1.pdf filename=2404.04941v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances in automated <b>essay</b> <b>scoring</b> (AES) have traditionally relied on labeled <b>essays,</b> <b>requiring</b> tremendous cost and expertise for their acquisition. Recently, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved great success in various tasks, but their potential is less explored in AES. In this paper, we propose Multi Trait Specialization <b>(MTS),</b> a <b>zero-shot</b> <b>prompting</b> framework to elicit <b>essay</b> <b>scoring</b> capabilities in <b>LLMs.</b> Specifically, we leverage <b>ChatGPT</b> to decompose writing proficiency into distinct traits and generate scoring criteria for each trait. Then, an <b>LLM</b> is <b>prompted</b> to extract trait scores from several conversational rounds, each round scoring one of the traits based on the scoring criteria. Finally, we derive the overall score via trait averaging and min-max scaling. Experimental results on two <b>benchmark</b> datasets demonstrate that <b>MTS</b> consistently outperforms straightforward <b>prompting</b> (Vanilla) in average QWK across all <b>LLMs</b> and datasets, with maximum gains of 0.437 on TOEFL11 and 0.355 on ASAP. Additionally, with the help of <b>MTS,</b> the small-sized Llama2-13b-chat substantially outperforms <b>ChatGPT,</b> facilitating an effective deployment in real applications.</p></p class="citation"></blockquote><h3 id=418--4152-fractal-fine-grained-scoring-from-aggregate-text-labels-yukti-makhija-et-al-2024>(4/18 | 4/152) FRACTAL: Fine-Grained Scoring from Aggregate Text Labels (Yukti Makhija et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yukti Makhija, Priyanka Agrawal, Rishi Saket, Aravindan Raghuveer. (2024)<br><strong>FRACTAL: Fine-Grained Scoring from Aggregate Text Labels</strong><br><button class=copy-to-clipboard title="FRACTAL: Fine-Grained Scoring from Aggregate Text Labels" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Multiple Instance Learning, Question Answering, Reasoning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04817v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04817v1.pdf filename=2404.04817v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are being increasingly tuned to power complex generation tasks such as writing, fact-seeking, querying and <b>reasoning.</b> Traditionally, human or model feedback for evaluating and further tuning <b>LLM</b> performance has been provided at the response level, enabling faster and more cost-effective assessments. However, recent works (Amplayo et al. [2022], Wu et al. [2023]) indicate that sentence-level labels may provide more accurate and interpretable feedback for <b>LLM</b> optimization. In this work, we introduce methods to disaggregate response-level labels into sentence-level (pseudo-)labels. Our approach leverages <b>multiple</b> <b>instance</b> <b>learning</b> (MIL) and learning from label proportions (LLP) techniques in conjunction with prior information (e.g., document-sentence cosine similarity) to train a specialized model for sentence-level scoring. We also employ techniques which use model predictions to pseudo-label the train-set at the sentence-level for model training to further improve performance. We conduct extensive evaluations of our methods across six datasets and four tasks: retrieval, <b>question</b> <b>answering,</b> <b>summarization,</b> and math <b>reasoning.</b> Our results demonstrate improved performance compared to <b>multiple</b> <b>baselines</b> <b>across</b> most of these tasks. Our work is the first to develop response-level feedback to sentence-level scoring techniques, leveraging sentence-level prior information, along with comprehensive evaluations on <b>multiple</b> <b>tasks</b> <b>as</b> well as end-to-end <b>finetuning</b> evaluation showing performance comparable to a model trained on fine-grained human annotated labels.</p></p class="citation"></blockquote><h3 id=518--5152-semeval-2024-task-2-safe-biomedical-natural-language-inference-for-clinical-trials-mael-jullien-et-al-2024>(5/18 | 5/152) SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials (Mael Jullien et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mael Jullien, Marco Valentino, André Freitas. (2024)<br><strong>SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials</strong><br><button class=copy-to-clipboard title="SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Natural Language Inference, Natural Language Inference, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04963v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04963v1.pdf filename=2404.04963v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are at the forefront of NLP achievements but fall short in dealing with shortcut learning, factual inconsistency, and vulnerability to adversarial inputs.These shortcomings are especially critical in medical contexts, where they can misrepresent actual model capabilities. Addressing this, we present SemEval-2024 Task 2: Safe Biomedical <b>Natural</b> <b>Language</b> <b>Inference</b> for ClinicalTrials. Our contributions include the refined NLI4CT-P dataset (i.e., <b>Natural</b> <b>Language</b> <b>Inference</b> for Clinical Trials - Perturbed), designed to challenge <b>LLMs</b> with interventional and causal <b>reasoning</b> tasks, along with a comprehensive evaluation of methods and results for participant submissions. A total of 106 participants registered for the task contributing to over 1200 individual submissions and 25 system overview papers. This initiative aims to advance the robustness and applicability of <b>NLI</b> models in healthcare, ensuring safer and more dependable AI assistance in clinical decision-making. We anticipate that the dataset, models, and outcomes of this task can support future research in the field of biomedical <b>NLI.</b> The dataset, competition leaderboard, and website are publicly available.</p></p class="citation"></blockquote><h3 id=618--6152-f-malloc-feed-forward-memory-allocation-for-continual-learning-in-neural-machine-translation-junhong-wu-et-al-2024>(6/18 | 6/152) F-MALLOC: Feed-forward Memory Allocation for Continual Learning in Neural Machine Translation (Junhong Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhong Wu, Yuchen Liu, Chengqing Zong. (2024)<br><strong>F-MALLOC: Feed-forward Memory Allocation for Continual Learning in Neural Machine Translation</strong><br><button class=copy-to-clipboard title="F-MALLOC: Feed-forward Memory Allocation for Continual Learning in Neural Machine Translation" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Continual Learning, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04846v1.pdf filename=2404.04846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the evolving landscape of <b>Neural</b> <b>Machine</b> <b>Translation</b> <b>(NMT),</b> the pretrain-then-finetune paradigm has yielded impressive results. However, the persistent challenge of Catastrophic Forgetting (CF) remains a hurdle. While previous work has introduced <b>Continual</b> <b>Learning</b> (CL) methods to address CF, these approaches grapple with the delicate balance between avoiding forgetting and maintaining system extensibility. To address this, we propose a CL method, named $\textbf{F-MALLOC}$ ($\textbf{F}$eed-forward $\textbf{M}$emory $\textbf{ALLOC}ation)$. F-MALLOC is inspired by recent insights highlighting that feed-forward layers emulate <b>neural</b> <b>memories</b> <b>and</b> encapsulate crucial translation knowledge. It decomposes feed-forward layers into discrete memory cells and allocates these memories to different tasks. By learning to allocate and safeguard these memories, our method effectively alleviates CF while ensuring robust extendability. Besides, we propose a comprehensive assessment protocol for multi-stage CL of <b>NMT</b> systems. Experiments conducted following this new protocol showcase the superior performance of F-MALLOC, evidenced by higher <b>BLEU</b> scores and almost zero forgetting.</p></p class="citation"></blockquote><h3 id=718--7152-advancing-geometric-problem-solving-a-comprehensive-benchmark-for-multimodal-model-evaluation-kai-sun-et-al-2024>(7/18 | 7/152) Advancing Geometric Problem Solving: A Comprehensive Benchmark for Multimodal Model Evaluation (Kai Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Sun, Yushi Bai, Nianyi Lin. (2024)<br><strong>Advancing Geometric Problem Solving: A Comprehensive Benchmark for Multimodal Model Evaluation</strong><br><button class=copy-to-clipboard title="Advancing Geometric Problem Solving: A Comprehensive Benchmark for Multimodal Model Evaluation" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 49<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Claude, GPT, GPT-4, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05091v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05091v1.pdf filename=2404.05091v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we present the MM-MATH dataset, a novel <b>benchmark</b> developed to rigorously evaluate the performance of advanced large language and <b>multimodal</b> models - including but not limited to <b>GPT-4,</b> <b>GPT-4V,</b> and <b>Claude</b> - within the domain of geometric computation. This dataset comprises 5,929 meticulously crafted geometric problems, each paired with a corresponding image, aimed at mirroring the complexity and requirements typical of ninth-grade mathematics. The motivation behind MM-MATH stems from the burgeoning interest and significant strides in <b>multimodal</b> technology, which necessitates a paradigm shift in assessment methodologies from mere outcome analysis to a more holistic evaluation encompassing <b>reasoning</b> and procedural correctness. Despite impressive gains in various <b>benchmark</b> performances, our analysis uncovers a persistent and notable deficiency in these models&rsquo; ability to parse and interpret geometric information accurately from images, accounting for over 60% of observed errors. By deploying a dual-focused evaluation approach, examining both the end results and the underlying problem-solving processes, we unearthed a marked discrepancy between the capabilities of current <b>multimodal</b> models and human-level proficiency. The introduction of MM-MATH represents a tripartite contribution to the field: it not only serves as a comprehensive and challenging <b>benchmark</b> for assessing geometric problem-solving prowess but also illuminates critical gaps in textual and visual comprehension that current models exhibit. Through this endeavor, we aspire to catalyze further research and development aimed at bridging these gaps, thereby advancing the state of <b>multimodal</b> model capabilities to new heights.</p></p class="citation"></blockquote><h3 id=818--8152-mlake-multilingual-knowledge-editing-benchmark-for-large-language-models-zihao-wei-et-al-2024>(8/18 | 8/152) MLaKE: Multilingual Knowledge Editing Benchmark for Large Language Models (Zihao Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihao Wei, Jingcheng Deng, Liang Pang, Hanxing Ding, Huawei Shen, Xueqi Cheng. (2024)<br><strong>MLaKE: Multilingual Knowledge Editing Benchmark for Large Language Models</strong><br><button class=copy-to-clipboard title="MLaKE: Multilingual Knowledge Editing Benchmark for Large Language Models" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 36<br>Keywords: Benchmarking, Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04990v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04990v1.pdf filename=2404.04990v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The extensive utilization of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> underscores the crucial necessity for precise and contemporary knowledge embedded within their intrinsic parameters. Existing research on knowledge editing primarily concentrates on monolingual scenarios, neglecting the complexities presented by multilingual contexts and multi-hop <b>reasoning.</b> To address these challenges, our study introduces MLaKE (Multilingual Language Knowledge Editing), a novel <b>benchmark</b> comprising 4072 multi-hop and 5360 single-hop questions designed to evaluate the adaptability of knowledge editing methods across five languages: English, Chinese, Japanese, French, and German. MLaKE aggregates fact chains from Wikipedia across languages and utilizes <b>LLMs</b> to generate questions in both free-form and multiple-choice. We evaluate the multilingual knowledge editing generalization capabilities of existing methods on MLaKE. Existing knowledge editing methods demonstrate higher success rates in English samples compared to other languages. However, their generalization capabilities are limited in multi-language experiments. Notably, existing knowledge editing methods often show relatively high generalization for languages within the same language family compared to languages from different language families. These results underscore the imperative need for advancements in multilingual knowledge editing and we hope MLaKE can serve as a valuable resource for <b>benchmarking</b> and solution development.</p></p class="citation"></blockquote><h3 id=918--9152-lucky-52-how-many-languages-are-needed-to-instruction-fine-tune-large-language-models-shaoxiong-ji-et-al-2024>(9/18 | 9/152) Lucky 52: How Many Languages Are Needed to Instruction Fine-Tune Large Language Models? (Shaoxiong Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaoxiong Ji, Pinzhen Chen. (2024)<br><strong>Lucky 52: How Many Languages Are Needed to Instruction Fine-Tune Large Language Models?</strong><br><button class=copy-to-clipboard title="Lucky 52: How Many Languages Are Needed to Instruction Fine-Tune Large Language Models?" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04850v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04850v1.pdf filename=2404.04850v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> <b>large</b> <b>language</b> <b>models</b> for multilingual downstream tasks requires a diverse set of languages to capture the nuances and structures of different linguistic contexts effectively. While the specific number varies depending on the desired scope and target languages, we argue that the number of languages, language exposure, and similarity that incorporate the selection of languages for <b>fine-tuning</b> are some important aspects to examine. By <b>fine-tuning</b> <b>large</b> <b>multilingual</b> <b>models</b> on 1 to 52 languages, this paper answers one question: How many languages are needed in instruction <b>fine-tuning</b> for multilingual tasks? We investigate how multilingual instruction <b>fine-tuned</b> models behave on multilingual <b>benchmarks</b> with an increasing number of languages and discuss our findings from the perspective of language exposure and similarity.</p></p class="citation"></blockquote><h3 id=1018--10152-towards-understanding-the-influence-of-reward-margin-on-preference-model-performance-bowen-qin-et-al-2024>(10/18 | 10/152) Towards Understanding the Influence of Reward Margin on Preference Model Performance (Bowen Qin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Qin, Duanyu Feng, Xi Yang. (2024)<br><strong>Towards Understanding the Influence of Reward Margin on Preference Model Performance</strong><br><button class=copy-to-clipboard title="Towards Understanding the Influence of Reward Margin on Preference Model Performance" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04932v1.pdf filename=2404.04932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> is a widely used framework for the training of language models. However, the process of using <b>RLHF</b> to develop a language model that is well-aligned presents challenges, especially when it comes to optimizing the reward model. Our research has found that existing reward models, when trained using the traditional ranking objective based on human preference data, often struggle to effectively distinguish between responses that are more or less favorable in real-world scenarios. To bridge this gap, our study introduces a novel method to estimate the preference differences without the need for detailed, exhaustive labels from human annotators. Our experimental results provide empirical evidence that incorporating margin values into the training process significantly improves the effectiveness of reward models. This comparative analysis not only demonstrates the superiority of our approach in terms of reward prediction accuracy but also highlights its effectiveness in practical applications.</p></p class="citation"></blockquote><h3 id=1118--11152-radial-networks-dynamic-layer-routing-for-high-performance-large-language-models-jordan-dotzel-et-al-2024>(11/18 | 11/152) Radial Networks: Dynamic Layer Routing for High-Performance Large Language Models (Jordan Dotzel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jordan Dotzel, Yash Akhauri, Ahmed S. AbouElhamayed, Carly Jiang, Mohamed Abdelfattah, Zhiru Zhang. (2024)<br><strong>Radial Networks: Dynamic Layer Routing for High-Performance Large Language Models</strong><br><button class=copy-to-clipboard title="Radial Networks: Dynamic Layer Routing for High-Performance Large Language Models" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04900v1.pdf filename=2404.04900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> often struggle with strict memory, latency, and power demands. To meet these demands, various forms of dynamic sparsity have been proposed that reduce compute on an input-by-input basis. These methods improve over static methods by exploiting the variance across individual inputs, which has steadily grown with the exponential increase in training data. Yet, the increasing depth within modern models, currently with hundreds of layers, has opened opportunities for dynamic layer sparsity, which skips the computation for entire layers. In this work, we explore the practicality of layer sparsity by profiling residual connections and establish the relationship between model depth and layer sparsity. For example, the residual blocks in the OPT-66B model have a median contribution of 5% to its output. We then take advantage of this dynamic sparsity and propose Radial Networks, which perform token-level routing between layers guided by a trained router module. These networks can be used in a post-training <b>distillation</b> from sequential networks or trained from scratch to co-learn the router and layer weights. They enable scaling to larger model sizes by decoupling the number of layers from the dynamic depth of the network, and their design allows for layer reuse. By varying the compute token by token, they reduce the overall resources needed for generating entire sequences. Overall, this leads to larger capacity networks with significantly lower compute and serving costs for <b>large</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=1218--12152-data-bias-according-to-bipol-men-are-naturally-right-and-it-is-the-role-of-women-to-follow-their-lead-irene-pagliai-et-al-2024>(12/18 | 12/152) Data Bias According to Bipol: Men are Naturally Right and It is the Role of Women to Follow Their Lead (Irene Pagliai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Irene Pagliai, Goya van Boven, Tosin Adewumi, Lama Alkhaled, Namrata Gurung, Isabella Södergren, Elisa Barney. (2024)<br><strong>Data Bias According to Bipol: Men are Naturally Right and It is the Role of Women to Follow Their Lead</strong><br><button class=copy-to-clipboard title="Data Bias According to Bipol: Men are Naturally Right and It is the Role of Women to Follow Their Lead" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04838v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04838v1.pdf filename=2404.04838v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce new <b>large</b> <b>labeled</b> <b>datasets</b> on bias in 3 languages and show in experiments that bias exists in all 10 datasets of 5 languages evaluated, including <b>benchmark</b> datasets on the English GLUE/SuperGLUE leaderboards. The 3 new languages give a total of almost 6 million labeled samples and we <b>benchmark</b> on these datasets using SotA multilingual pretrained models: mT5 and mBERT. The challenge of social bias, based on prejudice, is ubiquitous, as recent events with AI and <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown. Motivated by this challenge, we set out to estimate bias in multiple datasets. We compare some recent bias metrics and use bipol, which has explainability in the metric. We also confirm the unverified assumption that bias exists in toxic comments by randomly sampling 200 samples from a toxic dataset population using the confidence level of 95% and error margin of 7%. Thirty gold samples were randomly distributed in the 200 samples to secure the quality of the annotation. Our findings confirm that many of the datasets have male bias (prejudice against women), besides other types of bias. We publicly release our new datasets, lexica, models, and codes.</p></p class="citation"></blockquote><h3 id=1318--13152-seer-moe-sparse-expert-efficiency-through-regularization-for-mixture-of-experts-alexandre-muzio-et-al-2024>(13/18 | 13/152) SEER-MoE: Sparse Expert Efficiency through Regularization for Mixture-of-Experts (Alexandre Muzio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandre Muzio, Alex Sun, Churan He. (2024)<br><strong>SEER-MoE: Sparse Expert Efficiency through Regularization for Mixture-of-Experts</strong><br><button class=copy-to-clipboard title="SEER-MoE: Sparse Expert Efficiency through Regularization for Mixture-of-Experts" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05089v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05089v1.pdf filename=2404.05089v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancement of deep learning has led to the emergence of Mixture-of-Experts (MoEs) models, known for their dynamic allocation of computational resources based on input. Despite their promise, MoEs face challenges, particularly in terms of memory requirements. To address this, our work introduces SEER-MoE, a novel two-stage framework for reducing both the memory footprint and compute requirements of pre-trained MoE models. The first stage involves <b>pruning</b> the total number of experts using a heavy-hitters counting guidance, while the second stage employs a regularization-based <b>fine-tuning</b> strategy to recover accuracy loss and reduce the number of activated experts during inference. Our empirical studies demonstrate the effectiveness of our method, resulting in a sparse MoEs model optimized for inference efficiency with minimal accuracy trade-offs.</p></p class="citation"></blockquote><h3 id=1418--14152-a-two-dimensional-feature-engineering-method-for-relation-extraction-hao-wang-et-al-2024>(14/18 | 14/152) A Two Dimensional Feature Engineering Method for Relation Extraction (Hao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Wang, Yanping Chen, Weizhe Yang, Yongbin Qin, Ruizhang Huang. (2024)<br><strong>A Two Dimensional Feature Engineering Method for Relation Extraction</strong><br><button class=copy-to-clipboard title="A Two Dimensional Feature Engineering Method for Relation Extraction" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Relation Extraction, Table Filling<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04959v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04959v1.pdf filename=2404.04959v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Transforming a sentence into a two-dimensional (2D) representation (e.g., the <b>table</b> <b>filling)</b> has the ability to unfold a semantic plane, where an element of the plane is a word-pair representation of a sentence which may denote a possible <b>relation</b> <b>representation</b> composed of two named entities. The 2D representation is effective in resolving overlapped <b>relation</b> <b>instances.</b> However, in related works, the representation is directly transformed from a raw input. It is weak to utilize prior knowledge, which is important to support the <b>relation</b> <b>extraction</b> task. In this paper, we propose a two-dimensional feature engineering method in the 2D sentence representation for <b>relation</b> <b>extraction.</b> Our proposed method is evaluated on three public datasets (ACE05 Chinese, ACE05 English, and SanWen) and achieves the state-of-the-art performance. The results indicate that two-dimensional feature engineering can take advantage of a two-dimensional sentence representation and make full use of prior knowledge in traditional feature engineering. Our code is publicly available at <a href=https://github.com/Wang-ck123/A-Two-Dimensional-Feature-Engineering-Method-for-Entity-Relation-Extraction>https://github.com/Wang-ck123/A-Two-Dimensional-Feature-Engineering-Method-for-Entity-Relation-Extraction</a></p></p class="citation"></blockquote><h3 id=1518--15152-silversight-a-multi-task-chinese-financial-large-language-model-based-on-adaptive-semantic-space-learning-yuhang-zhou-et-al-2024>(15/18 | 15/152) SilverSight: A Multi-Task Chinese Financial Large Language Model Based on Adaptive Semantic Space Learning (Yuhang Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhang Zhou, Zeping Li, Siyu Tian, Yuchen Ni, Sen Liu, Guangnan Ye, Hongfeng Chai. (2024)<br><strong>SilverSight: A Multi-Task Chinese Financial Large Language Model Based on Adaptive Semantic Space Learning</strong><br><button class=copy-to-clipboard title="SilverSight: A Multi-Task Chinese Financial Large Language Model Based on Adaptive Semantic Space Learning" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CE, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04949v1.pdf filename=2404.04949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are increasingly being applied across various specialized fields, leveraging their extensive knowledge to empower a multitude of scenarios within these domains. However, each field encompasses a variety of specific tasks that require learning, and the diverse, heterogeneous data across these domains can lead to conflicts during model task transfer. In response to this challenge, our study introduces an Adaptive Semantic Space Learning (ASSL) framework, which utilizes the adaptive reorganization of data distributions within the semantic space to enhance the performance and selection efficacy of multi-expert models. Utilizing this framework, we trained a financial multi-task <b>LLM</b> named &ldquo;SilverSight&rdquo;. Our research findings demonstrate that our framework can achieve results close to those obtained with full data training using only 10% of the data, while also exhibiting strong generalization capabilities.</p></p class="citation"></blockquote><h3 id=1618--16152-multilingual-large-language-model-a-survey-of-resources-taxonomy-and-frontiers-libo-qin-et-al-2024>(16/18 | 16/152) Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers (Libo Qin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Libo Qin, Qiguang Chen, Yuhang Zhou, Zhi Chen, Yinghui Li, Lizi Liao, Min Li, Wanxiang Che, Philip S. Yu. (2024)<br><strong>Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers</strong><br><button class=copy-to-clipboard title="Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04925v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04925v1.pdf filename=2404.04925v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multilingual <b>Large</b> <b>Language</b> <b>Models</b> are capable of using powerful <b>Large</b> <b>Language</b> <b>Models</b> to handle and respond to queries in multiple languages, which achieves remarkable success in multilingual natural language processing tasks. Despite these breakthroughs, there still remains a lack of a comprehensive survey to <b>summarize</b> existing approaches and recent developments in this field. To this end, in this paper, we present a thorough review and provide a unified perspective to <b>summarize</b> the recent progress as well as emerging trends in multilingual <b>large</b> <b>language</b> <b>models</b> (MLLMs) literature. The contributions of this paper can be <b>summarized:</b> (1) First survey: to our knowledge, we take the first step and present a thorough review in MLLMs research field according to multi-lingual alignment; (2) New taxonomy: we offer a new and unified perspective to <b>summarize</b> the current progress of MLLMs; (3) New frontiers: we highlight several emerging frontiers and discuss the corresponding challenges; (4) Abundant resources: we collect abundant open-source resources, including relevant papers, data corpora, and leaderboards. We hope our work can provide the community with quick access and spur breakthrough research in MLLMs.</p></p class="citation"></blockquote><h3 id=1718--17152-slpl-shroom-at-semeval2024-task-06-a-comprehensive-study-on-models-ability-to-detect-hallucination-pouya-fallah-et-al-2024>(17/18 | 17/152) SLPL SHROOM at SemEval2024 Task 06: A comprehensive study on models ability to detect hallucination (Pouya Fallah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pouya Fallah, Soroush Gooran, Mohammad Jafarinasab, Pouya Sadeghi, Reza Farnia, Amirreza Tarabkhah, Zainab Sadat Taghavi, Hossein Sameti. (2024)<br><strong>SLPL SHROOM at SemEval2024 Task 06: A comprehensive study on models ability to detect hallucination</strong><br><button class=copy-to-clipboard title="SLPL SHROOM at SemEval2024 Task 06: A comprehensive study on models ability to detect hallucination" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Hallucination Detection, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04845v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04845v2.pdf filename=2404.04845v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models, particularly generative models, are susceptible to <b>hallucinations,</b> <b>generating</b> outputs that contradict factual knowledge or the source text. This study explores methods for detecting <b>hallucinations</b> <b>in</b> three SemEval-2024 Task 6 tasks: <b>Machine</b> <b>Translation,</b> Definition Modeling, and Paraphrase Generation. We evaluate two methods: semantic similarity between the generated text and factual references, and an ensemble of language models that judge each other&rsquo;s outputs. Our results show that semantic similarity achieves moderate accuracy and correlation scores in trial data, while the ensemble method offers insights into the complexities of <b>hallucination</b> <b>detection</b> but falls short of expectations. This work highlights the challenges of <b>hallucination</b> <b>detection</b> and underscores the need for further research in this critical area.</p></p class="citation"></blockquote><h3 id=1818--18152-generating-uncontextualized-and-contextualized-questions-for-document-level-event-argument-extraction-md-nayem-uddin-et-al-2024>(18/18 | 18/152) Generating Uncontextualized and Contextualized Questions for Document-Level Event Argument Extraction (Md Nayem Uddin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Nayem Uddin, Enfa Rose George, Eduardo Blanco, Steven Corman. (2024)<br><strong>Generating Uncontextualized and Contextualized Questions for Document-Level Event Argument Extraction</strong><br><button class=copy-to-clipboard title="Generating Uncontextualized and Contextualized Questions for Document-Level Event Argument Extraction" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Event Argument Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04770v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04770v1.pdf filename=2404.04770v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents multiple question generation strategies for document-level <b>event</b> <b>argument</b> <b>extraction.</b> These strategies do not require human involvement and result in uncontextualized questions as well as contextualized questions grounded on the <b>event</b> <b>and</b> <b>document</b> of interest. Experimental results show that combining uncontextualized and contextualized questions is beneficial, especially when <b>event</b> <b>triggers</b> <b>and</b> arguments appear in different sentences. Our approach does not have corpus-specific components, in particular, the question generation strategies transfer across corpora. We also present a qualitative analysis of the most common errors made by our best model.</p></p class="citation"></blockquote><h2 id=cscv-47>cs.CV (47)</h2><h3 id=147--19152-vmambamorph-a-visual-mamba-based-framework-with-cross-scan-module-for-deformable-3d-image-registration-ziyang-wang-et-al-2024>(1/47 | 19/152) VMambaMorph: a Visual Mamba-based Framework with Cross-Scan Module for Deformable 3D Image Registration (Ziyang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyang Wang, Jian-Qing Zheng, Chao Ma, Tao Guo. (2024)<br><strong>VMambaMorph: a Visual Mamba-based Framework with Cross-Scan Module for Deformable 3D Image Registration</strong><br><button class=copy-to-clipboard title="VMambaMorph: a Visual Mamba-based Framework with Cross-Scan Module for Deformable 3D Image Registration" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 83<br>Keywords: Convolutional Neural Network, Convolutional Neural Network, Vision Transformer, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Mixed Reality (MR), Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05105v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05105v1.pdf filename=2404.05105v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image registration, a critical process in medical imaging, involves aligning different sets of medical imaging data into a single unified coordinate system. Deep learning networks, such as the <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)-based</b> VoxelMorph, <b>Vision</b> <b>Transformer</b> (ViT)-based TransMorph, and State Space Model (SSM)-based MambaMorph, have demonstrated effective performance in this domain. The recent Visual State Space Model (VMamba), which incorporates a cross-scan module with SSM, has exhibited promising improvements in modeling global-range dependencies with efficient computational cost in computer <b>vision</b> <b>tasks.</b> This paper hereby introduces an exploration of VMamba with image registration, named VMambaMorph. This novel hybrid VMamba-CNN network is designed specifically for 3D image registration. Utilizing a U-shaped network architecture, VMambaMorph computes the deformation field based on target and source volumes. The VMamba-based block with 2D cross-scan module is redesigned for 3D volumetric feature processing, and a fine-grained feature extraction module is proposed for high-dimensional feature learning. We validate VMambaMorph using a public <b>benchmark</b> brain <b>MR-CT</b> registration dataset, comparing its performance against current state-of-the-art methods. The results indicate that VMambaMorph achieves competitive registration quality. The code for VMambaMorph is available on GitHub.</p></p class="citation"></blockquote><h3 id=247--20152-gvt-a-graph-based-vision-transformer-with-talking-heads-utilizing-sparsity-trained-from-scratch-on-small-datasets-dongjing-shan-et-al-2024>(2/47 | 20/152) GvT: A Graph-based Vision Transformer with Talking-Heads Utilizing Sparsity, Trained from Scratch on Small Datasets (Dongjing Shan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongjing Shan, guiqiang chen. (2024)<br><strong>GvT: A Graph-based Vision Transformer with Talking-Heads Utilizing Sparsity, Trained from Scratch on Small Datasets</strong><br><button class=copy-to-clipboard title="GvT: A Graph-based Vision Transformer with Talking-Heads Utilizing Sparsity, Trained from Scratch on Small Datasets" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 83<br>Keywords: Convolutional Neural Network, Convolutional Neural Network, Vision Transformer, Graph, Convolution, Convolutional Neural Network, Convolutional Neural Network, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04924v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04924v1.pdf filename=2404.04924v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision</b> <b>Transformers</b> (ViTs) have achieved impressive results in large-scale image classification. However, when training from scratch on small datasets, there is still a significant performance gap between ViTs and <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs),</b> which is attributed to the lack of inductive bias. To address this issue, we propose a <b>Graph-based</b> <b>Vision</b> <b>Transformer</b> (GvT) that utilizes <b>graph</b> <b>convolutional</b> <b>projection</b> <b>and</b> <b>graph-pooling.</b> In each block, queries and keys are calculated through <b>graph</b> <b>convolutional</b> <b>projection</b> <b>based</b> on the spatial adjacency matrix, while dot-product attention is used in another <b>graph</b> <b>convolution</b> to generate values. When using more attention heads, the queries and keys become lower-dimensional, making their dot product an uninformative matching function. To overcome this low-rank bottleneck in attention heads, we employ talking-heads technology based on bilinear pooled features and sparse selection of attention tensors. This allows interaction among filtered attention scores and enables each attention mechanism to depend on all queries and keys. Additionally, we apply <b>graph-pooling</b> between two intermediate blocks to reduce the number of tokens and aggregate semantic information more effectively. Our experimental results show that GvT produces comparable or superior outcomes to deep <b>convolutional</b> <b>networks</b> <b>and</b> surpasses <b>vision</b> <b>transformers</b> without pre-training on large datasets. The code for our proposed model is publicly available on the website.</p></p class="citation"></blockquote><h3 id=347--21152-pairaug-what-can-augmented-image-text-pairs-do-for-radiology-yutong-xie-et-al-2024>(3/47 | 21/152) PairAug: What Can Augmented Image-Text Pairs Do for Radiology? (Yutong Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yutong Xie, Qi Chen, Sinuo Wang, Minh-Son To, Iris Lee, Ee Win Khoo, Kerolos Hendy, Daniel Koh, Yong Xia, Qi Wu. (2024)<br><strong>PairAug: What Can Augmented Image-Text Pairs Do for Radiology?</strong><br><button class=copy-to-clipboard title="PairAug: What Can Augmented Image-Text Pairs Do for Radiology?" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 80<br>Keywords: Data Augmentation, Fine-tuning, Zero-shot, Image2text, Text Augmentation, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04960v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04960v1.pdf filename=2404.04960v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current <b>vision-language</b> pre-training (VLP) methodologies predominantly depend on paired <b>image-text</b> datasets, a resource that is challenging to acquire in radiology due to privacy considerations and labelling complexities. <b>Data</b> <b>augmentation</b> provides a practical solution to overcome the issue of <b>data</b> <b>scarcity,</b> however, most augmentation methods exhibit a limited focus, prioritising either image or <b>text</b> <b>augmentation</b> exclusively. Acknowledging this limitation, our objective is to devise a framework capable of concurrently augmenting medical image and <b>text</b> <b>data.</b> <b>We</b> design a Pairwise Augmentation (PairAug) approach that contains an Inter-patient Augmentation (InterAug) branch and an Intra-patient Augmentation (IntraAug) branch. Specifically, the InterAug branch of our approach generates radiology images using synthesised yet plausible reports derived from a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM).</b> The generated pairs can be considered a collection of new patient cases since they are artificially created and may not exist in the original dataset. In contrast, the IntraAug branch uses newly generated reports to manipulate images. This process allows us to create new paired <b>data</b> <b>for</b> each individual with diverse medical conditions. Our extensive experiments on various downstream tasks covering medical image classification <b>zero-shot</b> and <b>fine-tuning</b> analysis demonstrate that our PairAug, concurrently expanding both image and <b>text</b> <b>data,</b> <b>substantially</b> outperforms image-/text-only expansion baselines and advanced medical VLP baselines. Our code is released at \url{https://github.com/YtongXie/PairAug}.</p></p class="citation"></blockquote><h3 id=447--22152-bootstrapping-chest-ct-image-understanding-by-distilling-knowledge-from-x-ray-expert-models-weiwei-cao-et-al-2024>(4/47 | 22/152) Bootstrapping Chest CT Image Understanding by Distilling Knowledge from X-ray Expert Models (Weiwei Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiwei Cao, Jianpeng Zhang, Yingda Xia, Tony C. W. Mok, Zi Li, Xianghua Ye, Le Lu, Jian Zheng, Yuxing Tang, Ling Zhang. (2024)<br><strong>Bootstrapping Chest CT Image Understanding by Distilling Knowledge from X-ray Expert Models</strong><br><button class=copy-to-clipboard title="Bootstrapping Chest CT Image Understanding by Distilling Knowledge from X-ray Expert Models" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Contrastive Learning, Fine-tuning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Zero-shot, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04936v1.pdf filename=2404.04936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Radiologists highly desire fully automated versatile AI for medical imaging interpretation. However, the lack of extensively annotated large-scale multi-disease datasets has hindered the achievement of this goal. In this paper, we explore the feasibility of leveraging language as a naturally high-quality supervision for chest CT imaging. In light of the limited availability of image-report pairs, we bootstrap the understanding of 3D chest CT images by <b>distilling</b> chest-related diagnostic <b>knowledge</b> <b>from</b> an extensively pre-trained 2D X-ray expert model. Specifically, we propose a language-guided retrieval method to match each 3D CT image with its semantically closest 2D X-ray image, and perform pair-wise and semantic relation <b>knowledge</b> <b>distillation.</b> Subsequently, we use <b>contrastive</b> <b>learning</b> to align images and reports within the same patient while distinguishing them from the other patients. However, the challenge arises when patients have similar semantic diagnoses, such as healthy patients, potentially confusing if treated as negatives. We introduce a robust <b>contrastive</b> <b>learning</b> that identifies and corrects these false negatives. We train our model with over 12,000 pairs of chest CT images and radiology reports. Extensive experiments across multiple scenarios, including <b>zero-shot</b> <b>learning,</b> report generation, and <b>fine-tuning</b> processes, demonstrate the model&rsquo;s feasibility in interpreting chest CT images.</p></p class="citation"></blockquote><h3 id=547--23152-genearl-a-training-free-generative-framework-for-multimodal-event-argument-role-labeling-hritik-bansal-et-al-2024>(5/47 | 23/152) GenEARL: A Training-Free Generative Framework for Multimodal Event Argument Role Labeling (Hritik Bansal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hritik Bansal, Po-Nien Kung, P. Jeffrey Brantingham, Kai-Wei Chang, Nanyun Peng. (2024)<br><strong>GenEARL: A Training-Free Generative Framework for Multimodal Event Argument Role Labeling</strong><br><button class=copy-to-clipboard title="GenEARL: A Training-Free Generative Framework for Multimodal Event Argument Role Labeling" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 66<br>Keywords: Multi-modal, Multi-modal, Zero-shot, Reasoning, Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04763v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04763v1.pdf filename=2404.04763v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> event argument role labeling (EARL), a task that assigns a role for each event participant (object) in an image is a complex challenge. It requires <b>reasoning</b> over the entire image, the depicted event, and the interactions between various objects participating in the event. Existing models heavily rely on high-quality event-annotated training data to understand the event semantics and structures, and they fail to generalize to new event types and domains. In this paper, we propose GenEARL, a training-free generative framework that harness the power of the modern generative models to understand event task descriptions given image contexts to perform the EARL task. Specifically, GenEARL comprises two stages of generative <b>prompting</b> with a frozen <b>vision-language</b> model (VLM) and a frozen <b>large</b> <b>language</b> <b>model</b> <b>(LLM).</b> First, a generative VLM learns the semantics of the event argument roles and generates event-centric object descriptions based on the image. Subsequently, a <b>LLM</b> is <b>prompted</b> with the generated object descriptions with a predefined template for EARL (i.e., assign an object with an event argument role). We show that GenEARL outperforms the contrastive pretraining (CLIP) baseline by 9.4% and 14.2% accuracy for <b>zero-shot</b> EARL on the M2E2 and SwiG datasets, respectively. In addition, we outperform CLIP-Event by 22% precision on M2E2 dataset. The framework also allows flexible adaptation and generalization to unseen domains.</p></p class="citation"></blockquote><h3 id=647--24152-fpl-filtered-pseudo-label-based-unsupervised-cross-modality-adaptation-for-3d-medical-image-segmentation-jianghao-wu-et-al-2024>(6/47 | 24/152) FPL+: Filtered Pseudo Label-based Unsupervised Cross-Modality Adaptation for 3D Medical Image Segmentation (Jianghao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianghao Wu, Dong Guo, Guotai Wang, Qiang Yue, Huijun Yu, Kang Li, Shaoting Zhang. (2024)<br><strong>FPL+: Filtered Pseudo Label-based Unsupervised Cross-Modality Adaptation for 3D Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="FPL+: Filtered Pseudo Label-based Unsupervised Cross-Modality Adaptation for 3D Medical Image Segmentation" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Adversarial Learning, Data Augmentation, Multi-modal, Supervised Learning, Supervised Learning, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04971v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04971v1.pdf filename=2404.04971v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adapting a medical image segmentation model to a new <b>domain</b> <b>is</b> important for improving its cross-domain transferability, and due to the expensive annotation process, <b>Unsupervised</b> <b>Domain</b> <b>Adaptation</b> (UDA) is appealing where only unlabeled images are needed for the adaptation. Existing UDA methods are mainly based on image or feature alignment with <b>adversarial</b> <b>training</b> for regularization, and they are limited by insufficient supervision in the target <b>domain.</b> <b>In</b> this paper, we propose an enhanced Filtered Pseudo Label (FPL+)-based UDA method for 3D medical image segmentation. It first uses cross-domain <b>data</b> <b>augmentation</b> to translate labeled images in the source <b>domain</b> <b>to</b> a dual-domain training set consisting of a pseudo source-domain set and a pseudo target-domain set. To leverage the dual-domain augmented images to train a pseudo label generator, <b>domain-specific</b> <b>batch</b> normalization layers are used to deal with the <b>domain</b> <b>shift</b> while learning the <b>domain-invariant</b> <b>structure</b> features, generating high-quality pseudo labels for target-domain images. We then combine labeled source-domain images and target-domain images with pseudo labels to train a final segmentor, where image-level weighting based on uncertainty estimation and pixel-level weighting based on dual-domain consensus are proposed to mitigate the adverse effect of noisy pseudo labels. Experiments on three public <b>multi-modal</b> datasets for Vestibular Schwannoma, brain tumor and whole heart segmentation show that our method surpassed ten state-of-the-art UDA methods, and it even achieved better results than fully <b>supervised</b> <b>learning</b> in the target <b>domain</b> <b>in</b> some cases.</p></p class="citation"></blockquote><h3 id=747--25152-dinobloom-a-foundation-model-for-generalizable-cell-embeddings-in-hematology-valentin-koch-et-al-2024>(7/47 | 25/152) DinoBloom: A Foundation Model for Generalizable Cell Embeddings in Hematology (Valentin Koch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Valentin Koch, Sophia J. Wagner, Salome Kazeminia, Ece Sancar, Matthias Hehr, Julia Schnabel, Tingying Peng, Carsten Marr. (2024)<br><strong>DinoBloom: A Foundation Model for Generalizable Cell Embeddings in Hematology</strong><br><button class=copy-to-clipboard title="DinoBloom: A Foundation Model for Generalizable Cell Embeddings in Hematology" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Foundation Model, Multiple Instance Learning, Supervised Learning, Transfer Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05022v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05022v1.pdf filename=2404.05022v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In hematology, computational models offer significant potential to improve diagnostic accuracy, streamline workflows, and reduce the tedious work of analyzing single cells in peripheral blood or bone marrow smears. However, clinical adoption of computational models has been hampered by the lack of generalization due to large batch effects, small dataset sizes, and poor performance in <b>transfer</b> <b>learning</b> from natural images. To address these challenges, we introduce DinoBloom, the first <b>foundation</b> <b>model</b> for single cell images in hematology, utilizing a tailored DINOv2 pipeline. Our model is built upon an extensive collection of 13 diverse, publicly available datasets of peripheral blood and bone marrow smears, the most substantial open-source cohort in hematology so far, comprising over 380,000 white blood cell images. To assess its generalization capability, we evaluate it on an external dataset with a challenging domain shift. We show that our model outperforms existing medical and non-medical vision models in (i) linear probing and k-nearest neighbor evaluations for cell-type classification on blood and bone marrow smears and (ii) weakly <b>supervised</b> <b>multiple</b> <b>instance</b> <b>learning</b> for acute myeloid leukemia subtyping by a large margin. A family of four DinoBloom models (small, base, large, and giant) can be adapted for a wide range of downstream applications, be a strong baseline for classification problems, and facilitate the assessment of batch effects in new datasets. All models are available at github.com/marrlab/DinoBloom.</p></p class="citation"></blockquote><h3 id=847--26152-monotakd-teaching-assistant-knowledge-distillation-for-monocular-3d-object-detection-hou-i-liu-et-al-2024>(8/47 | 26/152) MonoTAKD: Teaching Assistant Knowledge Distillation for Monocular 3D Object Detection (Hou-I Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hou-I Liu, Christine Wu, Jen-Hao Cheng, Wenhao Chai, Shian-Yun Wang, Gaowen Liu, Jenq-Neng Hwang, Hong-Han Shuai, Wen-Huang Cheng. (2024)<br><strong>MonoTAKD: Teaching Assistant Knowledge Distillation for Monocular 3D Object Detection</strong><br><button class=copy-to-clipboard title="MonoTAKD: Teaching Assistant Knowledge Distillation for Monocular 3D Object Detection" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 48<br>Keywords: Object Detection, Benchmarking, Geometry, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04910v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04910v1.pdf filename=2404.04910v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monocular 3D <b>object</b> <b>detection</b> (Mono3D) is an indispensable research topic in autonomous driving, thanks to the cost-effective monocular camera sensors and its wide range of applications. Since the image perspective has depth ambiguity, the challenges of Mono3D lie in understanding 3D scene <b>geometry</b> and reconstructing 3D <b>object</b> <b>information</b> from a single image. Previous methods attempted to transfer 3D information directly from the LiDAR-based teacher to the camera-based student. However, a considerable gap in feature representation makes direct cross-modal <b>distillation</b> inefficient, resulting in a significant performance deterioration between the LiDAR-based teacher and the camera-based student. To address this issue, we propose the Teaching Assistant <b>Knowledge</b> <b>Distillation</b> (MonoTAKD) to break down the learning objective by integrating intra-modal <b>distillation</b> with cross-modal residual <b>distillation.</b> In particular, we employ a strong camera-based teaching assistant model to <b>distill</b> powerful visual <b>knowledge</b> <b>effectively</b> through intra-modal <b>distillation.</b> Subsequently, we introduce the cross-modal residual <b>distillation</b> to transfer the 3D spatial cues. By acquiring both visual <b>knowledge</b> <b>and</b> 3D spatial cues, the predictions of our approach are rigorously evaluated on the KITTI 3D <b>object</b> <b>detection</b> <b>benchmark</b> and achieve state-of-the-art performance in Mono3D.</p></p class="citation"></blockquote><h3 id=947--27152-facial-affective-behavior-analysis-with-instruction-tuning-yifan-li-et-al-2024>(9/47 | 27/152) Facial Affective Behavior Analysis with Instruction Tuning (Yifan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Li, Anh Dao, Wentao Bao, Zhen Tan, Tianlong Chen, Huan Liu, Yu Kong. (2024)<br><strong>Facial Affective Behavior Analysis with Instruction Tuning</strong><br><button class=copy-to-clipboard title="Facial Affective Behavior Analysis with Instruction Tuning" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Benchmarking, Multi-modal, Instruction Following, Reasoning, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05052v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05052v1.pdf filename=2404.05052v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Facial affective behavior analysis (FABA) is crucial for understanding human mental states from images. However, traditional approaches primarily deploy models to discriminate among discrete emotion categories, and lack the fine granularity and <b>reasoning</b> capability for complex facial behaviors. The advent of <b>Multi-modal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) has been proven successful in general visual understanding tasks. However, directly harnessing MLLMs for FABA is challenging due to the scarcity of datasets and <b>benchmarks,</b> neglecting facial prior knowledge, and low training efficiency. To address these challenges, we introduce (i) an <b>instruction-following</b> <b>dataset</b> for two FABA tasks, e.g., emotion and action unit recognition, (ii) a <b>benchmark</b> FABA-Bench with a new metric considering both recognition and generation ability, and (iii) a new MLLM &ldquo;EmoLA&rdquo; as a strong baseline to the community. Our initiative on the dataset and <b>benchmarks</b> reveal the nature and rationale of facial affective behaviors, i.e., fine-grained facial movement, interpretability, and <b>reasoning.</b> Moreover, to build an effective and efficient FABA MLLM, we introduce a facial prior expert module with face structure knowledge and a low-rank adaptation module into pre-trained MLLM. We conduct extensive experiments on FABA-Bench and four commonly-used FABA datasets. The results demonstrate that the proposed facial prior expert can boost the performance and EmoLA achieves the best results on our FABA-Bench. On commonly-used FABA datasets, EmoLA is competitive rivaling task-specific state-of-the-art models.</p></p class="citation"></blockquote><h3 id=1047--28152-weakly-supervised-deep-hyperspherical-quantization-for-image-retrieval-jinpeng-wang-et-al-2024>(10/47 | 28/152) Weakly Supervised Deep Hyperspherical Quantization for Image Retrieval (Jinpeng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinpeng Wang, Bin Chen, Qiang Zhang, Zaiqiao Meng, Shangsong Liang, Shu-Tao Xia. (2024)<br><strong>Weakly Supervised Deep Hyperspherical Quantization for Image Retrieval</strong><br><button class=copy-to-clipboard title="Weakly Supervised Deep Hyperspherical Quantization for Image Retrieval" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-IR, cs.CV<br>Keyword Score: 43<br>Keywords: Graph, Quantization, Supervised Learning, Weakly-supervised Learning, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04998v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04998v1.pdf filename=2404.04998v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>quantization</b> methods have shown high efficiency on large-scale image retrieval. However, current models heavily rely on ground-truth information, hindering the application of <b>quantization</b> in label-hungry scenarios. A more realistic demand is to learn from inexhaustible uploaded images that are associated with informal tags provided by amateur users. Though such sketchy tags do not obviously reveal the labels, they actually contain useful semantic information for supervising deep <b>quantization.</b> To this end, we propose <b>Weakly-Supervised</b> Deep Hyperspherical <b>Quantization</b> (WSDHQ), which is the first work to learn deep <b>quantization</b> from weakly tagged images. Specifically, 1) we use <b>word</b> <b>embeddings</b> to represent the tags and enhance their semantic information based on a tag correlation <b>graph.</b> 2) To better preserve semantic information in <b>quantization</b> codes and reduce <b>quantization</b> error, we jointly learn semantics-preserving embeddings and <b>supervised</b> quantizer on hypersphere by employing a well-designed fusion layer and tailor-made loss functions. Extensive experiments show that WSDHQ can achieve state-of-art performance on <b>weakly-supervised</b> compact coding. Code is available at <a href=https://github.com/gimpong/AAAI21-WSDHQ>https://github.com/gimpong/AAAI21-WSDHQ</a>.</p></p class="citation"></blockquote><h3 id=1147--29152-reconstructing-retinal-visual-images-from-3t-fmri-data-enhanced-by-unsupervised-learning-yujian-xiong-et-al-2024>(11/47 | 29/152) Reconstructing Retinal Visual Images from 3T fMRI Data Enhanced by Unsupervised Learning (Yujian Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yujian Xiong, Wenhui Zhu, Zhong-Lin Lu, Yalin Wang. (2024)<br><strong>Reconstructing Retinal Visual Images from 3T fMRI Data Enhanced by Unsupervised Learning</strong><br><button class=copy-to-clipboard title="Reconstructing Retinal Visual Images from 3T fMRI Data Enhanced by Unsupervised Learning" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05107v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05107v1.pdf filename=2404.05107v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The reconstruction of human visual inputs from brain activity, particularly through functional Magnetic Resonance Imaging (fMRI), holds promising avenues for unraveling the mechanisms of the human visual system. Despite the significant strides made by deep learning methods in improving the quality and interpretability of visual reconstruction, there remains a substantial demand for high-quality, long-duration, subject-specific 7-Tesla fMRI experiments. The challenge arises in integrating diverse smaller 3-Tesla datasets or accommodating new subjects with brief and low-quality fMRI scans. In response to these constraints, we propose a novel framework that generates enhanced 3T fMRI data through an <b>unsupervised</b> <b>Generative</b> <b>Adversarial</b> <b>Network</b> <b>(GAN),</b> leveraging unpaired training across two distinct fMRI datasets in 7T and 3T, respectively. This approach aims to overcome the limitations of the scarcity of high-quality 7-Tesla data and the challenges associated with brief and low-quality scans in 3-Tesla experiments. In this paper, we demonstrate the reconstruction capabilities of the enhanced 3T fMRI data, highlighting its proficiency in generating superior input visual images compared to data-intensive methods trained and tested on a single subject.</p></p class="citation"></blockquote><h3 id=1247--30152-high-discriminative-attribute-feature-learning-for-generalized-zero-shot-learning-yu-lei-et-al-2024>(12/47 | 30/152) High-Discriminative Attribute Feature Learning for Generalized Zero-Shot Learning (Yu Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Lei, Guoshuai Sheng, Fangfang Li, Quanxue Gao, Cheng Deng, Qin Li. (2024)<br><strong>High-Discriminative Attribute Feature Learning for Generalized Zero-Shot Learning</strong><br><button class=copy-to-clipboard title="High-Discriminative Attribute Feature Learning for Generalized Zero-Shot Learning" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Zero-shot, Transformer, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04953v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04953v1.pdf filename=2404.04953v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Zero-shot</b> <b>learning(ZSL)</b> aims to recognize new classes without prior exposure to their samples, relying on semantic knowledge from observed classes. However, current attention-based models may overlook the transferability of visual features and the distinctiveness of attribute localization when learning regional features in images. Additionally, they often overlook shared attributes among different objects. Highly discriminative attribute features are crucial for identifying and distinguishing unseen classes. To address these issues, we propose an innovative approach called High-Discriminative Attribute Feature Learning for Generalized <b>Zero-Shot</b> <b>Learning</b> (HDAFL). HDAFL optimizes visual features by learning attribute features to obtain discriminative visual embeddings. Specifically, HDAFL utilizes multiple <b>convolutional</b> kernels to automatically learn discriminative regions highly correlated with attributes in images, eliminating irrelevant interference in image features. Furthermore, we introduce a <b>Transformer-based</b> attribute discrimination encoder to enhance the discriminative capability among attributes. Simultaneously, the method employs contrastive loss to alleviate dataset biases and enhance the transferability of visual features, facilitating better semantic transfer between seen and unseen classes. Experimental results demonstrate the effectiveness of HDAFL across three widely used datasets.</p></p class="citation"></blockquote><h3 id=1347--31152-few-shot-object-detection-research-advances-and-challenges-zhimeng-xin-et-al-2024>(13/47 | 31/152) Few-Shot Object Detection: Research Advances and Challenges (Zhimeng Xin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhimeng Xin, Shiming Chen, Tianxu Wu, Yuanjie Shao, Weiping Ding, Xinge You. (2024)<br><strong>Few-Shot Object Detection: Research Advances and Challenges</strong><br><button class=copy-to-clipboard title="Few-Shot Object Detection: Research Advances and Challenges" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Few-shot, Few-shot Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04799v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04799v1.pdf filename=2404.04799v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Object</b> <b>detection</b> as a subfield within computer vision has achieved remarkable progress, which aims to accurately identify and locate a specific <b>object</b> <b>from</b> images or videos. Such methods rely on large-scale labeled training samples for each <b>object</b> <b>category</b> to ensure accurate detection, but obtaining extensive annotated data is a labor-intensive and expensive process in many real-world scenarios. To tackle this challenge, researchers have explored <b>few-shot</b> <b>object</b> <b>detection</b> (FSOD) that combines <b>few-shot</b> <b>learning</b> and <b>object</b> <b>detection</b> techniques to rapidly adapt to novel <b>objects</b> <b>with</b> limited annotated samples. This paper presents a comprehensive survey to review the significant advancements in the field of FSOD in recent years and <b>summarize</b> the existing challenges and solutions. Specifically, we first introduce the background and definition of FSOD to emphasize potential value in advancing the field of computer vision. We then propose a novel FSOD taxonomy method and survey the plentifully remarkable FSOD algorithms based on this fact to report a comprehensive overview that facilitates a deeper understanding of the FSOD problem and the development of innovative solutions. Finally, we discuss the advantages and limitations of these algorithms to <b>summarize</b> the challenges, potential research direction, and development trend of <b>object</b> <b>detection</b> in the data scarcity scenario.</p></p class="citation"></blockquote><h3 id=1447--32152-havtr-improving-video-text-retrieval-through-augmentation-using-large-foundation-models-yimu-wang-et-al-2024>(14/47 | 32/152) HaVTR: Improving Video-Text Retrieval Through Augmentation Using Large Foundation Models (Yimu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yimu Wang, Shuai Yuan, Xiangru Jian, Wei Pang, Mushi Wang, Ning Yu. (2024)<br><strong>HaVTR: Improving Video-Text Retrieval Through Augmentation Using Large Foundation Models</strong><br><button class=copy-to-clipboard title="HaVTR: Improving Video-Text Retrieval Through Augmentation Using Large Foundation Models" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-IR, cs-LG, cs.CV<br>Keyword Score: 38<br>Keywords: Benchmarking, Foundation Model, Representation Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05083v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05083v1.pdf filename=2404.05083v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While recent progress in video-text retrieval has been driven by the exploration of powerful model architectures and training strategies, the <b>representation</b> <b>learning</b> ability of video-text retrieval models is still limited due to low-quality and scarce training data annotations. To address this issue, we present a novel video-text learning paradigm, HaVTR, which augments video and text data to learn more generalized features. Specifically, we first adopt a simple augmentation method, which generates self-similar data by randomly duplicating or dropping subwords and frames. In addition, inspired by the recent advancement in visual and language generative models, we propose a more powerful augmentation method through textual paraphrasing and video stylization using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and visual generative models (VGMs). Further, to bring richer information into video and text, we propose a hallucination-based augmentation method, where we use <b>LLMs</b> and VGMs to generate and add new relevant information to the original data. Benefiting from the enriched data, extensive experiments on several video-text retrieval <b>benchmarks</b> demonstrate the superiority of HaVTR over existing methods.</p></p class="citation"></blockquote><h3 id=1547--33152-scalable-and-efficient-hierarchical-visual-topological-mapping-saravanabalagi-ramachandran-et-al-2024>(15/47 | 33/152) Scalable and Efficient Hierarchical Visual Topological Mapping (Saravanabalagi Ramachandran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saravanabalagi Ramachandran, Jonathan Horgan, Ganesh Sistu, John McDonald. (2024)<br><strong>Scalable and Efficient Hierarchical Visual Topological Mapping</strong><br><button class=copy-to-clipboard title="Scalable and Efficient Hierarchical Visual Topological Mapping" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 33<br>Keywords: Autoencoder, Benchmarking, Unsupervised Learning, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05023v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05023v1.pdf filename=2404.05023v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hierarchical topological representations can significantly reduce search times within mapping and localization algorithms. Although recent research has shown the potential for such approaches, limited consideration has been given to the suitability and comparative performance of different global feature representations within this context. In this work, we evaluate state-of-the-art hand-crafted and learned global descriptors using a hierarchical topological mapping technique on <b>benchmark</b> datasets and present results of a comprehensive evaluation of the impact of the global descriptor used. Although learned descriptors have been incorporated into place recognition methods to improve retrieval accuracy and enhance overall recall, the problem of scalability and efficiency when applied to longer trajectories has not been adequately addressed in a majority of research studies. Based on our empirical analysis of multiple runs, we identify that continuity and distinctiveness are crucial characteristics for an optimal global descriptor that enable efficient and scalable hierarchical mapping, and present a methodology for quantifying and contrasting these characteristics across different global descriptors. Our study demonstrates that the use of global descriptors based on an <b>unsupervised</b> learned <b>Variational</b> <b>Autoencoder</b> (VAE) excels in these characteristics and achieves significantly lower runtime. It runs on a consumer grade desktop, up to 2.3x faster than the second best global descriptor, NetVLAD, and up to 9.5x faster than the hand-crafted descriptor, PHOG, on the longest track evaluated (St Lucia, 17.6 km), without sacrificing overall recall performance.</p></p class="citation"></blockquote><h3 id=1647--34152-hyperbolic-learning-with-synthetic-captions-for-open-world-detection-fanjie-kong-et-al-2024>(16/47 | 34/152) Hyperbolic Learning with Synthetic Captions for Open-World Detection (Fanjie Kong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fanjie Kong, Yanbei Chen, Jiarui Cai, Davide Modolo. (2024)<br><strong>Hyperbolic Learning with Synthetic Captions for Open-World Detection</strong><br><button class=copy-to-clipboard title="Hyperbolic Learning with Synthetic Captions for Open-World Detection" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Object Detection, Benchmarking, Grounding, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05016v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05016v1.pdf filename=2404.05016v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-world detection poses significant challenges, as it requires the detection of any <b>object</b> <b>using</b> either <b>object</b> <b>class</b> labels or free-form texts. Existing related works often use large-scale manual annotated caption datasets for training, which are extremely expensive to collect. Instead, we propose to transfer knowledge from <b>vision-language</b> models (VLMs) to enrich the open-vocabulary descriptions automatically. Specifically, we bootstrap dense synthetic captions using pre-trained VLMs to provide rich descriptions on different regions in images, and incorporate these captions to train a novel detector that generalizes to novel concepts. To mitigate the noise caused by hallucination in synthetic captions, we also propose a novel hyperbolic <b>vision-language</b> learning approach to impose a hierarchy between visual and caption embeddings. We call our detector ``HyperLearner&rsquo;&rsquo;. We conduct extensive experiments on a wide variety of open-world detection <b>benchmarks</b> (COCO, LVIS, <b>Object</b> <b>Detection</b> in the Wild, RefCOCO) and our results show that our model consistently outperforms existing state-of-the-art methods, such as GLIP, GLIPv2 and <b>Grounding</b> DINO, when using the same backbone.</p></p class="citation"></blockquote><h3 id=1747--35152-mixture-of-low-rank-experts-for-transferable-ai-generated-image-detection-zihan-liu-et-al-2024>(17/47 | 35/152) Mixture of Low-rank Experts for Transferable AI-Generated Image Detection (Zihan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihan Liu, Hanyi Wang, Yaoyu Kang, Shilin Wang. (2024)<br><strong>Mixture of Low-rank Experts for Transferable AI-Generated Image Detection</strong><br><button class=copy-to-clipboard title="Mixture of Low-rank Experts for Transferable AI-Generated Image Detection" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04883v1.pdf filename=2404.04883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative models have shown a giant leap in synthesizing photo-realistic images with minimal expertise, sparking concerns about the authenticity of online information. This study aims to develop a universal AI-generated image detector capable of identifying images from diverse sources. Existing methods struggle to generalize across unseen generative models when provided with limited sample sources. Inspired by the <b>zero-shot</b> transferability of pre-trained <b>vision-language</b> models, we seek to harness the nontrivial visual-world knowledge and descriptive proficiency of CLIP-ViT to generalize over unknown domains. This paper presents a novel parameter-efficient <b>fine-tuning</b> approach, mixture of low-rank experts, to fully exploit CLIP-ViT&rsquo;s potential while preserving knowledge and expanding capacity for transferable detection. We adapt only the MLP layers of deeper ViT blocks via an integration of shared and separate LoRAs within an MoE-based structure. Extensive experiments on public <b>benchmarks</b> show that our method achieves superiority over state-of-the-art approaches in cross-generator generalization and robustness to perturbations. Remarkably, our best-performing ViT-L/14 variant requires training only 0.08% of its parameters to surpass the leading baseline by +3.64% mAP and +12.72% avg.Acc across unseen diffusion and autoregressive models. This even outperforms the baseline with just 0.28% of the training data. Our code and pre-trained models will be available at <a href=https://github.com/zhliuworks/CLIPMoLE>https://github.com/zhliuworks/CLIPMoLE</a>.</p></p class="citation"></blockquote><h3 id=1847--36152-animatezoo-zero-shot-video-generation-of-cross-species-animation-via-subject-alignment-yuanfeng-xu-et-al-2024>(18/47 | 36/152) AnimateZoo: Zero-shot Video Generation of Cross-Species Animation via Subject Alignment (Yuanfeng Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanfeng Xu, Yuhao Chen, Zhongzhan Huang, Zijian He, Guangrun Wang, Philip Torr, Liang Lin. (2024)<br><strong>AnimateZoo: Zero-shot Video Generation of Cross-Species Animation via Subject Alignment</strong><br><button class=copy-to-clipboard title="AnimateZoo: Zero-shot Video Generation of Cross-Species Animation via Subject Alignment" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04946v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04946v1.pdf filename=2404.04946v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent video editing advancements rely on accurate pose sequences to animate subjects. However, these efforts are not suitable for cross-species animation due to pose misalignment between species (for example, the poses of a cat differs greatly from that of a pig due to differences in body structure). In this paper, we present AnimateZoo, a <b>zero-shot</b> diffusion-based video generator to address this challenging cross-species animation issue, aiming to accurately produce animal animations while preserving the background. The key technique used in our AnimateZoo is subject alignment, which includes two steps. First, we improve appearance feature extraction by integrating a Laplacian detail booster and a <b>prompt-tuning</b> identity extractor. These components are specifically designed to capture essential appearance information, including identity and fine details. Second, we align shape features and address conflicts from differing subjects by introducing a scale-information remover. This ensures accurate cross-species animation. Moreover, we introduce two high-quality animal video datasets featuring a wide variety of species. Trained on these extensive datasets, our model is capable of generating videos characterized by accurate movements, consistent appearance, and high-fidelity frames, without the need for the pre-inference <b>fine-tuning</b> that prior arts required. Extensive experiments showcase the outstanding performance of our method in cross-species action following tasks, demonstrating exceptional shape adaptation capability. The project page is available at <a href=https://justinxu0.github.io/AnimateZoo/>https://justinxu0.github.io/AnimateZoo/</a>.</p></p class="citation"></blockquote><h3 id=1947--37152-anomaly-detection-in-electrocardiograms-advancing-clinical-diagnosis-through-self-supervised-learning-aofan-jiang-et-al-2024>(19/47 | 37/152) Anomaly Detection in Electrocardiograms: Advancing Clinical Diagnosis Through Self-Supervised Learning (Aofan Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aofan Jiang, Chaoqin Huang, Qing Cao, Yuchen Xu, Zi Zeng, Kang Chen, Ya Zhang, Yanfeng Wang. (2024)<br><strong>Anomaly Detection in Electrocardiograms: Advancing Clinical Diagnosis Through Self-Supervised Learning</strong><br><button class=copy-to-clipboard title="Anomaly Detection in Electrocardiograms: Advancing Clinical Diagnosis Through Self-Supervised Learning" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Anomaly Detection, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04935v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04935v1.pdf filename=2404.04935v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The electrocardiogram (ECG) is an essential tool for diagnosing heart disease, with computer-aided systems improving diagnostic accuracy and reducing healthcare costs. Despite advancements, existing systems often miss rare cardiac anomalies that could be precursors to serious, life-threatening issues or alterations in the cardiac macro/microstructure. We address this gap by focusing on <b>self-supervised</b> <b>anomaly</b> <b>detection</b> (AD), training exclusively on normal ECGs to recognize deviations indicating anomalies. We introduce a novel <b>self-supervised</b> <b>learning</b> framework for ECG AD, utilizing a vast dataset of normal ECGs to autonomously detect and localize cardiac anomalies. It proposes a novel masking and restoration technique alongside a multi-scale cross-attention module, enhancing the model&rsquo;s ability to integrate global and local signal features. The framework emphasizes accurate localization of anomalies within ECG signals, ensuring the method&rsquo;s clinical relevance and reliability. To reduce the impact of individual variability, the approach further incorporates crucial patient-specific information from ECG reports, such as age and gender, thus enabling accurate identification of a broad spectrum of cardiac anomalies, including rare ones. Utilizing an extensive dataset of 478,803 ECG graphic reports from real-world clinical practice, our method has demonstrated exceptional effectiveness in AD across all tested conditions, regardless of their frequency of occurrence, significantly outperforming existing models. It achieved superior performance metrics, including an AUROC of 91.2%, an F1 score of 83.7%, a sensitivity rate of 84.2%, a specificity of 83.0%, and a precision of 75.6% with a fixed recall rate of 90%. It has also demonstrated robust localization capabilities, with an AUROC of 76.5% and a Dice coefficient of 65.3% for <b>anomaly</b> <b>localization.</b></p></p class="citation"></blockquote><h3 id=2047--38152-rethinking-diffusion-model-for-multi-contrast-mri-super-resolution-guangyuan-li-et-al-2024>(20/47 | 38/152) Rethinking Diffusion Model for Multi-Contrast MRI Super-Resolution (Guangyuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangyuan Li, Chen Rao, Juncheng Mo, Zhanjie Zhang, Wei Xing, Lei Zhao. (2024)<br><strong>Rethinking Diffusion Model for Multi-Contrast MRI Super-Resolution</strong><br><button class=copy-to-clipboard title="Rethinking Diffusion Model for Multi-Contrast MRI Super-Resolution" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Mixed Reality (MR), Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04785v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04785v1.pdf filename=2404.04785v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>diffusion</b> <b>models</b> (DM) have been applied in magnetic resonance imaging (MRI) super-resolution (SR) reconstruction, exhibiting impressive performance, especially with regard to detailed reconstruction. However, the current DM-based SR reconstruction methods still face the following issues: (1) They require a large number of iterations to reconstruct the final image, which is inefficient and consumes a significant amount of computational resources. (2) The results reconstructed by these methods are often misaligned with the real high-resolution images, leading to remarkable distortion in the reconstructed <b>MR</b> images. To address the aforementioned issues, we propose an efficient <b>diffusion</b> <b>model</b> for multi-contrast MRI SR, named as DiffMSR. Specifically, we apply DM in a highly compact low-dimensional latent space to generate prior knowledge with high-frequency detail information. The highly compact latent space ensures that DM requires only a few simple iterations to produce accurate prior knowledge. In addition, we design the Prior-Guide Large Window <b>Transformer</b> (PLWformer) as the decoder for DM, which can extend the receptive field while fully utilizing the prior knowledge generated by DM to ensure that the reconstructed <b>MR</b> image remains undistorted. Extensive experiments on public and clinical datasets demonstrate that our DiffMSR outperforms state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=2147--39152-fgaif-aligning-large-vision-language-models-with-fine-grained-ai-feedback-liqiang-jing-et-al-2024>(21/47 | 39/152) FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback (Liqiang Jing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liqiang Jing, Xinya Du. (2024)<br><strong>FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback</strong><br><button class=copy-to-clipboard title="FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Reinforcement Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05046v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05046v1.pdf filename=2404.05046v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>Vision-Language</b> Models (LVLMs) have demonstrated proficiency in tackling a variety of visual-language tasks. However, current LVLMs suffer from misalignment between text and image modalities which causes three kinds of hallucination problems, i.e., object existence, object attribute, and object relationship. To tackle this issue, existing methods mainly utilize <b>Reinforcement</b> <b>Learning</b> (RL) to align modalities in LVLMs. However, they still suffer from three main limitations: (1) General feedback can not indicate the hallucination type contained in the response; (2) Sparse rewards only give the sequence-level reward for the whole response; and (3)Annotation cost is time-consuming and labor-intensive. To handle these limitations, we propose an innovative method to align modalities in LVLMs through Fine-Grained Artificial Intelligence Feedback (FGAIF), which mainly consists of three steps: AI-based Feedback Collection, Fine-grained Reward Model Training, and <b>Reinforcement</b> <b>Learning</b> with Fine-grained Reward. Specifically, We first utilize AI tools to predict the types of hallucination for each segment in the response and obtain a collection of fine-grained feedback. Then, based on the collected reward data, three specialized reward models are trained to produce dense rewards. Finally, a novel fine-grained feedback module is integrated into the Proximal Policy Optimization (PPO) algorithm. Extensive experiments are conducted on hallucination and general <b>benchmarks,</b> demonstrating the superior performance of our proposed method. Notably, compared with previous models trained with the RL-based aligning method, our proposed method is effective even with fewer parameters.</p></p class="citation"></blockquote><h3 id=2247--40152-x-vars-introducing-explainability-in-football-refereeing-with-multi-modal-large-language-model-jan-held-et-al-2024>(22/47 | 40/152) X-VARS: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Model (Jan Held et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan Held, Hani Itani, Anthony Cioppa, Silvio Giancola, Bernard Ghanem, Marc Van Droogenbroeck. (2024)<br><strong>X-VARS: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Model</strong><br><button class=copy-to-clipboard title="X-VARS: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Model" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Multi-modal, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06332v1.pdf filename=2404.06332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of artificial intelligence has led to significant improvements in automated decision-making. However, the increased performance of models often comes at the cost of explainability and transparency of their decision-making processes. In this paper, we investigate the capabilities of <b>large</b> <b>language</b> <b>models</b> to explain decisions, using football refereeing as a testing ground, given its decision complexity and subjectivity. We introduce the Explainable Video Assistant Referee System, X-VARS, a <b>multi-modal</b> <b>large</b> <b>language</b> <b>model</b> designed for understanding football videos from the point of view of a referee. X-VARS can perform a multitude of tasks, including video description, <b>question</b> <b>answering,</b> action recognition, and conducting meaningful conversations based on video content and in accordance with the Laws of the Game for football referees. We validate X-VARS on our novel dataset, SoccerNet-XFoul, which consists of more than 22k video-question-answer triplets annotated by over 70 experienced football referees. Our experiments and human study illustrate the impressive capabilities of X-VARS in interpreting complex football clips. Furthermore, we highlight the potential of X-VARS to reach human performance and support football referees in the future.</p></p class="citation"></blockquote><h3 id=2347--41152-airshot-efficient-few-shot-detection-for-autonomous-exploration-zihan-wang-et-al-2024>(23/47 | 41/152) AirShot: Efficient Few-Shot Detection for Autonomous Exploration (Zihan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihan Wang, Bowen Li, Chen Wang, Sebastian Scherer. (2024)<br><strong>AirShot: Efficient Few-Shot Detection for Autonomous Exploration</strong><br><button class=copy-to-clipboard title="AirShot: Efficient Few-Shot Detection for Autonomous Exploration" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05069v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05069v1.pdf filename=2404.05069v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> <b>object</b> <b>detection</b> has drawn increasing attention in the field of robotic exploration, where robots are required to find unseen <b>objects</b> <b>with</b> a few online provided examples. Despite recent efforts have been made to yield online processing capabilities, slow inference speeds of low-powered robots fail to meet the demands of real-time detection-making them impractical for autonomous exploration. Existing methods still face performance and efficiency challenges, mainly due to unreliable features and exhaustive class loops. In this work, we propose a new paradigm AirShot, and discover that, by fully exploiting the valuable correlation map, AirShot can result in a more robust and faster <b>few-shot</b> <b>object</b> <b>detection</b> system, which is more applicable to robotics community. The core module Top Prediction Filter (TPF) can operate on multi-scale correlation maps in both the training and inference stages. During training, TPF supervises the generation of a more representative correlation map, while during inference, it reduces looping iterations by selecting top-ranked classes, thus cutting down on computational costs with better performance. Surprisingly, this dual functionality exhibits general effectiveness and efficiency on various off-the-shelf models. Exhaustive experiments on COCO2017, VOC2014, and SubT datasets demonstrate that TPF can significantly boost the efficacy and efficiency of most off-the-shelf models, achieving up to 36.4% precision improvements along with 56.3% faster inference speed. Code and Data are at: <a href=https://github.com/ImNotPrepared/AirShot>https://github.com/ImNotPrepared/AirShot</a>.</p></p class="citation"></blockquote><h3 id=2447--42152-camera-based-remote-physiology-sensing-for-hundreds-of-subjects-across-skin-tones-jiankai-tang-et-al-2024>(24/47 | 42/152) Camera-Based Remote Physiology Sensing for Hundreds of Subjects Across Skin Tones (Jiankai Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiankai Tang, Xinyi Li, Jiacheng Liu, Xiyuxing Zhang, Zeyu Wang, Yuntao Wang. (2024)<br><strong>Camera-Based Remote Physiology Sensing for Hundreds of Subjects Across Skin Tones</strong><br><button class=copy-to-clipboard title="Camera-Based Remote Physiology Sensing for Hundreds of Subjects Across Skin Tones" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05003v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05003v1.pdf filename=2404.05003v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remote photoplethysmography (rPPG) emerges as a promising method for non-invasive, convenient measurement of vital signs, utilizing the widespread presence of cameras. Despite advancements, existing datasets fall short in terms of size and diversity, limiting comprehensive evaluation under diverse conditions. This paper presents an in-depth analysis of the VitalVideo dataset, the largest real-world rPPG dataset to date, encompassing 893 subjects and 6 Fitzpatrick skin tones. Our experimentation with six <b>unsupervised</b> methods and three <b>supervised</b> models demonstrates that datasets comprising a few hundred subjects(i.e., 300 for UBFC-rPPG, 500 for PURE, and 700 for MMPD-Simple) are sufficient for effective rPPG model training. Our findings highlight the importance of diversity and consistency in skin tones for precise performance evaluation across different datasets.</p></p class="citation"></blockquote><h3 id=2547--43152-msmsfnet-a-multi-stream-and-multi-scale-fusion-net-for-edge-detection-chenguang-liu-et-al-2024>(25/47 | 43/152) Msmsfnet: a multi-stream and multi-scale fusion net for edge detection (Chenguang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenguang Liu, Chisheng Wang, Feifei Dong, Xin Su, Chuanhua Zhu, Dejin Zhang, Qingquan Li. (2024)<br><strong>Msmsfnet: a multi-stream and multi-scale fusion net for edge detection</strong><br><button class=copy-to-clipboard title="Msmsfnet: a multi-stream and multi-scale fusion net for edge detection" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fairness, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04856v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04856v1.pdf filename=2404.04856v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Edge detection is a long standing problem in computer vision. Recent deep learning based algorithms achieve state of-the-art performance in publicly available datasets. Despite the efficiency of these algorithms, their performance, however, relies heavily on the pretrained weights of the backbone network on the ImageNet dataset. This limits heavily the design space of deep learning based edge detectors. Whenever we want to devise a new model, we have to train this new model on the ImageNet dataset first, and then fine tune the model using the edge detection datasets. The comparison would be unfair otherwise. However, it is usually not feasible for many researchers to train a model on the ImageNet dataset due to the limited computation resources. In this work, we study the performance that can be achieved by state-of-the-art deep learning based edge detectors in publicly available datasets when they are trained from scratch, and devise a new network architecture, the multi-stream and multi scale fusion net (msmsfnet), for edge detection. We show in our experiments that by training all models from scratch to ensure the <b>fairness</b> of comparison, out model outperforms state-of-the art deep learning based edge detectors in three publicly available datasets.</p></p class="citation"></blockquote><h3 id=2647--44152-strictly-id-preserved-and-controllable-accessory-advertising-image-generation-youze-xue-et-al-2024>(26/47 | 44/152) Strictly-ID-Preserved and Controllable Accessory Advertising Image Generation (Youze Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Youze Xue, Binghui Chen, Yifeng Geng, Xuansong Xie, Jiansheng Chen, Hongbing Ma. (2024)<br><strong>Strictly-ID-Preserved and Controllable Accessory Advertising Image Generation</strong><br><button class=copy-to-clipboard title="Strictly-ID-Preserved and Controllable Accessory Advertising Image Generation" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04828v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04828v1.pdf filename=2404.04828v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Customized generative <b>text-to-image</b> models have the ability to produce images that closely resemble a given subject. However, in the context of generating advertising images for e-commerce scenarios, it is crucial that the generated subject&rsquo;s identity aligns perfectly with the product being advertised. In order to address the need for strictly-ID preserved advertising image generation, we have developed a Control-Net based customized image generation pipeline and have taken earring model advertising as an example. Our approach facilitates a seamless interaction between the earrings and the model&rsquo;s face, while ensuring that the identity of the earrings remains intact. Furthermore, to achieve a diverse and controllable display, we have proposed a multi-branch cross-attention architecture, which allows for control over the scale, pose, and appearance of the model, going beyond the limitations of text <b>prompts.</b> Our method manages to achieve fine-grained control of the generated model&rsquo;s face, resulting in controllable and captivating advertising effects.</p></p class="citation"></blockquote><h3 id=2747--45152-light-the-night-a-multi-condition-diffusion-framework-for-unpaired-low-light-enhancement-in-autonomous-driving-jinlong-li-et-al-2024>(27/47 | 45/152) Light the Night: A Multi-Condition Diffusion Framework for Unpaired Low-Light Enhancement in Autonomous Driving (Jinlong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinlong Li, Baolu Li, Zhengzhong Tu, Xinyu Liu, Qing Guo, Felix Juefei-Xu, Runsheng Xu, Hongkai Yu. (2024)<br><strong>Light the Night: A Multi-Condition Diffusion Framework for Unpaired Low-Light Enhancement in Autonomous Driving</strong><br><button class=copy-to-clipboard title="Light the Night: A Multi-Condition Diffusion Framework for Unpaired Low-Light Enhancement in Autonomous Driving" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04804v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04804v1.pdf filename=2404.04804v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vision-centric perception systems for autonomous driving have gained considerable attention recently due to their cost-effectiveness and scalability, especially compared to LiDAR-based systems. However, these systems often struggle in low-light conditions, potentially compromising their performance and safety. To address this, our paper introduces LightDiff, a domain-tailored framework designed to enhance the low-light image quality for autonomous driving applications. Specifically, we employ a multi-condition controlled <b>diffusion</b> <b>model.</b> LightDiff works without any human-collected paired data, leveraging a dynamic data degradation process instead. It incorporates a novel multi-condition adapter that adaptively controls the input weights from different modalities, including depth maps, RGB images, and text captions, to effectively illuminate dark scenes while maintaining context consistency. Furthermore, to align the enhanced images with the detection model&rsquo;s knowledge, LightDiff employs perception-specific scores as rewards to guide the <b>diffusion</b> <b>training</b> process through <b>reinforcement</b> <b>learning.</b> Extensive experiments on the nuScenes datasets demonstrate that LightDiff can significantly improve the performance of several state-of-the-art 3D detectors in night-time conditions while achieving high visual quality scores, highlighting its potential to safeguard autonomous driving.</p></p class="citation"></blockquote><h3 id=2847--46152-a-unified-diffusion-framework-for-scene-aware-human-motion-estimation-from-sparse-signals-jiangnan-tang-et-al-2024>(28/47 | 46/152) A Unified Diffusion Framework for Scene-aware Human Motion Estimation from Sparse Signals (Jiangnan Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiangnan Tang, Jingya Wang, Kaiyang Ji, Lan Xu, Jingyi Yu, Ye Shi. (2024)<br><strong>A Unified Diffusion Framework for Scene-aware Human Motion Estimation from Sparse Signals</strong><br><button class=copy-to-clipboard title="A Unified Diffusion Framework for Scene-aware Human Motion Estimation from Sparse Signals" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Autoencoder, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04890v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04890v1.pdf filename=2404.04890v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating full-body human motion via sparse tracking signals from head-mounted displays and hand controllers in 3D scenes is crucial to applications in AR/VR. One of the biggest challenges to this task is the one-to-many mapping from sparse observations to dense full-body motions, which endowed inherent ambiguities. To help resolve this ambiguous problem, we introduce a new framework to combine rich contextual information provided by scenes to benefit full-body motion tracking from sparse observations. To estimate plausible human motions given sparse tracking signals and 3D scenes, we develop $\text{S}^2$Fusion, a unified framework fusing \underline{S}cene and sparse \underline{S}ignals with a conditional dif\underline{Fusion} model. $\text{S}^2$Fusion first extracts the spatial-temporal relations residing in the sparse signals via a periodic <b>autoencoder,</b> and then produces time-alignment feature embedding as additional inputs. Subsequently, by drawing initial noisy motion from a pre-trained prior, $\text{S}^2$Fusion utilizes conditional diffusion to fuse scene <b>geometry</b> and sparse tracking signals to generate full-body scene-aware motions. The sampling procedure of $\text{S}^2$Fusion is further guided by a specially designed scene-penetration loss and phase-matching loss, which effectively regularizes the motion of the lower body even in the absence of any tracking signals, making the generated motion much more plausible and coherent. Extensive experimental results have demonstrated that our $\text{S}^2$Fusion outperforms the state-of-the-art in terms of estimation quality and smoothness.</p></p class="citation"></blockquote><h3 id=2947--47152-a-clinical-oriented-multi-level-contrastive-learning-method-for-disease-diagnosis-in-low-quality-medical-images-qingshan-hou-et-al-2024>(29/47 | 47/152) A Clinical-oriented Multi-level Contrastive Learning Method for Disease Diagnosis in Low-quality Medical Images (Qingshan Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingshan Hou, Shuai Cheng, Peng Cao, Jinzhu Yang, Xiaoli Liu, Osmar R. Zaiane, Yih Chung Tham. (2024)<br><strong>A Clinical-oriented Multi-level Contrastive Learning Method for Disease Diagnosis in Low-quality Medical Images</strong><br><button class=copy-to-clipboard title="A Clinical-oriented Multi-level Contrastive Learning Method for Disease Diagnosis in Low-quality Medical Images" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Contrastive Learning, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04887v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04887v1.pdf filename=2404.04887v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Representation</b> <b>learning</b> offers a conduit to elucidate distinctive features within the latent space and interpret the deep models. However, the randomness of lesion distribution and the complexity of low-quality factors in medical images pose great challenges for models to extract key lesion features. Disease diagnosis methods guided by <b>contrastive</b> <b>learning</b> (CL) have shown significant advantages in lesion feature <b>representation.</b> <b>Nevertheless,</b> the effectiveness of CL is highly dependent on the quality of the positive and negative sample pairs. In this work, we propose a clinical-oriented multi-level CL framework that aims to enhance the model&rsquo;s capacity to extract lesion features and discriminate between lesion and low-quality factors, thereby enabling more accurate disease diagnosis from low-quality medical images. Specifically, we first construct multi-level positive and negative pairs to enhance the model&rsquo;s comprehensive recognition capability of lesion features by integrating information from different levels and qualities of medical images. Moreover, to improve the quality of the learned lesion embeddings, we introduce a dynamic hard sample mining method based on self-paced learning. The proposed CL framework is validated on two public medical image datasets, EyeQ and Chest X-ray, demonstrating superior performance compared to other state-of-the-art disease diagnostic methods.</p></p class="citation"></blockquote><h3 id=3047--48152-dual-scale-transformer-for-large-scale-single-pixel-imaging-gang-qu-et-al-2024>(30/47 | 48/152) Dual-Scale Transformer for Large-Scale Single-Pixel Imaging (Gang Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gang Qu, Ping Wang, Xin Yuan. (2024)<br><strong>Dual-Scale Transformer for Large-Scale Single-Pixel Imaging</strong><br><button class=copy-to-clipboard title="Dual-Scale Transformer for Large-Scale Single-Pixel Imaging" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Graph, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05001v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05001v1.pdf filename=2404.05001v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Single-pixel imaging (SPI) is a potential computational imaging technique which produces image by solving an illposed reconstruction problem from few measurements captured by a single-pixel detector. Deep learning has achieved impressive success on SPI reconstruction. However, previous poor reconstruction performance and impractical imaging model limit its real-world applications. In this paper, we propose a deep unfolding network with hybrid-attention <b>Transformer</b> on Kronecker SPI model, dubbed HATNet, to improve the imaging quality of real SPI cameras. Specifically, we unfold the computation <b>graph</b> of the iterative shrinkagethresholding algorithm (ISTA) into two alternative modules: efficient tensor gradient descent and hybrid-attention multiscale denoising. By virtue of Kronecker SPI, the gradient descent module can avoid high computational overheads rooted in previous gradient descent modules based on vectorized SPI. The denoising module is an encoder-decoder architecture powered by dual-scale spatial attention for high- and low-frequency aggregation and channel attention for global information recalibration. Moreover, we build a SPI prototype to verify the effectiveness of the proposed method. Extensive experiments on synthetic and real data demonstrate that our method achieves the state-of-the-art performance. The source code and pre-trained models are available at <a href=https://github.com/Gang-Qu/HATNet-SPI>https://github.com/Gang-Qu/HATNet-SPI</a>.</p></p class="citation"></blockquote><h3 id=3147--49152-platesegfl-a-privacy-preserving-license-plate-detection-using-federated-segmentation-learning-md-shahriar-rahman-anuvab-et-al-2024>(31/47 | 49/152) PlateSegFL: A Privacy-Preserving License Plate Detection Using Federated Segmentation Learning (Md. Shahriar Rahman Anuvab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md. Shahriar Rahman Anuvab, Mishkat Sultana, Md. Atif Hossain, Shashwata Das, Suvarthi Chowdhury, Rafeed Rahman, Dibyo Fabian Dofadar, Shahriar Rahman Rana. (2024)<br><strong>PlateSegFL: A Privacy-Preserving License Plate Detection Using Federated Segmentation Learning</strong><br><button class=copy-to-clipboard title="PlateSegFL: A Privacy-Preserving License Plate Detection Using Federated Segmentation Learning" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05049v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05049v1.pdf filename=2404.05049v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic License Plate Recognition (ALPR) is an integral component of an intelligent transport system with extensive applications in secure transportation, vehicle-to-vehicle communication, stolen vehicles detection, traffic violations, and traffic flow management. The existing license plate detection system focuses on one-shot learners or pre-trained models that operate with a geometric bounding box, limiting the model&rsquo;s performance. Furthermore, continuous video data streams uploaded to the central server result in network and complexity issues. To combat this, PlateSegFL was introduced, which implements U-Net-based segmentation along with <b>Federated</b> <b>Learning</b> (FL). U-Net is well-suited for multi-class image segmentation tasks because it can analyze a large number of classes and generate a pixel-level segmentation map for each class. <b>Federated</b> <b>Learning</b> is used to reduce the quantity of data required while safeguarding the user&rsquo;s privacy. Different computing platforms, such as mobile phones, are able to collaborate on the development of a standard prediction model where it makes efficient use of one&rsquo;s time; incorporates more diverse data; delivers projections in real-time; and requires no physical effort from the user; resulting around 95% F1 score.</p></p class="citation"></blockquote><h3 id=3247--50152-magictime-time-lapse-video-generation-models-as-metamorphic-simulators-shenghai-yuan-et-al-2024>(32/47 | 50/152) MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators (Shenghai Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shenghai Yuan, Jinfa Huang, Yujun Shi, Yongqi Xu, Ruijie Zhu, Bin Lin, Xinhua Cheng, Li Yuan, Jiebo Luo. (2024)<br><strong>MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators</strong><br><button class=copy-to-clipboard title="MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05014v1.pdf filename=2404.05014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in Text-to-Video generation (T2V) have achieved remarkable success in synthesizing high-quality general videos from textual descriptions. A largely overlooked problem in T2V is that existing models have not adequately encoded physical knowledge of the real world, thus generated videos tend to have limited motion and poor variations. In this paper, we propose \textbf{MagicTime}, a metamorphic time-lapse video generation model, which learns real-world physics knowledge from time-lapse videos and implements metamorphic generation. First, we design a MagicAdapter scheme to decouple spatial and temporal training, encode more physical knowledge from metamorphic videos, and transform pre-trained T2V models to generate metamorphic videos. Second, we introduce a Dynamic Frames Extraction strategy to adapt to metamorphic time-lapse videos, which have a wider variation range and cover dramatic object metamorphic processes, thus embodying more physical knowledge than general videos. Finally, we introduce a Magic Text-Encoder to improve the understanding of metamorphic video <b>prompts.</b> Furthermore, we create a time-lapse video-text dataset called \textbf{ChronoMagic}, specifically curated to unlock the metamorphic video generation ability. Extensive experiments demonstrate the superiority and effectiveness of MagicTime for generating high-quality and dynamic metamorphic videos, suggesting time-lapse video generation is a promising path toward building metamorphic simulators of the physical world.</p></p class="citation"></blockquote><h3 id=3347--51152-fantastic-animals-and-where-to-find-them-segment-any-marine-animal-with-dual-sam-pingping-zhang-et-al-2024>(33/47 | 51/152) Fantastic Animals and Where to Find Them: Segment Any Marine Animal with Dual SAM (Pingping Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pingping Zhang, Tianyu Yan, Yang Liu, Huchuan Lu. (2024)<br><strong>Fantastic Animals and Where to Find Them: Segment Any Marine Animal with Dual SAM</strong><br><button class=copy-to-clipboard title="Fantastic Animals and Where to Find Them: Segment Any Marine Animal with Dual SAM" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04996v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04996v1.pdf filename=2404.04996v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As an important pillar of underwater intelligence, Marine Animal Segmentation (MAS) involves segmenting animals within marine environments. Previous methods don&rsquo;t excel in extracting long-range contextual features and overlook the connectivity between discrete pixels. Recently, Segment Anything Model (SAM) offers a universal framework for general segmentation tasks. Unfortunately, trained with natural images, SAM does not obtain the prior knowledge from marine images. In addition, the single-position <b>prompt</b> of SAM is very insufficient for prior guidance. To address these issues, we propose a novel feature learning framework, named Dual-SAM for high-performance MAS. To this end, we first introduce a dual structure with SAM&rsquo;s paradigm to enhance feature learning of marine images. Then, we propose a Multi-level Coupled <b>Prompt</b> (MCP) strategy to instruct comprehensive underwater prior information, and enhance the multi-level features of SAM&rsquo;s encoder with adapters. Subsequently, we design a Dilated Fusion Attention Module (DFAM) to progressively integrate multi-level features from SAM&rsquo;s encoder. Finally, instead of directly predicting the masks of marine animals, we propose a Criss-Cross Connectivity Prediction (C$^3$P) paradigm to capture the inter-connectivity between discrete pixels. With dual decoders, it generates pseudo-labels and achieves mutual supervision for complementary feature representations, resulting in considerable improvements over previous techniques. Extensive experiments verify that our proposed method achieves state-of-the-art performances on five widely-used MAS datasets. The code is available at <a href=https://github.com/Drchip61/Dual_SAM>https://github.com/Drchip61/Dual_SAM</a>.</p></p class="citation"></blockquote><h3 id=3447--52152-dynamic-distinction-learning-adaptive-pseudo-anomalies-for-video-anomaly-detection-demetris-lappas-et-al-2024>(34/47 | 52/152) Dynamic Distinction Learning: Adaptive Pseudo Anomalies for Video Anomaly Detection (Demetris Lappas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Demetris Lappas, Vasileios Argyriou, Dimitrios Makris. (2024)<br><strong>Dynamic Distinction Learning: Adaptive Pseudo Anomalies for Video Anomaly Detection</strong><br><button class=copy-to-clipboard title="Dynamic Distinction Learning: Adaptive Pseudo Anomalies for Video Anomaly Detection" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04986v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04986v1.pdf filename=2404.04986v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Dynamic Distinction Learning (DDL) for Video <b>Anomaly</b> <b>Detection,</b> a novel video <b>anomaly</b> <b>detection</b> methodology that combines pseudo-anomalies, dynamic <b>anomaly</b> <b>weighting,</b> and a distinction loss function to improve detection accuracy. By training on pseudo-anomalies, our approach adapts to the variability of normal and anomalous behaviors without fixed <b>anomaly</b> <b>thresholds.</b> Our model showcases superior performance on the Ped2, Avenue and ShanghaiTech datasets, where individual models are tailored for each scene. These achievements highlight DDL&rsquo;s effectiveness in advancing <b>anomaly</b> <b>detection,</b> offering a scalable and adaptable solution for video surveillance challenges.</p></p class="citation"></blockquote><h3 id=3547--53152-gaussian-shading-provable-performance-lossless-image-watermarking-for-diffusion-models-zijin-yang-et-al-2024>(35/47 | 53/152) Gaussian Shading: Provable Performance-Lossless Image Watermarking for Diffusion Models (Zijin Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijin Yang, Kai Zeng, Kejiang Chen, Han Fang, Weiming Zhang, Nenghai Yu. (2024)<br><strong>Gaussian Shading: Provable Performance-Lossless Image Watermarking for Diffusion Models</strong><br><button class=copy-to-clipboard title="Gaussian Shading: Provable Performance-Lossless Image Watermarking for Diffusion Models" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CR, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04956v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04956v1.pdf filename=2404.04956v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ethical concerns surrounding copyright protection and inappropriate content generation pose challenges for the practical implementation of <b>diffusion</b> <b>models.</b> One effective solution involves watermarking the generated images. However, existing methods often compromise the model performance or require additional training, which is undesirable for operators and users. To address this issue, we propose Gaussian Shading, a <b>diffusion</b> <b>model</b> watermarking technique that is both performance-lossless and training-free, while serving the dual purpose of copyright protection and tracing of offending content. Our watermark embedding is free of model parameter modifications and thus is plug-and-play. We map the watermark to latent representations following a standard Gaussian distribution, which is indistinguishable from latent representations obtained from the non-watermarked <b>diffusion</b> <b>model.</b> Therefore we can achieve watermark embedding with lossless performance, for which we also provide theoretical proof. Furthermore, since the watermark is intricately linked with image semantics, it exhibits resilience to lossy processing and erasure attempts. The watermark can be extracted by Denoising <b>Diffusion</b> <b>Implicit</b> Models (DDIM) inversion and inverse sampling. We evaluate Gaussian Shading on multiple versions of Stable <b>Diffusion,</b> <b>and</b> the results demonstrate that Gaussian Shading not only is performance-lossless but also outperforms existing methods in terms of robustness.</p></p class="citation"></blockquote><h3 id=3647--54152-unimd-towards-unifying-moment-retrieval-and-temporal-action-detection-yingsen-zeng-et-al-2024>(36/47 | 54/152) UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection (Yingsen Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingsen Zeng, Yujie Zhong, Chengjian Feng, Lin Ma. (2024)<br><strong>UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection</strong><br><button class=copy-to-clipboard title="UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Mixed Reality (MR)<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04933v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04933v1.pdf filename=2404.04933v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Temporal Action Detection (TAD) focuses on detecting pre-defined actions, while Moment Retrieval <b>(MR)</b> aims to identify the events described by open-ended natural language within untrimmed videos. Despite that they focus on different events, we observe they have a significant connection. For instance, most descriptions in <b>MR</b> involve multiple actions from TAD. In this paper, we aim to investigate the potential synergy between TAD and <b>MR.</b> Firstly, we propose a unified architecture, termed Unified Moment Detection (UniMD), for both TAD and <b>MR.</b> It transforms the inputs of the two tasks, namely actions for TAD or events for <b>MR,</b> into a common embedding space, and utilizes two novel query-dependent decoders to generate a uniform output of classification score and temporal segments. Secondly, we explore the efficacy of two task fusion learning approaches, pre-training and co-training, in order to enhance the mutual benefits between TAD and <b>MR.</b> Extensive experiments demonstrate that the proposed task fusion learning scheme enables the two tasks to help each other and outperform the separately trained counterparts. Impressively, UniMD achieves state-of-the-art results on three paired datasets Ego4D, Charades-STA, and ActivityNet. Our code will be released at <a href=https://github.com/yingsen1/UniMD>https://github.com/yingsen1/UniMD</a>.</p></p class="citation"></blockquote><h3 id=3747--55152-codecnerf-toward-fast-encoding-and-decoding-compact-and-high-quality-novel-view-synthesis-gyeongjin-kang-et-al-2024>(37/47 | 55/152) CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis (Gyeongjin Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gyeongjin Kang, Younggeun Lee, Eunbyung Park. (2024)<br><strong>CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis</strong><br><button class=copy-to-clipboard title="CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04913v1.pdf filename=2404.04913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Radiance Fields (NeRF) have achieved huge success in effectively capturing and representing 3D objects and scenes. However, several factors have impeded its further proliferation as next-generation 3D media. To establish a ubiquitous presence in everyday media formats, such as images and videos, it is imperative to devise a solution that effectively fulfills three key objectives: fast encoding and decoding time, compact model sizes, and high-quality renderings. Despite significant advancements, a comprehensive algorithm that adequately addresses all objectives has yet to be fully realized. In this work, we present CodecNeRF, a neural codec for NeRF representations, consisting of a novel encoder and decoder architecture that can generate a NeRF representation in a single forward pass. Furthermore, inspired by the recent parameter-efficient <b>finetuning</b> approaches, we develop a novel <b>finetuning</b> method to efficiently adapt the generated NeRF representations to a new test instance, leading to high-quality image renderings and compact code sizes. The proposed CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF, achieved unprecedented compression performance of more than 150x and 20x reduction in encoding time while maintaining (or improving) the image quality on widely used 3D object datasets, such as ShapeNet and Objaverse.</p></p class="citation"></blockquote><h3 id=3847--56152-dual-camera-smooth-zoom-on-mobile-phones-renlong-wu-et-al-2024>(38/47 | 56/152) Dual-Camera Smooth Zoom on Mobile Phones (Renlong Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renlong Wu, Zhilu Zhang, Yu Yang, Wangmeng Zuo. (2024)<br><strong>Dual-Camera Smooth Zoom on Mobile Phones</strong><br><button class=copy-to-clipboard title="Dual-Camera Smooth Zoom on Mobile Phones" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04908v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04908v1.pdf filename=2404.04908v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When zooming between dual cameras on a mobile, noticeable jumps in geometric content and image color occur in the preview, inevitably affecting the user&rsquo;s zoom experience. In this work, we introduce a new task, ie, dual-camera smooth zoom (DCSZ) to achieve a smooth zoom preview. The frame interpolation (FI) technique is a potential solution but struggles with ground-truth collection. To address the issue, we suggest a data factory solution where continuous virtual cameras are assembled to generate DCSZ data by rendering reconstructed 3D models of the scene. In particular, we propose a novel dual-camera smooth zoom Gaussian Splatting (ZoomGS), where a camera-specific encoding is introduced to construct a specific 3D model for each virtual camera. With the proposed data factory, we construct a synthetic dataset for DCSZ, and we utilize it to <b>fine-tune</b> FI models. In addition, we collect real-world dual-zoom images without ground-truth for evaluation. Extensive experiments are conducted with multiple FI methods. The results show that the <b>fine-tuned</b> FI models achieve a significant performance improvement over the original ones on DCSZ task. The datasets, codes, and pre-trained models will be publicly available.</p></p class="citation"></blockquote><h3 id=3947--57152-dl-ewf-deep-learning-empowering-womens-fashion-with-grounded-segment-anything-segmentation-for-body-shape-classification-fatemeh-asghari-et-al-2024>(39/47 | 57/152) DL-EWF: Deep Learning Empowering Women&rsquo;s Fashion with Grounded-Segment-Anything Segmentation for Body Shape Classification (Fatemeh Asghari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fatemeh Asghari, Mohammad Reza Soheili, Faezeh Gholamrezaie. (2024)<br><strong>DL-EWF: Deep Learning Empowering Women&rsquo;s Fashion with Grounded-Segment-Anything Segmentation for Body Shape Classification</strong><br><button class=copy-to-clipboard title="DL-EWF: Deep Learning Empowering Women's Fashion with Grounded-Segment-Anything Segmentation for Body Shape Classification" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Deep Neural Network, Deep Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04891v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04891v1.pdf filename=2404.04891v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The global fashion industry plays a pivotal role in the global economy, and addressing fundamental issues within the industry is crucial for developing innovative solutions. One of the most pressing challenges in the fashion industry is the mismatch between body shapes and the garments of individuals they purchase. This issue is particularly prevalent among individuals with non-ideal body shapes, exacerbating the challenges faced. Considering inter-individual variability in body shapes is essential for designing and producing garments that are widely accepted by consumers. Traditional methods for determining human body shape are limited due to their low accuracy, high costs, and time-consuming nature. New approaches, utilizing digital imaging and <b>deep</b> <b>neural</b> <b>networks</b> <b>(DNN),</b> have been introduced to identify human body shape. In this study, the Style4BodyShape dataset is used for classifying body shapes into five categories: Rectangle, Triangle, Inverted Triangle, Hourglass, and Apple. In this paper, the body shape segmentation of a person is extracted from the image, disregarding the surroundings and background. Then, Various pre-trained models, such as ResNet18, ResNet34, ResNet50, VGG16, VGG19, and Inception v3, are used to classify the segmentation results. Among these pre-trained models, the Inception V3 model demonstrates superior performance regarding f1-score evaluation metric and accuracy compared to the other models.</p></p class="citation"></blockquote><h3 id=4047--58152-byteedit-boost-comply-and-accelerate-generative-image-editing-yuxi-ren-et-al-2024>(40/47 | 58/152) ByteEdit: Boost, Comply and Accelerate Generative Image Editing (Yuxi Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxi Ren, Jie Wu, Yanzuo Lu, Huafeng Kuang, Xin Xia, Xionghui Wang, Qianqian Wang, Yixing Zhu, Pan Xie, Shiyin Wang, Xuefeng Xiao, Yitong Wang, Min Zheng, Lean Fu. (2024)<br><strong>ByteEdit: Boost, Comply and Accelerate Generative Image Editing</strong><br><button class=copy-to-clipboard title="ByteEdit: Boost, Comply and Accelerate Generative Image Editing" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04860v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04860v1.pdf filename=2404.04860v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in diffusion-based generative image editing have sparked a profound revolution, reshaping the landscape of image outpainting and inpainting tasks. Despite these strides, the field grapples with inherent challenges, including: i) inferior quality; ii) poor consistency; iii) insufficient instrcution adherence; iv) suboptimal generation efficiency. To address these obstacles, we present ByteEdit, an innovative feedback learning framework meticulously designed to Boost, Comply, and Accelerate Generative Image Editing tasks. ByteEdit seamlessly integrates image reward models dedicated to enhancing aesthetics and <b>image-text</b> alignment, while also introducing a dense, pixel-level reward model tailored to foster coherence in the output. Furthermore, we propose a pioneering adversarial and progressive feedback learning strategy to expedite the model&rsquo;s inference speed. Through extensive large-scale user evaluations, we demonstrate that ByteEdit surpasses leading generative image editing products, including Adobe, Canva, and MeiTu, in both generation quality and consistency. ByteEdit-Outpainting exhibits a remarkable enhancement of 388% and 135% in quality and consistency, respectively, when compared to the baseline model. Experiments also verfied that our acceleration models maintains excellent performance results in terms of quality and consistency.</p></p class="citation"></blockquote><h3 id=4147--59152-shoemodel-learning-to-wear-on-the-user-specified-shoes-via-diffusion-model-binghui-chen-et-al-2024>(41/47 | 59/152) ShoeModel: Learning to Wear on the User-specified Shoes via Diffusion Model (Binghui Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Binghui Chen, Wenyu Li, Yifeng Geng, Xuansong Xie, Wangmeng Zuo. (2024)<br><strong>ShoeModel: Learning to Wear on the User-specified Shoes via Diffusion Model</strong><br><button class=copy-to-clipboard title="ShoeModel: Learning to Wear on the User-specified Shoes via Diffusion Model" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04833v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04833v1.pdf filename=2404.04833v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the development of the large-scale <b>diffusion</b> <b>model,</b> Artificial Intelligence Generated Content (AIGC) techniques are popular recently. However, how to truly make it serve our daily lives remains an open question. To this end, in this paper, we focus on employing AIGC techniques in one filed of E-commerce marketing, i.e., generating hyper-realistic advertising images for displaying user-specified shoes by human. Specifically, we propose a shoe-wearing system, called Shoe-Model, to generate plausible images of human legs interacting with the given shoes. It consists of three modules: (1) shoe wearable-area detection module (WD), (2) leg-pose synthesis module (LpS) and the final (3) shoe-wearing image generation module (SW). Them three are performed in ordered stages. Compared to baselines, our ShoeModel is shown to generalize better to different type of shoes and has ability of keeping the ID-consistency of the given shoes, as well as automatically producing reasonable interactions with human. Extensive experiments show the effectiveness of our proposed shoe-wearing system. Figure 1 shows the input and output examples of our ShoeModel.</p></p class="citation"></blockquote><h3 id=4247--60152-3d-building-reconstruction-from-monocular-remote-sensing-images-with-multi-level-supervisions-weijia-li-et-al-2024>(42/47 | 60/152) 3D Building Reconstruction from Monocular Remote Sensing Images with Multi-level Supervisions (Weijia Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weijia Li, Haote Yang, Zhenghao Hu, Juepeng Zheng, Gui-Song Xia, Conghui He. (2024)<br><strong>3D Building Reconstruction from Monocular Remote Sensing Images with Multi-level Supervisions</strong><br><button class=copy-to-clipboard title="3D Building Reconstruction from Monocular Remote Sensing Images with Multi-level Supervisions" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04823v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04823v1.pdf filename=2404.04823v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D building reconstruction from monocular remote sensing images is an important and challenging research problem that has received increasing attention in recent years, owing to its low cost of data acquisition and availability for large-scale applications. However, existing methods rely on expensive 3D-annotated samples for fully-supervised training, restricting their application to large-scale cross-city scenarios. In this work, we propose MLS-BRN, a multi-level <b>supervised</b> building reconstruction network that can flexibly utilize training samples with different annotation levels to achieve better reconstruction results in an end-to-end manner. To alleviate the demand on full 3D supervision, we design two new modules, Pseudo Building Bbox Calculator and Roof-Offset guided Footprint Extractor, as well as new tasks and training strategies for different types of samples. Experimental results on several public and new datasets demonstrate that our proposed MLS-BRN achieves competitive performance using much fewer 3D-annotated samples, and significantly improves the footprint extraction and 3D reconstruction performance compared with current state-of-the-art. The code and datasets of this work will be released at <a href=https://github.com/opendatalab/MLS-BRN.git>https://github.com/opendatalab/MLS-BRN.git</a>.</p></p class="citation"></blockquote><h3 id=4347--61152-joint-reconstruction-of-3d-human-and-object-via-contact-based-refinement-transformer-hyeongjin-nam-et-al-2024>(43/47 | 61/152) Joint Reconstruction of 3D Human and Object via Contact-Based Refinement Transformer (Hyeongjin Nam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyeongjin Nam, Daniel Sungho Jung, Gyeongsik Moon, Kyoung Mu Lee. (2024)<br><strong>Joint Reconstruction of 3D Human and Object via Contact-Based Refinement Transformer</strong><br><button class=copy-to-clipboard title="Joint Reconstruction of 3D Human and Object via Contact-Based Refinement Transformer" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04819v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04819v1.pdf filename=2404.04819v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human-object contact serves as a strong cue to understand how humans physically interact with objects. Nevertheless, it is not widely explored to utilize human-object contact information for the joint reconstruction of 3D human and object from a single image. In this work, we present a novel joint 3D human-object reconstruction method (CONTHO) that effectively exploits contact information between humans and objects. There are two core designs in our system: 1) 3D-guided contact estimation and 2) contact-based 3D human and object refinement. First, for accurate human-object contact estimation, CONTHO initially reconstructs 3D humans and objects and utilizes them as explicit 3D guidance for contact estimation. Second, to refine the initial reconstructions of 3D human and object, we propose a novel contact-based refinement <b>Transformer</b> that effectively aggregates human features and object features based on the estimated human-object contact. The proposed contact-based refinement prevents the learning of erroneous correlation between human and object, which enables accurate 3D reconstruction. As a result, our CONTHO achieves state-of-the-art performance in both human-object contact estimation and joint reconstruction of 3D human and object. The code is publicly available at <a href=https://github.com/dqj5182/CONTHO_RELEASE>https://github.com/dqj5182/CONTHO_RELEASE</a>.</p></p class="citation"></blockquote><h3 id=4447--62152-hilo-detailed-and-robust-3d-clothed-human-reconstruction-with-high-and-low-frequency-information-of-parametric-models-yifan-yang-et-al-2024>(44/47 | 62/152) HiLo: Detailed and Robust 3D Clothed Human Reconstruction with High-and Low-Frequency Information of Parametric Models (Yifan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Yang, Dong Liu, Shuhai Zhang, Zeshuai Deng, Zixiong Huang, Mingkui Tan. (2024)<br><strong>HiLo: Detailed and Robust 3D Clothed Human Reconstruction with High-and Low-Frequency Information of Parametric Models</strong><br><button class=copy-to-clipboard title="HiLo: Detailed and Robust 3D Clothed Human Reconstruction with High-and Low-Frequency Information of Parametric Models" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04876v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04876v1.pdf filename=2404.04876v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconstructing 3D clothed human involves creating a detailed <b>geometry</b> of individuals in clothing, with applications ranging from virtual try-on, movies, to games. To enable practical and widespread applications, recent advances propose to generate a clothed human from an RGB image. However, they struggle to reconstruct detailed and robust avatars simultaneously. We empirically find that the high-frequency (HF) and low-frequency (LF) information from a parametric model has the potential to enhance <b>geometry</b> details and improve robustness to noise, respectively. Based on this, we propose HiLo, namely clothed human reconstruction with high- and low-frequency information, which contains two components. 1) To recover detailed <b>geometry</b> using HF information, we propose a progressive HF Signed Distance Function to enhance the detailed 3D <b>geometry</b> of a clothed human. We analyze that our progressive learning manner alleviates large gradients that hinder model convergence. 2) To achieve robust reconstruction against inaccurate estimation of the parametric model by using LF information, we propose a spatial interaction implicit function. This function effectively exploits the complementary spatial information from a low-resolution voxel grid of the parametric model. Experimental results demonstrate that HiLo outperforms the state-of-the-art methods by 10.43% and 9.54% in terms of Chamfer distance on the Thuman2.0 and CAPE datasets, respectively. Additionally, HiLo demonstrates robustness to noise from the parametric model, challenging poses, and various clothing styles.</p></p class="citation"></blockquote><h3 id=4547--63152-logo-a-long-form-video-dataset-for-group-action-quality-assessment-shiyi-zhang-et-al-2024>(45/47 | 63/152) LOGO: A Long-Form Video Dataset for Group Action Quality Assessment (Shiyi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiyi Zhang, Wenxun Dai, Sujia Wang, Xiangwei Shen, Jiwen Lu, Jie Zhou, Yansong Tang. (2024)<br><strong>LOGO: A Long-Form Video Dataset for Group Action Quality Assessment</strong><br><button class=copy-to-clipboard title="LOGO: A Long-Form Video Dataset for Group Action Quality Assessment" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05029v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05029v1.pdf filename=2404.05029v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Action quality assessment (AQA) has become an emerging topic since it can be extensively applied in numerous scenarios. However, most existing methods and datasets focus on single-person short-sequence scenes, hindering the application of AQA in more complex situations. To address this issue, we construct a new multi-person long-form video dataset for action quality assessment named LOGO. Distinguished in scenario complexity, our dataset contains 200 videos from 26 artistic swimming events with 8 athletes in each sample along with an average duration of 204.2 seconds. As for richness in annotations, LOGO includes formation labels to depict group information of multiple athletes and detailed annotations on action procedures. Furthermore, we propose a simple yet effective method to model relations among athletes and reason about the potential temporal logic in long-form videos. Specifically, we design a group-aware attention module, which can be easily plugged into existing AQA methods, to enrich the clip-wise representations based on contextual group information. To <b>benchmark</b> LOGO, we systematically conduct investigations on the performance of several popular methods in AQA and action segmentation. The results reveal the challenges our dataset brings. Extensive experiments also show that our approach achieves state-of-the-art on the LOGO dataset. The dataset and code will be released at \url{https://github.com/shiyi-zh0408/LOGO }.</p></p class="citation"></blockquote><h3 id=4647--64152-efficient-learnable-collaborative-attention-for-single-image-super-resolution-yigang-zhao-chaowei-zheng-et-al-2024>(46/47 | 64/152) Efficient Learnable Collaborative Attention for Single Image Super-Resolution (Yigang Zhao Chaowei Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yigang Zhao Chaowei Zheng, Jiannan Su, GuangyongChen, MinGan. (2024)<br><strong>Efficient Learnable Collaborative Attention for Single Image Super-Resolution</strong><br><button class=copy-to-clipboard title="Efficient Learnable Collaborative Attention for Single Image Super-Resolution" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04922v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04922v1.pdf filename=2404.04922v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Non-Local Attention (NLA) is a powerful technique for capturing long-range feature correlations in deep single image super-resolution (SR). However, NLA suffers from high computational complexity and memory consumption, as it requires aggregating all non-local feature information for each query response and recalculating the similarity weight distribution for different abstraction levels of features. To address these challenges, we propose a novel Learnable Collaborative Attention (LCoA) that introduces inductive bias into non-local modeling. Our LCoA consists of two components: Learnable Sparse Pattern (LSP) and Collaborative Attention (CoA). LSP uses the k-means <b>clustering</b> algorithm to dynamically adjust the sparse attention pattern of deep features, which reduces the number of non-local modeling rounds compared with existing sparse solutions. CoA leverages the sparse attention pattern and weights learned by LSP, and co-optimizes the similarity matrix across different abstraction levels, which avoids redundant similarity matrix calculations. The experimental results show that our LCoA can reduce the non-local modeling time by about 83% in the inference stage. In addition, we integrate our LCoA into a deep Learnable Collaborative Attention Network (LCoAN), which achieves competitive performance in terms of inference time, memory consumption, and reconstruction quality compared with other state-of-the-art SR methods.</p></p class="citation"></blockquote><h3 id=4747--65152-gauu-scene-v2-expanse-lidar-image-dataset-shows-unreliable-geometric-reconstruction-using-gaussian-splatting-and-nerf-butian-xiong-et-al-2024>(47/47 | 65/152) GauU-Scene V2: Expanse Lidar Image Dataset Shows Unreliable Geometric Reconstruction Using Gaussian Splatting and NeRF (Butian Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Butian Xiong, Nanjun Zheng, Zhen Li. (2024)<br><strong>GauU-Scene V2: Expanse Lidar Image Dataset Shows Unreliable Geometric Reconstruction Using Gaussian Splatting and NeRF</strong><br><button class=copy-to-clipboard title="GauU-Scene V2: Expanse Lidar Image Dataset Shows Unreliable Geometric Reconstruction Using Gaussian Splatting and NeRF" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04880v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04880v1.pdf filename=2404.04880v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel large-scale scene reconstruction <b>benchmark</b> that utilizes newly developed 3D representation approaches: Gaussian Splatting and Neural Radiance Fields, on our expansive GauU-Scene V2 dataset. GauU-Scene V2 encompasses over 6.5 square kilometers and features a comprehensive RGB dataset coupled with LiDAR ground truth. This dataset offers a unique blend of urban and academic environments for advanced spatial analysis, covering more than 6.5 km2. We also provide detailed supplementary information on data collection protocols. Furthermore, we present an easy-to-follow pipeline to align the COLMAP sparse point cloud with the detailed LiDAR dataset. Our evaluation of U-Scene, which includes a detailed analysis across various novel viewpoints using image-based metrics such as SSIM, LPIPS, and PSNR, shows contradictory results when applying geometric-based metrics, such as Chamfer distance. This leads to doubts about the reliability of current image-based measurement matrices and geometric extraction methods on Gaussian Splatting. We also make the dataset available on the following anonymous project page</p></p class="citation"></blockquote><h2 id=cslg-26>cs.LG (26)</h2><h3 id=126--66152-contextual-chart-generation-for-cyber-deception-david-d-nguyen-et-al-2024>(1/26 | 66/152) Contextual Chart Generation for Cyber Deception (David D. Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David D. Nguyen, David Liebowitz, Surya Nepal, Salil S. Kanhere, Sharif Abuadbba. (2024)<br><strong>Contextual Chart Generation for Cyber Deception</strong><br><button class=copy-to-clipboard title="Contextual Chart Generation for Cyber Deception" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 83<br>Keywords: Autoencoder, Multi-modal, Bag-of-Words, ChatGPT, GPT-4, Transformer, Text Generation, Large Language Model, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04854v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04854v1.pdf filename=2404.04854v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Honeyfiles are <b>security</b> assets designed to attract and detect intruders on compromised systems. Honeyfiles are a type of honeypot that mimic real, sensitive documents, creating the illusion of the presence of valuable data. Interaction with a honeyfile reveals the presence of an intruder, and can provide insights into their goals and intentions. Their practical use, however, is limited by the time, cost and effort associated with manually creating realistic content. The introduction of <b>large</b> <b>language</b> <b>models</b> has made high-quality <b>text</b> <b>generation</b> accessible, but honeyfiles contain a variety of content including charts, tables and images. This content needs to be plausible and realistic, as well as semantically consistent both within honeyfiles and with the real documents they mimic, to successfully deceive an intruder. In this paper, we focus on an important component of the honeyfile content generation problem: document charts. Charts are ubiquitous in corporate documents and are commonly used to communicate quantitative and scientific data. Existing image generation models, such as DALL-E, are rather prone to generating charts with incomprehensible <b>text</b> <b>and</b> unconvincing data. We take a <b>multi-modal</b> approach to this problem by combining two purpose-built generative models: a multitask <b>Transformer</b> and a specialized multi-head <b>autoencoder.</b> The <b>Transformer</b> generates realistic captions and plot <b>text,</b> <b>while</b> the <b>autoencoder</b> generates the underlying tabular data for the plot. To advance the field of automated honeyplot generation, we also release a new document-chart dataset and propose a novel metric Keyword Semantic Matching (KSM). This metric measures the semantic consistency between keywords of a corpus and a smaller bag of words. Extensive experiments demonstrate excellent performance against multiple <b>large</b> <b>language</b> <b>models,</b> including <b>ChatGPT</b> and <b>GPT4.</b></p></p class="citation"></blockquote><h3 id=226--67152-initial-exploration-of-zero-shot-privacy-utility-tradeoffs-in-tabular-data-using-gpt-4-bishwas-mandal-et-al-2024>(2/26 | 67/152) Initial Exploration of Zero-Shot Privacy Utility Tradeoffs in Tabular Data Using GPT-4 (Bishwas Mandal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bishwas Mandal, George Amariucai, Shuangqing Wei. (2024)<br><strong>Initial Exploration of Zero-Shot Privacy Utility Tradeoffs in Tabular Data Using GPT-4</strong><br><button class=copy-to-clipboard title="Initial Exploration of Zero-Shot Privacy Utility Tradeoffs in Tabular Data Using GPT-4" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Fairness, Zero-shot, GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05047v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05047v1.pdf filename=2404.05047v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the application of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> specifically <b>GPT-4,</b> to scenarios involving the tradeoff between privacy and utility in tabular data. Our approach entails <b>prompting</b> <b>GPT-4</b> by transforming tabular data points into textual format, followed by the inclusion of precise sanitization instructions in a <b>zero-shot</b> manner. The primary objective is to sanitize the tabular data in such a way that it hinders existing machine learning models from accurately inferring private features while allowing models to accurately infer utility-related attributes. We explore various sanitization instructions. Notably, we discover that this relatively simple approach yields performance comparable to more complex adversarial optimization methods used for managing privacy-utility tradeoffs. Furthermore, while the <b>prompts</b> successfully obscure private features from the detection capabilities of existing machine learning models, we observe that this obscuration alone does not necessarily meet a range of <b>fairness</b> metrics. Nevertheless, our research indicates the potential effectiveness of <b>LLMs</b> in adhering to these <b>fairness</b> metrics, with some of our experimental results aligning with those achieved by well-established adversarial optimization techniques.</p></p class="citation"></blockquote><h3 id=326--68152-timegpt-in-load-forecasting-a-large-time-series-model-perspective-wenlong-liao-et-al-2024>(3/26 | 68/152) TimeGPT in Load Forecasting: A Large Time Series Model Perspective (Wenlong Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenlong Liao, Fernando Porte-Agel, Jiannong Fang, Christian Rehtanz, Shouxiang Wang, Dechang Yang, Zhe Yang. (2024)<br><strong>TimeGPT in Load Forecasting: A Large Time Series Model Perspective</strong><br><button class=copy-to-clipboard title="TimeGPT in Load Forecasting: A Large Time Series Model Perspective" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, Simulation, Simulator, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04885v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04885v1.pdf filename=2404.04885v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning models have made significant progress in load forecasting, but their forecast accuracy is limited in cases where historical load data is scarce. Inspired by the outstanding performance of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in computer vision and natural language processing, this paper aims to discuss the potential of <b>large</b> <b>time</b> <b>series</b> models in load forecasting with scarce historical data. Specifically, the <b>large</b> <b>time</b> <b>series</b> model is constructed as a time series generative pre-trained <b>transformer</b> (TimeGPT), which is trained on massive and diverse time series datasets consisting of 100 billion data points (e.g., finance, transportation, banking, web traffic, weather, energy, healthcare, etc.). Then, the scarce historical load data is used to <b>fine-tune</b> the TimeGPT, which helps it to adapt to the data distribution and characteristics associated with load forecasting. <b>Simulation</b> results show that TimeGPT outperforms the <b>benchmarks</b> (e.g., popular machine learning models and statistical models) for load forecasting on several real datasets with scarce training samples, particularly for short look-ahead times. However, it cannot be guaranteed that TimeGPT is always superior to <b>benchmarks</b> for load forecasting with scarce data, since the performance of TimeGPT may be affected by the distribution differences between the load data and the training data. In practical applications, we can divide the historical data into a training set and a validation set, and then use the validation set loss to decide whether TimeGPT is the best choice for a specific dataset.</p></p class="citation"></blockquote><h3 id=426--69152-adapting-llms-for-efficient-context-processing-through-soft-prompt-compression-cangqing-wang-et-al-2024>(4/26 | 69/152) Adapting LLMs for Efficient Context Processing through Soft Prompt Compression (Cangqing Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cangqing Wang, Yutian Yang, Ruisi Li, Dan Sun, Ruicong Cai, Yuzhu Zhang, Chengqian Fu, Lillian Floyd. (2024)<br><strong>Adapting LLMs for Efficient Context Processing through Soft Prompt Compression</strong><br><button class=copy-to-clipboard title="Adapting LLMs for Efficient Context Processing through Soft Prompt Compression" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Benchmarking, Text Generation, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04997v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04997v1.pdf filename=2404.04997v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has inaugurated a transformative epoch in natural language processing, fostering unprecedented proficiency in <b>text</b> <b>generation,</b> comprehension, and contextual scrutiny. Nevertheless, effectively handling extensive contexts, crucial for myriad applications, poses a formidable obstacle owing to the intrinsic constraints of the models&rsquo; context window sizes and the computational burdens entailed by their operations. This investigation presents an innovative framework that strategically tailors <b>LLMs</b> for streamlined context processing by harnessing the synergies among natural language <b>summarization,</b> soft <b>prompt</b> compression, and augmented utility preservation mechanisms. Our methodology, dubbed SoftPromptComp, amalgamates natural language <b>prompts</b> extracted from <b>summarization</b> methodologies with dynamically generated soft <b>prompts</b> to forge a concise yet semantically robust depiction of protracted contexts. This depiction undergoes further refinement via a weighting mechanism optimizing information retention and utility for subsequent tasks. We substantiate that our framework markedly diminishes computational overhead and enhances <b>LLMs&rsquo;</b> efficacy across various <b>benchmarks,</b> while upholding or even augmenting the caliber of the produced content. By amalgamating soft <b>prompt</b> compression with sophisticated <b>summarization,</b> SoftPromptComp confronts the dual challenges of managing lengthy contexts and ensuring model scalability. Our findings point towards a propitious trajectory for augmenting <b>LLMs&rsquo;</b> applicability and efficiency, rendering them more versatile and pragmatic for real-world applications. This research enriches the ongoing discourse on optimizing language models, providing insights into the potency of soft <b>prompts</b> and <b>summarization</b> techniques as pivotal instruments for the forthcoming generation of NLP solutions.</p></p class="citation"></blockquote><h3 id=526--70152-timecsl-unsupervised-contrastive-learning-of-general-shapelets-for-explorable-time-series-analysis-zhiyu-liang-et-al-2024>(5/26 | 70/152) TimeCSL: Unsupervised Contrastive Learning of General Shapelets for Explorable Time Series Analysis (Zhiyu Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyu Liang, Chen Liang, Zheng Liang, Hongzhi Wang, Bo Zheng. (2024)<br><strong>TimeCSL: Unsupervised Contrastive Learning of General Shapelets for Explorable Time Series Analysis</strong><br><button class=copy-to-clipboard title="TimeCSL: Unsupervised Contrastive Learning of General Shapelets for Explorable Time Series Analysis" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DB, cs-LG, cs.LG<br>Keyword Score: 48<br>Keywords: Anomaly Detection, Clustering, Contrastive Learning, Representation Learning, Self-supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05057v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05057v1.pdf filename=2404.05057v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> (a.k.a. <b>Self-supervised)</b> <b>representation</b> <b>learning</b> (URL) has emerged as a new paradigm for time series analysis, because it has the ability to learn generalizable time series <b>representation</b> <b>beneficial</b> for many downstream tasks without using labels that are usually difficult to obtain. Considering that existing approaches have limitations in the design of the <b>representation</b> <b>encoder</b> and the learning objective, we have proposed <b>Contrastive</b> <b>Shapelet</b> Learning (CSL), the first URL method that learns the general-purpose shapelet-based <b>representation</b> <b>through</b> <b>unsupervised</b> <b>contrastive</b> <b>learning,</b> and shown its superior performance in several analysis tasks, such as time series classification, <b>clustering,</b> and <b>anomaly</b> <b>detection.</b> In this paper, we develop TimeCSL, an end-to-end system that makes full use of the general and interpretable shapelets learned by CSL to achieve explorable time series analysis in a unified pipeline. We introduce the system components and demonstrate how users interact with TimeCSL to solve different analysis tasks in the unified pipeline, and gain insight into their time series by exploring the learned shapelets and representation.</p></p class="citation"></blockquote><h3 id=626--71152-graph-neural-networks-for-binary-programming-moshe-eliasof-et-al-2024>(6/26 | 71/152) Graph Neural Networks for Binary Programming (Moshe Eliasof et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Moshe Eliasof, Eldad Haber. (2024)<br><strong>Graph Neural Networks for Binary Programming</strong><br><button class=copy-to-clipboard title="Graph Neural Networks for Binary Programming" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-ET, cs-LG, cs.LG<br>Keyword Score: 48<br>Keywords: Node Classification, Graph, Graph Neural Network, Graph Neural Network, Representation Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04874v1.pdf filename=2404.04874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates a link between <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> and Binary Programming (BP) problems, laying the groundwork for <b>GNNs</b> to approximate solutions for these computationally challenging problems. By analyzing the sensitivity of BP problems, we are able to frame the solution of BP problems as a heterophilic <b>node</b> <b>classification</b> task. We then propose Binary-Programming <b>GNN</b> (BPGNN), an architecture that integrates <b>graph</b> <b>representation</b> <b>learning</b> techniques with BP-aware features to approximate BP solutions efficiently. Additionally, we introduce a <b>self-supervised</b> data generation mechanism, to enable efficient and tractable training data acquisition even for large-scale BP problems. Experimental evaluations of BPGNN across diverse BP problem sizes showcase its superior performance compared to exhaustive search and heuristic approaches. Finally, we discuss open challenges in the under-explored field of BP problems with <b>GNNs.</b></p></p class="citation"></blockquote><h3 id=726--72152-temporal-generalization-estimation-in-evolving-graphs-bin-lu-et-al-2024>(7/26 | 72/152) Temporal Generalization Estimation in Evolving Graphs (Bin Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bin Lu, Tingyan Ma, Xiaoying Gan, Xinbing Wang, Yunqiang Zhu, Chenghu Zhou, Shiyu Liang. (2024)<br><strong>Temporal Generalization Estimation in Evolving Graphs</strong><br><button class=copy-to-clipboard title="Temporal Generalization Estimation in Evolving Graphs" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Self-supervised Learning, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04969v1.pdf filename=2404.04969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> are widely deployed in vast fields, but they often struggle to maintain accurate representations as <b>graphs</b> <b>evolve.</b> <b>We</b> theoretically establish a lower bound, proving that under mild conditions, representation distortion inevitably occurs over time. To estimate the temporal distortion without human annotation after deployment, one naive approach is to pre-train a recurrent model (e.g., <b>RNN)</b> before deployment and use this model afterwards, but the estimation is far from satisfactory. In this paper, we analyze the representation distortion from an information theory perspective, and attribute it primarily to inaccurate feature extraction during evolution. Consequently, we introduce Smart, a straightforward and effective baseline enhanced by an adaptive feature extractor through <b>self-supervised</b> <b>graph</b> <b>reconstruction.</b> <b>In</b> synthetic random <b>graphs,</b> <b>we</b> <b>further</b> refine the former lower bound to show the inevitable distortion over time and empirically observe that Smart achieves good estimation performance. Moreover, we observe that Smart consistently shows outstanding generalization estimation on four real-world evolving <b>graphs.</b> <b>The</b> <b>ablation</b> studies underscore the necessity of <b>graph</b> <b>reconstruction.</b> <b>For</b> example, on OGB-arXiv dataset, the estimation metric MAPE deteriorates from 2.19% to 8.00% without reconstruction.</p></p class="citation"></blockquote><h3 id=826--73152-active-test-time-adaptation-theoretical-analyses-and-an-algorithm-shurui-gui-et-al-2024>(8/26 | 73/152) Active Test-Time Adaptation: Theoretical Analyses and An Algorithm (Shurui Gui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shurui Gui, Xiner Li, Shuiwang Ji. (2024)<br><strong>Active Test-Time Adaptation: Theoretical Analyses and An Algorithm</strong><br><button class=copy-to-clipboard title="Active Test-Time Adaptation: Theoretical Analyses and An Algorithm" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Active Learning, Distribution Shift, Distribution Shift, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05094v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05094v1.pdf filename=2404.05094v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Test-time adaptation (TTA) addresses <b>distribution</b> <b>shifts</b> for streaming test data in <b>unsupervised</b> settings. Currently, most TTA methods can only deal with minor shifts and rely heavily on heuristic and empirical studies. To advance TTA under <b>domain</b> <b>shifts,</b> we propose the novel problem setting of <b>active</b> <b>test-time</b> adaptation (ATTA) that integrates <b>active</b> <b>learning</b> within the fully TTA setting. We provide a learning theory analysis, demonstrating that incorporating limited labeled test instances enhances overall performances across test <b>domains</b> <b>with</b> a theoretical guarantee. We also present a sample entropy balancing for implementing ATTA while avoiding catastrophic forgetting (CF). We introduce a simple yet effective ATTA algorithm, known as SimATTA, using real-time sample selection techniques. Extensive experimental results confirm consistency with our theoretical analyses and show that the proposed ATTA method yields substantial performance improvements over TTA methods while maintaining efficiency and shares similar effectiveness to the more demanding <b>active</b> <b>domain</b> <b>adaptation</b> (ADA) methods. Our code is available at <a href=https://github.com/divelab/ATTA>https://github.com/divelab/ATTA</a></p></p class="citation"></blockquote><h3 id=926--74152-on-the-learnability-of-out-of-distribution-detection-zhen-fang-et-al-2024>(9/26 | 74/152) On the Learnability of Out-of-distribution Detection (Zhen Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Fang, Yixuan Li, Feng Liu, Bo Han, Jie Lu. (2024)<br><strong>On the Learnability of Out-of-distribution Detection</strong><br><button class=copy-to-clipboard title="On the Learnability of Out-of-distribution Detection" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04865v1.pdf filename=2404.04865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Supervised</b> <b>learning</b> aims to train a classifier under the assumption that training and test data are from the same distribution. To ease the above assumption, researchers have studied a more realistic setting: <b>out-of-distribution</b> (OOD) detection, where test data may come from classes that are unknown during training (i.e., OOD data). Due to the unavailability and diversity of OOD data, good generalization ability is crucial for effective OOD detection algorithms, and corresponding learning theory is still an open problem. To study the generalization of OOD detection, this paper investigates the probably approximately correct (PAC) learning theory of OOD detection that fits the commonly used evaluation metrics in the literature. First, we find a necessary condition for the learnability of OOD detection. Then, using this condition, we prove several impossibility theorems for the learnability of OOD detection under some scenarios. Although the impossibility theorems are frustrating, we find that some conditions of these impossibility theorems may not hold in some practical scenarios. Based on this observation, we next give several necessary and sufficient conditions to characterize the learnability of OOD detection in some practical scenarios. Lastly, we offer theoretical support for representative OOD detection works based on our OOD theory.</p></p class="citation"></blockquote><h3 id=1026--75152-mixup-domain-adaptations-for-dynamic-remaining-useful-life-predictions-muhammad-tanzil-furqon-et-al-2024>(10/26 | 75/152) Mixup Domain Adaptations for Dynamic Remaining Useful Life Predictions (Muhammad Tanzil Furqon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Tanzil Furqon, Mahardhika Pratama, Lin Liu, Habibullah, Kutluyil Dogancay. (2024)<br><strong>Mixup Domain Adaptations for Dynamic Remaining Useful Life Predictions</strong><br><button class=copy-to-clipboard title="Mixup Domain Adaptations for Dynamic Remaining Useful Life Predictions" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Self-supervised Learning, Self-supervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04824v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04824v1.pdf filename=2404.04824v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remaining Useful Life (RUL) predictions play vital role for asset planning and maintenance leading to many benefits to industries such as reduced downtime, low maintenance costs, etc. Although various efforts have been devoted to study this topic, most existing works are restricted for i.i.d conditions assuming the same condition of the training phase and the deployment phase. This paper proposes a solution to this problem where a mix-up <b>domain</b> <b>adaptation</b> (MDAN) is put forward. MDAN encompasses a three-staged mechanism where the mix-up strategy is not only performed to regularize the source and target <b>domains</b> <b>but</b> also applied to establish an intermediate mix-up <b>domain</b> <b>where</b> the source and target <b>domains</b> <b>are</b> aligned. The <b>self-supervised</b> <b>learning</b> strategy is implemented to prevent the supervision collapse problem. Rigorous evaluations have been performed where MDAN is compared to recently published works for dynamic RUL predictions. MDAN outperforms its counterparts with substantial margins in 12 out of 12 cases. In addition, MDAN is evaluated with the bearing machine dataset where it beats prior art with significant gaps in 8 of 12 cases. Source codes of MDAN are made publicly available in \url{https://github.com/furqon3009/MDAN}.</p></p class="citation"></blockquote><h3 id=1126--76152-inference-time-rule-eraser-distilling-and-removing-bias-rules-to-mitigate-bias-in-deployed-models-yi-zhang-et-al-2024>(11/26 | 76/152) Inference-Time Rule Eraser: Distilling and Removing Bias Rules to Mitigate Bias in Deployed Models (Yi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Zhang, Jitao Sang. (2024)<br><strong>Inference-Time Rule Eraser: Distilling and Removing Bias Rules to Mitigate Bias in Deployed Models</strong><br><button class=copy-to-clipboard title="Inference-Time Rule Eraser: Distilling and Removing Bias Rules to Mitigate Bias in Deployed Models" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fairness, Fine-tuning, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04814v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04814v1.pdf filename=2404.04814v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fairness</b> is critical for artificial intelligence systems, especially for those deployed in high-stakes applications such as hiring and justice. Existing efforts toward <b>fairness</b> in machine learning <b>fairness</b> require retraining or <b>fine-tuning</b> the neural network weights to meet the <b>fairness</b> criteria. However, this is often not feasible in practice for regular model users due to the inability to access and modify model weights. In this paper, we propose a more flexible <b>fairness</b> paradigm, Inference-Time Rule Eraser, or simply Eraser, which considers the case where model weights can not be accessed and tackles <b>fairness</b> issues from the perspective of biased rules removal at inference-time. We first verified the feasibility of modifying the model output to wipe the biased rule through Bayesian analysis, and deduced Inference-Time Rule Eraser via subtracting the logarithmic value associated with unfair rules (i.e., the model&rsquo;s response to biased features) from the model&rsquo;s logits output as a means of removing biased rules. Moreover, we present a specific implementation of Rule Eraser that involves two stages: (1) limited queries are performed on the model with inaccessible weights to <b>distill</b> its biased rules into an additional patched model, and (2) during inference time, the biased rules already <b>distilled</b> into the patched model are excluded from the output of the original model, guided by the removal strategy outlined in Rule Eraser. Exhaustive experimental evaluation demonstrates the effectiveness and superior performance of the proposed Rule Eraser in addressing <b>fairness</b> concerns.</p></p class="citation"></blockquote><h3 id=1226--77152-data-stream-sampling-with-fuzzy-task-boundaries-and-noisy-labels-yu-hsi-chen-2024>(12/26 | 77/152) Data Stream Sampling with Fuzzy Task Boundaries and Noisy Labels (Yu-Hsi Chen, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu-Hsi Chen. (2024)<br><strong>Data Stream Sampling with Fuzzy Task Boundaries and Noisy Labels</strong><br><button class=copy-to-clipboard title="Data Stream Sampling with Fuzzy Task Boundaries and Noisy Labels" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Continual Learning, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04871v1.pdf filename=2404.04871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of <b>continual</b> <b>learning,</b> the presence of noisy labels within data streams represents a notable obstacle to model reliability and <b>fairness.</b> We focus on the data stream scenario outlined in pertinent literature, characterized by fuzzy task boundaries and noisy labels. To address this challenge, we introduce a novel and intuitive sampling method called Noisy Test Debiasing (NTD) to mitigate noisy labels in evolving data streams and establish a fair and robust <b>continual</b> <b>learning</b> algorithm. NTD is straightforward to implement, making it feasible across various scenarios. Our experiments <b>benchmark</b> four datasets, including two synthetic noise datasets (CIFAR10 and CIFAR100) and real-world noise datasets (mini-WebVision and Food-101N). The results validate the efficacy of NTD for online <b>continual</b> <b>learning</b> in scenarios with noisy labels in data streams. Compared to the previous leading approach, NTD achieves a training speedup enhancement over two times while maintaining or surpassing accuracy levels. Moreover, NTD utilizes less than one-fifth of the GPU memory resources compared to previous leading methods.</p></p class="citation"></blockquote><h3 id=1326--78152-squeezeattention-2d-management-of-kv-cache-in-llm-inference-via-layer-wise-optimal-budget-zihao-wang-et-al-2024>(13/26 | 78/152) SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget (Zihao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihao Wang, Shaoduo Gan. (2024)<br><strong>SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget</strong><br><button class=copy-to-clipboard title="SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04793v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04793v1.pdf filename=2404.04793v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optimizing the Key-Value (KV) cache of the <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> has been considered critical to saving the cost of inference. Most of the existing KV-cache compression algorithms attempted to sparsify the sequence of tokens by taking advantage of the different importance of tokens. In this work, we found that by identifying the importance of attention layers, we could optimize the KV-cache jointly from two dimensions. Based on our observations regarding layer-wise importance in inference, we propose SqueezeAttention to precisely optimize the allocation of KV-cache budget among layers on-the-fly and then incorporate three representative token sparsification algorithms to compress the KV-cache for each layer with its very own budget. By optimizing the KV-cache from both sequence&rsquo;s and layer&rsquo;s dimensions, SqueezeAttention achieves around 30% to 70% of the memory reductions and up to 2.2 times of throughput improvements in a wide range of <b>LLMs</b> and <b>benchmarks.</b> The code is available at <a href=https://github.com/hetailang/SqueezeAttention>https://github.com/hetailang/SqueezeAttention</a>.</p></p class="citation"></blockquote><h3 id=1426--79152-a-note-on-lora-vlad-fomenko-et-al-2024>(14/26 | 79/152) A Note on LoRA (Vlad Fomenko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vlad Fomenko, Han Yu, Jongho Lee, Stanley Hsieh, Weizhu Chen. (2024)<br><strong>A Note on LoRA</strong><br><button class=copy-to-clipboard title="A Note on LoRA" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05086v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05086v1.pdf filename=2404.05086v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>LoRA (Low-Rank Adaptation) has emerged as a preferred method for efficiently adapting <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with remarkable simplicity and efficacy. This note extends the original LoRA paper by offering new perspectives that were not initially discussed and presents a series of insights for deploying LoRA at scale. Without introducing new experiments, we aim to improve the understanding and application of LoRA.</p></p class="citation"></blockquote><h3 id=1526--80152-a-robust-assessment-for-invariant-representations-wenlu-tang-et-al-2024>(15/26 | 80/152) A robust assessment for invariant representations (Wenlu Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenlu Tang, Zicheng Liu. (2024)<br><strong>A robust assessment for invariant representations</strong><br><button class=copy-to-clipboard title="A robust assessment for invariant representations" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Data Augmentation, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05058v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05058v1.pdf filename=2404.05058v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The performance of machine learning models can be impacted by changes in <b>data</b> <b>over</b> time. A promising approach to address this challenge is invariant learning, with a particular focus on a method known as invariant risk minimization (IRM). This technique aims to identify a stable <b>data</b> <b>representation</b> that remains effective with <b>out-of-distribution</b> (OOD) <b>data.</b> <b>While</b> numerous studies have developed IRM-based methods adaptive to <b>data</b> <b>augmentation</b> scenarios, there has been limited attention on directly assessing how well these representations preserve their invariant performance under varying conditions. In our paper, we propose a novel method to evaluate invariant performance, specifically tailored for IRM-based methods. We establish a bridge between the conditional expectation of an invariant predictor across different environments through the likelihood ratio. Our proposed criterion offers a robust basis for evaluating invariant performance. We validate our approach with theoretical support and demonstrate its effectiveness through extensive numerical studies.These experiments illustrate how our method can assess the invariant performance of various representation techniques.</p></p class="citation"></blockquote><h3 id=1626--81152-percentile-criterion-optimization-in-offline-reinforcement-learning-elita-a-lobo-et-al-2024>(16/26 | 81/152) Percentile Criterion Optimization in Offline Reinforcement Learning (Elita A. Lobo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elita A. Lobo, Cyrus Cousins, Yair Zick, Marek Petrik. (2024)<br><strong>Percentile Criterion Optimization in Offline Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Percentile Criterion Optimization in Offline Reinforcement Learning" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05055v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05055v1.pdf filename=2404.05055v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>reinforcement</b> <b>learning,</b> robust policies for high-stakes decision-making problems with limited data are usually computed by optimizing the \emph{percentile criterion}. The percentile criterion is approximately solved by constructing an \emph{ambiguity set} that contains the true model with high probability and optimizing the policy for the worst model in the set. Since the percentile criterion is non-convex, constructing ambiguity sets is often challenging. Existing work uses \emph{Bayesian credible regions} as ambiguity sets, but they are often unnecessarily large and result in learning overly conservative policies. To overcome these shortcomings, we propose a novel Value-at-Risk based dynamic programming algorithm to optimize the percentile criterion without explicitly constructing any ambiguity sets. Our theoretical and empirical results show that our algorithm implicitly constructs much smaller ambiguity sets and learns less conservative robust policies.</p></p class="citation"></blockquote><h3 id=1726--82152-regularized-conditional-diffusion-model-for-multi-task-preference-alignment-xudong-yu-et-al-2024>(17/26 | 82/152) Regularized Conditional Diffusion Model for Multi-Task Preference Alignment (Xudong Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xudong Yu, Chenjia Bai, Haoran He, Changhong Wang, Xuelong Li. (2024)<br><strong>Regularized Conditional Diffusion Model for Multi-Task Preference Alignment</strong><br><button class=copy-to-clipboard title="Regularized Conditional Diffusion Model for Multi-Task Preference Alignment" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Diffusion Model, Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04920v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04920v1.pdf filename=2404.04920v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sequential decision-making is desired to align with human intents and exhibit versatility across various tasks. Previous methods formulate it as a conditional generation process, utilizing return-conditioned <b>diffusion</b> <b>models</b> to directly model trajectory distributions. Nevertheless, the return-conditioned paradigm relies on pre-defined reward functions, facing challenges when applied in multi-task settings characterized by varying reward functions (versatility) and showing limited controllability concerning human preferences (alignment). In this work, we adopt multi-task preferences as a unified condition for both single- and multi-task decision-making, and propose preference representations aligned with preference labels. The learned representations are used to guide the conditional generation process of <b>diffusion</b> <b>models,</b> and we introduce an auxiliary objective to maximize the <b>mutual</b> <b>information</b> between representations and corresponding generated trajectories, improving alignment between trajectories and preferences. Extensive experiments in D4RL and Meta-World demonstrate that our method presents favorable performance in single- and multi-task scenarios, and exhibits superior alignment with preferences.</p></p class="citation"></blockquote><h3 id=1826--83152-gradient-based-design-of-computational-granular-crystals-atoosa-parsa-et-al-2024>(18/26 | 83/152) Gradient-based Design of Computational Granular Crystals (Atoosa Parsa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atoosa Parsa, Corey S. O&rsquo;Hern, Rebecca Kramer-Bottiglio, Josh Bongard. (2024)<br><strong>Gradient-based Design of Computational Granular Crystals</strong><br><button class=copy-to-clipboard title="Gradient-based Design of Computational Granular Crystals" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-ET, cs-LG, cs-NE, cs.LG<br>Keyword Score: 15<br>Keywords: Geometry, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04825v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04825v1.pdf filename=2404.04825v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is growing interest in engineering unconventional computing devices that leverage the intrinsic dynamics of physical substrates to perform fast and energy-efficient computations. Granular metamaterials are one such substrate that has emerged as a promising platform for building wave-based information processing devices with the potential to integrate sensing, actuation, and computation. Their high-dimensional and nonlinear dynamics result in nontrivial and sometimes counter-intuitive wave responses that can be shaped by the material properties, <b>geometry,</b> and configuration of individual grains. Such highly tunable rich dynamics can be utilized for mechanical computing in special-purpose applications. However, there are currently no general frameworks for the inverse design of large-scale granular materials. Here, we build upon the similarity between the spatiotemporal dynamics of wave propagation in material and the computational dynamics of <b>Recurrent</b> <b>Neural</b> <b>Networks</b> to develop a gradient-based optimization framework for harmonically driven granular crystals. We showcase how our framework can be utilized to design basic logic gates where mechanical vibrations carry the information at predetermined frequencies. We compare our design methodology with classic gradient-free methods and find that our approach discovers higher-performing configurations with less computational effort. Our findings show that a gradient-based optimization method can greatly expand the design space of metamaterials and provide the opportunity to systematically traverse the parameter space to find materials with the desired functionalities.</p></p class="citation"></blockquote><h3 id=1926--84152-chiplet-placement-order-exploration-based-on-learning-to-rank-with-graph-representation-zhihui-deng-et-al-2024>(19/26 | 84/152) Chiplet Placement Order Exploration Based on Learning to Rank with Graph Representation (Zhihui Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihui Deng, Yuanyuan Duan, Leilai Shao, Xiaolei Zhu. (2024)<br><strong>Chiplet Placement Order Exploration Based on Learning to Rank with Graph Representation</strong><br><button class=copy-to-clipboard title="Chiplet Placement Order Exploration Based on Learning to Rank with Graph Representation" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-AR, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04943v1.pdf filename=2404.04943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chiplet-based systems, integrating various silicon dies manufactured at different integrated circuit technology nodes on a carrier interposer, have garnered significant attention in recent years due to their cost-effectiveness and competitive performance. The widespread adoption of <b>reinforcement</b> <b>learning</b> as a sequential placement method has introduced a new challenge in determining the optimal placement order for each chiplet. The order in which chiplets are placed on the interposer influences the spatial resources available for earlier and later placed chiplets, making the placement results highly sensitive to the sequence of chiplet placement. To address these challenges, we propose a learning to rank approach with <b>graph</b> representation, building upon the <b>reinforcement</b> <b>learning</b> framework RLPlanner. This method aims to select the optimal chiplet placement order for each chiplet-based system. Experimental results demonstrate that compared to placement order obtained solely based on the descending order of the chiplet area and the number of interconnect wires between the chiplets, utilizing the placement order obtained from the learning to rank network leads to further improvements in system temperature and inter-chiplet wirelength. Specifically, applying the top-ranked placement order obtained from the learning to rank network results in a 10.05% reduction in total inter-chiplet wirelength and a 1.01% improvement in peak system temperature during the chiplet placement process.</p></p class="citation"></blockquote><h3 id=2026--85152-fuzzy-k-means-clustering-without-cluster-centroids-han-lu-et-al-2024>(20/26 | 85/152) Fuzzy K-Means Clustering without Cluster Centroids (Han Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Lu, Fangfang Li, Quanxue Gao, Cheng Deng, Chris Ding, Qianqian Wang. (2024)<br><strong>Fuzzy K-Means Clustering without Cluster Centroids</strong><br><button class=copy-to-clipboard title="Fuzzy K-Means Clustering without Cluster Centroids" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Clustering, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04940v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04940v1.pdf filename=2404.04940v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fuzzy K-Means <b>clustering</b> is a critical technique in <b>unsupervised</b> data analysis. However, the performance of popular Fuzzy K-Means algorithms is sensitive to the selection of initial cluster centroids and is also affected by noise when updating mean cluster centroids. To address these challenges, this paper proposes a novel Fuzzy K-Means <b>clustering</b> algorithm that entirely eliminates the reliance on cluster centroids, obtaining membership matrices solely through distance matrix computation. This innovation enhances flexibility in distance measurement between sample points, thus improving the algorithm&rsquo;s performance and robustness. The paper also establishes theoretical connections between the proposed model and popular Fuzzy K-Means <b>clustering</b> techniques. Experimental results on several real datasets demonstrate the effectiveness of the algorithm.</p></p class="citation"></blockquote><h3 id=2126--86152-test-time-training-for-depression-detection-sri-harsha-dumpala-et-al-2024>(21/26 | 86/152) Test-Time Training for Depression Detection (Sri Harsha Dumpala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sri Harsha Dumpala, Chandramouli Shama Sastry, Rudolf Uher, Sageev Oore. (2024)<br><strong>Test-Time Training for Depression Detection</strong><br><button class=copy-to-clipboard title="Test-Time Training for Depression Detection" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SD, cs.LG, eess-AS<br>Keyword Score: 10<br>Keywords: Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05071v1.pdf filename=2404.05071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous works on depression detection use datasets collected in similar environments to train and test the models. In practice, however, the train and test <b>distributions</b> <b>cannot</b> be guaranteed to be identical. <b>Distribution</b> <b>shifts</b> can be introduced due to variations such as recording environment (e.g., background noise) and demographics (e.g., gender, age, etc). Such <b>distributional</b> <b>shifts</b> can surprisingly lead to severe performance degradation of the depression detection models. In this paper, we analyze the application of test-time training (TTT) to improve robustness of models trained for depression detection. When compared to regular testing of the models, we find TTT can significantly improve the robustness of the model under a variety of <b>distributional</b> <b>shifts</b> introduced due to: (a) background-noise, (b) gender-bias, and (c) data collection and curation procedure (i.e., train and test samples are from separate datasets).</p></p class="citation"></blockquote><h3 id=2226--87152-shortcut-connected-expert-parallelism-for-accelerating-mixture-of-experts-weilin-cai-et-al-2024>(22/26 | 87/152) Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts (Weilin Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, Jiayi Huang. (2024)<br><strong>Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts</strong><br><button class=copy-to-clipboard title="Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05019v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05019v1.pdf filename=2404.05019v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Expert parallelism has been introduced as a strategy to distribute the computational workload of sparsely-gated mixture-of-experts (MoE) models across multiple computing devices, facilitating the execution of these increasingly large-scale models. However, the All-to-All communication intrinsic to expert parallelism constitutes a significant overhead, diminishing the MoE models&rsquo; efficiency. Current optimization approaches offer some relief, yet they are constrained by the sequential interdependence of communication and computation operations. To address this limitation, we present a novel shortcut-connected MoE architecture with overlapping parallel strategy, designated as ScMoE, which effectively decouples communication from its conventional sequence, allowing for a substantial overlap of 70% to 100% with computation. When compared with the prevalent top-2 MoE architecture, ScMoE demonstrates training speed improvements of 30% and 11%, and inference improvements of 40% and 15%, in our PCIe and NVLink hardware environments, respectively, where communication constitutes 60% and 15% of the total MoE time consumption. On the other hand, extensive experiments and theoretical analyses indicate that ScMoE not only achieves comparable but in some instances surpasses the model quality of existing approaches in vision and language tasks.</p></p class="citation"></blockquote><h3 id=2326--88152-signal-noise-separation-using-unsupervised-reservoir-computing-jaesung-choi-et-al-2024>(23/26 | 88/152) Signal-noise separation using unsupervised reservoir computing (Jaesung Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaesung Choi, Pilwon Kim. (2024)<br><strong>Signal-noise separation using unsupervised reservoir computing</strong><br><button class=copy-to-clipboard title="Signal-noise separation using unsupervised reservoir computing" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP, nlin-CD<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04870v1.pdf filename=2404.04870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Removing noise from a signal without knowing the characteristics of the noise is a challenging task. This paper introduces a signal-noise separation method based on time series prediction. We use Reservoir Computing (RC) to extract the maximum portion of &ldquo;predictable information&rdquo; from a given signal. Reproducing the deterministic component of the signal using RC, we estimate the noise distribution from the difference between the original signal and reconstructed one. The method is based on a machine learning approach and requires no prior knowledge of either the deterministic signal or the noise distribution. It provides a way to identify additivity/multiplicativity of noise and to estimate the signal-to-noise ratio (SNR) indirectly. The method works successfully for combinations of various signal and noise, including chaotic signal and highly oscillating sinusoidal signal which are corrupted by non-Gaussian additive/ multiplicative noise. The separation performances are robust and notably outstanding for signals with strong noise, even for those with negative SNR.</p></p class="citation"></blockquote><h3 id=2426--89152-skill-transfer-and-discovery-for-sim-to-real-learning-a-representation-based-viewpoint-haitong-ma-et-al-2024>(24/26 | 89/152) Skill Transfer and Discovery for Sim-to-Real Learning: A Representation-Based Viewpoint (Haitong Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haitong Ma, Zhaolin Ren, Bo Dai, Na Li. (2024)<br><strong>Skill Transfer and Discovery for Sim-to-Real Learning: A Representation-Based Viewpoint</strong><br><button class=copy-to-clipboard title="Skill Transfer and Discovery for Sim-to-Real Learning: A Representation-Based Viewpoint" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-RO, cs.LG<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05051v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05051v1.pdf filename=2404.05051v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study sim-to-real skill transfer and discovery in the context of robotics control using <b>representation</b> <b>learning.</b> We draw inspiration from spectral decomposition of Markov decision processes. The spectral decomposition brings about <b>representation</b> <b>that</b> can linearly represent the state-action value function induced by any policies, thus can be regarded as skills. The skill <b>representations</b> <b>are</b> transferable across arbitrary tasks with the same transition dynamics. Moreover, to handle the sim-to-real gap in the dynamics, we propose a skill discovery algorithm that learns new skills caused by the sim-to-real gap from real-world data. We promote the discovery of new skills by enforcing orthogonal constraints between the skills to learn and the skills from simulators, and then synthesize the policy using the enlarged skill sets. We demonstrate our methodology by transferring quadrotor controllers from simulators to Crazyflie 2.1 quadrotors. We show that we can learn the skill <b>representations</b> <b>from</b> a single simulator task and transfer these to multiple different real-world tasks including hovering, taking off, landing and trajectory tracking. Our skill discovery approach helps narrow the sim-to-real gap and improve the real-world controller performance by up to 30.2%.</p></p class="citation"></blockquote><h3 id=2526--90152-demystifying-lazy-training-of-neural-networks-from-a-macroscopic-viewpoint-yuqing-li-et-al-2024>(25/26 | 90/152) Demystifying Lazy Training of Neural Networks from a Macroscopic Viewpoint (Yuqing Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqing Li, Tao Luo, Qixuan Zhou. (2024)<br><strong>Demystifying Lazy Training of Neural Networks from a Macroscopic Viewpoint</strong><br><button class=copy-to-clipboard title="Demystifying Lazy Training of Neural Networks from a Macroscopic Viewpoint" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 5<br>Keywords: Deep Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04859v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04859v1.pdf filename=2404.04859v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we advance the understanding of neural network training dynamics by examining the intricate interplay of various factors introduced by weight parameters in the initialization process. Motivated by the foundational work of Luo et al. (J. Mach. Learn. Res., Vol. 22, Iss. 1, No. 71, pp 3327-3373), we explore the gradient descent dynamics of neural networks through the lens of macroscopic limits, where we analyze its behavior as width $m$ tends to infinity. Our study presents a unified approach with refined techniques designed for multi-layer fully connected neural networks, which can be readily extended to other neural network architectures. Our investigation reveals that gradient descent can rapidly drive <b>deep</b> <b>neural</b> <b>networks</b> to zero training loss, irrespective of the specific initialization schemes employed by weight parameters, provided that the initial scale of the output function $\kappa$ surpasses a certain threshold. This regime, characterized as the theta-lazy area, accentuates the predominant influence of the initial scale $\kappa$ over other factors on the training behavior of neural networks. Furthermore, our approach draws inspiration from the Neural Tangent Kernel (NTK) paradigm, and we expand its applicability. While NTK typically assumes that $\lim_{m\to\infty}\frac{\log \kappa}{\log m}=\frac{1}{2}$, and imposes each weight parameters to scale by the factor $\frac{1}{\sqrt{m}}$, in our theta-lazy regime, we discard the factor and relax the conditions to $\lim_{m\to\infty}\frac{\log \kappa}{\log m}>0$. Similar to NTK, the behavior of overparameterized neural networks within the theta-lazy regime trained by gradient descent can be effectively described by a specific kernel. Through rigorous analysis, our investigation illuminates the pivotal role of $\kappa$ in governing the training dynamics of neural networks.</p></p class="citation"></blockquote><h3 id=2626--91152-the-sample-complexity-of-gradient-descent-in-stochastic-convex-optimization-roi-livni-2024>(26/26 | 91/152) The Sample Complexity of Gradient Descent in Stochastic Convex Optimization (Roi Livni, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roi Livni. (2024)<br><strong>The Sample Complexity of Gradient Descent in Stochastic Convex Optimization</strong><br><button class=copy-to-clipboard title="The Sample Complexity of Gradient Descent in Stochastic Convex Optimization" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04931v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04931v2.pdf filename=2404.04931v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We analyze the <b>sample</b> <b>complexity</b> of full-batch Gradient Descent (GD) in the setup of non-smooth Stochastic Convex Optimization. We show that the generalization error of GD, with common choice of hyper-parameters, can be $\tilde \Theta(d/m + 1/\sqrt{m})$, where $d$ is the dimension and $m$ is the <b>sample</b> <b>size.</b> This matches the <b>sample</b> <b>complexity</b> of \emph{worst-case} empirical risk minimizers. That means that, in contrast with other algorithms, GD has no advantage over naive ERMs. Our bound follows from a new generalization bound that depends on both the dimension as well as the learning rate and number of iterations. Our bound also shows that, for general hyper-parameters, when the dimension is strictly larger than number of <b>samples,</b> <b>$T=\Omega(1/\epsilon^4)$</b> iterations are necessary to avoid overfitting. This resolves an open problem by Schlisserman et al.23 and Amir er Al.21, and improves over previous lower bounds that demonstrated that the <b>sample</b> <b>size</b> must be at least square root of the dimension.</p></p class="citation"></blockquote><h2 id=q-bioto-1>q-bio.TO (1)</h2><h3 id=11--92152-primary-liver-cancer-classification-from-routine-tumour-biopsy-using-weakly-supervised-deep-learning-aurélie-beaufrère-et-al-2024>(1/1 | 92/152) Primary liver cancer classification from routine tumour biopsy using weakly supervised deep learning (Aurélie Beaufrère et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aurélie Beaufrère, Nora Ouzir, Paul Emile Zafar, Astrid Laurent-Bellue, Miguel Albuquerque, Gwladys Lubuela, Jules Grégory, Catherine Guettier, Kévin Mondet, Jean-Christophe Pesquet, Valérie Paradis. (2024)<br><strong>Primary liver cancer classification from routine tumour biopsy using weakly supervised deep learning</strong><br><button class=copy-to-clipboard title="Primary liver cancer classification from routine tumour biopsy using weakly supervised deep learning" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.TO<br>Categories: I-5; I-4; I-2, cs-AI, cs-CV, q-bio-TO, q-bio.TO<br>Keyword Score: 73<br>Keywords: Clustering, Convolution, Supervised Learning, Supervised Learning, Unsupervised Learning, Weakly-supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04983v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04983v1.pdf filename=2404.04983v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The diagnosis of primary liver cancers (PLCs) can be challenging, especially on biopsies and for combined hepatocellular-cholangiocarcinoma (cHCC-CCA). We automatically classified PLCs on routine-stained biopsies using a <b>weakly</b> <b>supervised</b> <b>learning</b> method. Weak tumour/non-tumour annotations served as labels for training a Resnet18 neural network, and the network&rsquo;s last <b>convolutional</b> layer was used to extract new tumour tile features. Without knowledge of the precise labels of the malignancies, we then applied an <b>unsupervised</b> <b>clustering</b> algorithm. Our model identified specific features of hepatocellular carcinoma (HCC) and intrahepatic cholangiocarcinoma (iCCA). Despite no specific features of cHCC-CCA being recognized, the identification of HCC and iCCA tiles within a slide could facilitate the diagnosis of primary liver cancers, particularly cHCC-CCA. Method and results: 166 PLC biopsies were divided into training, internal and external validation sets: 90, 29 and 47 samples. Two liver pathologists reviewed each whole-slide hematein eosin saffron (HES)-stained image (WSI). After annotating the tumour/non-tumour areas, 256x256 pixel tiles were extracted from the WSIs and used to train a ResNet18. The network was used to extract new tile features. An <b>unsupervised</b> <b>clustering</b> algorithm was then applied to the new tile features. In a two-cluster model, Clusters 0 and 1 contained mainly HCC and iCCA histological features. The diagnostic agreement between the pathological diagnosis and the model predictions in the internal and external validation sets was 100% (11/11) and 96% (25/26) for HCC and 78% (7/9) and 87% (13/15) for iCCA, respectively. For cHCC-CCA, we observed a highly variable proportion of tiles from each cluster (Cluster 0: 5-97%; Cluster 1: 2-94%).</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--93152-clinical-trials-protocol-authoring-using-llms-morteza-maleki-2024>(1/1 | 93/152) Clinical Trials Protocol Authoring using LLMs (Morteza Maleki, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Morteza Maleki. (2024)<br><strong>Clinical Trials Protocol Authoring using LLMs</strong><br><button class=copy-to-clipboard title="Clinical Trials Protocol Authoring using LLMs" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 60<br>Keywords: Generative AI, GPT, GPT-4, Text Generation, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05044v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05044v1.pdf filename=2404.05044v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This report embarks on a mission to revolutionize clinical trial protocol development through the integration of advanced AI technologies. With a focus on leveraging the capabilities of <b>generative</b> <b>AI,</b> specifically <b>GPT-4,</b> this initiative aimed to streamline and enhance the efficiency and accuracy of clinical trial protocols. The methodology encompassed a detailed analysis and preparation of comprehensive drug and study level metadata, followed by the deployment of <b>GPT-4</b> for automated protocol section generation. Results demonstrated a significant improvement in protocol authoring, highlighted by increases in efficiency, accuracy, and the customization of protocols to specific trial requirements. Challenges encountered during model selection and <b>prompt</b> engineering were systematically addressed, leading to refined methodologies that capitalized on the advanced <b>text</b> <b>generation</b> capabilities of <b>GPT-4.</b> This project not only showcases the practical applications and benefits of <b>generative</b> <b>AI</b> in clinical trial design but also sets a foundation for future innovations in the field.</p></p class="citation"></blockquote><h2 id=cssd-1>cs.SD (1)</h2><h3 id=11--94152-cross-domain-audio-deepfake-detection-dataset-and-analysis-yuang-li-et-al-2024>(1/1 | 94/152) Cross-Domain Audio Deepfake Detection: Dataset and Analysis (Yuang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuang Li, Min Zhang, Mengxin Ren, Miaomiao Ma, Daimeng Wei, Hao Yang. (2024)<br><strong>Cross-Domain Audio Deepfake Detection: Dataset and Analysis</strong><br><button class=copy-to-clipboard title="Cross-Domain Audio Deepfake Detection: Dataset and Analysis" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 60<br>Keywords: Few-shot, Fine-tuning, Zero-shot, Text-to-speech, Text-to-speech, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04904v1.pdf filename=2404.04904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Audio deepfake detection (ADD) is essential for preventing the misuse of synthetic voices that may infringe on personal rights and privacy. Recent <b>zero-shot</b> <b>text-to-speech</b> <b>(TTS)</b> models pose higher risks as they can clone voices with a single utterance. However, the existing ADD datasets are outdated, leading to suboptimal generalization of detection models. In this paper, we construct a new cross-domain ADD dataset comprising over 300 hours of speech data that is generated by five advanced <b>zero-shot</b> <b>TTS</b> models. To simulate real-world scenarios, we employ diverse attack methods and audio <b>prompts</b> from different datasets. Experiments show that, through novel attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve equal error rates of 4.1% and 6.5% respectively. Additionally, we demonstrate our models&rsquo; outstanding <b>few-shot</b> ADD ability by <b>fine-tuning</b> with just one minute of target-domain data. Nonetheless, neural codec compressors greatly affect the detection accuracy, necessitating further research.</p></p class="citation"></blockquote><h2 id=eessiv-2>eess.IV (2)</h2><h3 id=12--95152-lhu-net-a-light-hybrid-u-net-for-cost-efficient-high-performance-volumetric-medical-image-segmentation-yousef-sadegheih-et-al-2024>(1/2 | 95/152) LHU-Net: A Light Hybrid U-Net for Cost-Efficient, High-Performance Volumetric Medical Image Segmentation (Yousef Sadegheih et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yousef Sadegheih, Afshin Bozorgpour, Pratibha Kumari, Reza Azad, Dorit Merhof. (2024)<br><strong>LHU-Net: A Light Hybrid U-Net for Cost-Efficient, High-Performance Volumetric Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="LHU-Net: A Light Hybrid U-Net for Cost-Efficient, High-Performance Volumetric Medical Image Segmentation" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 53<br>Keywords: Convolutional Neural Network, Convolutional Neural Network, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05102v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05102v1.pdf filename=2404.05102v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a result of the rise of <b>Transformer</b> architectures in medical image analysis, specifically in the domain of medical image segmentation, a multitude of hybrid models have been created that merge the advantages of <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> and <b>Transformers.</b> These hybrid models have achieved notable success by significantly improving segmentation accuracy. Yet, this progress often comes at the cost of increased model complexity, both in terms of parameters and computational demand. Moreover, many of these models fail to consider the crucial interplay between spatial and channel features, which could further refine and improve segmentation outcomes. To address this, we introduce LHU-Net, a Light Hybrid U-Net architecture optimized for volumetric medical image segmentation. LHU-Net is meticulously designed to prioritize spatial feature analysis in its initial layers before shifting focus to channel-based features in its deeper layers, ensuring a comprehensive feature extraction process. Rigorous evaluation across five <b>benchmark</b> datasets - Synapse, LA, Pancreas, ACDC, and BRaTS 2018 - underscores LHU-Net&rsquo;s superior performance, showcasing its dual capacity for efficiency and accuracy. Notably, LHU-Net sets new performance <b>benchmarks,</b> such as attaining a Dice score of 92.66 on the ACDC dataset, while simultaneously reducing parameters by 85% and quartering the computational load compared to existing state-of-the-art models. Achieved without any reliance on pre-training, additional data, or model ensemble, LHU-Net&rsquo;s effectiveness is further evidenced by its state-of-the-art performance across all evaluated datasets, utilizing fewer than 11 million parameters. This achievement highlights that balancing computational efficiency with high accuracy in medical image segmentation is feasible. Our implementation of LHU-Net is freely accessible to the research community on GitHub.</p></p class="citation"></blockquote><h3 id=22--96152-correcting-diffusion-based-perceptual-image-compression-with-privileged-end-to-end-decoder-yiyang-ma-et-al-2024>(2/2 | 96/152) Correcting Diffusion-Based Perceptual Image Compression with Privileged End-to-End Decoder (Yiyang Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiyang Ma, Wenhan Yang, Jiaying Liu. (2024)<br><strong>Correcting Diffusion-Based Perceptual Image Compression with Privileged End-to-End Decoder</strong><br><button class=copy-to-clipboard title="Correcting Diffusion-Based Perceptual Image Compression with Privileged End-to-End Decoder" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04916v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04916v1.pdf filename=2404.04916v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The images produced by <b>diffusion</b> <b>models</b> can attain excellent perceptual quality. However, it is challenging for <b>diffusion</b> <b>models</b> to guarantee distortion, hence the integration of <b>diffusion</b> <b>models</b> and image compression models still needs more comprehensive explorations. This paper presents a <b>diffusion-based</b> <b>image</b> compression method that employs a privileged end-to-end decoder model as correction, which achieves better perceptual quality while guaranteeing the distortion to an extent. We build a <b>diffusion</b> <b>model</b> and design a novel paradigm that combines the <b>diffusion</b> <b>model</b> and an end-to-end decoder, and the latter is responsible for transmitting the privileged information extracted at the encoder side. Specifically, we theoretically analyze the reconstruction process of the <b>diffusion</b> <b>models</b> at the encoder side with the original images being visible. Based on the analysis, we introduce an end-to-end <b>convolutional</b> decoder to provide a better approximation of the score function $\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t)$ at the encoder side and effectively transmit the combination. Experiments demonstrate the superiority of our method in both distortion and perception compared with previous perceptual compression methods.</p></p class="citation"></blockquote><h2 id=csro-6>cs.RO (6)</h2><h3 id=16--97152-robomp2-a-robotic-multimodal-perception-planning-framework-with-multimodal-large-language-models-qi-lv-et-al-2024>(1/6 | 97/152) RoboMP$^2$: A Robotic Multimodal Perception-Planning Framework with Multimodal Large Language Models (Qi Lv et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Lv, Hao Li, Xiang Deng, Rui Shao, Michael Yu Wang, Liqiang Nie. (2024)<br><strong>RoboMP$^2$: A Robotic Multimodal Perception-Planning Framework with Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="RoboMP$^2$: A Robotic Multimodal Perception-Planning Framework with Multimodal Large Language Models" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 49<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Reasoning, In-context Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04929v1.pdf filename=2404.04929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) have shown impressive <b>reasoning</b> abilities and general intelligence in various domains. It inspires researchers to train end-to-end MLLMs or utilize <b>large</b> <b>models</b> <b>to</b> generate policies with human-selected <b>prompts</b> for embodied agents. However, these methods exhibit limited generalization capabilities on unseen tasks or scenarios, and overlook the <b>multimodal</b> environment information which is critical for robots to make decisions. In this paper, we introduce a novel Robotic <b>Multimodal</b> Perception-Planning (RoboMP$^2$) framework for robotic manipulation which consists of a Goal-Conditioned <b>Multimodal</b> Preceptor (GCMP) and a Retrieval-Augmented <b>Multimodal</b> Planner (RAMP). Specially, GCMP captures environment states by employing a tailored MLLMs for embodied agents with the abilities of semantic <b>reasoning</b> and localization. RAMP utilizes coarse-to-fine retrieval method to find the $k$ most-relevant policies as <b>in-context</b> demonstrations to enhance the planner. Extensive experiments demonstrate the superiority of RoboMP$^2$ on both VIMA <b>benchmark</b> and real-world tasks, with around 10% improvement over the baselines.</p></p class="citation"></blockquote><h3 id=26--98152-prompting-multi-modal-tokens-to-enhance-end-to-end-autonomous-driving-imitation-learning-with-llms-yiqun-duan-et-al-2024>(2/6 | 98/152) Prompting Multi-Modal Tokens to Enhance End-to-End Autonomous Driving Imitation Learning with LLMs (Yiqun Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiqun Duan, Qiang Zhang, Renjing Xu. (2024)<br><strong>Prompting Multi-Modal Tokens to Enhance End-to-End Autonomous Driving Imitation Learning with LLMs</strong><br><button class=copy-to-clipboard title="Prompting Multi-Modal Tokens to Enhance End-to-End Autonomous Driving Imitation Learning with LLMs" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 43<br>Keywords: Multi-modal, Reinforcement Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04869v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04869v1.pdf filename=2404.04869v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The utilization of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> within the realm of <b>reinforcement</b> <b>learning,</b> particularly as planners, has garnered a significant degree of attention in recent scholarly literature. However, a substantial proportion of existing research predominantly focuses on planning models for robotics that transmute the outputs derived from perception models into linguistic forms, thus adopting a `pure-language&rsquo; strategy. In this research, we propose a hybrid End-to-End learning framework for autonomous driving by combining basic driving imitation learning with <b>LLMs</b> based on multi-modality <b>prompt</b> tokens. Instead of simply converting perception results from the separated train model into pure language input, our novelty lies in two aspects. 1) The end-to-end integration of visual and LiDAR sensory input into learnable multi-modality tokens, thereby intrinsically alleviating description bias by separated pre-trained perception models. 2) Instead of directly letting <b>LLMs</b> drive, this paper explores a hybrid setting of letting <b>LLMs</b> help the driving model correct mistakes and complicated scenarios. The results of our experiments suggest that the proposed methodology can attain driving scores of 49.21%, coupled with an impressive route completion rate of 91.34% in the offline evaluation conducted via CARLA. These performance metrics are comparable to the most advanced driving models.</p></p class="citation"></blockquote><h3 id=36--99152-enquery-ensemble-policies-for-diverse-query-generation-in-preference-alignment-of-robot-navigation-jorge-de-heuvel-et-al-2024>(3/6 | 99/152) EnQuery: Ensemble Policies for Diverse Query-Generation in Preference Alignment of Robot Navigation (Jorge de Heuvel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jorge de Heuvel, Florian Seiler, Maren Bennewitz. (2024)<br><strong>EnQuery: Ensemble Policies for Diverse Query-Generation in Preference Alignment of Robot Navigation</strong><br><button class=copy-to-clipboard title="EnQuery: Ensemble Policies for Diverse Query-Generation in Preference Alignment of Robot Navigation" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04852v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04852v1.pdf filename=2404.04852v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To align mobile robot navigation policies with user preferences through <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF),</b> reliable and behavior-diverse user queries are required. However, deterministic policies fail to generate a variety of navigation trajectory suggestions for a given navigation task configuration. We introduce EnQuery, a query generation approach using an ensemble of policies that achieve behavioral diversity through a regularization term. For a given navigation task, EnQuery produces multiple navigation trajectory suggestions, thereby optimizing the efficiency of preference data collection with fewer queries. Our methodology demonstrates superior performance in aligning navigation policies with user preferences in low-query regimes, offering enhanced policy convergence from sparse preference queries. The evaluation is complemented with a novel explainability representation, capturing full scene navigation behavior of the mobile robot in a single plot.</p></p class="citation"></blockquote><h3 id=46--100152-efficient-reinforcement-learning-of-task-planners-for-robotic-palletization-through-iterative-action-masking-learning-zheng-wu-et-al-2024>(4/6 | 100/152) Efficient Reinforcement Learning of Task Planners for Robotic Palletization through Iterative Action Masking Learning (Zheng Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheng Wu, Yichuan Li, Wei Zhan, Changliu Liu, Yun-Hui Liu, Masayoshi Tomizuka. (2024)<br><strong>Efficient Reinforcement Learning of Task Planners for Robotic Palletization through Iterative Action Masking Learning</strong><br><button class=copy-to-clipboard title="Efficient Reinforcement Learning of Task Planners for Robotic Palletization through Iterative Action Masking Learning" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04772v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04772v1.pdf filename=2404.04772v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of robotic systems for palletization in logistics scenarios is of paramount importance, addressing critical efficiency and precision demands in supply chain management. This paper investigates the application of <b>Reinforcement</b> <b>Learning</b> (RL) in enhancing task planning for such robotic systems. Confronted with the substantial challenge of a vast action space, which is a significant impediment to efficiently apply out-of-the-shelf RL methods, our study introduces a novel method of utilizing <b>supervised</b> <b>learning</b> to iteratively prune and manage the action space effectively. By reducing the complexity of the action space, our approach not only accelerates the learning phase but also ensures the effectiveness and reliability of the task planning in robotic palletization. The experimental results underscore the efficacy of this method, highlighting its potential in improving the performance of RL applications in complex and high-dimensional environments like logistics palletization.</p></p class="citation"></blockquote><h3 id=56--101152-adaptive-anchor-pairs-selection-in-a-tdoa-based-system-through-robot-localization-error-minimization-marcin-kolakowski-2024>(5/6 | 101/152) Adaptive Anchor Pairs Selection in a TDOA-based System Through Robot Localization Error Minimization (Marcin Kolakowski, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcin Kolakowski. (2024)<br><strong>Adaptive Anchor Pairs Selection in a TDOA-based System Through Robot Localization Error Minimization</strong><br><button class=copy-to-clipboard title="Adaptive Anchor Pairs Selection in a TDOA-based System Through Robot Localization Error Minimization" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05067v1.pdf filename=2404.05067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The following paper presents an adaptive anchor pairs selection method for ultra-wideband (UWB) Time Difference of Arrival (TDOA) based positioning systems. The method divides the area covered by the system into several zones and assigns them anchor pair sets. The pair sets are determined during calibration based on localization root mean square error (RMSE). The calibration assumes driving a mobile platform equipped with a LiDAR sensor and a UWB tag through the specified zones. The robot is localized separately based on a large set of different TDOA pairs and using a LiDAR, which acts as the reference. For each zone, the TDOA pairs set for which the registered RMSE is lowest is selected and used for localization in the routine system work. The proposed method has been tested with <b>simulations</b> and experiments. The results for both simulated static and experimental dynamic scenarios have proven that the adaptive selection of the anchor nodes leads to an increase in localization accuracy. In the experiment, the median trajectory error for a moving person localization was at a level of 25 cm.</p></p class="citation"></blockquote><h3 id=66--102152-learning-adaptive-multi-objective-robot-navigation-with-demonstrations-jorge-de-heuvel-et-al-2024>(6/6 | 102/152) Learning Adaptive Multi-Objective Robot Navigation with Demonstrations (Jorge de Heuvel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jorge de Heuvel, Tharun Sethuraman, Maren Bennewitz. (2024)<br><strong>Learning Adaptive Multi-Objective Robot Navigation with Demonstrations</strong><br><button class=copy-to-clipboard title="Learning Adaptive Multi-Objective Robot Navigation with Demonstrations" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04857v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04857v1.pdf filename=2404.04857v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Preference-aligned robot navigation in human environments is typically achieved through learning-based approaches, utilizing demonstrations and user feedback for personalization. However, personal preferences are subject to change and might even be context-dependent. Yet traditional <b>reinforcement</b> <b>learning</b> (RL) approaches with a static reward function often fall short in adapting to these varying user preferences. This paper introduces a framework that combines multi-objective <b>reinforcement</b> <b>learning</b> (MORL) with demonstration-based learning. Our approach allows for dynamic adaptation to changing user preferences without retraining. Through rigorous evaluations, including sim-to-real and robot-to-robot transfers, we demonstrate our framework&rsquo;s capability to reflect user preferences accurately while achieving high navigational performance in terms of collision avoidance and goal pursuance.</p></p class="citation"></blockquote><h2 id=cscr-8>cs.CR (8)</h2><h3 id=18--103152-hidden-you-malicious-goal-into-benigh-narratives-jailbreak-large-language-models-through-logic-chain-injection-zhilong-wang-et-al-2024>(1/8 | 103/152) Hidden You Malicious Goal Into Benigh Narratives: Jailbreak Large Language Models through Logic Chain Injection (Zhilong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhilong Wang, Yebo Cao, Peng Liu. (2024)<br><strong>Hidden You Malicious Goal Into Benigh Narratives: Jailbreak Large Language Models through Logic Chain Injection</strong><br><button class=copy-to-clipboard title="Hidden You Malicious Goal Into Benigh Narratives: Jailbreak Large Language Models through Logic Chain Injection" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: Large Language Model, Large Language Model, Prompt, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04849v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04849v1.pdf filename=2404.04849v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Jailbreak attacks on Language Model Models <b>(LLMs)</b> entail crafting <b>prompts</b> aimed at exploiting the models to generate malicious content. Existing jailbreak attacks can successfully deceive the <b>LLMs,</b> however they cannot deceive the human. This paper proposes a new type of jailbreak attacks which can deceive both the <b>LLMs</b> and human (i.e., <b>security</b> analyst). The key insight of our idea is borrowed from the social psychology - that is human are easily deceived if the lie is hidden in truth. Based on this insight, we proposed the logic-chain injection attacks to inject malicious intention into benign truth. Logic-chain injection attack firstly dissembles its malicious target into a chain of benign narrations, and then distribute narrations into a related benign article, with undoubted facts. In this way, newly generate <b>prompt</b> cannot only deceive the <b>LLMs,</b> but also deceive human.</p></p class="citation"></blockquote><h3 id=28--104152-safeguarding-voice-privacy-harnessing-near-ultrasonic-interference-to-protect-against-unauthorized-audio-recording-forrest-mckee-et-al-2024>(2/8 | 104/152) Safeguarding Voice Privacy: Harnessing Near-Ultrasonic Interference To Protect Against Unauthorized Audio Recording (Forrest McKee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Forrest McKee, David Noever. (2024)<br><strong>Safeguarding Voice Privacy: Harnessing Near-Ultrasonic Interference To Protect Against Unauthorized Audio Recording</strong><br><button class=copy-to-clipboard title="Safeguarding Voice Privacy: Harnessing Near-Ultrasonic Interference To Protect Against Unauthorized Audio Recording" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04769v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04769v1.pdf filename=2404.04769v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread adoption of voice-activated systems has modified routine human-machine interaction but has also introduced new vulnerabilities. This paper investigates the susceptibility of <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> algorithms in these systems to interference from near-ultrasonic noise. Building upon prior research that demonstrated the ability of near-ultrasonic frequencies (16 kHz - 22 kHz) to exploit the inherent properties of microelectromechanical systems (MEMS) microphones, our study explores alternative privacy enforcement means using this interference phenomenon. We expose a critical vulnerability in the most common microphones used in modern voice-activated devices, which inadvertently demodulate near-ultrasonic frequencies into the audible spectrum, disrupting the <b>ASR</b> process. Through a systematic analysis of the impact of near-ultrasonic noise on various <b>ASR</b> systems, we demonstrate that this vulnerability is consistent across different devices and under varying conditions, such as broadcast distance and specific phoneme structures. Our findings highlight the need to develop robust countermeasures to protect voice-activated systems from malicious exploitation of this vulnerability. Furthermore, we explore the potential applications of this phenomenon in enhancing privacy by disrupting unauthorized audio recording or eavesdropping. This research underscores the importance of a comprehensive approach to securing voice-activated systems, combining technological innovation, responsible development practices, and informed policy decisions to ensure the privacy and <b>security</b> of users in an increasingly connected world.</p></p class="citation"></blockquote><h3 id=38--105152-oss-malicious-package-analysis-in-the-wild-xiaoyan-zhou-et-al-2024>(3/8 | 105/152) OSS Malicious Package Analysis in the Wild (Xiaoyan Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyan Zhou, Ying Zhang, Wenjia Niu, Jiqiang Liu, Haining Wang, Qiang Li. (2024)<br><strong>OSS Malicious Package Analysis in the Wild</strong><br><button class=copy-to-clipboard title="OSS Malicious Package Analysis in the Wild" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-SE, cs.CR<br>Keyword Score: 28<br>Keywords: Graph, Knowledge Graph, Malware, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04991v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04991v1.pdf filename=2404.04991v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The open-source software (OSS) ecosystem suffers from various <b>security</b> threats and risks, and malicious packages play a central role in software supply chain (SSC) attacks. Although <b>malware</b> research has a history of over thirty years, less attention has been paid to OSS <b>malware.</b> Its existing research has three limitations: a lack of high-quality datasets, <b>malware</b> diversity, and attack campaign context. In this paper, we first build and curate the largest dataset of 23,425 malicious packages from scattered online sources. We then propose a <b>knowledge</b> <b>graph</b> to represent the OSS <b>malware</b> corpus and conduct malicious package analysis in the wild. Our main findings include (1) it is essential to collect malicious packages from various online sources because there is little data overlap between different sources; (2) despite the sheer volume of SSC attack campaigns, many malicious packages are similar, and unknown/sophisticated attack behaviors have yet to emerge or be detected; (3) OSS malicious package has its distinct life cycle, denoted as {changing->release->detection->removal}, and slightly changing the package (different name) is a widespread attack manner; (4) while malicious packages often lack context about how and who released them, <b>security</b> reports disclose the information about corresponding SSC attack campaigns.</p></p class="citation"></blockquote><h3 id=48--106152-pagpassgpt-pattern-guided-password-guessing-via-generative-pretrained-transformer-xingyu-su-et-al-2024>(4/8 | 106/152) PagPassGPT: Pattern Guided Password Guessing via Generative Pretrained Transformer (Xingyu Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyu Su, Xiaojie Zhu, Yang Li, Yong Li, Chi Chen, Paulo Esteves-Veríssimo. (2024)<br><strong>PagPassGPT: Pattern Guided Password Guessing via Generative Pretrained Transformer</strong><br><button class=copy-to-clipboard title="PagPassGPT: Pattern Guided Password Guessing via Generative Pretrained Transformer" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: GPT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04886v1.pdf filename=2404.04886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Amidst the surge in deep learning-based password guessing models, challenges of generating high-quality passwords and reducing duplicate passwords persist. To address these challenges, we present PagPassGPT, a password guessing model constructed on Generative Pretrained <b>Transformer</b> <b>(GPT).</b> It can perform pattern guided guessing by incorporating pattern structure information as background knowledge, resulting in a significant increase in the hit rate. Furthermore, we propose D&amp;C-GEN to reduce the repeat rate of generated passwords, which adopts the concept of a divide-and-conquer approach. The primary task of guessing passwords is recursively divided into non-overlapping subtasks. Each subtask inherits the knowledge from the parent task and predicts succeeding tokens. In comparison to the state-of-the-art model, our proposed scheme exhibits the capability to correctly guess 12% more passwords while producing 25% fewer duplicates.</p></p class="citation"></blockquote><h3 id=58--107152-optimizing-information-propagation-for-blockchain-empowered-mobile-aigc-a-graph-attention-network-approach-jiana-liao-et-al-2024>(5/8 | 107/152) Optimizing Information Propagation for Blockchain-empowered Mobile AIGC: A Graph Attention Network Approach (Jiana Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiana Liao, Jinbo Wen, Jiawen Kang, Yang Zhang, Jianbo Du, Qihao Li, Weiting Zhang, Dong Yang. (2024)<br><strong>Optimizing Information Propagation for Blockchain-empowered Mobile AIGC: A Graph Attention Network Approach</strong><br><button class=copy-to-clipboard title="Optimizing Information Propagation for Blockchain-empowered Mobile AIGC: A Graph Attention Network Approach" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-GT, cs.CR<br>Keyword Score: 13<br>Keywords: Graph Attention Networks, Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04937v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04937v1.pdf filename=2404.04937v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial Intelligence-Generated Content (AIGC) is a rapidly evolving field that utilizes advanced AI algorithms to generate content. Through integration with mobile edge networks, mobile AIGC networks have gained significant attention, which can provide real-time customized and personalized AIGC services and products. Since blockchains can facilitate decentralized and transparent data management, AIGC products can be securely managed by blockchain to avoid tampering and plagiarization. However, the evolution of blockchain-empowered mobile AIGC is still in its nascent phase, grappling with challenges such as improving information propagation efficiency to enable blockchain-empowered mobile AIGC. In this paper, we design a <b>Graph</b> Attention Network <b>(GAT)-based</b> information propagation optimization framework for blockchain-empowered mobile AIGC. We first innovatively apply age of information as a data-freshness metric to measure information propagation efficiency in public blockchains. Considering that <b>GATs</b> possess the excellent ability to process <b>graph-structured</b> data, we utilize the <b>GAT</b> to obtain the optimal information propagation trajectory. Numerical results demonstrate that the proposed scheme exhibits the most outstanding information propagation efficiency compared with traditional routing mechanisms.</p></p class="citation"></blockquote><h3 id=68--108152-stop-stealing-my-data-sanitizing-stego-channels-in-3d-printing-design-files-aleksandr-dolgavin-et-al-2024>(6/8 | 108/152) Stop Stealing My Data: Sanitizing Stego Channels in 3D Printing Design Files (Aleksandr Dolgavin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aleksandr Dolgavin, Mark Yampolskiy, Moti Yung. (2024)<br><strong>Stop Stealing My Data: Sanitizing Stego Channels in 3D Printing Design Files</strong><br><button class=copy-to-clipboard title="Stop Stealing My Data: Sanitizing Stego Channels in 3D Printing Design Files" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05106v1.pdf filename=2404.05106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increased adoption of additive manufacturing (AM) and the acceptance of AM outsourcing created an ecosystem in which the sending and receiving of digital designs by different actors became normal. It has recently been shown that the STL design files &ndash; most commonly used in AM &ndash; contain steganographic channels. Such channels can allow additional data to be embedded within the STL files without changing the printed model. These factors create a threat of misusing the design files as a covert communication channel to either exfiltrate stolen sensitive digital data from organizations or infiltrate malicious software into a secure environment. This paper addresses this <b>security</b> threat by designing and evaluating a \emph{sanitizer} that erases hidden content where steganographic channels might exist. The proposed sanitizer takes into account a set of specific constraints imposed by the application domain, such as not affecting the ability to manufacture part of the required quality using the sanitized design.</p></p class="citation"></blockquote><h3 id=78--109152-iniva-inclusive-and-incentive-compatible-vote-aggregation-arian-baloochestani-et-al-2024>(7/8 | 109/152) Iniva: Inclusive and Incentive-compatible Vote Aggregation (Arian Baloochestani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arian Baloochestani, Hanish Gogada, Leander Jehl, Hein Meling. (2024)<br><strong>Iniva: Inclusive and Incentive-compatible Vote Aggregation</strong><br><button class=copy-to-clipboard title="Iniva: Inclusive and Incentive-compatible Vote Aggregation" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-DC, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04948v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04948v1.pdf filename=2404.04948v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many blockchain platforms use committee-based consensus for scalability, finality, and <b>security.</b> In this consensus scheme, a committee decides which blocks get appended to the chain, typically through several voting phases. Platforms typically leverage the committee members&rsquo; recorded votes to reward, punish, or detect failures. A common approach is to let the block proposer decide which votes to include, opening the door to possible attacks. For example, a malicious proposer can omit votes from targeted committee members, resulting in lost profits and, ultimately, their departure from the system. This paper presents Iniva, an inclusive and incentive-compatible vote aggregation scheme that prevents such vote omission attacks. Iniva relies on a tree overlay with carefully selected fallback paths, making it robust against process failures without needing reconfiguration or additional redundancy. Our analysis shows that Iniva significantly reduces the chance to omit individual votes while ensuring that omitting many votes incurs a significant cost. In addition, our experimental results show that Iniva enjoys robustness, scalability, and reasonable throughput.</p></p class="citation"></blockquote><h3 id=88--110152-privacy-preserving-traceable-functional-encryption-for-inner-product-muyao-qiu-et-al-2024>(8/8 | 110/152) Privacy-Preserving Traceable Functional Encryption for Inner Product (Muyao Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muyao Qiu, Jinguang Han. (2024)<br><strong>Privacy-Preserving Traceable Functional Encryption for Inner Product</strong><br><button class=copy-to-clipboard title="Privacy-Preserving Traceable Functional Encryption for Inner Product" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04861v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04861v1.pdf filename=2404.04861v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Functional encryption introduces a new paradigm of public key encryption that decryption only reveals the function value of encrypted data. To curb key leakage issues and trace users in FE-IP, a new primitive called traceable functional encryption for inner product (TFE-IP) has been proposed. However, the privacy protection of user&rsquo;s identities has not been considered in the existing TFE-IP schemes. In order to balance privacy and accountability, we propose the concept of privacy-preserving traceable functional encryption for inner product (PPTFE-IP) and give a concrete construction. Our scheme provides the following features: (1) To prevent key sharing, a user&rsquo;s key is bound with both his/her identity and a vector; (2) The key generation center (KGC) and a user execute a two-party secure computing protocol to generate a key without the former knowing anything about the latter&rsquo;s identity; (3) Each user can verify the correctness of his/her key; (4) A user can calculate the inner product of the two vectors embedded in his/her key and in a ciphertext; (5) Only the tracer can trace the identity embedded in a key. The <b>security</b> of our scheme is formally reduced to well-known complexity assumptions, and the implementation is conducted to evaluate its efficiency. The novelty of our scheme is to protect users&rsquo; privacy and provide traceability if required.</p></p class="citation"></blockquote><h2 id=csse-6>cs.SE (6)</h2><h3 id=16--111152-csa-trans-code-structure-aware-transformer-for-ast-saeyoon-oh-et-al-2024>(1/6 | 111/152) CSA-Trans: Code Structure Aware Transformer for AST (Saeyoon Oh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saeyoon Oh, Shin Yoo. (2024)<br><strong>CSA-Trans: Code Structure Aware Transformer for AST</strong><br><button class=copy-to-clipboard title="CSA-Trans: Code Structure Aware Transformer for AST" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 33<br>Keywords: Graph, Transformer, Self-Attention, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05767v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05767v1.pdf filename=2404.05767v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When applying the <b>Transformer</b> architecture to source code, designing a good <b>self-attention</b> mechanism is critical as it affects how node relationship is extracted from the Abstract Syntax Trees (ASTs) of the source code. We present Code Structure Aware <b>Transformer</b> (CSA-Trans), which uses Code Structure Embedder (CSE) to generate specific PE for each node in AST. CSE generates node Positional Encoding (PE) using disentangled attention. To further extend the <b>self-attention</b> capability, we adopt Stochastic Block Model (SBM) attention. Our evaluation shows that our PE captures the relationships between AST nodes better than other <b>graph-related</b> PE techniques. We also show through quantitative and qualitative analysis that SBM attention is able to generate more node specific attention coefficients. We demonstrate that CSA-Trans outperforms 14 baselines in code <b>summarization</b> tasks for both Python and Java, while being 41.92% faster and 25.31% memory efficient in Java dataset compared to AST-Trans and SG-Trans respectively.</p></p class="citation"></blockquote><h3 id=26--112152-enhancing-llm-based-test-generation-for-hard-to-cover-branches-via-program-analysis-chen-yang-et-al-2024>(2/6 | 112/152) Enhancing LLM-based Test Generation for Hard-to-Cover Branches via Program Analysis (Chen Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Yang, Junjie Chen, Bin Lin, Jianyi Zhou, Ziqi Wang. (2024)<br><strong>Enhancing LLM-based Test Generation for Hard-to-Cover Branches via Program Analysis</strong><br><button class=copy-to-clipboard title="Enhancing LLM-based Test Generation for Hard-to-Cover Branches via Program Analysis" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04966v1.pdf filename=2404.04966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic test generation plays a critical role in software quality assurance. While the recent advances in Search-Based Software Testing (SBST) and <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown promise in generating useful tests, these techniques still struggle to cover certain branches. Reaching these hard-to-cover branches usually requires constructing complex objects and resolving intricate inter-procedural dependencies in branch conditions, which poses significant challenges for existing test generation techniques. In this work, we propose TELPA, a novel technique aimed at addressing these challenges. Its key insight lies in extracting real usage scenarios of the target method under test to learn how to construct complex objects and extracting methods entailing inter-procedural dependencies with hard-to-cover branches to learn the semantics of branch constraints. To enhance efficiency and effectiveness, TELPA identifies a set of ineffective tests as counter-examples for <b>LLMs</b> and employs a feedback-based process to iteratively refine these counter-examples. Then, TELPA integrates program analysis results and counter-examples into the <b>prompt,</b> guiding <b>LLMs</b> to gain deeper understandings of the semantics of the target method and generate diverse tests that can reach the hard-to-cover branches. Our experimental results on 27 open-source Python projects demonstrate that TELPA significantly outperforms the state-of-the-art SBST and <b>LLM-based</b> techniques, achieving an average improvement of 31.39% and 22.22% in terms of branch coverage.</p></p class="citation"></blockquote><h3 id=36--113152-llm-based-multi-agent-systems-for-software-engineering-vision-and-the-road-ahead-junda-he-et-al-2024>(3/6 | 113/152) LLM-Based Multi-Agent Systems for Software Engineering: Vision and the Road Ahead (Junda He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junda He, Christoph Treude, David Lo. (2024)<br><strong>LLM-Based Multi-Agent Systems for Software Engineering: Vision and the Road Ahead</strong><br><button class=copy-to-clipboard title="LLM-Based Multi-Agent Systems for Software Engineering: Vision and the Road Ahead" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04834v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04834v1.pdf filename=2404.04834v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integrating Large Language Models(LLMs) into autonomous agents marks a significant shift in the research landscape by offering cognitive abilities competitive to human planning and <b>reasoning.</b> This paper envisions the evolution of <b>LLM-based</b> Multi-Agent (LMA) systems in addressing complex and multi-faceted software engineering challenges. LMA systems introduce numerous benefits, including enhanced robustness through collaborative cross-examination, autonomous problem-solving, and scalable solutions to complex software projects. By examining the role of LMA systems in future software engineering practices, this vision paper highlights the potential applications and emerging challenges. We further point to specific opportunities for research and conclude with a research agenda with a set of research questions to guide future research directions.</p></p class="citation"></blockquote><h3 id=46--114152-how-do-oss-developers-utilize-architectural-solutions-from-qa-sites-an-empirical-study-musengamana-jean-de-dieu-et-al-2024>(4/6 | 114/152) How Do OSS Developers Utilize Architectural Solutions from Q&amp;A Sites: An Empirical Study (Musengamana Jean de Dieu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Musengamana Jean de Dieu, Peng Liang, Mojtaba Shahin. (2024)<br><strong>How Do OSS Developers Utilize Architectural Solutions from Q&amp;A Sites: An Empirical Study</strong><br><button class=copy-to-clipboard title="How Do OSS Developers Utilize Architectural Solutions from Q&A Sites: An Empirical Study" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05041v1.pdf filename=2404.05041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Developers utilize programming-related knowledge (e.g., code snippets) on Q&amp;A sites (e.g., Stack Overflow) that functionally matches the programming problems they encounter in their development. Despite extensive research on Q&amp;A sites, being a high-level and important type of development-related knowledge, architectural solutions (e.g., architecture tactics) and their utilization are rarely explored. To fill this gap, we conducted a mixed-methods study that includes a mining study and a survey study. For the mining study, we mined 984 commits and issues (i.e., 821 commits and 163 issues) from 893 Open-Source Software (OSS) projects on GitHub that explicitly referenced architectural solutions from Stack Overflow (SO) and Software Engineering Stack Exchange (SWESE). For the survey study, we identified practitioners involved in the utilization of these architectural solutions and surveyed 227 of them to further understand how practitioners utilize architectural solutions from Q&amp;A sites in their OSS development. Our main findings are that: (1) OSS practitioners use architectural solutions from Q&amp;A sites to solve a large variety (15 categories) of architectural problems, wherein Component design issue, Architectural anti-pattern, and <b>Security</b> issue are dominant; (2) Seven categories of architectural solutions from Q&amp;A sites have been utilized to solve those problems, among which Architectural refactoring, Use of frameworks, and Architectural tactic are the three most utilized architectural solutions; (3) Using architectural solutions from SO comes with a variety of challenges, e.g., OSS practitioners complain that they need to spend significant time to adapt such architectural solutions to address design concerns raised in their OSS development, and it is challenging to use architectural solutions that are not tailored to the design context of their OSS projects.</p></p class="citation"></blockquote><h3 id=56--115152-ai-for-devsecops-a-landscape-and-future-opportunities-michael-fu-et-al-2024>(5/6 | 115/152) AI for DevSecOps: A Landscape and Future Opportunities (Michael Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Fu, Jirat Pasuksmit, Chakkrit Tantithamthavorn. (2024)<br><strong>AI for DevSecOps: A Landscape and Future Opportunities</strong><br><button class=copy-to-clipboard title="AI for DevSecOps: A Landscape and Future Opportunities" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04839v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04839v1.pdf filename=2404.04839v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>DevOps has emerged as one of the most rapidly evolving software development paradigms. With the growing concerns surrounding <b>security</b> in software systems, the DevSecOps paradigm has gained prominence, urging practitioners to incorporate <b>security</b> practices seamlessly into the DevOps workflow. However, integrating <b>security</b> into the DevOps workflow can impact agility and impede delivery speed. Recently, the advancement of artificial intelligence (AI) has revolutionized automation in various software domains, including software <b>security.</b> AI-driven <b>security</b> approaches, particularly those leveraging machine learning or deep learning, hold promise in automating <b>security</b> workflows. They reduce manual efforts, which can be integrated into DevOps to ensure uninterrupted delivery speed and align with the DevSecOps paradigm simultaneously. This paper seeks to contribute to the critical intersection of AI and DevSecOps by presenting a comprehensive landscape of AI-driven <b>security</b> techniques applicable to DevOps and identifying avenues for enhancing <b>security,</b> trust, and efficiency in software development processes. We analyzed 99 research papers spanning from 2017 to 2023. Specifically, we address two key research questions (RQs). In RQ1, we identified 12 <b>security</b> tasks associated with the DevOps process and reviewed existing AI-driven <b>security</b> approaches. In RQ2, we discovered 15 challenges encountered by existing AI-driven <b>security</b> approaches and derived future research opportunities. Drawing insights from our findings, we discussed the state-of-the-art AI-driven <b>security</b> approaches, highlighted challenges in existing research, and proposed avenues for future opportunities.</p></p class="citation"></blockquote><h3 id=66--116152-a-data-to-product-multimodal-conceptual-framework-to-achieve-automated-software-evolution-for-context-rich-intelligent-applications-songhui-yue-2024>(6/6 | 116/152) A Data-to-Product Multimodal Conceptual Framework to Achieve Automated Software Evolution for Context-rich Intelligent Applications (Songhui Yue, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Songhui Yue. (2024)<br><strong>A Data-to-Product Multimodal Conceptual Framework to Achieve Automated Software Evolution for Context-rich Intelligent Applications</strong><br><button class=copy-to-clipboard title="A Data-to-Product Multimodal Conceptual Framework to Achieve Automated Software Evolution for Context-rich Intelligent Applications" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04821v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04821v1.pdf filename=2404.04821v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While AI is extensively transforming Software Engineering (SE) fields, SE is still in need of a framework to overall consider all phases to facilitate Automated Software Evolution (ASEv), particularly for intelligent applications that are context-rich, instead of conquering each division independently. Its complexity comes from the intricacy of the intelligent applications, the heterogeneity of the data sources, and the constant changes in the context. This study proposes a conceptual framework for achieving automated software evolution, emphasizing the importance of multimodality learning. A Selective Sequential Scope Model (3S) model is developed based on the conceptual framework, and it can be used to categorize existing and future research when it covers different SE phases and <b>multimodal</b> learning tasks. This research is a preliminary step toward the blueprint of a higher-level ASEv. The proposed conceptual framework can act as a practical guideline for practitioners to prepare themselves for diving into this area. Although the study is about intelligent applications, the framework and analysis methods may be adapted for other types of software as AI brings more intelligence into their life cycles.</p></p class="citation"></blockquote><h2 id=csit-9>cs.IT (9)</h2><h3 id=19--117152-graph-neural-network-meets-multi-agent-reinforcement-learning-fundamentals-applications-and-future-directions-ziheng-liu-et-al-2024>(1/9 | 117/152) Graph Neural Network Meets Multi-Agent Reinforcement Learning: Fundamentals, Applications, and Future Directions (Ziheng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziheng Liu, Jiayi Zhang, Enyu Shi, Zhilong Liu, Dusit Niyato, Bo Ai, Xuemin, Shen. (2024)<br><strong>Graph Neural Network Meets Multi-Agent Reinforcement Learning: Fundamentals, Applications, and Future Directions</strong><br><button class=copy-to-clipboard title="Graph Neural Network Meets Multi-Agent Reinforcement Learning: Fundamentals, Applications, and Future Directions" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 33<br>Keywords: Graph Attention Networks, Graph, Graph Neural Network, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04898v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04898v1.pdf filename=2404.04898v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-agent <b>reinforcement</b> <b>learning</b> (MARL) has become a fundamental component of next-generation wireless communication systems. Theoretically, although MARL has the advantages of low computational complexity and fast convergence rate, there exist several challenges including partial observability, non-stationary, and scalability. In this article, we investigate a novel MARL with <b>graph</b> <b>neural</b> <b>network-aided</b> communication (GNNComm-MARL) to address the aforementioned challenges by making use of <b>graph</b> <b>attention</b> <b>networks</b> to effectively sample neighborhoods and selectively aggregate messages. Furthermore, we thoroughly study the architecture of GNNComm-MARL and present a systematic design solution. We then present the typical applications of GNNComm-MARL from two aspects: resource allocation and mobility management. The results obtained unveil that GNNComm-MARL can achieve better performance with lower communication overhead compared to conventional communication schemes. Finally, several important research directions regarding GNNComm-MARL are presented to facilitate further investigation.</p></p class="citation"></blockquote><h3 id=29--118152-a-bird-eye-view-on-dna-storage-simulators-sanket-doshi-et-al-2024>(2/9 | 118/152) A Bird-Eye view on DNA Storage Simulators (Sanket Doshi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanket Doshi, Mihir Gohel, Manish K. Gupta. (2024)<br><strong>A Bird-Eye view on DNA Storage Simulators</strong><br><button class=copy-to-clipboard title="A Bird-Eye view on DNA Storage Simulators" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-CY, cs-ET, cs-IT, cs.IT, math-IT<br>Keyword Score: 23<br>Keywords: Clustering, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04877v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04877v1.pdf filename=2404.04877v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the current world due to the huge demand for storage, DNA-based storage solution sounds quite promising because of their longevity, low power consumption, and high capacity. However in real life storing data in the form of DNA is quite expensive, and challenging. Therefore researchers and developers develop such kind of software that helps simulate real-life DNA storage without worrying about the cost. This paper aims to review some of the software that performs DNA storage <b>simulations</b> in different domains. The paper also explains the core concepts such as synthesis, sequencing, <b>clustering,</b> reconstruction, GC window, K-mer window, etc and some overview on existing algorithms. Further, we present 3 different softwares on the basis of domain, implementation techniques, and customer/commercial usability.</p></p class="citation"></blockquote><h3 id=39--119152-probabilistic-examination-of-least-squares-error-in-low-bitwidth-cholesky-decomposition-alexander-osinsky-et-al-2024>(3/9 | 119/152) Probabilistic Examination of Least Squares Error in Low-bitwidth Cholesky Decomposition (Alexander Osinsky et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Osinsky, Roman Bychkov, Mikhail Trefilov, Vladimir Lyashev, Andrey Ivanov. (2024)<br><strong>Probabilistic Examination of Least Squares Error in Low-bitwidth Cholesky Decomposition</strong><br><button class=copy-to-clipboard title="Probabilistic Examination of Least Squares Error in Low-bitwidth Cholesky Decomposition" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05082v1.pdf filename=2404.05082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a new approach to justify a round-off error impact on the accuracy of the linear least squares (LS) solution using Cholesky decomposition. This decomposition is widely employed to inverse a matrix in the linear detector of the Multi-User multi-antenna receiver. The proposed stochastic bound is much closer to actual errors than other numerical bounds. It was tested with a half-precision format and validated in realistic scenarios. Experimental results demonstrate our approach predicts errors very close to those achieved by <b>simulations.</b> The proposed approach can be employed to analyze the resulting round-off error in many other applications.</p></p class="citation"></blockquote><h3 id=49--120152-towards-atomic-mimo-receivers-mingyao-cui-et-al-2024>(4/9 | 120/152) Towards Atomic MIMO Receivers (Mingyao Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyao Cui, Qunsong Zeng, Kaibin Huang. (2024)<br><strong>Towards Atomic MIMO Receivers</strong><br><button class=copy-to-clipboard title="Towards Atomic MIMO Receivers" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04864v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04864v1.pdf filename=2404.04864v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancement of Rydberg atoms in quantum sensing is driving a paradigm shift from classical receivers to atomic receivers. Capitalizing on the extreme sensitivity of Rydberg atoms to external disturbance, atomic receivers can measure radio-waves more precisely than classical receivers to support high-performance wireless communication and sensing. Although the atomic receiver is developing rapidly in quantum-sensing domain, its integration with wireless communications is still at a nascent stage. Particularly, systematic methods to enhance communication performance through this integration are largely uncharted. Motivated by this observation, we propose to incorporate atomic receivers into multiple-input-multiple-output (MIMO) communication to implement atomic-MIMO receivers. Specifically, we establish the framework of atomic-MIMO receivers exploiting the principle of quantum sensing, and reveal that its signal detection is intrinsically a non-linear biased phase-retrieval (PR) problem, as opposed to the linear model in classical MIMO systems. To this end, we modify the Gerchberg-Saxton (GS) algorithm, a typical PR solver, with a biased GS algorithm to solve the discovered biased PR problem. Moreover, we propose an Expectation-Maximization-GS (EM-GS) algorithm by introducing a high-pass filter constructed by Bessel functions into the iteration of GS, which improves the detection accuracy efficiently. Finally, the effectiveness of atomic MIMO receivers is demonstrated by theoretical analysis and numerical <b>simulation.</b></p></p class="citation"></blockquote><h3 id=59--121152-analog-digital-beam-focusing-for-line-of-sight-wide-aperture-mimo-with-spherical-wavefronts-jiyoung-yun-et-al-2024>(5/9 | 121/152) Analog-Digital Beam Focusing for Line of Sight Wide-Aperture MIMO with Spherical Wavefronts (Jiyoung Yun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiyoung Yun, Hojun Rho, Wan Choi. (2024)<br><strong>Analog-Digital Beam Focusing for Line of Sight Wide-Aperture MIMO with Spherical Wavefronts</strong><br><button class=copy-to-clipboard title="Analog-Digital Beam Focusing for Line of Sight Wide-Aperture MIMO with Spherical Wavefronts" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04842v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04842v1.pdf filename=2404.04842v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Enhancing high-speed wireless communication in the future relies significantly on harnessing high frequency bands effectively. These bands predominantly operate in line-of-sight (LoS) paths, necessitating well-configured antenna arrays and beamforming techniques for optimal spectrum utilization. Maximizing the potential of LoS multiple-input multiple-output (MIMO) systems, which are crucial for achieving high spectral efficiency, heavily depends on this. As the costs and power demands of mixed-signal devices in high frequency bands make a fully-digital architecture impractical for large-scale MIMO setups, our focus shifts to a hybrid analog-digital hardware configuration. Yet, analog processors&rsquo; limitations restrict flexibility within arrays, necessitating a nuanced understanding of hardware constraints for optimal antenna configuration design. We explore array design that optimizes the spectral efficiency of hybrid systems, considering hardware constraints. We propose an optimal antenna configuration, leveraging the prolate matrix structure of the LoS channel between two planar arrays. Building on the optimal array configuration, we introduce a low-complexity explicit analog-digital beam focusing scheme that exploits the asymptotic behavior of the LoS channel matrix in the near-field region. <b>Simulation</b> results validate that the proposed antenna configuration and beam focusing scheme achieves near-optimal performance across a range of signal-to-noise ratios with low computational complexity, even under arbitrary rotations relative to the communication link.</p></p class="citation"></blockquote><h3 id=69--122152-fourier-transform-based-wavenumber-domain-3d-imaging-in-ris-aided-communication-systems-yixuan-huang-et-al-2024>(6/9 | 122/152) Fourier Transform-based Wavenumber Domain 3D Imaging in RIS-aided Communication Systems (Yixuan Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixuan Huang, Jie Yang, Wankai Tang, Chao-Kai Wen, Shi Jin. (2024)<br><strong>Fourier Transform-based Wavenumber Domain 3D Imaging in RIS-aided Communication Systems</strong><br><button class=copy-to-clipboard title="Fourier Transform-based Wavenumber Domain 3D Imaging in RIS-aided Communication Systems" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04783v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04783v1.pdf filename=2404.04783v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Radio imaging is rapidly gaining prominence in the design of future communication systems, with the potential to utilize reconfigurable intelligent surfaces (RISs) as imaging apertures. Although the sparsity of targets in three-dimensional (3D) space has led most research to adopt compressed sensing (CS)-based imaging algorithms, these often require substantial computational and memory burdens. Drawing inspiration from conventional Fourier transform (FT)-based imaging methods, our research seeks to accelerate radio imaging in RIS-aided communication systems. To begin, we introduce a two-stage wavenumber domain 3D imaging technique: first, we modify RIS phase shifts to recover the equivalent channel response from the user equipment to the RIS array, subsequently employing traditional FT-based wavenumber domain methods to produce target images. We also determine the diffraction resolution limits of the system through k-space analysis, taking into account factors including system bandwidth, transmission direction, operating frequency, and the angle subtended by the RIS. Addressing the challenge of limited pilots in communication systems, we unveil an innovative algorithm that merges the strengths of both FT- and CS-based techniques by substituting the expansive sensing matrix with FT-based operators. Our <b>simulation</b> outcomes confirm that our proposed FT-based methods achieve high-quality images while demanding few time, memory, and communication resources.</p></p class="citation"></blockquote><h3 id=79--123152-single-server-pliable-private-information-retrieval-with-identifiable-side-information-megha-rayer-et-al-2024>(7/9 | 123/152) Single-Server Pliable Private Information Retrieval with Identifiable Side Information (Megha Rayer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Megha Rayer, Charul Rajput, B. Sundar Rajan. (2024)<br><strong>Single-Server Pliable Private Information Retrieval with Identifiable Side Information</strong><br><button class=copy-to-clipboard title="Single-Server Pliable Private Information Retrieval with Identifiable Side Information" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IR, cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04820v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04820v1.pdf filename=2404.04820v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Pliable Private <b>Information</b> <b>Retrieval</b> (PPIR) with a single server, messages are partitioned into $\Gamma$ non-overlapping classes \cite{ref5}. The user wants to retrieve a message from its desired class without revealing the identity of the desired class to the server. In \cite{ref6}, Obead et al. consider the problem of PPIR with Side <b>Information</b> <b>(PPIR-SI),</b> where the user now has side <b>information.</b> <b>The</b> user wants to retrieve any new message (not included in the side <b>information)</b> <b>from</b> its desired class without revealing the identity of the desired class and its side <b>information.</b> <b>A</b> scheme for the PPIR-SI is given in \cite{ref6} for the case when the users side <b>information</b> <b>is</b> unidentified, and this case is referred to as PPIR with Unidentifiable SI (PPIR-USI). In this paper, we study the problem of PPIR for the single server case when the side <b>information</b> <b>is</b> partially identifiable, and we term this case as PPIR with Identifiable Side <b>Information</b> <b>(PPIR-ISI).</b> The user is well aware of the identity of the side <b>information</b> <b>belonging</b> to $\eta$ number of classes, where $1\leq \eta \leq \Gamma$. We give a scheme for PPIR-ISI, and we prove that having identifiable side <b>information</b> <b>is</b> advantageous by comparing the rate of the proposed scheme to the rate of the PPIR-USI scheme given in \cite{ref6} for some cases. Further, we extend the problem of PPIR-ISI for multi-user case, where users can collaboratively generate the query sets, and we give a scheme for this problem.</p></p class="citation"></blockquote><h3 id=89--124152-soft-in-soft-out-decoding-of-spherical-codes-from-cartesian-powers-of-pam-constellations-reza-rafie-borujeny-et-al-2024>(8/9 | 124/152) Soft-in Soft-out Decoding of Spherical Codes from Cartesian Powers of PAM Constellations (Reza Rafie Borujeny et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reza Rafie Borujeny, Susanna E. Rumsey, Stark C. Draper, Frank R. Kschischang. (2024)<br><strong>Soft-in Soft-out Decoding of Spherical Codes from Cartesian Powers of PAM Constellations</strong><br><button class=copy-to-clipboard title="Soft-in Soft-out Decoding of Spherical Codes from Cartesian Powers of PAM Constellations" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04776v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04776v1.pdf filename=2404.04776v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For applications in concatenated coding for optical communications systems, we examine the encoding and soft-decoding of short spherical codes constructed as constant-energy shells of the Cartesian power of pulse amplitude modulation constellations. These are unions of permutation codes having the same average power. We construct a list decoder for permutation codes by adapting Murty&rsquo;s algorithm, which is then used to determine <b>mutual</b> <b>information</b> curves for these permutation codes. In the process, we discover a straightforward expression for determining the likelihood of large subcodes of permutation codes. We refer to these subcodes, obtained by all possible sign flips of a given permutation codeword, as orbits. We introduce a simple process, which we call orbit decoding with frozen symbols, that allows us to extract soft information from noisy permutation codewords. In a sample communication system with probabilistic amplitude shaping protected by a standard low-density parity-check code that employs short permutation codes, we demonstrate that orbit decoding with frozen symbols provides a gain of about 0.3 dB in signal-to-noise ratio compared to the traditional symbol-by-symbol decoding. By using spherical codes composed of unions of permutation codes, we can increase the input entropy compared to using permutation codes alone. In one scheme, we consider a union of a small number of permutation codes. In this case, orbit decoding with frozen symbols provides about 0.2 dB gain compared to the traditional method. In another scheme, we use all possible permutations to form a spherical code that exhibits a computationally feasible trellis representation. The soft information obtained using the BCJR algorithm outperforms the traditional symbol-by-symbol method by 0.1 dB.</p></p class="citation"></blockquote><h3 id=99--125152-holographic-integrated-data-and-energy-transfer-qingxiao-huang-et-al-2024>(9/9 | 125/152) Holographic Integrated Data and Energy Transfer (Qingxiao Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingxiao Huang, Jie Hu, Yizhe Zhao, Kun Yang. (2024)<br><strong>Holographic Integrated Data and Energy Transfer</strong><br><button class=copy-to-clipboard title="Holographic Integrated Data and Energy Transfer" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04927v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04927v1.pdf filename=2404.04927v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Thanks to the application of metamaterials, holographic multiple-input multiple-output (H-MIMO) is expected to achieve a higher spatial diversity gain by enabling the ability to generate any current distribution on the surface. With the aid of electromagnetic (EM) manipulation capability of H-MIMO, integrated data and energy transfer (IDET) system can fully exploits the EM channel to realize energy focusing and eliminate inter-user interference, which yields the concept of holographic IDET (H-IDET). In this paper, we invetigate the beamforming designs for H-IDET systems, where the sum-rate of data users (DUs) are maximized by guaranteeing the energy harvesting requirements of energy users (EUs). In order to solve the non-convex functional programming, a block coordinate descent (BCD) based scheme is proposed, wherein the Fourier transform and the equivalence between the signal-to-interference-plus-noise ratio (SINR) and the mean-square error (MSE) are also conceived, followed by the successive convex approximation (SCA) and an initialization scheme to enhance robustness. Numerical results illustrate that our proposed H-IDET scheme outperforms <b>benchmark</b> schemes, especially the one adopting traditional discrete antennas. Besides, the near-field focusing using EM channel model achieves better performance compared to that using the traditional channel model, especially for WPT where the EUs are usually close to the transmitter.</p></p class="citation"></blockquote><h2 id=eesssy-3>eess.SY (3)</h2><h3 id=13--126152-nanometer-scanning-with-micrometer-sensing-beating-quantization-constraints-in-lissajous-trajectory-tracking-matheus-lohse-et-al-2024>(1/3 | 126/152) Nanometer Scanning with Micrometer Sensing: Beating Quantization Constraints in Lissajous Trajectory Tracking (Matheus Lohse et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matheus Lohse, Rafael S. Castro, Aurelio T. Salton, Minyue Fu. (2024)<br><strong>Nanometer Scanning with Micrometer Sensing: Beating Quantization Constraints in Lissajous Trajectory Tracking</strong><br><button class=copy-to-clipboard title="Nanometer Scanning with Micrometer Sensing: Beating Quantization Constraints in Lissajous Trajectory Tracking" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Continuous Time, Continuous Time, Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04973v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04973v1.pdf filename=2404.04973v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the task of tracking Lissajous trajectories in the presence of <b>quantized</b> positioning sensors. To do so, theoretical results on tracking of <b>continuous</b> <b>time</b> periodic signals in the presence of output <b>quantization</b> are provided. With these results in hand, the application to Lissajous tracking is explored. The method proposed relies on the internal model principle and dispenses perfect knowledge of the system equations. Numerical results show that an arbitrary small scanning resolution is achievable despite large sensor <b>quantization</b> intervals.</p></p class="citation"></blockquote><h3 id=23--127152-opinion-dynamics-for-utility-maximizing-agents-exploring-the-impact-of-resource-penalty-prashil-wankhede-et-al-2024>(2/3 | 127/152) Opinion Dynamics for Utility Maximizing Agents: Exploring the Impact of Resource Penalty (Prashil Wankhede et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prashil Wankhede, Nirabhra Mandal, Sonia Martínez, Pavankumar Tallapragada. (2024)<br><strong>Opinion Dynamics for Utility Maximizing Agents: Exploring the Impact of Resource Penalty</strong><br><button class=copy-to-clipboard title="Opinion Dynamics for Utility Maximizing Agents: Exploring the Impact of Resource Penalty" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-GT, cs-MA, cs-SI, cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Continuous Time, Continuous Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04912v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04912v1.pdf filename=2404.04912v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a <b>continuous-time</b> <b>nonlinear</b> model of opinion dynamics with utility-maximizing agents connected via a social influence network. A distinguishing feature of the proposed model is the inclusion of an opinion-dependent resource-penalty term in the utilities, which limits the agents from holding opinions of large magnitude. The proposed utility functions also account for how the relative resources within the social group affect both an agent&rsquo;s stubbornness and social influence. Each agent myopically seeks to maximize its utility by revising its opinion in the gradient ascent direction of its utility function, thus leading to the proposed opinion dynamics. We show that, for any arbitrary social influence network, opinions are ultimately bounded. For networks with weak antagonistic relations, we show that there exists a globally exponentially stable equilibrium using contraction theory. We establish conditions for the existence of consensus equilibrium and analyze the relative dominance of the agents at consensus. We also conduct a game-theoretic analysis of the underlying opinion formation game, including on Nash equilibria and on prices of anarchy in terms of satisfaction ratios. Additionally, we also investigate the oscillatory behavior of opinions in a two-agent scenario. Finally, <b>simulations</b> illustrate our findings.</p></p class="citation"></blockquote><h3 id=33--128152-minimax-least-square-policy-iteration-for-cost-aware-defense-of-traffic-routing-against-unknown-threats-yuzhen-zhan-et-al-2024>(3/3 | 128/152) Minimax Least-Square Policy Iteration for Cost-Aware Defense of Traffic Routing against Unknown Threats (Yuzhen Zhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuzhen Zhan, Li Jin. (2024)<br><strong>Minimax Least-Square Policy Iteration for Cost-Aware Defense of Traffic Routing against Unknown Threats</strong><br><button class=copy-to-clipboard title="Minimax Least-Square Policy Iteration for Cost-Aware Defense of Traffic Routing against Unknown Threats" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05008v1.pdf filename=2404.05008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic routing is one of the representative control scheme in transportation, production lines, and data transmission. In the modern context of connectivity and autonomy, routing decisions are potentially vulnerable to malicious attacks. In this paper, we consider the dynamic routing problem over parallel traffic links in the face of such threats. An attacker is capable of increasing or destabilizing traffic queues by strategic manipulating the nominally optimal routing decisions. A defender is capable of securing the correct routing decision. Attacking and defensive actions induce technological costs. The defender has no prior information about the attacker&rsquo;s strategy. We develop an least-square policy iteration algorithm for the defender to compute a cost-aware and threat-adaptive defensive strategy. The policy evaluation step computes a weight vector that minimizes the sampled temporal-difference error. We derive a concrete theoretical upper bound on the evaluation error based on the theory of value function approximation. The policy improvement step solves a minimax problem and thus iteratively computes the Markov perfect equilibrium of the <b>security</b> game. We also discuss the training error of the entire policy iteration process.</p></p class="citation"></blockquote><h2 id=csar-2>cs.AR (2)</h2><h3 id=12--129152-explaining-eda-synthesis-errors-with-llms-siyu-qiu-et-al-2024>(1/2 | 129/152) Explaining EDA synthesis errors with LLMs (Siyu Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyu Qiu, Benjamin Tan, Hammond Pearce. (2024)<br><strong>Explaining EDA synthesis errors with LLMs</strong><br><button class=copy-to-clipboard title="Explaining EDA synthesis errors with LLMs" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AI, cs-AR, cs-PL, cs-SE, cs.AR<br>Keyword Score: 30<br>Keywords: Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07235v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07235v1.pdf filename=2404.07235v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training new engineers in digital design is a challenge, particularly when it comes to teaching the complex electronic design automation (EDA) tooling used in this domain. Learners will typically deploy designs in the Verilog and VHDL hardware description languages to Field Programmable Gate Arrays (FPGAs) from Altera (Intel) and Xilinx (AMD) via proprietary closed-source toolchains (Quartus Prime and Vivado, respectively). These tools are complex and difficult to use &ndash; yet, as they are the tools used in industry, they are an essential first step in this space. In this work, we examine how recent advances in artificial intelligence may be leveraged to address aspects of this challenge. Specifically, we investigate if <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> which have demonstrated text comprehension and <b>question-answering</b> <b>capabilities,</b> can be used to generate novice-friendly explanations of compile-time synthesis error messages from Quartus Prime and Vivado. To perform this study we generate 936 error message explanations using three OpenAI <b>LLMs</b> over 21 different buggy code samples. These are then graded for relevance and correctness, and we find that in approximately 71% of cases the <b>LLMs</b> give correct & complete explanations suitable for novice learners.</p></p class="citation"></blockquote><h3 id=22--130152-gdr-hgnn-a-heterogeneous-graph-neural-networks-accelerator-frontend-with-graph-decoupling-and-recoupling-runzhen-xue-et-al-2024>(2/2 | 130/152) GDR-HGNN: A Heterogeneous Graph Neural Networks Accelerator Frontend with Graph Decoupling and Recoupling (Runzhen Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runzhen Xue, Mingyu Yan, Dengke Han, Yihan Teng, Zhimin Tang, Xiaochun Ye, Dongrui Fan. (2024)<br><strong>GDR-HGNN: A Heterogeneous Graph Neural Networks Accelerator Frontend with Graph Decoupling and Recoupling</strong><br><button class=copy-to-clipboard title="GDR-HGNN: A Heterogeneous Graph Neural Networks Accelerator Frontend with Graph Decoupling and Recoupling" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 18<br>Keywords: Graph, Graph Neural Network, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04792v1.pdf filename=2404.04792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Heterogeneous <b>Graph</b> <b>Neural</b> <b>Networks</b> (HGNNs) have broadened the applicability of <b>graph</b> <b>representation</b> <b>learning</b> to heterogeneous <b>graphs.</b> <b>However,</b> <b>the</b> irregular memory access pattern of HGNNs leads to the buffer thrashing issue in HGNN accelerators. In this work, we identify an opportunity to address buffer thrashing in HGNN acceleration through an analysis of the topology of heterogeneous <b>graphs.</b> <b>To</b> <b>harvest</b> this opportunity, we propose a <b>graph</b> <b>restructuring</b> <b>method</b> and map it into a hardware frontend named GDR-HGNN. GDR-HGNN dynamically restructures the <b>graph</b> <b>on</b> <b>the</b> fly to enhance data locality for HGNN accelerators. Experimental results demonstrate that, with the assistance of GDR-HGNN, a leading HGNN accelerator achieves an average speedup of 14.6 times and 1.78 times compared to the state-of-the-art software framework running on A100 GPU and itself, respectively.</p></p class="citation"></blockquote><h2 id=csai-4>cs.AI (4)</h2><h3 id=14--131152-dwe-dual-way-matching-enhanced-framework-for-multimodal-entity-linking-shezheng-song-et-al-2024>(1/4 | 131/152) DWE+: Dual-Way Matching Enhanced Framework for Multimodal Entity Linking (Shezheng Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shezheng Song, Shasha Li, Shan Zhao, Xiaopeng Li, Chengyu Wang, Jie Yu, Jun Ma, Tianwei Yan, Bin Ji, Xiaoguang Mao. (2024)<br><strong>DWE+: Dual-Way Matching Enhanced Framework for Multimodal Entity Linking</strong><br><button class=copy-to-clipboard title="DWE+: Dual-Way Matching Enhanced Framework for Multimodal Entity Linking" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CV, cs.AI<br>Keyword Score: 26<br>Keywords: Contrastive Learning, Multi-modal, Multi-modal, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04818v1.pdf filename=2404.04818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> entity linking (MEL) aims to utilize <b>multimodal</b> information (usually textual and visual information) to link ambiguous mentions to unambiguous entities in knowledge base. Current methods facing main issues: (1)treating the entire image as input may contain redundant information. (2)the insufficient utilization of entity-related information, such as attributes in images. (3)semantic inconsistency between the entity in knowledge base and its representation. To this end, we propose DWE+ for <b>multimodal</b> entity linking. DWE+ could capture finer semantics and dynamically maintain semantic consistency with entities. This is achieved by three aspects: (a)we introduce a method for extracting fine-grained image features by partitioning the image into multiple local objects. Then, hierarchical <b>contrastive</b> <b>learning</b> is used to further align semantics between coarse-grained information(text and image) and fine-grained (mention and visual objects). (b)we explore ways to extract visual attributes from images to enhance fusion feature such as facial features and identity. (c)we leverage Wikipedia and <b>ChatGPT</b> to capture the entity representation, achieving semantic enrichment from both static and dynamic perspectives, which better reflects the real-world entity semantics. Experiments on Wikimel, Richpedia, and Wikidiverse datasets demonstrate the effectiveness of DWE+ in improving MEL performance. Specifically, we optimize these datasets and achieve state-of-the-art performance on the enhanced datasets. The code and enhanced datasets are released on <a href=https://github.com/season1blue/DWET>https://github.com/season1blue/DWET</a></p></p class="citation"></blockquote><h3 id=24--132152-towards-reliable-and-empathetic-depression-diagnosis-oriented-chats-kunyao-lan-et-al-2024>(2/4 | 132/152) Towards Reliable and Empathetic Depression-Diagnosis-Oriented Chats (Kunyao Lan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kunyao Lan, Cong Ming, Binwei Yao, Lu Chen, Mengyue Wu. (2024)<br><strong>Towards Reliable and Empathetic Depression-Diagnosis-Oriented Chats</strong><br><button class=copy-to-clipboard title="Towards Reliable and Empathetic Depression-Diagnosis-Oriented Chats" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 20<br>Keywords: Chatbot, Dialogue System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05012v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05012v1.pdf filename=2404.05012v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Chatbots</b> can serve as a viable tool for preliminary depression diagnosis via interactive conversations with potential patients. Nevertheless, the blend of task-oriented and chit-chat in diagnosis-related <b>dialogues</b> <b>necessitates</b> professional expertise and empathy. Such unique requirements challenge traditional <b>dialogue</b> <b>frameworks</b> geared towards single optimization goals. To address this, we propose an innovative ontology definition and generation framework tailored explicitly for depression diagnosis <b>dialogues,</b> <b>combining</b> the reliability of task-oriented conversations with the appeal of empathy-related chit-chat. We further apply the framework to D$^4$, the only existing public <b>dialogue</b> <b>dataset</b> on depression diagnosis-oriented chats. Exhaustive experimental results indicate significant improvements in task completion and emotional support generation in depression diagnosis, fostering a more comprehensive approach to task-oriented chat <b>dialogue</b> <b>system</b> development and its applications in digital mental health.</p></p class="citation"></blockquote><h3 id=34--133152-ai2apps-a-visual-ide-for-building-llm-based-ai-agent-applications-xin-pang-et-al-2024>(3/4 | 133/152) AI2Apps: A Visual IDE for Building LLM-based AI Agent Applications (Xin Pang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Pang, Zhucong Li, Jiaxiang Chen, Yuan Cheng, Yinghui Xu, Yuan Qi. (2024)<br><strong>AI2Apps: A Visual IDE for Building LLM-based AI Agent Applications</strong><br><button class=copy-to-clipboard title="AI2Apps: A Visual IDE for Building LLM-based AI Agent Applications" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-SE, cs.AI<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04902v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04902v1.pdf filename=2404.04902v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce AI2Apps, a Visual Integrated Development Environment (Visual IDE) with full-cycle capabilities that accelerates developers to build deployable <b>LLM-based</b> AI agent Applications. This Visual IDE prioritizes both the Integrity of its development tools and the Visuality of its components, ensuring a smooth and efficient building experience.On one hand, AI2Apps integrates a comprehensive development toolkit ranging from a prototyping canvas and AI-assisted code editor to agent debugger, management system, and deployment tools all within a web-based graphical user interface. On the other hand, AI2Apps visualizes reusable front-end and back-end code as intuitive drag-and-drop components. Furthermore, a plugin system named AI2Apps Extension (AAE) is designed for Extensibility, showcasing how a new plugin with 20 components enables web agent to mimic human-like browsing behavior. Our case study demonstrates substantial efficiency improvements, with AI2Apps reducing token consumption and API calls when debugging a specific sophisticated <b>multimodal</b> agent by approximately 90% and 80%, respectively. The AI2Apps, including an online demo, open-source code, and a screencast video, is now publicly accessible.</p></p class="citation"></blockquote><h3 id=44--134152-on-the-uniqueness-of-solution-for-the-bellman-equation-of-ltl-objectives-zetong-xuan-et-al-2024>(4/4 | 134/152) On the Uniqueness of Solution for the Bellman Equation of LTL Objectives (Zetong Xuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zetong Xuan, Alper Kamil Bozkurt, Miroslav Pajic, Yu Wang. (2024)<br><strong>On the Uniqueness of Solution for the Bellman Equation of LTL Objectives</strong><br><button class=copy-to-clipboard title="On the Uniqueness of Solution for the Bellman Equation of LTL Objectives" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-RO, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05074v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05074v1.pdf filename=2404.05074v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Surrogate rewards for linear temporal logic (LTL) objectives are commonly utilized in planning problems for LTL objectives. In a widely-adopted surrogate reward approach, two discount factors are used to ensure that the expected return approximates the satisfaction probability of the LTL objective. The expected return then can be estimated by methods using the Bellman updates such as <b>reinforcement</b> <b>learning.</b> However, the uniqueness of the solution to the Bellman equation with two discount factors has not been explicitly discussed. We demonstrate with an example that when one of the discount factors is set to one, as allowed in many previous works, the Bellman equation may have multiple solutions, leading to inaccurate evaluation of the expected return. We then propose a condition for the Bellman equation to have the expected return as the unique solution, requiring the solutions for states inside a rejecting bottom strongly connected component (BSCC) to be 0. We prove this condition is sufficient by showing that the solutions for the states with discounting can be separated from those for the states without discounting under this condition</p></p class="citation"></blockquote><h2 id=mathna-1>math.NA (1)</h2><h3 id=11--135152-generative-downscaling-of-pde-solvers-with-physics-guided-diffusion-models-yulong-lu-et-al-2024>(1/1 | 135/152) Generative downscaling of PDE solvers with physics-guided diffusion models (Yulong Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yulong Lu, Wuzhe Xu. (2024)<br><strong>Generative downscaling of PDE solvers with physics-guided diffusion models</strong><br><button class=copy-to-clipboard title="Generative downscaling of PDE solvers with physics-guided diffusion models" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 23<br>Keywords: Diffusion Model, Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05009v1.pdf filename=2404.05009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Solving partial differential equations (PDEs) on fine spatio-temporal scales for high-fidelity solutions is critical for numerous scientific breakthroughs. Yet, this process can be prohibitively expensive, owing to the inherent complexities of the problems, including nonlinearity and multiscale phenomena. To speed up large-scale computations, a process known as downscaling is employed, which generates high-fidelity approximate solutions from their low-fidelity counterparts. In this paper, we propose a novel Physics-Guided <b>Diffusion</b> <b>Model</b> (PGDM) for downscaling. Our model, initially trained on a dataset comprising low-and-high-fidelity paired solutions across coarse and fine scales, generates new high-fidelity approximations from any new low-fidelity inputs. These outputs are subsequently refined through <b>fine-tuning,</b> aimed at minimizing the physical discrepancies as defined by the discretized PDEs at the finer scale. We evaluate and <b>benchmark</b> our model&rsquo;s performance against other downscaling baselines in three categories of nonlinear PDEs. Our numerical experiments demonstrate that our model not only outperforms the baselines but also achieves a computational acceleration exceeding tenfold, while maintaining the same level of accuracy as the conventional fine-scale solvers.</p></p class="citation"></blockquote><h2 id=cslo-3>cs.LO (3)</h2><h3 id=13--136152-quantitative-weakest-hyper-pre-unifying-correctness-and-incorrectness-hyperproperties-via-predicate-transformers-linpeng-zhang-et-al-2024>(1/3 | 136/152) Quantitative Weakest Hyper Pre: Unifying Correctness and Incorrectness Hyperproperties via Predicate Transformers (Linpeng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linpeng Zhang, Noam Zilberstein, Benjamin Lucien Kaminski, Alexandra Silva. (2024)<br><strong>Quantitative Weakest Hyper Pre: Unifying Correctness and Incorrectness Hyperproperties via Predicate Transformers</strong><br><button class=copy-to-clipboard title="Quantitative Weakest Hyper Pre: Unifying Correctness and Incorrectness Hyperproperties via Predicate Transformers" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-CR, cs-FL, cs-LO, cs-PL, cs.LO<br>Keyword Score: 20<br>Keywords: Transformer, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05097v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05097v1.pdf filename=2404.05097v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel \emph{weakest pre calculus} for \emph{reasoning about quantitative hyperproperties} over \emph{nondeterministic and probabilistic} programs. Whereas existing calculi allow <b>reasoning</b> about the expected value that a quantity assumes after program termination from a \emph{single initial state}, we do so for \emph{initial sets of states} or \emph{initial probability distributions}. We thus (i)~obtain a weakest pre calculus for hyper Hoare logic and (ii)~enable <b>reasoning</b> about so-called \emph{hyperquantities} which include expected values but also quantities (e.g. variance) out of scope of previous work. As a byproduct, we obtain a novel strongest post for weighted programs that extends both existing strongest and strongest liberal post calculi. Our framework reveals novel dualities between forward and backward <b>transformers,</b> correctness and incorrectness, as well as nontermination and unreachability.</p></p class="citation"></blockquote><h3 id=23--137152-the-church-synthesis-problem-over-continuous-time-alexander-rabinovich-et-al-2024>(2/3 | 137/152) The Church Synthesis Problem over Continuous Time (Alexander Rabinovich et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Rabinovich, Daniel Fattal. (2024)<br><strong>The Church Synthesis Problem over Continuous Time</strong><br><button class=copy-to-clipboard title="The Church Synthesis Problem over Continuous Time" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 20<br>Keywords: Continuous Time, Continuous Time, Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04782v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04782v1.pdf filename=2404.04782v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Church Problem asks for the construction of a procedure which, given a logical specification A(I,O) between input omega-strings I and output omega-strings O, determines whether there exists an operator F that implements the specification in the sense that A(I, F(I)) holds for all inputs I. Buchi and Landweber provided a procedure to solve the Church problem for MSO specifications and operators computable by finite-state automata. We investigate a generalization of the Church synthesis problem to the <b>continuous</b> <b>time</b> of the non-negative reals. We show that in the <b>continuous</b> <b>time</b> there are phenomena which are very different from the canonical <b>discrete</b> <b>time</b> domain of the natural numbers.</p></p class="citation"></blockquote><h3 id=33--138152-gatlab-modeling-and-programming-with-generalized-algebraic-theories-owen-lynch-et-al-2024>(3/3 | 138/152) GATlab: Modeling and Programming with Generalized Algebraic Theories (Owen Lynch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Owen Lynch, Kris Brown, James Fairbanks, Evan Patterson. (2024)<br><strong>GATlab: Modeling and Programming with Generalized Algebraic Theories</strong><br><button class=copy-to-clipboard title="GATlab: Modeling and Programming with Generalized Algebraic Theories" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs-PL, cs.LO<br>Keyword Score: 10<br>Keywords: Graph Attention Networks<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04837v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04837v1.pdf filename=2404.04837v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Categories and categorical structures are increasingly recognized as useful abstractions for modeling in science and engineering. To uniformly implement category-theoretic mathematical models in software, we introduce GATlab, a domain-specific language for algebraic specification embedded in a technical programming language. GATlab is based on generalized algebraic theories <b>(GATs),</b> a logical system extending algebraic theories with dependent types so as to encompass category theory. Using GATlab, the programmer can specify generalized algebraic theories and their models, including both free models, based on symbolic expressions, and computational models, defined by arbitrary code in the host language. Moreover, the programmer can define maps between theories and use them to declaratively migrate models of one theory to models of another. In short, GATlab aims to provide a unified environment for both computer algebra and software interface design with generalized algebraic theories. In this paper, we describe the design, implementation, and applications of GATlab.</p></p class="citation"></blockquote><h2 id=cshc-3>cs.HC (3)</h2><h3 id=13--139152-reduction-of-forgetting-by-contextual-variation-during-encoding-using-360-degree-video-based-immersive-virtual-environments-takato-mizuho-et-al-2024>(1/3 | 139/152) Reduction of Forgetting by Contextual Variation During Encoding Using 360-Degree Video-Based Immersive Virtual Environments (Takato Mizuho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Takato Mizuho, Takuji Narumi, Hideaki Kuzuoka. (2024)<br><strong>Reduction of Forgetting by Contextual Variation During Encoding Using 360-Degree Video-Based Immersive Virtual Environments</strong><br><button class=copy-to-clipboard title="Reduction of Forgetting by Contextual Variation During Encoding Using 360-Degree Video-Based Immersive Virtual Environments" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Virtual Reality (VR), Virtual Reality (VR)<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05007v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05007v1.pdf filename=2404.05007v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recall impairment in a different environmental context from learning is called context-dependent forgetting. Two learning methods have been proposed to prevent context-dependent forgetting: reinstatement and decontextualization. Reinstatement matches the environmental context between learning and retrieval, whereas decontextualization involves repeated learning in various environmental contexts and eliminates the context dependency of memory. Conventionally, these methods have been validated by switching between physical rooms. However, in this study, we use immersive <b>virtual</b> <b>environments</b> (IVEs) as the environmental context assisted by <b>virtual</b> <b>reality</b> <b>(VR),</b> which is known for its low cost and high reproducibility compared to traditional manipulation. Whereas most existing studies using <b>VR</b> have failed to reveal the reinstatement effect, we test its occurrence using a 360-degree video-based IVE with improved familiarity and realism instead of a computer graphics-based IVE. Furthermore, we are the first to address decontextualization using <b>VR.</b> Our experiment showed that repeated learning in the same constant IVE as retrieval did not significantly reduce forgetting compared to repeated learning in different constant IVEs. Conversely, repeated learning in various IVEs significantly reduced forgetting than repeated learning in constant IVEs. These findings contribute to the design of IVEs for <b>VR-based</b> applications, particularly in educational settings.</p></p class="citation"></blockquote><h3 id=23--140152-chart-what-i-say-exploring-cross-modality-prompt-alignment-in-ai-assisted-chart-authoring-nazar-ponochevnyi-et-al-2024>(2/3 | 140/152) Chart What I Say: Exploring Cross-Modality Prompt Alignment in AI-Assisted Chart Authoring (Nazar Ponochevnyi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nazar Ponochevnyi, Anastasia Kuzminykh. (2024)<br><strong>Chart What I Say: Exploring Cross-Modality Prompt Alignment in AI-Assisted Chart Authoring</strong><br><button class=copy-to-clipboard title="Chart What I Say: Exploring Cross-Modality Prompt Alignment in AI-Assisted Chart Authoring" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05103v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05103v1.pdf filename=2404.05103v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent chart-authoring systems, such as Amazon Q in QuickSight and Copilot for Power BI, demonstrate an emergent focus on supporting natural language input to share meaningful insights from data through chart creation. Currently, chart-authoring systems tend to integrate voice input capabilities by relying on speech-to-text transcription, processing spoken and typed input similarly. However, cross-modality input comparisons in other interaction domains suggest that the structure of spoken and typed-in interactions could notably differ, reflecting variations in user expectations based on interface affordances. Thus, in this work, we compare spoken and typed instructions for chart creation. Findings suggest that while both text and voice instructions cover chart elements and element organization, voice descriptions have a variety of command formats, element characteristics, and complex linguistic features. Based on these findings, we developed guidelines for designing voice-based authoring-oriented systems and additional features that can be incorporated into existing text-based systems to support speech modality.</p></p class="citation"></blockquote><h3 id=33--141152-balancing-information-perception-with-yin-yang-agent-based-information-neutrality-model-for-recommendation-systems-mengyan-wang-et-al-2024>(3/3 | 141/152) Balancing Information Perception with Yin-Yang: Agent-Based Information Neutrality Model for Recommendation Systems (Mengyan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengyan Wang, Yuxuan Hu, Shiqing Wu, Weihua Li, Quan Bai, Verica Rupar. (2024)<br><strong>Balancing Information Perception with Yin-Yang: Agent-Based Information Neutrality Model for Recommendation Systems</strong><br><button class=copy-to-clipboard title="Balancing Information Perception with Yin-Yang: Agent-Based Information Neutrality Model for Recommendation Systems" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-IR, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04906v1.pdf filename=2404.04906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While preference-based <b>recommendation</b> algorithms effectively enhance user engagement by recommending personalized content, they often result in the creation of ``filter bubbles&rsquo;&rsquo;. These bubbles restrict the range of information users interact with, inadvertently reinforcing their existing viewpoints. Previous research has focused on modifying these underlying algorithms to tackle this issue. Yet, approaches that maintain the integrity of the original algorithms remain largely unexplored. This paper introduces an Agent-based Information Neutrality model grounded in the Yin-Yang theory, namely, AbIN. This innovative approach targets the imbalance in information perception within existing <b>recommendation</b> systems. It is designed to integrate with these preference-based systems, ensuring the delivery of <b>recommendations</b> with neutral information. Our empirical evaluation of this model proved its efficacy, showcasing its capacity to expand information diversity while respecting user preferences. Consequently, AbIN emerges as an instrumental tool in mitigating the negative impact of filter bubbles on information consumption.</p></p class="citation"></blockquote><h2 id=cspl-2>cs.PL (2)</h2><h3 id=12--142152-allo-a-programming-model-for-composable-accelerator-design-hongzheng-chen-et-al-2024>(1/2 | 142/152) Allo: A Programming Model for Composable Accelerator Design (Hongzheng Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongzheng Chen, Niansong Zhang, Shaojie Xiang, Zhichen Zeng, Mengjia Dai, Zhiru Zhang. (2024)<br><strong>Allo: A Programming Model for Composable Accelerator Design</strong><br><button class=copy-to-clipboard title="Allo: A Programming Model for Composable Accelerator Design" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-AR, cs-LG, cs-PL, cs.PL<br>Keyword Score: 13<br>Keywords: Benchmarking, GPT-2<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04815v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04815v1.pdf filename=2404.04815v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Special-purpose hardware accelerators are increasingly pivotal for sustaining performance improvements in emerging applications, especially as the benefits of technology scaling continue to diminish. However, designers currently lack effective tools and methodologies to construct complex, high-performance accelerator architectures in a productive manner. Existing high-level synthesis (HLS) tools often require intrusive source-level changes to attain satisfactory quality of results. Despite the introduction of several new accelerator design languages (ADLs) aiming to enhance or replace HLS, their advantages are more evident in relatively simple applications with a single kernel. Existing ADLs prove less effective for realistic hierarchical designs with multiple kernels, even if the design hierarchy is flattened. In this paper, we introduce Allo, a composable programming model for efficient spatial accelerator design. Allo decouples hardware customizations, including compute, memory, communication, and data type from algorithm specification, and encapsulates them as a set of customization primitives. Allo preserves the hierarchical structure of an input program by combining customizations from different functions in a bottom-up, type-safe manner. This approach facilitates holistic optimizations that span across function boundaries. We conduct comprehensive experiments on commonly-used HLS <b>benchmarks</b> and several realistic deep learning models. Our evaluation shows that Allo can outperform state-of-the-art HLS tools and ADLs on all test cases in the PolyBench. For the <b>GPT2</b> model, the inference latency of the Allo generated accelerator is 1.7x faster than the NVIDIA A100 GPU with 5.4x higher energy efficiency, demonstrating the capability of Allo to handle large-scale designs.</p></p class="citation"></blockquote><h3 id=22--143152-katch-a-fast-symbolic-verifier-for-netkat-mark-moeller-et-al-2024>(2/2 | 143/152) KATch: A Fast Symbolic Verifier for NetKAT (Mark Moeller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mark Moeller, Jules Jacobs, Olivier Savary Belanger, David Darais, Cole Schlesinger, Steffen Smolka, Nate Foster, Alexandra Silva. (2024)<br><strong>KATch: A Fast Symbolic Verifier for NetKAT</strong><br><button class=copy-to-clipboard title="KATch: A Fast Symbolic Verifier for NetKAT" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04760v1.pdf filename=2404.04760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We develop new data structures and algorithms for checking verification queries in NetKAT, a domain-specific language for specifying the behavior of network data planes. Our results extend the techniques obtained in prior work on symbolic automata and provide a framework for building efficient and scalable verification tools. We present \KATch, an implementation of these ideas in Scala, featuring an extended set of NetKAT operators that are useful for expressing network-wide specifications, and a verification engine that constructs a bisimulation or generates a counter-example showing that none exists. We evaluate the performance of our implementation on real-world and synthetic <b>benchmarks,</b> verifying properties such as reachability and slice isolation, typically returning a result in well under a second, which is orders of magnitude faster than previous approaches. Our advancements underscore NetKAT&rsquo;s potential as a practical, declarative language for network specification and verification.</p></p class="citation"></blockquote><h2 id=q-fincp-1>q-fin.CP (1)</h2><h3 id=11--144152-stockgpt-a-genai-model-for-stock-prediction-and-trading-dat-mai-2024>(1/1 | 144/152) StockGPT: A GenAI Model for Stock Prediction and Trading (Dat Mai, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dat Mai. (2024)<br><strong>StockGPT: A GenAI Model for Stock Prediction and Trading</strong><br><button class=copy-to-clipboard title="StockGPT: A GenAI Model for Stock Prediction and Trading" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.CP<br>Categories: cs-AI, q-fin-CP, q-fin-PM, q-fin-PR, q-fin-ST, q-fin.CP<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05101v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05101v2.pdf filename=2404.05101v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces StockGPT, an autoregressive ``number&rsquo;&rsquo; model trained and tested on 70 million daily U.S. stock returns over nearly 100 years. Treating each return series as a sequence of tokens, StockGPT automatically learns the hidden patterns predictive of future returns via its attention mechanism. On a held-out test sample from 2001 to 2023, a daily rebalanced long-short portfolio formed from StockGPT predictions earns an annual return of 119% with a Sharpe ratio of 6.5. The StockGPT-based portfolio completely spans momentum and long-/short-term reversals, eliminating the need for manually crafted price-based strategies, and also encompasses most leading stock market factors. This highlights the immense promise of <b>generative</b> <b>AI</b> in surpassing human in making complex financial investment decisions.</p></p class="citation"></blockquote><h2 id=econem-1>econ.EM (1)</h2><h3 id=11--145152-caviar-categorical-variable-embeddings-for-accurate-and-robust-inference-anirban-mukherjee-et-al-2024>(1/1 | 145/152) CAVIAR: Categorical-Variable Embeddings for Accurate and Robust Inference (Anirban Mukherjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anirban Mukherjee, Hannah Hanwen Chang. (2024)<br><strong>CAVIAR: Categorical-Variable Embeddings for Accurate and Robust Inference</strong><br><button class=copy-to-clipboard title="CAVIAR: Categorical-Variable Embeddings for Accurate and Robust Inference" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.EM<br>Categories: cs-LG, econ-EM, econ.EM<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04979v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04979v2.pdf filename=2404.04979v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social science research often hinges on the relationship between categorical variables and outcomes. We introduce CAVIAR, a novel method for embedding categorical variables that assume values in a high-dimensional ambient space but are sampled from an underlying manifold. Our theoretical and numerical analyses outline challenges posed by such categorical variables in causal inference. Specifically, dynamically varying and sparse levels can lead to violations of the Donsker conditions and a failure of the estimation functionals to converge to a tight <b>Gaussian</b> <b>process.</b> Traditional approaches, including the exclusion of rare categorical levels and principled variable selection models like LASSO, fall short. CAVIAR embeds the data into a lower-dimensional global coordinate system. The mapping can be derived from both structured and unstructured data, and ensures stable and robust estimates through dimensionality reduction. In a dataset of direct-to-consumer apparel sales, we illustrate how high-dimensional categorical variables, such as zip codes, can be succinctly represented, facilitating inference and analysis.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--146152-the-impact-of-virtual-laboratories-on-active-learning-and-engagement-in-cybersecurity-distance-education-victor-r-kebande-2024>(1/1 | 146/152) The Impact of Virtual Laboratories on Active Learning and Engagement in Cybersecurity Distance Education (Victor R. Kebande, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor R. Kebande. (2024)<br><strong>The Impact of Virtual Laboratories on Active Learning and Engagement in Cybersecurity Distance Education</strong><br><button class=copy-to-clipboard title="The Impact of Virtual Laboratories on Active Learning and Engagement in Cybersecurity Distance Education" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04952v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04952v1.pdf filename=2404.04952v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Virtual Laboratories (V Labs) have in the recent past become part and parcel of remote teaching in practical hands-on approaches, particularly in Cybersecurity distance courses. Their potential is meant to assist learners with hands-on practical laboratory exercises irrespective of geographical location. Nevertheless, adopting V Labs in didactic approaches in higher education has seen both merits and demerits. Based on this premise, this study investigates the impact of V Labs on <b>Active</b> <b>Learning</b> (AL) and engagement in cybersecurity distance education. A survey with a limited number of learners and educators who have had an experience with cybersecurity distance courses that leveraged V Labs in their practical Lab assignment, was conducted at Blekinge Tekniska H"ogskola, Sweden, to assess the impact of V Labs on AL and engagement in Cybersecurity Distance Education. 29% and 73% of the learners and educators, respectively responded to the survey administered remotely and with good internal consistency of questionnaires based on the Cronbalch Alpha; the results showed that learners and educators had a positive perception of using V Labs to enhance AL in cybersecurity distance education. The key concentration of the study was on AL and engagement and problem-solving abilities when V Labs are used. Both the learners and educators found the V Labs to be engaging, interactive, and effective in improving their understanding of cybersecurity concepts.</p></p class="citation"></blockquote><h2 id=eessas-1>eess.AS (1)</h2><h3 id=11--147152-gull-a-generative-multifunctional-audio-codec-yi-luo-et-al-2024>(1/1 | 147/152) Gull: A Generative Multifunctional Audio Codec (Yi Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Luo, Jianwei Yu, Hangting Chen, Rongzhi Gu, Chao Weng. (2024)<br><strong>Gull: A Generative Multifunctional Audio Codec</strong><br><button class=copy-to-clipboard title="Gull: A Generative Multifunctional Audio Codec" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-AI, cs-LG, cs-SD, eess-AS, eess-SP, eess.AS<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04947v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04947v1.pdf filename=2404.04947v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Gull, a generative multifunctional audio codec. Gull is a general purpose neural audio compression and decompression model which can be applied to a wide range of tasks and applications such as real-time communication, audio super-resolution, and codec language models. The key components of Gull include (1) universal-sample-rate modeling via subband modeling schemes motivated by recent progress in audio source separation, (2) gain-shape representations motivated by traditional audio codecs, (3) improved residual vector <b>quantization</b> modules for simpler training, (4) elastic decoder network that enables user-defined model size and complexity during inference time, (5) built-in ability for audio super-resolution without the increase of bitrate. We compare Gull with existing traditional and neural audio codecs and show that Gull is able to achieve on par or better performance across various sample rates, bitrates and model complexities in both subjective and objective evaluation metrics.</p></p class="citation"></blockquote><h2 id=mathds-1>math.DS (1)</h2><h3 id=11--148152-elementary-fractal-geometry-5-weak-separation-is-strong-separation-christoph-bandt-et-al-2024>(1/1 | 148/152) Elementary fractal geometry. 5. Weak separation is strong separation (Christoph Bandt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christoph Bandt, Michael F. Barnsley. (2024)<br><strong>Elementary fractal geometry. 5. Weak separation is strong separation</strong><br><button class=copy-to-clipboard title="Elementary fractal geometry. 5. Weak separation is strong separation" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.DS<br>Categories: 28A80, 11A63, 37B10, 54B15, 68Q45, cs-CL, math-DS, math.DS<br>Keyword Score: 8<br>Keywords: Graph, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04892v1.pdf filename=2404.04892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For self-similar sets, there are two important separation properties: the open set condition and the weak separation condition introduced by Zerner, which may be replaced by the formally stronger finite type property of Ngai and Wang. We show that any finite type self-similar set can be represented as a <b>graph-directed</b> construction obeying the open set condition. The proof is based on a combinatorial algorithm which performed well in computer experiments.</p></p class="citation"></blockquote><h2 id=physicsflu-dyn-1>physics.flu-dyn (1)</h2><h3 id=11--149152-exact-and-approximate-solutions-for-magnetohydrodynamic-flow-control-in-hele-shaw-cells-kyle-mckee-2024>(1/1 | 149/152) Exact and Approximate Solutions for Magnetohydrodynamic Flow Control in Hele-Shaw Cells (Kyle McKee, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyle McKee. (2024)<br><strong>Exact and Approximate Solutions for Magnetohydrodynamic Flow Control in Hele-Shaw Cells</strong><br><button class=copy-to-clipboard title="Exact and Approximate Solutions for Magnetohydrodynamic Flow Control in Hele-Shaw Cells" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-NA, math-MP, math-NA, math-ph, physics-app-ph, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04840v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04840v1.pdf filename=2404.04840v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Consider the motion of a thin layer of electrically conducting fluid, between two closely spaced parallel plates, in a classical Hele-Shaw <b>geometry.</b> Furthermore, let the system be immersed in a uniform external magnetic field (normal to the plates) and let electrical current be driven between conducting probes immersed in the fluid layer. In the present paper, we analyse the ensuing fluid flow at low Hartmann numbers. We first elucidate the mechanism of flow generation both physically and mathematically. We proceed by presenting mathematical solutions for a class of canonical multiply-connected geometries, in terms of the prime function developed by Crowdy (2020). Notably, those solutions can be written explicitly as series, and are thus exact, in doubly-connected geometries. Note that in higher connectivities, the prime function must be evaluated numerically. We then demonstrate how recently developed fast numerical methods may be applied to accurately determine the flow-field in arbitrary geometries when exact solutions are inaccessible.</p></p class="citation"></blockquote><h2 id=csne-1>cs.NE (1)</h2><h3 id=11--150152-dynamic-quality-diversity-search-roberto-gallotta-et-al-2024>(1/1 | 150/152) Dynamic Quality-Diversity Search (Roberto Gallotta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roberto Gallotta, Antonios Liapis, Georgios N. Yannakakis. (2024)<br><strong>Dynamic Quality-Diversity Search</strong><br><button class=copy-to-clipboard title="Dynamic Quality-Diversity Search" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05769v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05769v1.pdf filename=2404.05769v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evolutionary search via the quality-diversity (QD) paradigm can discover highly performing solutions in different behavioural niches, showing considerable potential in complex real-world scenarios such as evolutionary robotics. Yet most QD methods only tackle static tasks that are fixed over time, which is rarely the case in the real world. Unlike noisy environments, where the fitness of an individual changes slightly at every evaluation, dynamic environments simulate tasks where external factors at unknown and irregular intervals alter the performance of the individual with a severity that is unknown a priori. Literature on optimisation in dynamic environments is extensive, yet such environments have not been explored in the context of QD search. This paper introduces a novel and generalisable Dynamic QD methodology that aims to keep the archive of past solutions updated in the case of environment changes. Secondly, we present a novel characterisation of dynamic environments that can be easily applied to well-known <b>benchmarks,</b> with minor interventions to move them from a static task to a dynamic one. Our Dynamic QD intervention is applied on MAP-Elites and CMA-ME, two powerful QD algorithms, and we test the dynamic variants on different dynamic tasks.</p></p class="citation"></blockquote><h2 id=csds-1>cs.DS (1)</h2><h3 id=11--151152-chromatic-number-in-19999n-time-fast-deterministic-set-partitioning-under-the-asymptotic-rank-conjecture-andreas-björklund-et-al-2024>(1/1 | 151/152) Chromatic number in $1.9999^n$ time? Fast deterministic set partitioning under the asymptotic rank conjecture (Andreas Björklund et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andreas Björklund, Radu Curticapean, Thore Husfeldt, Petteri Kaski, Kevin Pratt. (2024)<br><strong>Chromatic number in $1.9999^n$ time? Fast deterministic set partitioning under the asymptotic rank conjecture</strong><br><button class=copy-to-clipboard title="Chromatic number in $1.9999^n$ time? Fast deterministic set partitioning under the asymptotic rank conjecture" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04987v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04987v1.pdf filename=2404.04987v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we further explore the recently discovered connection by Bj"{o}rklund and Kaski [STOC 2024] and Pratt [STOC 2024] between the asymptotic rank conjecture of Strassen [Progr. Math. 1994] and the three-way partitioning problem. We show that under the asymptotic rank conjecture, the chromatic number of an $n$-vertex <b>graph</b> can be computed deterministically in $O(1.99982^n)$ time, thus giving a conditional answer to a question of Zamir [ICALP 2021], and questioning the optimality of the $2^n\operatorname{poly}(n)$ time algorithm for chromatic number by Bj"{o}rklund, Husfeldt, and Koivisto [SICOMP 2009]. Our technique is a combination of earlier algorithms for detecting $k$-colorings for small $k$ and enumerating $k$-colorable subgraphs, with an extension and derandomisation of Pratt&rsquo;s tensor-based algorithm for balanced three-way partitioning to the unbalanced case.</p></p class="citation"></blockquote><h2 id=econth-1>econ.TH (1)</h2><h3 id=11--152152-a-many-to-one-job-market-more-about-the-core-and-the-competitive-salaries-ata-atay-et-al-2024>(1/1 | 152/152) A many-to-one job market: more about the core and the competitive salaries (Ata Atay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ata Atay, Marina Núñez, Tamás Solymosi. (2024)<br><strong>A many-to-one job market: more about the core and the competitive salaries</strong><br><button class=copy-to-clipboard title="A many-to-one job market: more about the core and the competitive salaries" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.TH<br>Categories: 05C57, 91A12, 91A43, cs-GT, econ-TH, econ.TH<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04847v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04847v1.pdf filename=2404.04847v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies many-to-one assignment markets, or matching markets with wages. Although it is well-known that the core of this model is non-empty, the structure of the core has not been fully investigated. To the known dissimilarities with the one-to-one assignment game, we add that the bargaining set does not coincide with the core and the kernel may not be included in the core. Besides, not all extreme core allocations can be obtained by means of a lexicographic maximization or a lexicographic minimization procedure, as it is the case in the one-to-one assignment game. The maximum and minimum competitive salaries are characterized in two ways: axiomatically and by means of easily verifiable properties of an associated directed <b>graph.</b> Regarding the remaining extreme core allocations of the many-to-one assignment game, we propose a lexicographic procedure that, for each order on the set of workers, sequentially maximizes or minimizes each worker&rsquo;s competitive salary. This procedure provides all extreme vectors of competitive salaries, that is all extreme core allocations.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240408000000/ title="arXiv @ 2024.04.08" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.04.08</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240410000000/ title="arXiv @ 2024.04.10" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.04.10</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-18>cs.CL (18)</a><ul><li><a href=#118--1152-low-resource-machine-translation-through-retrieval-augmented-llm-prompting-a-study-on-the-mambai-language-raphaël-merx-et-al-2024>(1/18 | 1/152) Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language (Raphaël Merx et al., 2024)</a></li><li><a href=#218--2152-how-much-reliable-is-chatgpts-prediction-on-information-extraction-under-input-perturbations-ishani-mondal-et-al-2024>(2/18 | 2/152) How much reliable is ChatGPT&rsquo;s prediction on Information Extraction under Input Perturbations? (Ishani Mondal et al., 2024)</a></li><li><a href=#318--3152-prompting-large-language-models-for-zero-shot-essay-scoring-via-multi-trait-specialization-sanwoo-lee-et-al-2024>(3/18 | 3/152) Prompting Large Language Models for Zero-shot Essay Scoring via Multi-trait Specialization (Sanwoo Lee et al., 2024)</a></li><li><a href=#418--4152-fractal-fine-grained-scoring-from-aggregate-text-labels-yukti-makhija-et-al-2024>(4/18 | 4/152) FRACTAL: Fine-Grained Scoring from Aggregate Text Labels (Yukti Makhija et al., 2024)</a></li><li><a href=#518--5152-semeval-2024-task-2-safe-biomedical-natural-language-inference-for-clinical-trials-mael-jullien-et-al-2024>(5/18 | 5/152) SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials (Mael Jullien et al., 2024)</a></li><li><a href=#618--6152-f-malloc-feed-forward-memory-allocation-for-continual-learning-in-neural-machine-translation-junhong-wu-et-al-2024>(6/18 | 6/152) F-MALLOC: Feed-forward Memory Allocation for Continual Learning in Neural Machine Translation (Junhong Wu et al., 2024)</a></li><li><a href=#718--7152-advancing-geometric-problem-solving-a-comprehensive-benchmark-for-multimodal-model-evaluation-kai-sun-et-al-2024>(7/18 | 7/152) Advancing Geometric Problem Solving: A Comprehensive Benchmark for Multimodal Model Evaluation (Kai Sun et al., 2024)</a></li><li><a href=#818--8152-mlake-multilingual-knowledge-editing-benchmark-for-large-language-models-zihao-wei-et-al-2024>(8/18 | 8/152) MLaKE: Multilingual Knowledge Editing Benchmark for Large Language Models (Zihao Wei et al., 2024)</a></li><li><a href=#918--9152-lucky-52-how-many-languages-are-needed-to-instruction-fine-tune-large-language-models-shaoxiong-ji-et-al-2024>(9/18 | 9/152) Lucky 52: How Many Languages Are Needed to Instruction Fine-Tune Large Language Models? (Shaoxiong Ji et al., 2024)</a></li><li><a href=#1018--10152-towards-understanding-the-influence-of-reward-margin-on-preference-model-performance-bowen-qin-et-al-2024>(10/18 | 10/152) Towards Understanding the Influence of Reward Margin on Preference Model Performance (Bowen Qin et al., 2024)</a></li><li><a href=#1118--11152-radial-networks-dynamic-layer-routing-for-high-performance-large-language-models-jordan-dotzel-et-al-2024>(11/18 | 11/152) Radial Networks: Dynamic Layer Routing for High-Performance Large Language Models (Jordan Dotzel et al., 2024)</a></li><li><a href=#1218--12152-data-bias-according-to-bipol-men-are-naturally-right-and-it-is-the-role-of-women-to-follow-their-lead-irene-pagliai-et-al-2024>(12/18 | 12/152) Data Bias According to Bipol: Men are Naturally Right and It is the Role of Women to Follow Their Lead (Irene Pagliai et al., 2024)</a></li><li><a href=#1318--13152-seer-moe-sparse-expert-efficiency-through-regularization-for-mixture-of-experts-alexandre-muzio-et-al-2024>(13/18 | 13/152) SEER-MoE: Sparse Expert Efficiency through Regularization for Mixture-of-Experts (Alexandre Muzio et al., 2024)</a></li><li><a href=#1418--14152-a-two-dimensional-feature-engineering-method-for-relation-extraction-hao-wang-et-al-2024>(14/18 | 14/152) A Two Dimensional Feature Engineering Method for Relation Extraction (Hao Wang et al., 2024)</a></li><li><a href=#1518--15152-silversight-a-multi-task-chinese-financial-large-language-model-based-on-adaptive-semantic-space-learning-yuhang-zhou-et-al-2024>(15/18 | 15/152) SilverSight: A Multi-Task Chinese Financial Large Language Model Based on Adaptive Semantic Space Learning (Yuhang Zhou et al., 2024)</a></li><li><a href=#1618--16152-multilingual-large-language-model-a-survey-of-resources-taxonomy-and-frontiers-libo-qin-et-al-2024>(16/18 | 16/152) Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers (Libo Qin et al., 2024)</a></li><li><a href=#1718--17152-slpl-shroom-at-semeval2024-task-06-a-comprehensive-study-on-models-ability-to-detect-hallucination-pouya-fallah-et-al-2024>(17/18 | 17/152) SLPL SHROOM at SemEval2024 Task 06: A comprehensive study on models ability to detect hallucination (Pouya Fallah et al., 2024)</a></li><li><a href=#1818--18152-generating-uncontextualized-and-contextualized-questions-for-document-level-event-argument-extraction-md-nayem-uddin-et-al-2024>(18/18 | 18/152) Generating Uncontextualized and Contextualized Questions for Document-Level Event Argument Extraction (Md Nayem Uddin et al., 2024)</a></li></ul></li><li><a href=#cscv-47>cs.CV (47)</a><ul><li><a href=#147--19152-vmambamorph-a-visual-mamba-based-framework-with-cross-scan-module-for-deformable-3d-image-registration-ziyang-wang-et-al-2024>(1/47 | 19/152) VMambaMorph: a Visual Mamba-based Framework with Cross-Scan Module for Deformable 3D Image Registration (Ziyang Wang et al., 2024)</a></li><li><a href=#247--20152-gvt-a-graph-based-vision-transformer-with-talking-heads-utilizing-sparsity-trained-from-scratch-on-small-datasets-dongjing-shan-et-al-2024>(2/47 | 20/152) GvT: A Graph-based Vision Transformer with Talking-Heads Utilizing Sparsity, Trained from Scratch on Small Datasets (Dongjing Shan et al., 2024)</a></li><li><a href=#347--21152-pairaug-what-can-augmented-image-text-pairs-do-for-radiology-yutong-xie-et-al-2024>(3/47 | 21/152) PairAug: What Can Augmented Image-Text Pairs Do for Radiology? (Yutong Xie et al., 2024)</a></li><li><a href=#447--22152-bootstrapping-chest-ct-image-understanding-by-distilling-knowledge-from-x-ray-expert-models-weiwei-cao-et-al-2024>(4/47 | 22/152) Bootstrapping Chest CT Image Understanding by Distilling Knowledge from X-ray Expert Models (Weiwei Cao et al., 2024)</a></li><li><a href=#547--23152-genearl-a-training-free-generative-framework-for-multimodal-event-argument-role-labeling-hritik-bansal-et-al-2024>(5/47 | 23/152) GenEARL: A Training-Free Generative Framework for Multimodal Event Argument Role Labeling (Hritik Bansal et al., 2024)</a></li><li><a href=#647--24152-fpl-filtered-pseudo-label-based-unsupervised-cross-modality-adaptation-for-3d-medical-image-segmentation-jianghao-wu-et-al-2024>(6/47 | 24/152) FPL+: Filtered Pseudo Label-based Unsupervised Cross-Modality Adaptation for 3D Medical Image Segmentation (Jianghao Wu et al., 2024)</a></li><li><a href=#747--25152-dinobloom-a-foundation-model-for-generalizable-cell-embeddings-in-hematology-valentin-koch-et-al-2024>(7/47 | 25/152) DinoBloom: A Foundation Model for Generalizable Cell Embeddings in Hematology (Valentin Koch et al., 2024)</a></li><li><a href=#847--26152-monotakd-teaching-assistant-knowledge-distillation-for-monocular-3d-object-detection-hou-i-liu-et-al-2024>(8/47 | 26/152) MonoTAKD: Teaching Assistant Knowledge Distillation for Monocular 3D Object Detection (Hou-I Liu et al., 2024)</a></li><li><a href=#947--27152-facial-affective-behavior-analysis-with-instruction-tuning-yifan-li-et-al-2024>(9/47 | 27/152) Facial Affective Behavior Analysis with Instruction Tuning (Yifan Li et al., 2024)</a></li><li><a href=#1047--28152-weakly-supervised-deep-hyperspherical-quantization-for-image-retrieval-jinpeng-wang-et-al-2024>(10/47 | 28/152) Weakly Supervised Deep Hyperspherical Quantization for Image Retrieval (Jinpeng Wang et al., 2024)</a></li><li><a href=#1147--29152-reconstructing-retinal-visual-images-from-3t-fmri-data-enhanced-by-unsupervised-learning-yujian-xiong-et-al-2024>(11/47 | 29/152) Reconstructing Retinal Visual Images from 3T fMRI Data Enhanced by Unsupervised Learning (Yujian Xiong et al., 2024)</a></li><li><a href=#1247--30152-high-discriminative-attribute-feature-learning-for-generalized-zero-shot-learning-yu-lei-et-al-2024>(12/47 | 30/152) High-Discriminative Attribute Feature Learning for Generalized Zero-Shot Learning (Yu Lei et al., 2024)</a></li><li><a href=#1347--31152-few-shot-object-detection-research-advances-and-challenges-zhimeng-xin-et-al-2024>(13/47 | 31/152) Few-Shot Object Detection: Research Advances and Challenges (Zhimeng Xin et al., 2024)</a></li><li><a href=#1447--32152-havtr-improving-video-text-retrieval-through-augmentation-using-large-foundation-models-yimu-wang-et-al-2024>(14/47 | 32/152) HaVTR: Improving Video-Text Retrieval Through Augmentation Using Large Foundation Models (Yimu Wang et al., 2024)</a></li><li><a href=#1547--33152-scalable-and-efficient-hierarchical-visual-topological-mapping-saravanabalagi-ramachandran-et-al-2024>(15/47 | 33/152) Scalable and Efficient Hierarchical Visual Topological Mapping (Saravanabalagi Ramachandran et al., 2024)</a></li><li><a href=#1647--34152-hyperbolic-learning-with-synthetic-captions-for-open-world-detection-fanjie-kong-et-al-2024>(16/47 | 34/152) Hyperbolic Learning with Synthetic Captions for Open-World Detection (Fanjie Kong et al., 2024)</a></li><li><a href=#1747--35152-mixture-of-low-rank-experts-for-transferable-ai-generated-image-detection-zihan-liu-et-al-2024>(17/47 | 35/152) Mixture of Low-rank Experts for Transferable AI-Generated Image Detection (Zihan Liu et al., 2024)</a></li><li><a href=#1847--36152-animatezoo-zero-shot-video-generation-of-cross-species-animation-via-subject-alignment-yuanfeng-xu-et-al-2024>(18/47 | 36/152) AnimateZoo: Zero-shot Video Generation of Cross-Species Animation via Subject Alignment (Yuanfeng Xu et al., 2024)</a></li><li><a href=#1947--37152-anomaly-detection-in-electrocardiograms-advancing-clinical-diagnosis-through-self-supervised-learning-aofan-jiang-et-al-2024>(19/47 | 37/152) Anomaly Detection in Electrocardiograms: Advancing Clinical Diagnosis Through Self-Supervised Learning (Aofan Jiang et al., 2024)</a></li><li><a href=#2047--38152-rethinking-diffusion-model-for-multi-contrast-mri-super-resolution-guangyuan-li-et-al-2024>(20/47 | 38/152) Rethinking Diffusion Model for Multi-Contrast MRI Super-Resolution (Guangyuan Li et al., 2024)</a></li><li><a href=#2147--39152-fgaif-aligning-large-vision-language-models-with-fine-grained-ai-feedback-liqiang-jing-et-al-2024>(21/47 | 39/152) FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback (Liqiang Jing et al., 2024)</a></li><li><a href=#2247--40152-x-vars-introducing-explainability-in-football-refereeing-with-multi-modal-large-language-model-jan-held-et-al-2024>(22/47 | 40/152) X-VARS: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Model (Jan Held et al., 2024)</a></li><li><a href=#2347--41152-airshot-efficient-few-shot-detection-for-autonomous-exploration-zihan-wang-et-al-2024>(23/47 | 41/152) AirShot: Efficient Few-Shot Detection for Autonomous Exploration (Zihan Wang et al., 2024)</a></li><li><a href=#2447--42152-camera-based-remote-physiology-sensing-for-hundreds-of-subjects-across-skin-tones-jiankai-tang-et-al-2024>(24/47 | 42/152) Camera-Based Remote Physiology Sensing for Hundreds of Subjects Across Skin Tones (Jiankai Tang et al., 2024)</a></li><li><a href=#2547--43152-msmsfnet-a-multi-stream-and-multi-scale-fusion-net-for-edge-detection-chenguang-liu-et-al-2024>(25/47 | 43/152) Msmsfnet: a multi-stream and multi-scale fusion net for edge detection (Chenguang Liu et al., 2024)</a></li><li><a href=#2647--44152-strictly-id-preserved-and-controllable-accessory-advertising-image-generation-youze-xue-et-al-2024>(26/47 | 44/152) Strictly-ID-Preserved and Controllable Accessory Advertising Image Generation (Youze Xue et al., 2024)</a></li><li><a href=#2747--45152-light-the-night-a-multi-condition-diffusion-framework-for-unpaired-low-light-enhancement-in-autonomous-driving-jinlong-li-et-al-2024>(27/47 | 45/152) Light the Night: A Multi-Condition Diffusion Framework for Unpaired Low-Light Enhancement in Autonomous Driving (Jinlong Li et al., 2024)</a></li><li><a href=#2847--46152-a-unified-diffusion-framework-for-scene-aware-human-motion-estimation-from-sparse-signals-jiangnan-tang-et-al-2024>(28/47 | 46/152) A Unified Diffusion Framework for Scene-aware Human Motion Estimation from Sparse Signals (Jiangnan Tang et al., 2024)</a></li><li><a href=#2947--47152-a-clinical-oriented-multi-level-contrastive-learning-method-for-disease-diagnosis-in-low-quality-medical-images-qingshan-hou-et-al-2024>(29/47 | 47/152) A Clinical-oriented Multi-level Contrastive Learning Method for Disease Diagnosis in Low-quality Medical Images (Qingshan Hou et al., 2024)</a></li><li><a href=#3047--48152-dual-scale-transformer-for-large-scale-single-pixel-imaging-gang-qu-et-al-2024>(30/47 | 48/152) Dual-Scale Transformer for Large-Scale Single-Pixel Imaging (Gang Qu et al., 2024)</a></li><li><a href=#3147--49152-platesegfl-a-privacy-preserving-license-plate-detection-using-federated-segmentation-learning-md-shahriar-rahman-anuvab-et-al-2024>(31/47 | 49/152) PlateSegFL: A Privacy-Preserving License Plate Detection Using Federated Segmentation Learning (Md. Shahriar Rahman Anuvab et al., 2024)</a></li><li><a href=#3247--50152-magictime-time-lapse-video-generation-models-as-metamorphic-simulators-shenghai-yuan-et-al-2024>(32/47 | 50/152) MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators (Shenghai Yuan et al., 2024)</a></li><li><a href=#3347--51152-fantastic-animals-and-where-to-find-them-segment-any-marine-animal-with-dual-sam-pingping-zhang-et-al-2024>(33/47 | 51/152) Fantastic Animals and Where to Find Them: Segment Any Marine Animal with Dual SAM (Pingping Zhang et al., 2024)</a></li><li><a href=#3447--52152-dynamic-distinction-learning-adaptive-pseudo-anomalies-for-video-anomaly-detection-demetris-lappas-et-al-2024>(34/47 | 52/152) Dynamic Distinction Learning: Adaptive Pseudo Anomalies for Video Anomaly Detection (Demetris Lappas et al., 2024)</a></li><li><a href=#3547--53152-gaussian-shading-provable-performance-lossless-image-watermarking-for-diffusion-models-zijin-yang-et-al-2024>(35/47 | 53/152) Gaussian Shading: Provable Performance-Lossless Image Watermarking for Diffusion Models (Zijin Yang et al., 2024)</a></li><li><a href=#3647--54152-unimd-towards-unifying-moment-retrieval-and-temporal-action-detection-yingsen-zeng-et-al-2024>(36/47 | 54/152) UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection (Yingsen Zeng et al., 2024)</a></li><li><a href=#3747--55152-codecnerf-toward-fast-encoding-and-decoding-compact-and-high-quality-novel-view-synthesis-gyeongjin-kang-et-al-2024>(37/47 | 55/152) CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis (Gyeongjin Kang et al., 2024)</a></li><li><a href=#3847--56152-dual-camera-smooth-zoom-on-mobile-phones-renlong-wu-et-al-2024>(38/47 | 56/152) Dual-Camera Smooth Zoom on Mobile Phones (Renlong Wu et al., 2024)</a></li><li><a href=#3947--57152-dl-ewf-deep-learning-empowering-womens-fashion-with-grounded-segment-anything-segmentation-for-body-shape-classification-fatemeh-asghari-et-al-2024>(39/47 | 57/152) DL-EWF: Deep Learning Empowering Women&rsquo;s Fashion with Grounded-Segment-Anything Segmentation for Body Shape Classification (Fatemeh Asghari et al., 2024)</a></li><li><a href=#4047--58152-byteedit-boost-comply-and-accelerate-generative-image-editing-yuxi-ren-et-al-2024>(40/47 | 58/152) ByteEdit: Boost, Comply and Accelerate Generative Image Editing (Yuxi Ren et al., 2024)</a></li><li><a href=#4147--59152-shoemodel-learning-to-wear-on-the-user-specified-shoes-via-diffusion-model-binghui-chen-et-al-2024>(41/47 | 59/152) ShoeModel: Learning to Wear on the User-specified Shoes via Diffusion Model (Binghui Chen et al., 2024)</a></li><li><a href=#4247--60152-3d-building-reconstruction-from-monocular-remote-sensing-images-with-multi-level-supervisions-weijia-li-et-al-2024>(42/47 | 60/152) 3D Building Reconstruction from Monocular Remote Sensing Images with Multi-level Supervisions (Weijia Li et al., 2024)</a></li><li><a href=#4347--61152-joint-reconstruction-of-3d-human-and-object-via-contact-based-refinement-transformer-hyeongjin-nam-et-al-2024>(43/47 | 61/152) Joint Reconstruction of 3D Human and Object via Contact-Based Refinement Transformer (Hyeongjin Nam et al., 2024)</a></li><li><a href=#4447--62152-hilo-detailed-and-robust-3d-clothed-human-reconstruction-with-high-and-low-frequency-information-of-parametric-models-yifan-yang-et-al-2024>(44/47 | 62/152) HiLo: Detailed and Robust 3D Clothed Human Reconstruction with High-and Low-Frequency Information of Parametric Models (Yifan Yang et al., 2024)</a></li><li><a href=#4547--63152-logo-a-long-form-video-dataset-for-group-action-quality-assessment-shiyi-zhang-et-al-2024>(45/47 | 63/152) LOGO: A Long-Form Video Dataset for Group Action Quality Assessment (Shiyi Zhang et al., 2024)</a></li><li><a href=#4647--64152-efficient-learnable-collaborative-attention-for-single-image-super-resolution-yigang-zhao-chaowei-zheng-et-al-2024>(46/47 | 64/152) Efficient Learnable Collaborative Attention for Single Image Super-Resolution (Yigang Zhao Chaowei Zheng et al., 2024)</a></li><li><a href=#4747--65152-gauu-scene-v2-expanse-lidar-image-dataset-shows-unreliable-geometric-reconstruction-using-gaussian-splatting-and-nerf-butian-xiong-et-al-2024>(47/47 | 65/152) GauU-Scene V2: Expanse Lidar Image Dataset Shows Unreliable Geometric Reconstruction Using Gaussian Splatting and NeRF (Butian Xiong et al., 2024)</a></li></ul></li><li><a href=#cslg-26>cs.LG (26)</a><ul><li><a href=#126--66152-contextual-chart-generation-for-cyber-deception-david-d-nguyen-et-al-2024>(1/26 | 66/152) Contextual Chart Generation for Cyber Deception (David D. Nguyen et al., 2024)</a></li><li><a href=#226--67152-initial-exploration-of-zero-shot-privacy-utility-tradeoffs-in-tabular-data-using-gpt-4-bishwas-mandal-et-al-2024>(2/26 | 67/152) Initial Exploration of Zero-Shot Privacy Utility Tradeoffs in Tabular Data Using GPT-4 (Bishwas Mandal et al., 2024)</a></li><li><a href=#326--68152-timegpt-in-load-forecasting-a-large-time-series-model-perspective-wenlong-liao-et-al-2024>(3/26 | 68/152) TimeGPT in Load Forecasting: A Large Time Series Model Perspective (Wenlong Liao et al., 2024)</a></li><li><a href=#426--69152-adapting-llms-for-efficient-context-processing-through-soft-prompt-compression-cangqing-wang-et-al-2024>(4/26 | 69/152) Adapting LLMs for Efficient Context Processing through Soft Prompt Compression (Cangqing Wang et al., 2024)</a></li><li><a href=#526--70152-timecsl-unsupervised-contrastive-learning-of-general-shapelets-for-explorable-time-series-analysis-zhiyu-liang-et-al-2024>(5/26 | 70/152) TimeCSL: Unsupervised Contrastive Learning of General Shapelets for Explorable Time Series Analysis (Zhiyu Liang et al., 2024)</a></li><li><a href=#626--71152-graph-neural-networks-for-binary-programming-moshe-eliasof-et-al-2024>(6/26 | 71/152) Graph Neural Networks for Binary Programming (Moshe Eliasof et al., 2024)</a></li><li><a href=#726--72152-temporal-generalization-estimation-in-evolving-graphs-bin-lu-et-al-2024>(7/26 | 72/152) Temporal Generalization Estimation in Evolving Graphs (Bin Lu et al., 2024)</a></li><li><a href=#826--73152-active-test-time-adaptation-theoretical-analyses-and-an-algorithm-shurui-gui-et-al-2024>(8/26 | 73/152) Active Test-Time Adaptation: Theoretical Analyses and An Algorithm (Shurui Gui et al., 2024)</a></li><li><a href=#926--74152-on-the-learnability-of-out-of-distribution-detection-zhen-fang-et-al-2024>(9/26 | 74/152) On the Learnability of Out-of-distribution Detection (Zhen Fang et al., 2024)</a></li><li><a href=#1026--75152-mixup-domain-adaptations-for-dynamic-remaining-useful-life-predictions-muhammad-tanzil-furqon-et-al-2024>(10/26 | 75/152) Mixup Domain Adaptations for Dynamic Remaining Useful Life Predictions (Muhammad Tanzil Furqon et al., 2024)</a></li><li><a href=#1126--76152-inference-time-rule-eraser-distilling-and-removing-bias-rules-to-mitigate-bias-in-deployed-models-yi-zhang-et-al-2024>(11/26 | 76/152) Inference-Time Rule Eraser: Distilling and Removing Bias Rules to Mitigate Bias in Deployed Models (Yi Zhang et al., 2024)</a></li><li><a href=#1226--77152-data-stream-sampling-with-fuzzy-task-boundaries-and-noisy-labels-yu-hsi-chen-2024>(12/26 | 77/152) Data Stream Sampling with Fuzzy Task Boundaries and Noisy Labels (Yu-Hsi Chen, 2024)</a></li><li><a href=#1326--78152-squeezeattention-2d-management-of-kv-cache-in-llm-inference-via-layer-wise-optimal-budget-zihao-wang-et-al-2024>(13/26 | 78/152) SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget (Zihao Wang et al., 2024)</a></li><li><a href=#1426--79152-a-note-on-lora-vlad-fomenko-et-al-2024>(14/26 | 79/152) A Note on LoRA (Vlad Fomenko et al., 2024)</a></li><li><a href=#1526--80152-a-robust-assessment-for-invariant-representations-wenlu-tang-et-al-2024>(15/26 | 80/152) A robust assessment for invariant representations (Wenlu Tang et al., 2024)</a></li><li><a href=#1626--81152-percentile-criterion-optimization-in-offline-reinforcement-learning-elita-a-lobo-et-al-2024>(16/26 | 81/152) Percentile Criterion Optimization in Offline Reinforcement Learning (Elita A. Lobo et al., 2024)</a></li><li><a href=#1726--82152-regularized-conditional-diffusion-model-for-multi-task-preference-alignment-xudong-yu-et-al-2024>(17/26 | 82/152) Regularized Conditional Diffusion Model for Multi-Task Preference Alignment (Xudong Yu et al., 2024)</a></li><li><a href=#1826--83152-gradient-based-design-of-computational-granular-crystals-atoosa-parsa-et-al-2024>(18/26 | 83/152) Gradient-based Design of Computational Granular Crystals (Atoosa Parsa et al., 2024)</a></li><li><a href=#1926--84152-chiplet-placement-order-exploration-based-on-learning-to-rank-with-graph-representation-zhihui-deng-et-al-2024>(19/26 | 84/152) Chiplet Placement Order Exploration Based on Learning to Rank with Graph Representation (Zhihui Deng et al., 2024)</a></li><li><a href=#2026--85152-fuzzy-k-means-clustering-without-cluster-centroids-han-lu-et-al-2024>(20/26 | 85/152) Fuzzy K-Means Clustering without Cluster Centroids (Han Lu et al., 2024)</a></li><li><a href=#2126--86152-test-time-training-for-depression-detection-sri-harsha-dumpala-et-al-2024>(21/26 | 86/152) Test-Time Training for Depression Detection (Sri Harsha Dumpala et al., 2024)</a></li><li><a href=#2226--87152-shortcut-connected-expert-parallelism-for-accelerating-mixture-of-experts-weilin-cai-et-al-2024>(22/26 | 87/152) Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts (Weilin Cai et al., 2024)</a></li><li><a href=#2326--88152-signal-noise-separation-using-unsupervised-reservoir-computing-jaesung-choi-et-al-2024>(23/26 | 88/152) Signal-noise separation using unsupervised reservoir computing (Jaesung Choi et al., 2024)</a></li><li><a href=#2426--89152-skill-transfer-and-discovery-for-sim-to-real-learning-a-representation-based-viewpoint-haitong-ma-et-al-2024>(24/26 | 89/152) Skill Transfer and Discovery for Sim-to-Real Learning: A Representation-Based Viewpoint (Haitong Ma et al., 2024)</a></li><li><a href=#2526--90152-demystifying-lazy-training-of-neural-networks-from-a-macroscopic-viewpoint-yuqing-li-et-al-2024>(25/26 | 90/152) Demystifying Lazy Training of Neural Networks from a Macroscopic Viewpoint (Yuqing Li et al., 2024)</a></li><li><a href=#2626--91152-the-sample-complexity-of-gradient-descent-in-stochastic-convex-optimization-roi-livni-2024>(26/26 | 91/152) The Sample Complexity of Gradient Descent in Stochastic Convex Optimization (Roi Livni, 2024)</a></li></ul></li><li><a href=#q-bioto-1>q-bio.TO (1)</a><ul><li><a href=#11--92152-primary-liver-cancer-classification-from-routine-tumour-biopsy-using-weakly-supervised-deep-learning-aurélie-beaufrère-et-al-2024>(1/1 | 92/152) Primary liver cancer classification from routine tumour biopsy using weakly supervised deep learning (Aurélie Beaufrère et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--93152-clinical-trials-protocol-authoring-using-llms-morteza-maleki-2024>(1/1 | 93/152) Clinical Trials Protocol Authoring using LLMs (Morteza Maleki, 2024)</a></li></ul></li><li><a href=#cssd-1>cs.SD (1)</a><ul><li><a href=#11--94152-cross-domain-audio-deepfake-detection-dataset-and-analysis-yuang-li-et-al-2024>(1/1 | 94/152) Cross-Domain Audio Deepfake Detection: Dataset and Analysis (Yuang Li et al., 2024)</a></li></ul></li><li><a href=#eessiv-2>eess.IV (2)</a><ul><li><a href=#12--95152-lhu-net-a-light-hybrid-u-net-for-cost-efficient-high-performance-volumetric-medical-image-segmentation-yousef-sadegheih-et-al-2024>(1/2 | 95/152) LHU-Net: A Light Hybrid U-Net for Cost-Efficient, High-Performance Volumetric Medical Image Segmentation (Yousef Sadegheih et al., 2024)</a></li><li><a href=#22--96152-correcting-diffusion-based-perceptual-image-compression-with-privileged-end-to-end-decoder-yiyang-ma-et-al-2024>(2/2 | 96/152) Correcting Diffusion-Based Perceptual Image Compression with Privileged End-to-End Decoder (Yiyang Ma et al., 2024)</a></li></ul></li><li><a href=#csro-6>cs.RO (6)</a><ul><li><a href=#16--97152-robomp2-a-robotic-multimodal-perception-planning-framework-with-multimodal-large-language-models-qi-lv-et-al-2024>(1/6 | 97/152) RoboMP$^2$: A Robotic Multimodal Perception-Planning Framework with Multimodal Large Language Models (Qi Lv et al., 2024)</a></li><li><a href=#26--98152-prompting-multi-modal-tokens-to-enhance-end-to-end-autonomous-driving-imitation-learning-with-llms-yiqun-duan-et-al-2024>(2/6 | 98/152) Prompting Multi-Modal Tokens to Enhance End-to-End Autonomous Driving Imitation Learning with LLMs (Yiqun Duan et al., 2024)</a></li><li><a href=#36--99152-enquery-ensemble-policies-for-diverse-query-generation-in-preference-alignment-of-robot-navigation-jorge-de-heuvel-et-al-2024>(3/6 | 99/152) EnQuery: Ensemble Policies for Diverse Query-Generation in Preference Alignment of Robot Navigation (Jorge de Heuvel et al., 2024)</a></li><li><a href=#46--100152-efficient-reinforcement-learning-of-task-planners-for-robotic-palletization-through-iterative-action-masking-learning-zheng-wu-et-al-2024>(4/6 | 100/152) Efficient Reinforcement Learning of Task Planners for Robotic Palletization through Iterative Action Masking Learning (Zheng Wu et al., 2024)</a></li><li><a href=#56--101152-adaptive-anchor-pairs-selection-in-a-tdoa-based-system-through-robot-localization-error-minimization-marcin-kolakowski-2024>(5/6 | 101/152) Adaptive Anchor Pairs Selection in a TDOA-based System Through Robot Localization Error Minimization (Marcin Kolakowski, 2024)</a></li><li><a href=#66--102152-learning-adaptive-multi-objective-robot-navigation-with-demonstrations-jorge-de-heuvel-et-al-2024>(6/6 | 102/152) Learning Adaptive Multi-Objective Robot Navigation with Demonstrations (Jorge de Heuvel et al., 2024)</a></li></ul></li><li><a href=#cscr-8>cs.CR (8)</a><ul><li><a href=#18--103152-hidden-you-malicious-goal-into-benigh-narratives-jailbreak-large-language-models-through-logic-chain-injection-zhilong-wang-et-al-2024>(1/8 | 103/152) Hidden You Malicious Goal Into Benigh Narratives: Jailbreak Large Language Models through Logic Chain Injection (Zhilong Wang et al., 2024)</a></li><li><a href=#28--104152-safeguarding-voice-privacy-harnessing-near-ultrasonic-interference-to-protect-against-unauthorized-audio-recording-forrest-mckee-et-al-2024>(2/8 | 104/152) Safeguarding Voice Privacy: Harnessing Near-Ultrasonic Interference To Protect Against Unauthorized Audio Recording (Forrest McKee et al., 2024)</a></li><li><a href=#38--105152-oss-malicious-package-analysis-in-the-wild-xiaoyan-zhou-et-al-2024>(3/8 | 105/152) OSS Malicious Package Analysis in the Wild (Xiaoyan Zhou et al., 2024)</a></li><li><a href=#48--106152-pagpassgpt-pattern-guided-password-guessing-via-generative-pretrained-transformer-xingyu-su-et-al-2024>(4/8 | 106/152) PagPassGPT: Pattern Guided Password Guessing via Generative Pretrained Transformer (Xingyu Su et al., 2024)</a></li><li><a href=#58--107152-optimizing-information-propagation-for-blockchain-empowered-mobile-aigc-a-graph-attention-network-approach-jiana-liao-et-al-2024>(5/8 | 107/152) Optimizing Information Propagation for Blockchain-empowered Mobile AIGC: A Graph Attention Network Approach (Jiana Liao et al., 2024)</a></li><li><a href=#68--108152-stop-stealing-my-data-sanitizing-stego-channels-in-3d-printing-design-files-aleksandr-dolgavin-et-al-2024>(6/8 | 108/152) Stop Stealing My Data: Sanitizing Stego Channels in 3D Printing Design Files (Aleksandr Dolgavin et al., 2024)</a></li><li><a href=#78--109152-iniva-inclusive-and-incentive-compatible-vote-aggregation-arian-baloochestani-et-al-2024>(7/8 | 109/152) Iniva: Inclusive and Incentive-compatible Vote Aggregation (Arian Baloochestani et al., 2024)</a></li><li><a href=#88--110152-privacy-preserving-traceable-functional-encryption-for-inner-product-muyao-qiu-et-al-2024>(8/8 | 110/152) Privacy-Preserving Traceable Functional Encryption for Inner Product (Muyao Qiu et al., 2024)</a></li></ul></li><li><a href=#csse-6>cs.SE (6)</a><ul><li><a href=#16--111152-csa-trans-code-structure-aware-transformer-for-ast-saeyoon-oh-et-al-2024>(1/6 | 111/152) CSA-Trans: Code Structure Aware Transformer for AST (Saeyoon Oh et al., 2024)</a></li><li><a href=#26--112152-enhancing-llm-based-test-generation-for-hard-to-cover-branches-via-program-analysis-chen-yang-et-al-2024>(2/6 | 112/152) Enhancing LLM-based Test Generation for Hard-to-Cover Branches via Program Analysis (Chen Yang et al., 2024)</a></li><li><a href=#36--113152-llm-based-multi-agent-systems-for-software-engineering-vision-and-the-road-ahead-junda-he-et-al-2024>(3/6 | 113/152) LLM-Based Multi-Agent Systems for Software Engineering: Vision and the Road Ahead (Junda He et al., 2024)</a></li><li><a href=#46--114152-how-do-oss-developers-utilize-architectural-solutions-from-qa-sites-an-empirical-study-musengamana-jean-de-dieu-et-al-2024>(4/6 | 114/152) How Do OSS Developers Utilize Architectural Solutions from Q&amp;A Sites: An Empirical Study (Musengamana Jean de Dieu et al., 2024)</a></li><li><a href=#56--115152-ai-for-devsecops-a-landscape-and-future-opportunities-michael-fu-et-al-2024>(5/6 | 115/152) AI for DevSecOps: A Landscape and Future Opportunities (Michael Fu et al., 2024)</a></li><li><a href=#66--116152-a-data-to-product-multimodal-conceptual-framework-to-achieve-automated-software-evolution-for-context-rich-intelligent-applications-songhui-yue-2024>(6/6 | 116/152) A Data-to-Product Multimodal Conceptual Framework to Achieve Automated Software Evolution for Context-rich Intelligent Applications (Songhui Yue, 2024)</a></li></ul></li><li><a href=#csit-9>cs.IT (9)</a><ul><li><a href=#19--117152-graph-neural-network-meets-multi-agent-reinforcement-learning-fundamentals-applications-and-future-directions-ziheng-liu-et-al-2024>(1/9 | 117/152) Graph Neural Network Meets Multi-Agent Reinforcement Learning: Fundamentals, Applications, and Future Directions (Ziheng Liu et al., 2024)</a></li><li><a href=#29--118152-a-bird-eye-view-on-dna-storage-simulators-sanket-doshi-et-al-2024>(2/9 | 118/152) A Bird-Eye view on DNA Storage Simulators (Sanket Doshi et al., 2024)</a></li><li><a href=#39--119152-probabilistic-examination-of-least-squares-error-in-low-bitwidth-cholesky-decomposition-alexander-osinsky-et-al-2024>(3/9 | 119/152) Probabilistic Examination of Least Squares Error in Low-bitwidth Cholesky Decomposition (Alexander Osinsky et al., 2024)</a></li><li><a href=#49--120152-towards-atomic-mimo-receivers-mingyao-cui-et-al-2024>(4/9 | 120/152) Towards Atomic MIMO Receivers (Mingyao Cui et al., 2024)</a></li><li><a href=#59--121152-analog-digital-beam-focusing-for-line-of-sight-wide-aperture-mimo-with-spherical-wavefronts-jiyoung-yun-et-al-2024>(5/9 | 121/152) Analog-Digital Beam Focusing for Line of Sight Wide-Aperture MIMO with Spherical Wavefronts (Jiyoung Yun et al., 2024)</a></li><li><a href=#69--122152-fourier-transform-based-wavenumber-domain-3d-imaging-in-ris-aided-communication-systems-yixuan-huang-et-al-2024>(6/9 | 122/152) Fourier Transform-based Wavenumber Domain 3D Imaging in RIS-aided Communication Systems (Yixuan Huang et al., 2024)</a></li><li><a href=#79--123152-single-server-pliable-private-information-retrieval-with-identifiable-side-information-megha-rayer-et-al-2024>(7/9 | 123/152) Single-Server Pliable Private Information Retrieval with Identifiable Side Information (Megha Rayer et al., 2024)</a></li><li><a href=#89--124152-soft-in-soft-out-decoding-of-spherical-codes-from-cartesian-powers-of-pam-constellations-reza-rafie-borujeny-et-al-2024>(8/9 | 124/152) Soft-in Soft-out Decoding of Spherical Codes from Cartesian Powers of PAM Constellations (Reza Rafie Borujeny et al., 2024)</a></li><li><a href=#99--125152-holographic-integrated-data-and-energy-transfer-qingxiao-huang-et-al-2024>(9/9 | 125/152) Holographic Integrated Data and Energy Transfer (Qingxiao Huang et al., 2024)</a></li></ul></li><li><a href=#eesssy-3>eess.SY (3)</a><ul><li><a href=#13--126152-nanometer-scanning-with-micrometer-sensing-beating-quantization-constraints-in-lissajous-trajectory-tracking-matheus-lohse-et-al-2024>(1/3 | 126/152) Nanometer Scanning with Micrometer Sensing: Beating Quantization Constraints in Lissajous Trajectory Tracking (Matheus Lohse et al., 2024)</a></li><li><a href=#23--127152-opinion-dynamics-for-utility-maximizing-agents-exploring-the-impact-of-resource-penalty-prashil-wankhede-et-al-2024>(2/3 | 127/152) Opinion Dynamics for Utility Maximizing Agents: Exploring the Impact of Resource Penalty (Prashil Wankhede et al., 2024)</a></li><li><a href=#33--128152-minimax-least-square-policy-iteration-for-cost-aware-defense-of-traffic-routing-against-unknown-threats-yuzhen-zhan-et-al-2024>(3/3 | 128/152) Minimax Least-Square Policy Iteration for Cost-Aware Defense of Traffic Routing against Unknown Threats (Yuzhen Zhan et al., 2024)</a></li></ul></li><li><a href=#csar-2>cs.AR (2)</a><ul><li><a href=#12--129152-explaining-eda-synthesis-errors-with-llms-siyu-qiu-et-al-2024>(1/2 | 129/152) Explaining EDA synthesis errors with LLMs (Siyu Qiu et al., 2024)</a></li><li><a href=#22--130152-gdr-hgnn-a-heterogeneous-graph-neural-networks-accelerator-frontend-with-graph-decoupling-and-recoupling-runzhen-xue-et-al-2024>(2/2 | 130/152) GDR-HGNN: A Heterogeneous Graph Neural Networks Accelerator Frontend with Graph Decoupling and Recoupling (Runzhen Xue et al., 2024)</a></li></ul></li><li><a href=#csai-4>cs.AI (4)</a><ul><li><a href=#14--131152-dwe-dual-way-matching-enhanced-framework-for-multimodal-entity-linking-shezheng-song-et-al-2024>(1/4 | 131/152) DWE+: Dual-Way Matching Enhanced Framework for Multimodal Entity Linking (Shezheng Song et al., 2024)</a></li><li><a href=#24--132152-towards-reliable-and-empathetic-depression-diagnosis-oriented-chats-kunyao-lan-et-al-2024>(2/4 | 132/152) Towards Reliable and Empathetic Depression-Diagnosis-Oriented Chats (Kunyao Lan et al., 2024)</a></li><li><a href=#34--133152-ai2apps-a-visual-ide-for-building-llm-based-ai-agent-applications-xin-pang-et-al-2024>(3/4 | 133/152) AI2Apps: A Visual IDE for Building LLM-based AI Agent Applications (Xin Pang et al., 2024)</a></li><li><a href=#44--134152-on-the-uniqueness-of-solution-for-the-bellman-equation-of-ltl-objectives-zetong-xuan-et-al-2024>(4/4 | 134/152) On the Uniqueness of Solution for the Bellman Equation of LTL Objectives (Zetong Xuan et al., 2024)</a></li></ul></li><li><a href=#mathna-1>math.NA (1)</a><ul><li><a href=#11--135152-generative-downscaling-of-pde-solvers-with-physics-guided-diffusion-models-yulong-lu-et-al-2024>(1/1 | 135/152) Generative downscaling of PDE solvers with physics-guided diffusion models (Yulong Lu et al., 2024)</a></li></ul></li><li><a href=#cslo-3>cs.LO (3)</a><ul><li><a href=#13--136152-quantitative-weakest-hyper-pre-unifying-correctness-and-incorrectness-hyperproperties-via-predicate-transformers-linpeng-zhang-et-al-2024>(1/3 | 136/152) Quantitative Weakest Hyper Pre: Unifying Correctness and Incorrectness Hyperproperties via Predicate Transformers (Linpeng Zhang et al., 2024)</a></li><li><a href=#23--137152-the-church-synthesis-problem-over-continuous-time-alexander-rabinovich-et-al-2024>(2/3 | 137/152) The Church Synthesis Problem over Continuous Time (Alexander Rabinovich et al., 2024)</a></li><li><a href=#33--138152-gatlab-modeling-and-programming-with-generalized-algebraic-theories-owen-lynch-et-al-2024>(3/3 | 138/152) GATlab: Modeling and Programming with Generalized Algebraic Theories (Owen Lynch et al., 2024)</a></li></ul></li><li><a href=#cshc-3>cs.HC (3)</a><ul><li><a href=#13--139152-reduction-of-forgetting-by-contextual-variation-during-encoding-using-360-degree-video-based-immersive-virtual-environments-takato-mizuho-et-al-2024>(1/3 | 139/152) Reduction of Forgetting by Contextual Variation During Encoding Using 360-Degree Video-Based Immersive Virtual Environments (Takato Mizuho et al., 2024)</a></li><li><a href=#23--140152-chart-what-i-say-exploring-cross-modality-prompt-alignment-in-ai-assisted-chart-authoring-nazar-ponochevnyi-et-al-2024>(2/3 | 140/152) Chart What I Say: Exploring Cross-Modality Prompt Alignment in AI-Assisted Chart Authoring (Nazar Ponochevnyi et al., 2024)</a></li><li><a href=#33--141152-balancing-information-perception-with-yin-yang-agent-based-information-neutrality-model-for-recommendation-systems-mengyan-wang-et-al-2024>(3/3 | 141/152) Balancing Information Perception with Yin-Yang: Agent-Based Information Neutrality Model for Recommendation Systems (Mengyan Wang et al., 2024)</a></li></ul></li><li><a href=#cspl-2>cs.PL (2)</a><ul><li><a href=#12--142152-allo-a-programming-model-for-composable-accelerator-design-hongzheng-chen-et-al-2024>(1/2 | 142/152) Allo: A Programming Model for Composable Accelerator Design (Hongzheng Chen et al., 2024)</a></li><li><a href=#22--143152-katch-a-fast-symbolic-verifier-for-netkat-mark-moeller-et-al-2024>(2/2 | 143/152) KATch: A Fast Symbolic Verifier for NetKAT (Mark Moeller et al., 2024)</a></li></ul></li><li><a href=#q-fincp-1>q-fin.CP (1)</a><ul><li><a href=#11--144152-stockgpt-a-genai-model-for-stock-prediction-and-trading-dat-mai-2024>(1/1 | 144/152) StockGPT: A GenAI Model for Stock Prediction and Trading (Dat Mai, 2024)</a></li></ul></li><li><a href=#econem-1>econ.EM (1)</a><ul><li><a href=#11--145152-caviar-categorical-variable-embeddings-for-accurate-and-robust-inference-anirban-mukherjee-et-al-2024>(1/1 | 145/152) CAVIAR: Categorical-Variable Embeddings for Accurate and Robust Inference (Anirban Mukherjee et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--146152-the-impact-of-virtual-laboratories-on-active-learning-and-engagement-in-cybersecurity-distance-education-victor-r-kebande-2024>(1/1 | 146/152) The Impact of Virtual Laboratories on Active Learning and Engagement in Cybersecurity Distance Education (Victor R. Kebande, 2024)</a></li></ul></li><li><a href=#eessas-1>eess.AS (1)</a><ul><li><a href=#11--147152-gull-a-generative-multifunctional-audio-codec-yi-luo-et-al-2024>(1/1 | 147/152) Gull: A Generative Multifunctional Audio Codec (Yi Luo et al., 2024)</a></li></ul></li><li><a href=#mathds-1>math.DS (1)</a><ul><li><a href=#11--148152-elementary-fractal-geometry-5-weak-separation-is-strong-separation-christoph-bandt-et-al-2024>(1/1 | 148/152) Elementary fractal geometry. 5. Weak separation is strong separation (Christoph Bandt et al., 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-1>physics.flu-dyn (1)</a><ul><li><a href=#11--149152-exact-and-approximate-solutions-for-magnetohydrodynamic-flow-control-in-hele-shaw-cells-kyle-mckee-2024>(1/1 | 149/152) Exact and Approximate Solutions for Magnetohydrodynamic Flow Control in Hele-Shaw Cells (Kyle McKee, 2024)</a></li></ul></li><li><a href=#csne-1>cs.NE (1)</a><ul><li><a href=#11--150152-dynamic-quality-diversity-search-roberto-gallotta-et-al-2024>(1/1 | 150/152) Dynamic Quality-Diversity Search (Roberto Gallotta et al., 2024)</a></li></ul></li><li><a href=#csds-1>cs.DS (1)</a><ul><li><a href=#11--151152-chromatic-number-in-19999n-time-fast-deterministic-set-partitioning-under-the-asymptotic-rank-conjecture-andreas-björklund-et-al-2024>(1/1 | 151/152) Chromatic number in $1.9999^n$ time? Fast deterministic set partitioning under the asymptotic rank conjecture (Andreas Björklund et al., 2024)</a></li></ul></li><li><a href=#econth-1>econ.TH (1)</a><ul><li><a href=#11--152152-a-many-to-one-job-market-more-about-the-core-and-the-competitive-salaries-ata-atay-et-al-2024>(1/1 | 152/152) A many-to-one job market: more about the core and the competitive salaries (Ata Atay et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>